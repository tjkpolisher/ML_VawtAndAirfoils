{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc91fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the steady-state simulation - Case 1: MLP with Optimal settings\n",
    "## Optimal Settings are like below:\n",
    "# 1. Train/Validation/Test dataset ratio = 0.7/0.2/0.1\n",
    "# 2. Cd scaling -> replaced as normalization for both Cl and Cd\n",
    "# 3. Seperate the ML models into the two models, a model only for Cl and the other for Cd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy import interpolate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1130c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining parameters and hyperparameters of the model\n",
    "\n",
    "n_units=256 # Number of units in the hidden layer of the MLP network\n",
    "n_layers=5\n",
    "input_size = 110 + 3 # Size of input for the network (110 coefficients and 3 other parameters, time, h, beta)\n",
    "lr = 1e-04 # Learning rate of the network\n",
    "test_rate = 0.1 # Defines the ratio of test dataset\n",
    "val_rate = 0.2 # Defines the ratio of validation dataset\n",
    "n_data = 16 # Number of txt files from which the aerodynamic coefficients are extracted\n",
    "batch_size = 200 # Mini-batch size\n",
    "l2_regularizer=1e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d84b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing working directory\n",
    "\n",
    "main_directory = 'D:\\\\VAWT_data\\\\flap_steady\\\\flap_steady'\n",
    "os.chdir(main_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic parameters\n",
    "\n",
    "c = 1 # Chord length\n",
    "h = np.array([0.01, 0.02, 0.03]) * c # Height of the Gurney flaps\n",
    "thickness = 0.02 * h # Thickness of the Gurney flaps\n",
    "beta = np.linspace(30, 90, 5).reshape((5,1))\n",
    "\n",
    "h = h[-1]\n",
    "beta = beta[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18aaa915",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h.reshape((-1,1))\n",
    "thickness = thickness.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9745480",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_interval = 0.001\n",
    "t_len = int((11-10) / t_interval)\n",
    "\n",
    "n_beta = len(beta)# Number of the Gurney flap inclination\n",
    "n_h = len(h) # Number of the height of the Gurney flaps\n",
    "n_cases = n_data * t_len # Total number of cases(Number of geometries * Number of angles of attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d7f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Input dataset\n",
    "# Defining time as input\n",
    "\n",
    "main_directory = 'D:\\\\VAWT_data'\n",
    "cm_dir = main_directory + \"\\\\blade_1_cm_data\"\n",
    "cm_list = os.listdir(cm_dir)\n",
    "os.chdir(cm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5014fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_target = [file for file in cm_list if file.endswith('.csv')]\n",
    "cm_target = sorted(cm_target, key=lambda s: int(re.search(r'\\d+',s).group()))\n",
    "cm_target = [cm_target[-8],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d3bbcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case15cm_blade1.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62d3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create input and output data\n",
    "### This function is the main framework where data are reordered with respect to the shape the NNs require.\n",
    "### Each input features are made by calling the corresponding functions, which generate the data,\n",
    "### e.g., time, cm, h, beta, coordinates of airfoil and Gurney flaps, etc.\n",
    "def genereate_input_output(cm_target, n_beta, t_len, normalize:bool=False):\n",
    "    \n",
    "    input_time_cm = time_and_cm(cm_target)\n",
    "    t = input_time_cm[:,0].reshape((-1, 1))\n",
    "    cm = input_time_cm[:,1].reshape((-1, 1))\n",
    "    \n",
    "    hh = generate_h(n_beta, t_len, normalize)\n",
    "    bb = generate_beta(n_beta, t_len, normalize)\n",
    "#     total_coords = generate_coordinates(n_cases)\n",
    "    \n",
    "    # Concatenate data for input dataset\n",
    "    #x = np.hstack((t, hh, bb, total_coords))\n",
    "    #x = np.hstack((t, total_coords))\n",
    "    #x = t\n",
    "    x = np.hstack((t, hh, bb))\n",
    "    \n",
    "    # Generating output dataset (depending on whether the data be normalized or not)\n",
    "    if normalize==True:\n",
    "        y = (cm-np.min(cm))/(np.max(cm)-np.min(cm))\n",
    "    else:\n",
    "        y = cm\n",
    "    print(\"Dimension - x: \", x.shape)\n",
    "    print(\"Dimension - y: \", y.shape)\n",
    "    \n",
    "    return x, y, t, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa96208",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating time for input, and Cm (moment coefficient) for output\n",
    "def time_and_cm(cm_target):\n",
    "    cm_df = pd.DataFrame()\n",
    "    for i, file in enumerate(cm_target):\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        data = pd.read_csv(file, header=None)\n",
    "        df = pd.concat([df, data], axis=0)\n",
    "        \n",
    "        time = df.iloc[:,0].values\n",
    "        cm = df.iloc[:,1].values\n",
    "        \n",
    "        time_beUsed = time[np.where(np.logical_and(time>=10, time<11))]\n",
    "        cm_beUsed = cm[np.where(np.logical_and(time>=10, time<11))]\n",
    "        \n",
    "        # Handle the time that is duplicated because of digits\n",
    "        # Also, outliers are regulated at the second conditional statement.\n",
    "        time_beUsed = handler_time(time_beUsed)\n",
    "        cm_beUsed = handler_cm(cm_beUsed)\n",
    "        \n",
    "        linear_func = interpolate.interp1d(time_beUsed, cm_beUsed,\n",
    "                                           bounds_error=False,kind='quadratic',\n",
    "                                           fill_value='extrapolate')\n",
    "        time_interp = np.arange(10, 11, t_interval).reshape((-1,1))\n",
    "        cm_interp=linear_func(time_interp).reshape((-1,1))\n",
    "        \n",
    "        cm_df = pd.concat([cm_df, pd.DataFrame(np.hstack((time_interp, cm_interp)))], axis=0)\n",
    "    \n",
    "    input_time_cm = cm_df.iloc[:,:].values\n",
    "    print(\"Dimension - time and Cm: \", input_time_cm.shape)\n",
    "    return input_time_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c96f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling duplicated time value\n",
    "def handler_time(time_beUsed):\n",
    "    for i in range(len(time_beUsed)):\n",
    "        if time_beUsed[i]==time_beUsed[i-1]:\n",
    "            time_beUsed[i] += 0.0005\n",
    "            \n",
    "    return time_beUsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d715ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outlier, (if there are)\n",
    "def handler_cm(cm_beUsed):\n",
    "    period = int(len(cm_beUsed) / 5)\n",
    "    for i in range(len(cm_beUsed)):\n",
    "        if np.abs(cm_beUsed[i]-cm_beUsed[i-1])>0.3:\n",
    "            cm_beUsed[i-1] = cm_beUsed[i-1 + period]\n",
    "            \n",
    "    return cm_beUsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "937cc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining beta, the Gurney flap inclination\n",
    "## In case of mere NACA0018, the bb in those indexes are considered as zero.\n",
    "def generate_beta(n_beta=n_beta, t_len=t_len, normalize=True):\n",
    "\n",
    "#     beta_0 = np.zeros((t_len,1)) # Values for sheer NACA0018\n",
    "    b_ = np.ones((t_len,1)) # Template for the inclination for a single h and single beta\n",
    "    bb_imp = np.zeros((t_len*n_beta,1))\n",
    "\n",
    "    for j in range(n_beta):\n",
    "        b_imp = b_ * beta[j]\n",
    "        bb_imp[t_len*j:t_len*(j+1),:] = b_imp[:,:]\n",
    "\n",
    "    bb_imp = bb_imp.reshape((-1,1))\n",
    "    bb = bb_imp\n",
    "    if normalize==True:\n",
    "        bb = bb / np.max(beta)\n",
    "    \n",
    "    print(\"Dimension - inclination(beta): \", bb.shape)\n",
    "\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6302058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the Gurney flap height\n",
    "## In case of mere NACA0018, the hh in those indexes are considered as zero.\n",
    "def generate_h(n_beta=n_beta, t_len=t_len, normalize:bool=True):\n",
    "    #hh = np.concatenate((np.zeros(t_len), h[0]*np.ones(n_beta*t_len), h[1]*np.ones(n_beta*t_len), h[2]*np.ones(n_beta*t_len)))\n",
    "    hh = h[0]*np.ones(n_beta*t_len)\n",
    "    hh = hh.reshape((-1,1))\n",
    "    \n",
    "    if normalize==True:\n",
    "        hh = hh / np.max(h)\n",
    "    \n",
    "    print(\"Dimension - heights of Gurney flaps: \", hh.shape)\n",
    "    return hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2d882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generates coordinate data of NACA0018 airfoil and Gurney flaps\n",
    "def generate_coordinates(n_cases):\n",
    "    origin_coord = \"D:\\\\AirfoilClCdCoordinates_out\\\\AirfoilClCdCoordinates_out\\\\airfoil15\"\n",
    "\n",
    "    csv_file_name = origin_coord + '\\\\airfoilOut15.txt'\n",
    "    data = pd.read_csv(csv_file_name, header=None)\n",
    "    \n",
    "    baseline_coord_high = data.iloc[0,:]\n",
    "    baseline_coord_low = data.iloc[1,:]\n",
    "    baseline_coord = np.hstack((np.flip(baseline_coord_high), baseline_coord_low)).reshape((1,-1))\n",
    "    airfoil_coord = np.repeat(baseline_coord, n_cases, axis=0)\n",
    "    print(\"Dimension - airfoil coordinates: \", airfoil_coord.shape)\n",
    "    \n",
    "    flap_coords= coord_with_flaps(n_cases)\n",
    "    total_coords = np.hstack((airfoil_coord, flap_coords))\n",
    "    \n",
    "    print(\"Dimension - total coordinates: \", total_coords.shape)\n",
    "    \n",
    "    return total_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a25f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data of Gurney flap coordinates\n",
    "def coord_with_flaps(n_cases):\n",
    "    flap_left = np.zeros((15,5))\n",
    "    flap_right = np.zeros((15,5))\n",
    "\n",
    "    for i in range(n_h):\n",
    "        # Defining coordinates of the flaps with respect to beta=90 degree.\n",
    "        yLeft = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "        yRight = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "        xLeft = 0.5*np.ones((5,1)) - 0.02*h[i]\n",
    "        xRight = 0.5*np.ones((5,1))\n",
    "\n",
    "        for j in range(n_beta):\n",
    "            betaValue = beta[j]\n",
    "\n",
    "            # Rotating transformation\n",
    "            rotateTransf = np.array([[np.cos(90-betaValue), -np.sin(90-betaValue)],\n",
    "                                     [np.sin(90-betaValue), np.cos(90-betaValue)]])\n",
    "            rotateTransf = rotateTransf.reshape((2,2))\n",
    "\n",
    "            LeftImp = np.hstack((xLeft-0.5, yLeft))\n",
    "            RightImp = np.hstack((xRight-0.5, yRight))\n",
    "\n",
    "            rotatedFlapLeft = rotateTransf @ LeftImp.T # shape: 2*5 (x-coordinates on first row, y-coordinates on second row)\n",
    "            rotatedFlapRight = rotateTransf @ RightImp.T\n",
    "\n",
    "            # All we need is the y-coordinates of the flaps\n",
    "            flap_left[5*i+j,:] = rotatedFlapLeft[1,:]\n",
    "            flap_right[5*i+j,:] = rotatedFlapRight[1,:]\n",
    "    \n",
    "    flap_coords = np.hstack((flap_left, np.flip(flap_right, axis=1)))\n",
    "    flap_coords2 = np.zeros((n_cases, 10))\n",
    "    \n",
    "    for i in range(t_len, n_cases):\n",
    "        flap_coords2[i,:] = flap_coords[i%15,:]\n",
    "    print(\"Dimension - coord with flaps: \", flap_coords2.shape)\n",
    "    \n",
    "    return flap_coords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc16d509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension - time and Cm:  (1000, 2)\n",
      "Dimension - heights of Gurney flaps:  (1000, 1)\n",
      "Dimension - inclination(beta):  (1000, 1)\n",
      "Dimension - x:  (1000, 3)\n",
      "Dimension - y:  (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generating x, y and cm (for denormalizing)\n",
    "x, y, t, cm = genereate_input_output(cm_target, n_beta, t_len, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "766ff9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0] -= 10\n",
    "x[:,0] /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7444ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(x, y, cm, test_rate, random_state=1, **kwargs):\n",
    "    if kwargs.get('validation')==True:\n",
    "        val_rate = kwargs.get('val_rate')\n",
    "        x_all, x_test, y_all, y_test, cm_all, cm_test = train_test_split(x, y, cm, test_size=test_rate, random_state=kwargs.get('random_state'))\n",
    "        x_train, x_val, y_train, y_val, cm_train, cm_val = train_test_split(x_all, y_all, cm_all, test_size=val_rate/(1-test_rate),\n",
    "                                                                            random_state=kwargs.get('random_state'))\n",
    "        return x_train, x_val, x_test, y_train, y_val, y_test, cm_train, cm_val, cm_test\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test, cm_train, cm_test = train_test_split(x, y, cm, test_size=test_rate, random_state=kwargs.get('random_state'))\n",
    "        return x_train, x_test, y_train, y_test, cm_train, cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec562b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test, cm_train, cm_val, cm_test = dataset_split(x, y, cm,\n",
    "                                                                                          test_rate, val_rate=val_rate,\n",
    "                                                                                          validation=True, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37290049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp_model(num_layer:int = 1): # This function can only generate model with at least 3 hidden layers\n",
    "    input_data = tf.keras.Input(shape=3)\n",
    "\n",
    "    # The first hidden layer\n",
    "    x_fc = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc1',\n",
    "                                #kernel_initializer='he_uniform',\n",
    "                                kernel_regularizer=regularizers.l2(l2_regularizer))(input_data)\n",
    "    \n",
    "    # The other hidden layers, which will be placed between the first hidden layer and the last hidden layer.\n",
    "    # The number of layers that the user desires is input of this function.\n",
    "    for i in range(0, num_layer-2):\n",
    "        x_fc = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc%d' % (i+2),\n",
    "                                     #kernel_initializer='he_uniform',\n",
    "                                     kernel_regularizer=regularizers.l2(l2_regularizer))(x_fc)\n",
    "    \n",
    "    # The last hidden layer\n",
    "    x_fc_final = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc%d' % num_layer,\n",
    "                                       #kernel_initializer='he_uniform',\n",
    "                                       kernel_regularizer=regularizers.l2(l2_regularizer))(x_fc)\n",
    "\n",
    "    # The output layer\n",
    "    output_data = tf.keras.layers.Dense(units=1, activation='linear', name='outputLayer')(x_fc_final)\n",
    "    \n",
    "    # MLP(FC layer)-based\n",
    "    model = tf.keras.Model(input_data, output_data)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3c482f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 256)               1024      \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " fc4 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " fc5 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " outputLayer (Dense)         (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,449\n",
      "Trainable params: 264,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_mlp_model(num_layer=n_layers)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30f3abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss = tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "360fbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221230\\\\Case13_WithParameters\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b6c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = model_directory + \"20221230unsteadyPrediction_MLP_Case13_WithParameters_val_\"+str(val_rate) + \"_test\"+str(test_rate)+ \"_\" +str(n_layers)+\"layers_\"+ str(n_units) +\"units_checkpoint.h5\"\n",
    "\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(ckpt_name, monitor=\"val_loss\", mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200, min_delta=1e-05,\n",
    "                                      restore_best_weights=True, verbose=1)\n",
    "rp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=100, factor=0.5,\n",
    "                                          min_delta = 1e-05, min_lr=1e-05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4cc904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = len(x_train)//batch_size\n",
    "VALIDATION_STEPS = len(x_val)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17d6b26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/3 [=========>....................] - ETA: 2s - loss: 0.1765 - rmse: 0.4200\n",
      "Epoch 1: val_loss improved from inf to 0.17644, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 2s 111ms/step - loss: 0.1808 - rmse: 0.4251 - val_loss: 0.1764 - val_rmse: 0.4199 - lr: 1.0000e-04\n",
      "Epoch 2/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1510 - rmse: 0.3884\n",
      "Epoch 2: val_loss improved from 0.17644 to 0.14653, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.1409 - rmse: 0.3752 - val_loss: 0.1465 - val_rmse: 0.3827 - lr: 1.0000e-04\n",
      "Epoch 3/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1482 - rmse: 0.3849\n",
      "Epoch 3: val_loss improved from 0.14653 to 0.12061, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.1208 - rmse: 0.3473 - val_loss: 0.1206 - val_rmse: 0.3471 - lr: 1.0000e-04\n",
      "Epoch 4/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1069 - rmse: 0.3268\n",
      "Epoch 4: val_loss improved from 0.12061 to 0.09900, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.1053 - rmse: 0.3244 - val_loss: 0.0990 - val_rmse: 0.3145 - lr: 1.0000e-04\n",
      "Epoch 5/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0732 - rmse: 0.2704\n",
      "Epoch 5: val_loss improved from 0.09900 to 0.08299, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0829 - rmse: 0.2877 - val_loss: 0.0830 - val_rmse: 0.2879 - lr: 1.0000e-04\n",
      "Epoch 6/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0687 - rmse: 0.2618\n",
      "Epoch 6: val_loss improved from 0.08299 to 0.07237, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0718 - rmse: 0.2678 - val_loss: 0.0724 - val_rmse: 0.2688 - lr: 1.0000e-04\n",
      "Epoch 7/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0613 - rmse: 0.2474\n",
      "Epoch 7: val_loss improved from 0.07237 to 0.06763, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0573 - rmse: 0.2392 - val_loss: 0.0676 - val_rmse: 0.2599 - lr: 1.0000e-04\n",
      "Epoch 8/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0627 - rmse: 0.2503\n",
      "Epoch 8: val_loss improved from 0.06763 to 0.06729, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0651 - rmse: 0.2549 - val_loss: 0.0673 - val_rmse: 0.2592 - lr: 1.0000e-04\n",
      "Epoch 9/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0617 - rmse: 0.2482\n",
      "Epoch 9: val_loss did not improve from 0.06729\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0657 - rmse: 0.2562 - val_loss: 0.0681 - val_rmse: 0.2608 - lr: 1.0000e-04\n",
      "Epoch 10/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0562 - rmse: 0.2368\n",
      "Epoch 10: val_loss did not improve from 0.06729\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0658 - rmse: 0.2563 - val_loss: 0.0682 - val_rmse: 0.2610 - lr: 1.0000e-04\n",
      "Epoch 11/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0620 - rmse: 0.2488\n",
      "Epoch 11: val_loss did not improve from 0.06729\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0608 - rmse: 0.2464 - val_loss: 0.0674 - val_rmse: 0.2594 - lr: 1.0000e-04\n",
      "Epoch 12/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0680 - rmse: 0.2606\n",
      "Epoch 12: val_loss improved from 0.06729 to 0.06697, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0659 - rmse: 0.2564 - val_loss: 0.0670 - val_rmse: 0.2586 - lr: 1.0000e-04\n",
      "Epoch 13/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0665 - rmse: 0.2577\n",
      "Epoch 13: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0625 - rmse: 0.2498 - val_loss: 0.0676 - val_rmse: 0.2599 - lr: 1.0000e-04\n",
      "Epoch 14/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0626 - rmse: 0.2499\n",
      "Epoch 14: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0693 - rmse: 0.2630 - val_loss: 0.0686 - val_rmse: 0.2616 - lr: 1.0000e-04\n",
      "Epoch 15/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0481 - rmse: 0.2192\n",
      "Epoch 15: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0551 - rmse: 0.2344 - val_loss: 0.0689 - val_rmse: 0.2623 - lr: 1.0000e-04\n",
      "Epoch 16/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0618 - rmse: 0.2483\n",
      "Epoch 16: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0637 - rmse: 0.2523 - val_loss: 0.0687 - val_rmse: 0.2619 - lr: 1.0000e-04\n",
      "Epoch 17/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0659 - rmse: 0.2566\n",
      "Epoch 17: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0651 - rmse: 0.2549 - val_loss: 0.0681 - val_rmse: 0.2607 - lr: 1.0000e-04\n",
      "Epoch 18/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0481 - rmse: 0.2192\n",
      "Epoch 18: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0576 - rmse: 0.2399 - val_loss: 0.0676 - val_rmse: 0.2599 - lr: 1.0000e-04\n",
      "Epoch 19/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0697 - rmse: 0.2639\n",
      "Epoch 19: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0631 - rmse: 0.2509 - val_loss: 0.0673 - val_rmse: 0.2591 - lr: 1.0000e-04\n",
      "Epoch 20/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0711 - rmse: 0.2665\n",
      "Epoch 20: val_loss did not improve from 0.06697\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0633 - rmse: 0.2514 - val_loss: 0.0670 - val_rmse: 0.2587 - lr: 1.0000e-04\n",
      "Epoch 21/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0518 - rmse: 0.2274\n",
      "Epoch 21: val_loss improved from 0.06697 to 0.06690, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0624 - rmse: 0.2496 - val_loss: 0.0669 - val_rmse: 0.2585 - lr: 1.0000e-04\n",
      "Epoch 22/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0615 - rmse: 0.2478\n",
      "Epoch 22: val_loss improved from 0.06690 to 0.06690, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0614 - rmse: 0.2476 - val_loss: 0.0669 - val_rmse: 0.2584 - lr: 1.0000e-04\n",
      "Epoch 23/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0630 - rmse: 0.2508\n",
      "Epoch 23: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0630 - rmse: 0.2509 - val_loss: 0.0670 - val_rmse: 0.2586 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0553 - rmse: 0.2349\n",
      "Epoch 24: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0622 - rmse: 0.2492 - val_loss: 0.0671 - val_rmse: 0.2588 - lr: 1.0000e-04\n",
      "Epoch 25/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - rmse: 0.2421\n",
      "Epoch 25: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0627 - rmse: 0.2502 - val_loss: 0.0671 - val_rmse: 0.2588 - lr: 1.0000e-04\n",
      "Epoch 26/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0591 - rmse: 0.2429\n",
      "Epoch 26: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0628 - rmse: 0.2503 - val_loss: 0.0671 - val_rmse: 0.2589 - lr: 1.0000e-04\n",
      "Epoch 27/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0582 - rmse: 0.2410\n",
      "Epoch 27: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0594 - rmse: 0.2435 - val_loss: 0.0671 - val_rmse: 0.2589 - lr: 1.0000e-04\n",
      "Epoch 28/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0668 - rmse: 0.2583\n",
      "Epoch 28: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0637 - rmse: 0.2522 - val_loss: 0.0670 - val_rmse: 0.2587 - lr: 1.0000e-04\n",
      "Epoch 29/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0657 - rmse: 0.2560\n",
      "Epoch 29: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0636 - rmse: 0.2520 - val_loss: 0.0669 - val_rmse: 0.2585 - lr: 1.0000e-04\n",
      "Epoch 30/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0532 - rmse: 0.2305\n",
      "Epoch 30: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0642 - rmse: 0.2532 - val_loss: 0.0669 - val_rmse: 0.2585 - lr: 1.0000e-04\n",
      "Epoch 31/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0579 - rmse: 0.2405\n",
      "Epoch 31: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0532 - rmse: 0.2304 - val_loss: 0.0670 - val_rmse: 0.2587 - lr: 1.0000e-04\n",
      "Epoch 32/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0662 - rmse: 0.2570\n",
      "Epoch 32: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0671 - rmse: 0.2589 - val_loss: 0.0671 - val_rmse: 0.2588 - lr: 1.0000e-04\n",
      "Epoch 33/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0671 - rmse: 0.2589\n",
      "Epoch 33: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0630 - rmse: 0.2507 - val_loss: 0.0671 - val_rmse: 0.2588 - lr: 1.0000e-04\n",
      "Epoch 34/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0566 - rmse: 0.2376\n",
      "Epoch 34: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0590 - rmse: 0.2427 - val_loss: 0.0671 - val_rmse: 0.2588 - lr: 1.0000e-04\n",
      "Epoch 35/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0684 - rmse: 0.2614\n",
      "Epoch 35: val_loss did not improve from 0.06690\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0598 - rmse: 0.2443 - val_loss: 0.0670 - val_rmse: 0.2586 - lr: 1.0000e-04\n",
      "Epoch 36/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0669 - rmse: 0.2584\n",
      "Epoch 36: val_loss improved from 0.06690 to 0.06687, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0662 - rmse: 0.2571 - val_loss: 0.0669 - val_rmse: 0.2584 - lr: 1.0000e-04\n",
      "Epoch 37/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0704 - rmse: 0.2651\n",
      "Epoch 37: val_loss improved from 0.06687 to 0.06670, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0651 - rmse: 0.2549 - val_loss: 0.0667 - val_rmse: 0.2581 - lr: 1.0000e-04\n",
      "Epoch 38/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0437 - rmse: 0.2087\n",
      "Epoch 38: val_loss did not improve from 0.06670\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0593 - rmse: 0.2434 - val_loss: 0.0668 - val_rmse: 0.2582 - lr: 1.0000e-04\n",
      "Epoch 39/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0560 - rmse: 0.2365\n",
      "Epoch 39: val_loss did not improve from 0.06670\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0599 - rmse: 0.2446 - val_loss: 0.0668 - val_rmse: 0.2584 - lr: 1.0000e-04\n",
      "Epoch 40/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0590 - rmse: 0.2427\n",
      "Epoch 40: val_loss did not improve from 0.06670\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0630 - rmse: 0.2508 - val_loss: 0.0668 - val_rmse: 0.2583 - lr: 1.0000e-04\n",
      "Epoch 41/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0629 - rmse: 0.2505\n",
      "Epoch 41: val_loss improved from 0.06670 to 0.06667, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0642 - rmse: 0.2532 - val_loss: 0.0667 - val_rmse: 0.2580 - lr: 1.0000e-04\n",
      "Epoch 42/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0484 - rmse: 0.2197\n",
      "Epoch 42: val_loss did not improve from 0.06667\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0630 - rmse: 0.2509 - val_loss: 0.0667 - val_rmse: 0.2581 - lr: 1.0000e-04\n",
      "Epoch 43/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0563 - rmse: 0.2370\n",
      "Epoch 43: val_loss did not improve from 0.06667\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0568 - rmse: 0.2381 - val_loss: 0.0667 - val_rmse: 0.2581 - lr: 1.0000e-04\n",
      "Epoch 44/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0569 - rmse: 0.2383\n",
      "Epoch 44: val_loss did not improve from 0.06667\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0632 - rmse: 0.2511 - val_loss: 0.0668 - val_rmse: 0.2583 - lr: 1.0000e-04\n",
      "Epoch 45/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0543 - rmse: 0.2328\n",
      "Epoch 45: val_loss did not improve from 0.06667\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0597 - rmse: 0.2441 - val_loss: 0.0671 - val_rmse: 0.2588 - lr: 1.0000e-04\n",
      "Epoch 46/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0755 - rmse: 0.2747\n",
      "Epoch 46: val_loss did not improve from 0.06667\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0617 - rmse: 0.2482 - val_loss: 0.0668 - val_rmse: 0.2583 - lr: 1.0000e-04\n",
      "Epoch 47/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0771 - rmse: 0.2775\n",
      "Epoch 47: val_loss improved from 0.06667 to 0.06660, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0680 - rmse: 0.2606 - val_loss: 0.0666 - val_rmse: 0.2579 - lr: 1.0000e-04\n",
      "Epoch 48/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0581 - rmse: 0.2409\n",
      "Epoch 48: val_loss improved from 0.06660 to 0.06653, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0586 - rmse: 0.2419 - val_loss: 0.0665 - val_rmse: 0.2577 - lr: 1.0000e-04\n",
      "Epoch 49/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0600 - rmse: 0.2447\n",
      "Epoch 49: val_loss improved from 0.06653 to 0.06643, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0618 - rmse: 0.2483 - val_loss: 0.0664 - val_rmse: 0.2575 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0621 - rmse: 0.2491\n",
      "Epoch 50: val_loss improved from 0.06643 to 0.06629, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0634 - rmse: 0.2515 - val_loss: 0.0663 - val_rmse: 0.2573 - lr: 1.0000e-04\n",
      "Epoch 51/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0635 - rmse: 0.2517\n",
      "Epoch 51: val_loss did not improve from 0.06629\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0585 - rmse: 0.2416 - val_loss: 0.0665 - val_rmse: 0.2577 - lr: 1.0000e-04\n",
      "Epoch 52/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0705 - rmse: 0.2654\n",
      "Epoch 52: val_loss did not improve from 0.06629\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0636 - rmse: 0.2519 - val_loss: 0.0668 - val_rmse: 0.2583 - lr: 1.0000e-04\n",
      "Epoch 53/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0665 - rmse: 0.2578\n",
      "Epoch 53: val_loss did not improve from 0.06629\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0588 - rmse: 0.2423 - val_loss: 0.0670 - val_rmse: 0.2586 - lr: 1.0000e-04\n",
      "Epoch 54/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0794 - rmse: 0.2816\n",
      "Epoch 54: val_loss did not improve from 0.06629\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0673 - rmse: 0.2593 - val_loss: 0.0670 - val_rmse: 0.2587 - lr: 1.0000e-04\n",
      "Epoch 55/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0596 - rmse: 0.2439\n",
      "Epoch 55: val_loss did not improve from 0.06629\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0579 - rmse: 0.2405 - val_loss: 0.0666 - val_rmse: 0.2579 - lr: 1.0000e-04\n",
      "Epoch 56/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0765 - rmse: 0.2764\n",
      "Epoch 56: val_loss improved from 0.06629 to 0.06621, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0637 - rmse: 0.2521 - val_loss: 0.0662 - val_rmse: 0.2571 - lr: 1.0000e-04\n",
      "Epoch 57/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0536 - rmse: 0.2312\n",
      "Epoch 57: val_loss did not improve from 0.06621\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0593 - rmse: 0.2432 - val_loss: 0.0663 - val_rmse: 0.2573 - lr: 1.0000e-04\n",
      "Epoch 58/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0759 - rmse: 0.2754\n",
      "Epoch 58: val_loss improved from 0.06621 to 0.06608, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0644 - rmse: 0.2537 - val_loss: 0.0661 - val_rmse: 0.2569 - lr: 1.0000e-04\n",
      "Epoch 59/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0561 - rmse: 0.2367\n",
      "Epoch 59: val_loss improved from 0.06608 to 0.06604, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0629 - rmse: 0.2505 - val_loss: 0.0660 - val_rmse: 0.2568 - lr: 1.0000e-04\n",
      "Epoch 60/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0596 - rmse: 0.2439\n",
      "Epoch 60: val_loss improved from 0.06604 to 0.06604, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0603 - rmse: 0.2455 - val_loss: 0.0660 - val_rmse: 0.2568 - lr: 1.0000e-04\n",
      "Epoch 61/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0650 - rmse: 0.2548\n",
      "Epoch 61: val_loss did not improve from 0.06604\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0606 - rmse: 0.2460 - val_loss: 0.0660 - val_rmse: 0.2568 - lr: 1.0000e-04\n",
      "Epoch 62/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0669 - rmse: 0.2585\n",
      "Epoch 62: val_loss did not improve from 0.06604\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0629 - rmse: 0.2506 - val_loss: 0.0662 - val_rmse: 0.2571 - lr: 1.0000e-04\n",
      "Epoch 63/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0615 - rmse: 0.2477\n",
      "Epoch 63: val_loss did not improve from 0.06604\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0578 - rmse: 0.2403 - val_loss: 0.0661 - val_rmse: 0.2570 - lr: 1.0000e-04\n",
      "Epoch 64/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0748 - rmse: 0.2733\n",
      "Epoch 64: val_loss improved from 0.06604 to 0.06598, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0647 - rmse: 0.2542 - val_loss: 0.0660 - val_rmse: 0.2567 - lr: 1.0000e-04\n",
      "Epoch 65/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0616 - rmse: 0.2480\n",
      "Epoch 65: val_loss did not improve from 0.06598\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0615 - rmse: 0.2477 - val_loss: 0.0660 - val_rmse: 0.2567 - lr: 1.0000e-04\n",
      "Epoch 66/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0610 - rmse: 0.2469\n",
      "Epoch 66: val_loss improved from 0.06598 to 0.06590, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0625 - rmse: 0.2499 - val_loss: 0.0659 - val_rmse: 0.2565 - lr: 1.0000e-04\n",
      "Epoch 67/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0569 - rmse: 0.2383\n",
      "Epoch 67: val_loss did not improve from 0.06590\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0585 - rmse: 0.2416 - val_loss: 0.0659 - val_rmse: 0.2565 - lr: 1.0000e-04\n",
      "Epoch 68/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0683 - rmse: 0.2611\n",
      "Epoch 68: val_loss improved from 0.06590 to 0.06584, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0630 - rmse: 0.2507 - val_loss: 0.0658 - val_rmse: 0.2564 - lr: 1.0000e-04\n",
      "Epoch 69/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0567 - rmse: 0.2379\n",
      "Epoch 69: val_loss improved from 0.06584 to 0.06566, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0628 - rmse: 0.2505 - val_loss: 0.0657 - val_rmse: 0.2561 - lr: 1.0000e-04\n",
      "Epoch 70/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0523 - rmse: 0.2286\n",
      "Epoch 70: val_loss did not improve from 0.06566\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0620 - rmse: 0.2489 - val_loss: 0.0659 - val_rmse: 0.2565 - lr: 1.0000e-04\n",
      "Epoch 71/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0548 - rmse: 0.2338\n",
      "Epoch 71: val_loss did not improve from 0.06566\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0587 - rmse: 0.2420 - val_loss: 0.0658 - val_rmse: 0.2563 - lr: 1.0000e-04\n",
      "Epoch 72/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0648 - rmse: 0.2544\n",
      "Epoch 72: val_loss did not improve from 0.06566\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0613 - rmse: 0.2473 - val_loss: 0.0657 - val_rmse: 0.2562 - lr: 1.0000e-04\n",
      "Epoch 73/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0696 - rmse: 0.2636\n",
      "Epoch 73: val_loss did not improve from 0.06566\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0641 - rmse: 0.2530 - val_loss: 0.0658 - val_rmse: 0.2563 - lr: 1.0000e-04\n",
      "Epoch 74/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0434 - rmse: 0.2081\n",
      "Epoch 74: val_loss did not improve from 0.06566\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0568 - rmse: 0.2381 - val_loss: 0.0658 - val_rmse: 0.2564 - lr: 1.0000e-04\n",
      "Epoch 75/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0623 - rmse: 0.2494\n",
      "Epoch 75: val_loss improved from 0.06566 to 0.06558, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0625 - rmse: 0.2498 - val_loss: 0.0656 - val_rmse: 0.2559 - lr: 1.0000e-04\n",
      "Epoch 76/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0615 - rmse: 0.2479\n",
      "Epoch 76: val_loss improved from 0.06558 to 0.06537, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0605 - rmse: 0.2457 - val_loss: 0.0654 - val_rmse: 0.2555 - lr: 1.0000e-04\n",
      "Epoch 77/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0665 - rmse: 0.2577\n",
      "Epoch 77: val_loss improved from 0.06537 to 0.06520, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0589 - rmse: 0.2425 - val_loss: 0.0652 - val_rmse: 0.2551 - lr: 1.0000e-04\n",
      "Epoch 78/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0740 - rmse: 0.2718\n",
      "Epoch 78: val_loss improved from 0.06520 to 0.06509, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0658 - rmse: 0.2563 - val_loss: 0.0651 - val_rmse: 0.2549 - lr: 1.0000e-04\n",
      "Epoch 79/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0567 - rmse: 0.2380\n",
      "Epoch 79: val_loss did not improve from 0.06509\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0570 - rmse: 0.2386 - val_loss: 0.0652 - val_rmse: 0.2551 - lr: 1.0000e-04\n",
      "Epoch 80/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0595 - rmse: 0.2436\n",
      "Epoch 80: val_loss did not improve from 0.06509\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0627 - rmse: 0.2501 - val_loss: 0.0653 - val_rmse: 0.2553 - lr: 1.0000e-04\n",
      "Epoch 81/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0568 - rmse: 0.2381\n",
      "Epoch 81: val_loss did not improve from 0.06509\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0594 - rmse: 0.2435 - val_loss: 0.0653 - val_rmse: 0.2553 - lr: 1.0000e-04\n",
      "Epoch 82/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0696 - rmse: 0.2636\n",
      "Epoch 82: val_loss improved from 0.06509 to 0.06502, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0621 - rmse: 0.2490 - val_loss: 0.0650 - val_rmse: 0.2548 - lr: 1.0000e-04\n",
      "Epoch 83/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0598 - rmse: 0.2443\n",
      "Epoch 83: val_loss improved from 0.06502 to 0.06491, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0606 - rmse: 0.2460 - val_loss: 0.0649 - val_rmse: 0.2546 - lr: 1.0000e-04\n",
      "Epoch 84/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0640 - rmse: 0.2527\n",
      "Epoch 84: val_loss improved from 0.06491 to 0.06479, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0614 - rmse: 0.2476 - val_loss: 0.0648 - val_rmse: 0.2544 - lr: 1.0000e-04\n",
      "Epoch 85/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0538 - rmse: 0.2317\n",
      "Epoch 85: val_loss did not improve from 0.06479\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0585 - rmse: 0.2417 - val_loss: 0.0649 - val_rmse: 0.2545 - lr: 1.0000e-04\n",
      "Epoch 86/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0736 - rmse: 0.2711\n",
      "Epoch 86: val_loss improved from 0.06479 to 0.06459, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0642 - rmse: 0.2532 - val_loss: 0.0646 - val_rmse: 0.2540 - lr: 1.0000e-04\n",
      "Epoch 87/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0595 - rmse: 0.2436\n",
      "Epoch 87: val_loss did not improve from 0.06459\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0620 - rmse: 0.2489 - val_loss: 0.0647 - val_rmse: 0.2541 - lr: 1.0000e-04\n",
      "Epoch 88/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0528 - rmse: 0.2295\n",
      "Epoch 88: val_loss did not improve from 0.06459\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0582 - rmse: 0.2410 - val_loss: 0.0649 - val_rmse: 0.2545 - lr: 1.0000e-04\n",
      "Epoch 89/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0677 - rmse: 0.2600\n",
      "Epoch 89: val_loss did not improve from 0.06459\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0592 - rmse: 0.2431 - val_loss: 0.0649 - val_rmse: 0.2545 - lr: 1.0000e-04\n",
      "Epoch 90/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0684 - rmse: 0.2613\n",
      "Epoch 90: val_loss did not improve from 0.06459\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0623 - rmse: 0.2494 - val_loss: 0.0648 - val_rmse: 0.2543 - lr: 1.0000e-04\n",
      "Epoch 91/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0565 - rmse: 0.2374\n",
      "Epoch 91: val_loss improved from 0.06459 to 0.06443, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0592 - rmse: 0.2432 - val_loss: 0.0644 - val_rmse: 0.2536 - lr: 1.0000e-04\n",
      "Epoch 92/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0588 - rmse: 0.2423\n",
      "Epoch 92: val_loss improved from 0.06443 to 0.06407, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0612 - rmse: 0.2472 - val_loss: 0.0641 - val_rmse: 0.2529 - lr: 1.0000e-04\n",
      "Epoch 93/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0566 - rmse: 0.2376\n",
      "Epoch 93: val_loss improved from 0.06407 to 0.06391, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0583 - rmse: 0.2412 - val_loss: 0.0639 - val_rmse: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 94/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0723 - rmse: 0.2686\n",
      "Epoch 94: val_loss improved from 0.06391 to 0.06382, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0625 - rmse: 0.2499 - val_loss: 0.0638 - val_rmse: 0.2524 - lr: 1.0000e-04\n",
      "Epoch 95/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0627 - rmse: 0.2502\n",
      "Epoch 95: val_loss did not improve from 0.06382\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0612 - rmse: 0.2471 - val_loss: 0.0639 - val_rmse: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 96/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0516 - rmse: 0.2268\n",
      "Epoch 96: val_loss did not improve from 0.06382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0593 - rmse: 0.2434 - val_loss: 0.0639 - val_rmse: 0.2527 - lr: 1.0000e-04\n",
      "Epoch 97/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0674 - rmse: 0.2595\n",
      "Epoch 97: val_loss improved from 0.06382 to 0.06381, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0622 - rmse: 0.2492 - val_loss: 0.0638 - val_rmse: 0.2524 - lr: 1.0000e-04\n",
      "Epoch 98/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0472 - rmse: 0.2169\n",
      "Epoch 98: val_loss improved from 0.06381 to 0.06380, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0564 - rmse: 0.2372 - val_loss: 0.0638 - val_rmse: 0.2524 - lr: 1.0000e-04\n",
      "Epoch 99/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0550 - rmse: 0.2343\n",
      "Epoch 99: val_loss did not improve from 0.06380\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0601 - rmse: 0.2450 - val_loss: 0.0642 - val_rmse: 0.2531 - lr: 1.0000e-04\n",
      "Epoch 100/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0620 - rmse: 0.2489\n",
      "Epoch 100: val_loss improved from 0.06380 to 0.06366, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0609 - rmse: 0.2466 - val_loss: 0.0637 - val_rmse: 0.2521 - lr: 1.0000e-04\n",
      "Epoch 101/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0619 - rmse: 0.2487\n",
      "Epoch 101: val_loss did not improve from 0.06366\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0612 - rmse: 0.2472 - val_loss: 0.0638 - val_rmse: 0.2524 - lr: 1.0000e-04\n",
      "Epoch 102/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0525 - rmse: 0.2289\n",
      "Epoch 102: val_loss did not improve from 0.06366\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0529 - rmse: 0.2298 - val_loss: 0.0639 - val_rmse: 0.2526 - lr: 1.0000e-04\n",
      "Epoch 103/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0696 - rmse: 0.2636\n",
      "Epoch 103: val_loss improved from 0.06366 to 0.06335, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0657 - rmse: 0.2561 - val_loss: 0.0634 - val_rmse: 0.2515 - lr: 1.0000e-04\n",
      "Epoch 104/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0652 - rmse: 0.2552\n",
      "Epoch 104: val_loss improved from 0.06335 to 0.06310, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0594 - rmse: 0.2434 - val_loss: 0.0631 - val_rmse: 0.2510 - lr: 1.0000e-04\n",
      "Epoch 105/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0592 - rmse: 0.2430\n",
      "Epoch 105: val_loss did not improve from 0.06310\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0573 - rmse: 0.2392 - val_loss: 0.0633 - val_rmse: 0.2515 - lr: 1.0000e-04\n",
      "Epoch 106/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0728 - rmse: 0.2697\n",
      "Epoch 106: val_loss did not improve from 0.06310\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0647 - rmse: 0.2542 - val_loss: 0.0634 - val_rmse: 0.2516 - lr: 1.0000e-04\n",
      "Epoch 107/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0567 - rmse: 0.2379\n",
      "Epoch 107: val_loss did not improve from 0.06310\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0566 - rmse: 0.2378 - val_loss: 0.0632 - val_rmse: 0.2512 - lr: 1.0000e-04\n",
      "Epoch 108/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0625 - rmse: 0.2499\n",
      "Epoch 108: val_loss improved from 0.06310 to 0.06299, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0596 - rmse: 0.2440 - val_loss: 0.0630 - val_rmse: 0.2508 - lr: 1.0000e-04\n",
      "Epoch 109/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0560 - rmse: 0.2365\n",
      "Epoch 109: val_loss did not improve from 0.06299\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0610 - rmse: 0.2467 - val_loss: 0.0632 - val_rmse: 0.2511 - lr: 1.0000e-04\n",
      "Epoch 110/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0497 - rmse: 0.2226\n",
      "Epoch 110: val_loss improved from 0.06299 to 0.06281, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0543 - rmse: 0.2329 - val_loss: 0.0628 - val_rmse: 0.2504 - lr: 1.0000e-04\n",
      "Epoch 111/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0650 - rmse: 0.2548\n",
      "Epoch 111: val_loss improved from 0.06281 to 0.06269, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0653 - rmse: 0.2553 - val_loss: 0.0627 - val_rmse: 0.2502 - lr: 1.0000e-04\n",
      "Epoch 112/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0568 - rmse: 0.2382\n",
      "Epoch 112: val_loss did not improve from 0.06269\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0558 - rmse: 0.2360 - val_loss: 0.0627 - val_rmse: 0.2503 - lr: 1.0000e-04\n",
      "Epoch 113/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - rmse: 0.2420\n",
      "Epoch 113: val_loss improved from 0.06269 to 0.06239, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0592 - rmse: 0.2431 - val_loss: 0.0624 - val_rmse: 0.2496 - lr: 1.0000e-04\n",
      "Epoch 114/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0576 - rmse: 0.2398\n",
      "Epoch 114: val_loss improved from 0.06239 to 0.06218, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0613 - rmse: 0.2474 - val_loss: 0.0622 - val_rmse: 0.2492 - lr: 1.0000e-04\n",
      "Epoch 115/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0553 - rmse: 0.2349\n",
      "Epoch 115: val_loss improved from 0.06218 to 0.06206, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0582 - rmse: 0.2411 - val_loss: 0.0621 - val_rmse: 0.2489 - lr: 1.0000e-04\n",
      "Epoch 116/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0609 - rmse: 0.2466\n",
      "Epoch 116: val_loss improved from 0.06206 to 0.06193, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0565 - rmse: 0.2374 - val_loss: 0.0619 - val_rmse: 0.2487 - lr: 1.0000e-04\n",
      "Epoch 117/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0522 - rmse: 0.2282\n",
      "Epoch 117: val_loss improved from 0.06193 to 0.06180, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0576 - rmse: 0.2397 - val_loss: 0.0618 - val_rmse: 0.2484 - lr: 1.0000e-04\n",
      "Epoch 118/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0650 - rmse: 0.2547\n",
      "Epoch 118: val_loss improved from 0.06180 to 0.06146, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0615 - rmse: 0.2478 - val_loss: 0.0615 - val_rmse: 0.2477 - lr: 1.0000e-04\n",
      "Epoch 119/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0518 - rmse: 0.2274\n",
      "Epoch 119: val_loss did not improve from 0.06146\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0548 - rmse: 0.2338 - val_loss: 0.0616 - val_rmse: 0.2481 - lr: 1.0000e-04\n",
      "Epoch 120/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0605 - rmse: 0.2458\n",
      "Epoch 120: val_loss improved from 0.06146 to 0.06131, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0603 - rmse: 0.2453 - val_loss: 0.0613 - val_rmse: 0.2474 - lr: 1.0000e-04\n",
      "Epoch 121/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0674 - rmse: 0.2593\n",
      "Epoch 121: val_loss did not improve from 0.06131\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0580 - rmse: 0.2406 - val_loss: 0.0613 - val_rmse: 0.2475 - lr: 1.0000e-04\n",
      "Epoch 122/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0593 - rmse: 0.2433\n",
      "Epoch 122: val_loss did not improve from 0.06131\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0594 - rmse: 0.2435 - val_loss: 0.0619 - val_rmse: 0.2486 - lr: 1.0000e-04\n",
      "Epoch 123/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - rmse: 0.2420\n",
      "Epoch 123: val_loss did not improve from 0.06131\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0562 - rmse: 0.2369 - val_loss: 0.0614 - val_rmse: 0.2476 - lr: 1.0000e-04\n",
      "Epoch 124/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0587 - rmse: 0.2420\n",
      "Epoch 124: val_loss improved from 0.06131 to 0.06092, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0591 - rmse: 0.2429 - val_loss: 0.0609 - val_rmse: 0.2466 - lr: 1.0000e-04\n",
      "Epoch 125/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0520 - rmse: 0.2279\n",
      "Epoch 125: val_loss did not improve from 0.06092\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0573 - rmse: 0.2392 - val_loss: 0.0614 - val_rmse: 0.2475 - lr: 1.0000e-04\n",
      "Epoch 126/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0613 - rmse: 0.2474\n",
      "Epoch 126: val_loss improved from 0.06092 to 0.06069, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0606 - rmse: 0.2460 - val_loss: 0.0607 - val_rmse: 0.2461 - lr: 1.0000e-04\n",
      "Epoch 127/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0524 - rmse: 0.2288\n",
      "Epoch 127: val_loss improved from 0.06069 to 0.06058, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0570 - rmse: 0.2386 - val_loss: 0.0606 - val_rmse: 0.2459 - lr: 1.0000e-04\n",
      "Epoch 128/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0612 - rmse: 0.2472\n",
      "Epoch 128: val_loss did not improve from 0.06058\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0563 - rmse: 0.2370 - val_loss: 0.0608 - val_rmse: 0.2464 - lr: 1.0000e-04\n",
      "Epoch 129/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0575 - rmse: 0.2396\n",
      "Epoch 129: val_loss improved from 0.06058 to 0.06050, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0544 - rmse: 0.2330 - val_loss: 0.0605 - val_rmse: 0.2458 - lr: 1.0000e-04\n",
      "Epoch 130/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0759 - rmse: 0.2753\n",
      "Epoch 130: val_loss improved from 0.06050 to 0.06003, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0634 - rmse: 0.2516 - val_loss: 0.0600 - val_rmse: 0.2448 - lr: 1.0000e-04\n",
      "Epoch 131/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0572 - rmse: 0.2390\n",
      "Epoch 131: val_loss did not improve from 0.06003\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0580 - rmse: 0.2406 - val_loss: 0.0601 - val_rmse: 0.2450 - lr: 1.0000e-04\n",
      "Epoch 132/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0514 - rmse: 0.2266\n",
      "Epoch 132: val_loss did not improve from 0.06003\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0539 - rmse: 0.2320 - val_loss: 0.0605 - val_rmse: 0.2458 - lr: 1.0000e-04\n",
      "Epoch 133/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0609 - rmse: 0.2466\n",
      "Epoch 133: val_loss did not improve from 0.06003\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0556 - rmse: 0.2356 - val_loss: 0.0601 - val_rmse: 0.2449 - lr: 1.0000e-04\n",
      "Epoch 134/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0645 - rmse: 0.2538\n",
      "Epoch 134: val_loss improved from 0.06003 to 0.05944, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0553 - rmse: 0.2350 - val_loss: 0.0594 - val_rmse: 0.2436 - lr: 1.0000e-04\n",
      "Epoch 135/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0648 - rmse: 0.2544\n",
      "Epoch 135: val_loss did not improve from 0.05944\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0601 - rmse: 0.2449 - val_loss: 0.0596 - val_rmse: 0.2440 - lr: 1.0000e-04\n",
      "Epoch 136/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0647 - rmse: 0.2541\n",
      "Epoch 136: val_loss did not improve from 0.05944\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0562 - rmse: 0.2368 - val_loss: 0.0599 - val_rmse: 0.2445 - lr: 1.0000e-04\n",
      "Epoch 137/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0558 - rmse: 0.2360\n",
      "Epoch 137: val_loss improved from 0.05944 to 0.05902, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0542 - rmse: 0.2326 - val_loss: 0.0590 - val_rmse: 0.2427 - lr: 1.0000e-04\n",
      "Epoch 138/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0683 - rmse: 0.2612\n",
      "Epoch 138: val_loss improved from 0.05902 to 0.05899, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0561 - rmse: 0.2367 - val_loss: 0.0590 - val_rmse: 0.2427 - lr: 1.0000e-04\n",
      "Epoch 139/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0604 - rmse: 0.2456\n",
      "Epoch 139: val_loss improved from 0.05899 to 0.05865, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0575 - rmse: 0.2397 - val_loss: 0.0586 - val_rmse: 0.2420 - lr: 1.0000e-04\n",
      "Epoch 140/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0550 - rmse: 0.2342\n",
      "Epoch 140: val_loss improved from 0.05865 to 0.05818, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0568 - rmse: 0.2382 - val_loss: 0.0582 - val_rmse: 0.2410 - lr: 1.0000e-04\n",
      "Epoch 141/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0586 - rmse: 0.2419\n",
      "Epoch 141: val_loss did not improve from 0.05818\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0555 - rmse: 0.2354 - val_loss: 0.0584 - val_rmse: 0.2414 - lr: 1.0000e-04\n",
      "Epoch 142/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0571 - rmse: 0.2388\n",
      "Epoch 142: val_loss improved from 0.05818 to 0.05818, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0561 - rmse: 0.2367 - val_loss: 0.0582 - val_rmse: 0.2410 - lr: 1.0000e-04\n",
      "Epoch 143/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0570 - rmse: 0.2386\n",
      "Epoch 143: val_loss improved from 0.05818 to 0.05781, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0515 - rmse: 0.2266 - val_loss: 0.0578 - val_rmse: 0.2402 - lr: 1.0000e-04\n",
      "Epoch 144/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0678 - rmse: 0.2603\n",
      "Epoch 144: val_loss improved from 0.05781 to 0.05751, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0584 - rmse: 0.2415 - val_loss: 0.0575 - val_rmse: 0.2396 - lr: 1.0000e-04\n",
      "Epoch 145/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0519 - rmse: 0.2275\n",
      "Epoch 145: val_loss did not improve from 0.05751\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0550 - rmse: 0.2342 - val_loss: 0.0584 - val_rmse: 0.2415 - lr: 1.0000e-04\n",
      "Epoch 146/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0574 - rmse: 0.2394\n",
      "Epoch 146: val_loss improved from 0.05751 to 0.05735, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0543 - rmse: 0.2329 - val_loss: 0.0573 - val_rmse: 0.2393 - lr: 1.0000e-04\n",
      "Epoch 147/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0538 - rmse: 0.2317\n",
      "Epoch 147: val_loss improved from 0.05735 to 0.05693, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0562 - rmse: 0.2369 - val_loss: 0.0569 - val_rmse: 0.2384 - lr: 1.0000e-04\n",
      "Epoch 148/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0500 - rmse: 0.2234\n",
      "Epoch 148: val_loss did not improve from 0.05693\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0534 - rmse: 0.2309 - val_loss: 0.0570 - val_rmse: 0.2385 - lr: 1.0000e-04\n",
      "Epoch 149/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0527 - rmse: 0.2295\n",
      "Epoch 149: val_loss did not improve from 0.05693\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0554 - rmse: 0.2352 - val_loss: 0.0571 - val_rmse: 0.2387 - lr: 1.0000e-04\n",
      "Epoch 150/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0469 - rmse: 0.2164\n",
      "Epoch 150: val_loss improved from 0.05693 to 0.05615, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0501 - rmse: 0.2237 - val_loss: 0.0562 - val_rmse: 0.2368 - lr: 1.0000e-04\n",
      "Epoch 151/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0605 - rmse: 0.2458\n",
      "Epoch 151: val_loss improved from 0.05615 to 0.05576, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0587 - rmse: 0.2421 - val_loss: 0.0558 - val_rmse: 0.2359 - lr: 1.0000e-04\n",
      "Epoch 152/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0456 - rmse: 0.2132\n",
      "Epoch 152: val_loss did not improve from 0.05576\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0499 - rmse: 0.2232 - val_loss: 0.0567 - val_rmse: 0.2380 - lr: 1.0000e-04\n",
      "Epoch 153/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0561 - rmse: 0.2367\n",
      "Epoch 153: val_loss improved from 0.05576 to 0.05526, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0530 - rmse: 0.2300 - val_loss: 0.0553 - val_rmse: 0.2349 - lr: 1.0000e-04\n",
      "Epoch 154/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0541 - rmse: 0.2324\n",
      "Epoch 154: val_loss did not improve from 0.05526\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0511 - rmse: 0.2259 - val_loss: 0.0558 - val_rmse: 0.2360 - lr: 1.0000e-04\n",
      "Epoch 155/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0537 - rmse: 0.2316\n",
      "Epoch 155: val_loss did not improve from 0.05526\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0573 - rmse: 0.2391 - val_loss: 0.0557 - val_rmse: 0.2358 - lr: 1.0000e-04\n",
      "Epoch 156/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0455 - rmse: 0.2130\n",
      "Epoch 156: val_loss improved from 0.05526 to 0.05426, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0500 - rmse: 0.2234 - val_loss: 0.0543 - val_rmse: 0.2327 - lr: 1.0000e-04\n",
      "Epoch 157/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0487 - rmse: 0.2204\n",
      "Epoch 157: val_loss did not improve from 0.05426\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0540 - rmse: 0.2321 - val_loss: 0.0555 - val_rmse: 0.2354 - lr: 1.0000e-04\n",
      "Epoch 158/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0454 - rmse: 0.2128\n",
      "Epoch 158: val_loss did not improve from 0.05426\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0539 - rmse: 0.2319 - val_loss: 0.0559 - val_rmse: 0.2362 - lr: 1.0000e-04\n",
      "Epoch 159/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0531 - rmse: 0.2303\n",
      "Epoch 159: val_loss improved from 0.05426 to 0.05397, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0475 - rmse: 0.2178 - val_loss: 0.0540 - val_rmse: 0.2321 - lr: 1.0000e-04\n",
      "Epoch 160/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0539 - rmse: 0.2320\n",
      "Epoch 160: val_loss improved from 0.05397 to 0.05356, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0530 - rmse: 0.2299 - val_loss: 0.0536 - val_rmse: 0.2312 - lr: 1.0000e-04\n",
      "Epoch 161/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0612 - rmse: 0.2471\n",
      "Epoch 161: val_loss did not improve from 0.05356\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0530 - rmse: 0.2299 - val_loss: 0.0537 - val_rmse: 0.2316 - lr: 1.0000e-04\n",
      "Epoch 162/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0378 - rmse: 0.1942\n",
      "Epoch 162: val_loss improved from 0.05356 to 0.05290, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0486 - rmse: 0.2203 - val_loss: 0.0529 - val_rmse: 0.2298 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0472 - rmse: 0.2170\n",
      "Epoch 163: val_loss improved from 0.05290 to 0.05204, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0477 - rmse: 0.2181 - val_loss: 0.0520 - val_rmse: 0.2279 - lr: 1.0000e-04\n",
      "Epoch 164/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0508 - rmse: 0.2251\n",
      "Epoch 164: val_loss improved from 0.05204 to 0.05162, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0507 - rmse: 0.2250 - val_loss: 0.0516 - val_rmse: 0.2270 - lr: 1.0000e-04\n",
      "Epoch 165/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0487 - rmse: 0.2204\n",
      "Epoch 165: val_loss improved from 0.05162 to 0.05161, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0495 - rmse: 0.2223 - val_loss: 0.0516 - val_rmse: 0.2270 - lr: 1.0000e-04\n",
      "Epoch 166/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0439 - rmse: 0.2092\n",
      "Epoch 166: val_loss improved from 0.05161 to 0.05067, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0505 - rmse: 0.2244 - val_loss: 0.0507 - val_rmse: 0.2249 - lr: 1.0000e-04\n",
      "Epoch 167/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0447 - rmse: 0.2113\n",
      "Epoch 167: val_loss did not improve from 0.05067\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0437 - rmse: 0.2088 - val_loss: 0.0519 - val_rmse: 0.2276 - lr: 1.0000e-04\n",
      "Epoch 168/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0576 - rmse: 0.2398\n",
      "Epoch 168: val_loss did not improve from 0.05067\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0497 - rmse: 0.2227 - val_loss: 0.0510 - val_rmse: 0.2256 - lr: 1.0000e-04\n",
      "Epoch 169/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0485 - rmse: 0.2199\n",
      "Epoch 169: val_loss improved from 0.05067 to 0.05000, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0478 - rmse: 0.2184 - val_loss: 0.0500 - val_rmse: 0.2234 - lr: 1.0000e-04\n",
      "Epoch 170/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0455 - rmse: 0.2130\n",
      "Epoch 170: val_loss improved from 0.05000 to 0.04926, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0474 - rmse: 0.2175 - val_loss: 0.0493 - val_rmse: 0.2217 - lr: 1.0000e-04\n",
      "Epoch 171/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0477 - rmse: 0.2181\n",
      "Epoch 171: val_loss improved from 0.04926 to 0.04924, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0443 - rmse: 0.2101 - val_loss: 0.0492 - val_rmse: 0.2217 - lr: 1.0000e-04\n",
      "Epoch 172/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0507 - rmse: 0.2250\n",
      "Epoch 172: val_loss improved from 0.04924 to 0.04890, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0484 - rmse: 0.2198 - val_loss: 0.0489 - val_rmse: 0.2209 - lr: 1.0000e-04\n",
      "Epoch 173/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0464 - rmse: 0.2151\n",
      "Epoch 173: val_loss improved from 0.04890 to 0.04740, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0465 - rmse: 0.2154 - val_loss: 0.0474 - val_rmse: 0.2175 - lr: 1.0000e-04\n",
      "Epoch 174/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0384 - rmse: 0.1956\n",
      "Epoch 174: val_loss did not improve from 0.04740\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0420 - rmse: 0.2046 - val_loss: 0.0474 - val_rmse: 0.2176 - lr: 1.0000e-04\n",
      "Epoch 175/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0509 - rmse: 0.2254\n",
      "Epoch 175: val_loss did not improve from 0.04740\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0465 - rmse: 0.2153 - val_loss: 0.0478 - val_rmse: 0.2184 - lr: 1.0000e-04\n",
      "Epoch 176/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0447 - rmse: 0.2111\n",
      "Epoch 176: val_loss improved from 0.04740 to 0.04579, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0446 - rmse: 0.2110 - val_loss: 0.0458 - val_rmse: 0.2138 - lr: 1.0000e-04\n",
      "Epoch 177/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0434 - rmse: 0.2080\n",
      "Epoch 177: val_loss did not improve from 0.04579\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0430 - rmse: 0.2072 - val_loss: 0.0460 - val_rmse: 0.2141 - lr: 1.0000e-04\n",
      "Epoch 178/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0483 - rmse: 0.2196\n",
      "Epoch 178: val_loss did not improve from 0.04579\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0458 - rmse: 0.2138 - val_loss: 0.0467 - val_rmse: 0.2158 - lr: 1.0000e-04\n",
      "Epoch 179/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0399 - rmse: 0.1994\n",
      "Epoch 179: val_loss improved from 0.04579 to 0.04466, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0416 - rmse: 0.2038 - val_loss: 0.0447 - val_rmse: 0.2111 - lr: 1.0000e-04\n",
      "Epoch 180/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0435 - rmse: 0.2083\n",
      "Epoch 180: val_loss improved from 0.04466 to 0.04364, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0428 - rmse: 0.2065 - val_loss: 0.0436 - val_rmse: 0.2087 - lr: 1.0000e-04\n",
      "Epoch 181/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0401 - rmse: 0.2001\n",
      "Epoch 181: val_loss did not improve from 0.04364\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0412 - rmse: 0.2027 - val_loss: 0.0440 - val_rmse: 0.2096 - lr: 1.0000e-04\n",
      "Epoch 182/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0419 - rmse: 0.2044\n",
      "Epoch 182: val_loss improved from 0.04364 to 0.04267, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0429 - rmse: 0.2068 - val_loss: 0.0427 - val_rmse: 0.2063 - lr: 1.0000e-04\n",
      "Epoch 183/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0409 - rmse: 0.2020\n",
      "Epoch 183: val_loss did not improve from 0.04267\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0381 - rmse: 0.1950 - val_loss: 0.0434 - val_rmse: 0.2080 - lr: 1.0000e-04\n",
      "Epoch 184/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0440 - rmse: 0.2096\n",
      "Epoch 184: val_loss improved from 0.04267 to 0.04087, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0407 - rmse: 0.2014 - val_loss: 0.0409 - val_rmse: 0.2019 - lr: 1.0000e-04\n",
      "Epoch 185/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0359 - rmse: 0.1893\n",
      "Epoch 185: val_loss improved from 0.04087 to 0.04065, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0391 - rmse: 0.1974 - val_loss: 0.0406 - val_rmse: 0.2014 - lr: 1.0000e-04\n",
      "Epoch 186/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0399 - rmse: 0.1995\n",
      "Epoch 186: val_loss did not improve from 0.04065\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0394 - rmse: 0.1983 - val_loss: 0.0413 - val_rmse: 0.2030 - lr: 1.0000e-04\n",
      "Epoch 187/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0408 - rmse: 0.2018\n",
      "Epoch 187: val_loss improved from 0.04065 to 0.04017, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0379 - rmse: 0.1943 - val_loss: 0.0402 - val_rmse: 0.2002 - lr: 1.0000e-04\n",
      "Epoch 188/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0387 - rmse: 0.1965\n",
      "Epoch 188: val_loss improved from 0.04017 to 0.03897, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0377 - rmse: 0.1939 - val_loss: 0.0390 - val_rmse: 0.1972 - lr: 1.0000e-04\n",
      "Epoch 189/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0392 - rmse: 0.1978\n",
      "Epoch 189: val_loss improved from 0.03897 to 0.03891, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0375 - rmse: 0.1934 - val_loss: 0.0389 - val_rmse: 0.1970 - lr: 1.0000e-04\n",
      "Epoch 190/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0365 - rmse: 0.1908\n",
      "Epoch 190: val_loss improved from 0.03891 to 0.03826, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0372 - rmse: 0.1926 - val_loss: 0.0383 - val_rmse: 0.1954 - lr: 1.0000e-04\n",
      "Epoch 191/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0375 - rmse: 0.1934\n",
      "Epoch 191: val_loss did not improve from 0.03826\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0373 - rmse: 0.1928 - val_loss: 0.0386 - val_rmse: 0.1962 - lr: 1.0000e-04\n",
      "Epoch 192/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0337 - rmse: 0.1833\n",
      "Epoch 192: val_loss improved from 0.03826 to 0.03649, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0340 - rmse: 0.1842 - val_loss: 0.0365 - val_rmse: 0.1908 - lr: 1.0000e-04\n",
      "Epoch 193/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0357 - rmse: 0.1886\n",
      "Epoch 193: val_loss improved from 0.03649 to 0.03525, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0351 - rmse: 0.1872 - val_loss: 0.0352 - val_rmse: 0.1875 - lr: 1.0000e-04\n",
      "Epoch 194/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0339 - rmse: 0.1839\n",
      "Epoch 194: val_loss did not improve from 0.03525\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0342 - rmse: 0.1848 - val_loss: 0.0355 - val_rmse: 0.1881 - lr: 1.0000e-04\n",
      "Epoch 195/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0343 - rmse: 0.1849\n",
      "Epoch 195: val_loss improved from 0.03525 to 0.03432, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0329 - rmse: 0.1811 - val_loss: 0.0343 - val_rmse: 0.1850 - lr: 1.0000e-04\n",
      "Epoch 196/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0338 - rmse: 0.1836\n",
      "Epoch 196: val_loss improved from 0.03432 to 0.03366, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0323 - rmse: 0.1795 - val_loss: 0.0337 - val_rmse: 0.1832 - lr: 1.0000e-04\n",
      "Epoch 197/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0347 - rmse: 0.1859\n",
      "Epoch 197: val_loss did not improve from 0.03366\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0324 - rmse: 0.1799 - val_loss: 0.0339 - val_rmse: 0.1838 - lr: 1.0000e-04\n",
      "Epoch 198/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0282 - rmse: 0.1676\n",
      "Epoch 198: val_loss improved from 0.03366 to 0.03291, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0318 - rmse: 0.1781 - val_loss: 0.0329 - val_rmse: 0.1811 - lr: 1.0000e-04\n",
      "Epoch 199/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0311 - rmse: 0.1760\n",
      "Epoch 199: val_loss improved from 0.03291 to 0.03147, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0320 - rmse: 0.1787 - val_loss: 0.0315 - val_rmse: 0.1771 - lr: 1.0000e-04\n",
      "Epoch 200/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0245 - rmse: 0.1562\n",
      "Epoch 200: val_loss did not improve from 0.03147\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0291 - rmse: 0.1704 - val_loss: 0.0319 - val_rmse: 0.1782 - lr: 1.0000e-04\n",
      "Epoch 201/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0302 - rmse: 0.1736\n",
      "Epoch 201: val_loss improved from 0.03147 to 0.03040, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0301 - rmse: 0.1733 - val_loss: 0.0304 - val_rmse: 0.1741 - lr: 1.0000e-04\n",
      "Epoch 202/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0277 - rmse: 0.1661\n",
      "Epoch 202: val_loss did not improve from 0.03040\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0302 - rmse: 0.1735 - val_loss: 0.0305 - val_rmse: 0.1742 - lr: 1.0000e-04\n",
      "Epoch 203/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0254 - rmse: 0.1591\n",
      "Epoch 203: val_loss improved from 0.03040 to 0.03000, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0278 - rmse: 0.1663 - val_loss: 0.0300 - val_rmse: 0.1729 - lr: 1.0000e-04\n",
      "Epoch 204/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0264 - rmse: 0.1621\n",
      "Epoch 204: val_loss improved from 0.03000 to 0.02843, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0276 - rmse: 0.1660 - val_loss: 0.0284 - val_rmse: 0.1683 - lr: 1.0000e-04\n",
      "Epoch 205/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0299 - rmse: 0.1726\n",
      "Epoch 205: val_loss did not improve from 0.02843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0274 - rmse: 0.1654 - val_loss: 0.0294 - val_rmse: 0.1711 - lr: 1.0000e-04\n",
      "Epoch 206/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0263 - rmse: 0.1619\n",
      "Epoch 206: val_loss improved from 0.02843 to 0.02798, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0268 - rmse: 0.1635 - val_loss: 0.0280 - val_rmse: 0.1670 - lr: 1.0000e-04\n",
      "Epoch 207/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0259 - rmse: 0.1607\n",
      "Epoch 207: val_loss improved from 0.02798 to 0.02669, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0266 - rmse: 0.1627 - val_loss: 0.0267 - val_rmse: 0.1631 - lr: 1.0000e-04\n",
      "Epoch 208/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0239 - rmse: 0.1544\n",
      "Epoch 208: val_loss did not improve from 0.02669\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0248 - rmse: 0.1573 - val_loss: 0.0282 - val_rmse: 0.1677 - lr: 1.0000e-04\n",
      "Epoch 209/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0302 - rmse: 0.1734\n",
      "Epoch 209: val_loss improved from 0.02669 to 0.02557, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0268 - rmse: 0.1634 - val_loss: 0.0256 - val_rmse: 0.1596 - lr: 1.0000e-04\n",
      "Epoch 210/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0162 - rmse: 0.1270\n",
      "Epoch 210: val_loss improved from 0.02557 to 0.02534, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0217 - rmse: 0.1469 - val_loss: 0.0253 - val_rmse: 0.1589 - lr: 1.0000e-04\n",
      "Epoch 211/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0245 - rmse: 0.1563\n",
      "Epoch 211: val_loss improved from 0.02534 to 0.02469, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0257 - rmse: 0.1599 - val_loss: 0.0247 - val_rmse: 0.1568 - lr: 1.0000e-04\n",
      "Epoch 212/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0244 - rmse: 0.1560\n",
      "Epoch 212: val_loss improved from 0.02469 to 0.02454, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0237 - rmse: 0.1535 - val_loss: 0.0245 - val_rmse: 0.1563 - lr: 1.0000e-04\n",
      "Epoch 213/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0228 - rmse: 0.1507\n",
      "Epoch 213: val_loss improved from 0.02454 to 0.02325, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0223 - rmse: 0.1490 - val_loss: 0.0232 - val_rmse: 0.1521 - lr: 1.0000e-04\n",
      "Epoch 214/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0288 - rmse: 0.1694\n",
      "Epoch 214: val_loss did not improve from 0.02325\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0227 - rmse: 0.1502 - val_loss: 0.0246 - val_rmse: 0.1567 - lr: 1.0000e-04\n",
      "Epoch 215/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0245 - rmse: 0.1561\n",
      "Epoch 215: val_loss improved from 0.02325 to 0.02212, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0221 - rmse: 0.1482 - val_loss: 0.0221 - val_rmse: 0.1484 - lr: 1.0000e-04\n",
      "Epoch 216/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0219 - rmse: 0.1477\n",
      "Epoch 216: val_loss did not improve from 0.02212\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0229 - rmse: 0.1511 - val_loss: 0.0230 - val_rmse: 0.1512 - lr: 1.0000e-04\n",
      "Epoch 217/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0225 - rmse: 0.1497\n",
      "Epoch 217: val_loss improved from 0.02212 to 0.02147, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0208 - rmse: 0.1437 - val_loss: 0.0215 - val_rmse: 0.1462 - lr: 1.0000e-04\n",
      "Epoch 218/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0236 - rmse: 0.1532\n",
      "Epoch 218: val_loss did not improve from 0.02147\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0217 - rmse: 0.1471 - val_loss: 0.0216 - val_rmse: 0.1466 - lr: 1.0000e-04\n",
      "Epoch 219/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0179 - rmse: 0.1333\n",
      "Epoch 219: val_loss improved from 0.02147 to 0.02053, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0202 - rmse: 0.1418 - val_loss: 0.0205 - val_rmse: 0.1429 - lr: 1.0000e-04\n",
      "Epoch 220/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0177 - rmse: 0.1328\n",
      "Epoch 220: val_loss improved from 0.02053 to 0.01967, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0187 - rmse: 0.1363 - val_loss: 0.0197 - val_rmse: 0.1399 - lr: 1.0000e-04\n",
      "Epoch 221/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0190 - rmse: 0.1374\n",
      "Epoch 221: val_loss did not improve from 0.01967\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0196 - rmse: 0.1396 - val_loss: 0.0203 - val_rmse: 0.1420 - lr: 1.0000e-04\n",
      "Epoch 222/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0163 - rmse: 0.1274\n",
      "Epoch 222: val_loss improved from 0.01967 to 0.01915, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0183 - rmse: 0.1349 - val_loss: 0.0191 - val_rmse: 0.1380 - lr: 1.0000e-04\n",
      "Epoch 223/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0195 - rmse: 0.1393\n",
      "Epoch 223: val_loss improved from 0.01915 to 0.01857, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0171 - rmse: 0.1303 - val_loss: 0.0186 - val_rmse: 0.1359 - lr: 1.0000e-04\n",
      "Epoch 224/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0194 - rmse: 0.1388\n",
      "Epoch 224: val_loss improved from 0.01857 to 0.01809, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0188 - rmse: 0.1367 - val_loss: 0.0181 - val_rmse: 0.1341 - lr: 1.0000e-04\n",
      "Epoch 225/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0177 - rmse: 0.1328\n",
      "Epoch 225: val_loss improved from 0.01809 to 0.01741, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0173 - rmse: 0.1312 - val_loss: 0.0174 - val_rmse: 0.1316 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0178 - rmse: 0.1329\n",
      "Epoch 226: val_loss did not improve from 0.01741\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0165 - rmse: 0.1282 - val_loss: 0.0179 - val_rmse: 0.1333 - lr: 1.0000e-04\n",
      "Epoch 227/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0167 - rmse: 0.1290\n",
      "Epoch 227: val_loss improved from 0.01741 to 0.01608, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0173 - rmse: 0.1310 - val_loss: 0.0161 - val_rmse: 0.1264 - lr: 1.0000e-04\n",
      "Epoch 228/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0157 - rmse: 0.1251\n",
      "Epoch 228: val_loss did not improve from 0.01608\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0157 - rmse: 0.1251 - val_loss: 0.0176 - val_rmse: 0.1322 - lr: 1.0000e-04\n",
      "Epoch 229/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0160 - rmse: 0.1260\n",
      "Epoch 229: val_loss improved from 0.01608 to 0.01531, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0156 - rmse: 0.1246 - val_loss: 0.0153 - val_rmse: 0.1233 - lr: 1.0000e-04\n",
      "Epoch 230/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0168 - rmse: 0.1294\n",
      "Epoch 230: val_loss improved from 0.01531 to 0.01513, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0161 - rmse: 0.1266 - val_loss: 0.0151 - val_rmse: 0.1226 - lr: 1.0000e-04\n",
      "Epoch 231/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0151 - rmse: 0.1224\n",
      "Epoch 231: val_loss did not improve from 0.01513\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0148 - rmse: 0.1214 - val_loss: 0.0166 - val_rmse: 0.1283 - lr: 1.0000e-04\n",
      "Epoch 232/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0163 - rmse: 0.1274\n",
      "Epoch 232: val_loss improved from 0.01513 to 0.01415, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0149 - rmse: 0.1218 - val_loss: 0.0142 - val_rmse: 0.1186 - lr: 1.0000e-04\n",
      "Epoch 233/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0135 - rmse: 0.1158\n",
      "Epoch 233: val_loss did not improve from 0.01415\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0140 - rmse: 0.1180 - val_loss: 0.0147 - val_rmse: 0.1207 - lr: 1.0000e-04\n",
      "Epoch 234/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0143 - rmse: 0.1192\n",
      "Epoch 234: val_loss improved from 0.01415 to 0.01348, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0138 - rmse: 0.1170 - val_loss: 0.0135 - val_rmse: 0.1157 - lr: 1.0000e-04\n",
      "Epoch 235/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0137 - rmse: 0.1168\n",
      "Epoch 235: val_loss improved from 0.01348 to 0.01248, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0139 - rmse: 0.1174 - val_loss: 0.0125 - val_rmse: 0.1113 - lr: 1.0000e-04\n",
      "Epoch 236/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0122 - rmse: 0.1099\n",
      "Epoch 236: val_loss did not improve from 0.01248\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0126 - rmse: 0.1117 - val_loss: 0.0135 - val_rmse: 0.1159 - lr: 1.0000e-04\n",
      "Epoch 237/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0123 - rmse: 0.1104\n",
      "Epoch 237: val_loss improved from 0.01248 to 0.01199, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0124 - rmse: 0.1111 - val_loss: 0.0120 - val_rmse: 0.1091 - lr: 1.0000e-04\n",
      "Epoch 238/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0114 - rmse: 0.1063\n",
      "Epoch 238: val_loss improved from 0.01199 to 0.01147, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0115 - rmse: 0.1069 - val_loss: 0.0115 - val_rmse: 0.1066 - lr: 1.0000e-04\n",
      "Epoch 239/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0118 - rmse: 0.1081\n",
      "Epoch 239: val_loss did not improve from 0.01147\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0120 - rmse: 0.1089 - val_loss: 0.0117 - val_rmse: 0.1075 - lr: 1.0000e-04\n",
      "Epoch 240/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0128 - rmse: 0.1125\n",
      "Epoch 240: val_loss improved from 0.01147 to 0.01108, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0109 - rmse: 0.1039 - val_loss: 0.0111 - val_rmse: 0.1048 - lr: 1.0000e-04\n",
      "Epoch 241/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0116 - rmse: 0.1072\n",
      "Epoch 241: val_loss did not improve from 0.01108\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0111 - rmse: 0.1048 - val_loss: 0.0119 - val_rmse: 0.1085 - lr: 1.0000e-04\n",
      "Epoch 242/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0119 - rmse: 0.1085\n",
      "Epoch 242: val_loss improved from 0.01108 to 0.00987, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0107 - rmse: 0.1030 - val_loss: 0.0099 - val_rmse: 0.0988 - lr: 1.0000e-04\n",
      "Epoch 243/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0102 - rmse: 0.1005\n",
      "Epoch 243: val_loss improved from 0.00987 to 0.00949, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0103 - rmse: 0.1012 - val_loss: 0.0095 - val_rmse: 0.0969 - lr: 1.0000e-04\n",
      "Epoch 244/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0094 - rmse: 0.0962\n",
      "Epoch 244: val_loss did not improve from 0.00949\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0100 - rmse: 0.0993 - val_loss: 0.0107 - val_rmse: 0.1030 - lr: 1.0000e-04\n",
      "Epoch 245/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0105 - rmse: 0.1021\n",
      "Epoch 245: val_loss improved from 0.00949 to 0.00894, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0097 - rmse: 0.0980 - val_loss: 0.0089 - val_rmse: 0.0940 - lr: 1.0000e-04\n",
      "Epoch 246/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0108 - rmse: 0.1034\n",
      "Epoch 246: val_loss did not improve from 0.00894\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0097 - rmse: 0.0979 - val_loss: 0.0095 - val_rmse: 0.0967 - lr: 1.0000e-04\n",
      "Epoch 247/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0088 - rmse: 0.0934\n",
      "Epoch 247: val_loss improved from 0.00894 to 0.00828, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0087 - rmse: 0.0929 - val_loss: 0.0083 - val_rmse: 0.0904 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0092 - rmse: 0.0953\n",
      "Epoch 248: val_loss improved from 0.00828 to 0.00795, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0089 - rmse: 0.0938 - val_loss: 0.0080 - val_rmse: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 249/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0073 - rmse: 0.0852\n",
      "Epoch 249: val_loss did not improve from 0.00795\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0081 - rmse: 0.0893 - val_loss: 0.0082 - val_rmse: 0.0901 - lr: 1.0000e-04\n",
      "Epoch 250/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0096 - rmse: 0.0975\n",
      "Epoch 250: val_loss improved from 0.00795 to 0.00742, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0086 - rmse: 0.0920 - val_loss: 0.0074 - val_rmse: 0.0856 - lr: 1.0000e-04\n",
      "Epoch 251/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0081 - rmse: 0.0897\n",
      "Epoch 251: val_loss improved from 0.00742 to 0.00737, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0080 - rmse: 0.0887 - val_loss: 0.0074 - val_rmse: 0.0853 - lr: 1.0000e-04\n",
      "Epoch 252/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0075 - rmse: 0.0862\n",
      "Epoch 252: val_loss did not improve from 0.00737\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0076 - rmse: 0.0865 - val_loss: 0.0078 - val_rmse: 0.0878 - lr: 1.0000e-04\n",
      "Epoch 253/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0078 - rmse: 0.0876\n",
      "Epoch 253: val_loss improved from 0.00737 to 0.00697, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0076 - rmse: 0.0864 - val_loss: 0.0070 - val_rmse: 0.0829 - lr: 1.0000e-04\n",
      "Epoch 254/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0080 - rmse: 0.0890\n",
      "Epoch 254: val_loss did not improve from 0.00697\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0079 - rmse: 0.0881 - val_loss: 0.0071 - val_rmse: 0.0835 - lr: 1.0000e-04\n",
      "Epoch 255/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0058 - rmse: 0.0757\n",
      "Epoch 255: val_loss improved from 0.00697 to 0.00627, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0060 - rmse: 0.0765 - val_loss: 0.0063 - val_rmse: 0.0785 - lr: 1.0000e-04\n",
      "Epoch 256/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0076 - rmse: 0.0869\n",
      "Epoch 256: val_loss did not improve from 0.00627\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0072 - rmse: 0.0841 - val_loss: 0.0065 - val_rmse: 0.0799 - lr: 1.0000e-04\n",
      "Epoch 257/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0062 - rmse: 0.0782\n",
      "Epoch 257: val_loss did not improve from 0.00627\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0066 - rmse: 0.0804 - val_loss: 0.0063 - val_rmse: 0.0788 - lr: 1.0000e-04\n",
      "Epoch 258/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0076 - rmse: 0.0864\n",
      "Epoch 258: val_loss did not improve from 0.00627\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0068 - rmse: 0.0816 - val_loss: 0.0066 - val_rmse: 0.0806 - lr: 1.0000e-04\n",
      "Epoch 259/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0087 - rmse: 0.0926\n",
      "Epoch 259: val_loss did not improve from 0.00627\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0079 - rmse: 0.0884 - val_loss: 0.0072 - val_rmse: 0.0845 - lr: 1.0000e-04\n",
      "Epoch 260/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0063 - rmse: 0.0785\n",
      "Epoch 260: val_loss did not improve from 0.00627\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0062 - rmse: 0.0783 - val_loss: 0.0066 - val_rmse: 0.0807 - lr: 1.0000e-04\n",
      "Epoch 261/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0057 - rmse: 0.0752\n",
      "Epoch 261: val_loss improved from 0.00627 to 0.00591, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0065 - rmse: 0.0800 - val_loss: 0.0059 - val_rmse: 0.0763 - lr: 1.0000e-04\n",
      "Epoch 262/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0063 - rmse: 0.0786\n",
      "Epoch 262: val_loss improved from 0.00591 to 0.00527, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0057 - rmse: 0.0751 - val_loss: 0.0053 - val_rmse: 0.0719 - lr: 1.0000e-04\n",
      "Epoch 263/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0059 - rmse: 0.0761\n",
      "Epoch 263: val_loss improved from 0.00527 to 0.00489, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0060 - rmse: 0.0766 - val_loss: 0.0049 - val_rmse: 0.0692 - lr: 1.0000e-04\n",
      "Epoch 264/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0059 - rmse: 0.0760\n",
      "Epoch 264: val_loss did not improve from 0.00489\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0058 - rmse: 0.0755 - val_loss: 0.0050 - val_rmse: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 265/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0049 - rmse: 0.0691\n",
      "Epoch 265: val_loss improved from 0.00489 to 0.00467, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0054 - rmse: 0.0727 - val_loss: 0.0047 - val_rmse: 0.0676 - lr: 1.0000e-04\n",
      "Epoch 266/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0059 - rmse: 0.0762\n",
      "Epoch 266: val_loss did not improve from 0.00467\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0050 - rmse: 0.0699 - val_loss: 0.0052 - val_rmse: 0.0712 - lr: 1.0000e-04\n",
      "Epoch 267/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0055 - rmse: 0.0735\n",
      "Epoch 267: val_loss improved from 0.00467 to 0.00456, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0057 - rmse: 0.0746 - val_loss: 0.0046 - val_rmse: 0.0668 - lr: 1.0000e-04\n",
      "Epoch 268/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0045 - rmse: 0.0665\n",
      "Epoch 268: val_loss did not improve from 0.00456\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0049 - rmse: 0.0692 - val_loss: 0.0047 - val_rmse: 0.0676 - lr: 1.0000e-04\n",
      "Epoch 269/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0046 - rmse: 0.0669\n",
      "Epoch 269: val_loss improved from 0.00456 to 0.00417, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0051 - rmse: 0.0704 - val_loss: 0.0042 - val_rmse: 0.0638 - lr: 1.0000e-04\n",
      "Epoch 270/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0037 - rmse: 0.0604\n",
      "Epoch 270: val_loss improved from 0.00417 to 0.00410, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0046 - rmse: 0.0672 - val_loss: 0.0041 - val_rmse: 0.0633 - lr: 1.0000e-04\n",
      "Epoch 271/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0049 - rmse: 0.0691\n",
      "Epoch 271: val_loss did not improve from 0.00410\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0044 - rmse: 0.0658 - val_loss: 0.0041 - val_rmse: 0.0636 - lr: 1.0000e-04\n",
      "Epoch 272/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0042 - rmse: 0.0643\n",
      "Epoch 272: val_loss did not improve from 0.00410\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0046 - rmse: 0.0668 - val_loss: 0.0045 - val_rmse: 0.0667 - lr: 1.0000e-04\n",
      "Epoch 273/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0053 - rmse: 0.0720\n",
      "Epoch 273: val_loss improved from 0.00410 to 0.00402, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0047 - rmse: 0.0682 - val_loss: 0.0040 - val_rmse: 0.0626 - lr: 1.0000e-04\n",
      "Epoch 274/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0042 - rmse: 0.0639\n",
      "Epoch 274: val_loss improved from 0.00402 to 0.00401, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0046 - rmse: 0.0669 - val_loss: 0.0040 - val_rmse: 0.0626 - lr: 1.0000e-04\n",
      "Epoch 275/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0040 - rmse: 0.0626\n",
      "Epoch 275: val_loss improved from 0.00401 to 0.00371, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0041 - rmse: 0.0635 - val_loss: 0.0037 - val_rmse: 0.0601 - lr: 1.0000e-04\n",
      "Epoch 276/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0042 - rmse: 0.0637\n",
      "Epoch 276: val_loss did not improve from 0.00371\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0042 - rmse: 0.0638 - val_loss: 0.0039 - val_rmse: 0.0617 - lr: 1.0000e-04\n",
      "Epoch 277/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0045 - rmse: 0.0660\n",
      "Epoch 277: val_loss improved from 0.00371 to 0.00351, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0041 - rmse: 0.0629 - val_loss: 0.0035 - val_rmse: 0.0584 - lr: 1.0000e-04\n",
      "Epoch 278/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0049 - rmse: 0.0695\n",
      "Epoch 278: val_loss did not improve from 0.00351\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0041 - rmse: 0.0631 - val_loss: 0.0037 - val_rmse: 0.0599 - lr: 1.0000e-04\n",
      "Epoch 279/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0038 - rmse: 0.0608\n",
      "Epoch 279: val_loss improved from 0.00351 to 0.00343, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0039 - rmse: 0.0614 - val_loss: 0.0034 - val_rmse: 0.0577 - lr: 1.0000e-04\n",
      "Epoch 280/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0045 - rmse: 0.0660\n",
      "Epoch 280: val_loss did not improve from 0.00343\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0041 - rmse: 0.0632 - val_loss: 0.0038 - val_rmse: 0.0609 - lr: 1.0000e-04\n",
      "Epoch 281/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0044 - rmse: 0.0657\n",
      "Epoch 281: val_loss improved from 0.00343 to 0.00328, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0040 - rmse: 0.0626 - val_loss: 0.0033 - val_rmse: 0.0564 - lr: 1.0000e-04\n",
      "Epoch 282/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0040 - rmse: 0.0627\n",
      "Epoch 282: val_loss did not improve from 0.00328\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0041 - rmse: 0.0631 - val_loss: 0.0036 - val_rmse: 0.0593 - lr: 1.0000e-04\n",
      "Epoch 283/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0036 - rmse: 0.0592\n",
      "Epoch 283: val_loss improved from 0.00328 to 0.00323, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0036 - rmse: 0.0590 - val_loss: 0.0032 - val_rmse: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 284/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0045 - rmse: 0.0662\n",
      "Epoch 284: val_loss did not improve from 0.00323\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0039 - rmse: 0.0617 - val_loss: 0.0034 - val_rmse: 0.0571 - lr: 1.0000e-04\n",
      "Epoch 285/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0042 - rmse: 0.0643\n",
      "Epoch 285: val_loss improved from 0.00323 to 0.00309, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0037 - rmse: 0.0602 - val_loss: 0.0031 - val_rmse: 0.0547 - lr: 1.0000e-04\n",
      "Epoch 286/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0031 - rmse: 0.0551\n",
      "Epoch 286: val_loss did not improve from 0.00309\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0036 - rmse: 0.0588 - val_loss: 0.0033 - val_rmse: 0.0567 - lr: 1.0000e-04\n",
      "Epoch 287/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0033 - rmse: 0.0562\n",
      "Epoch 287: val_loss improved from 0.00309 to 0.00308, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0035 - rmse: 0.0579 - val_loss: 0.0031 - val_rmse: 0.0547 - lr: 1.0000e-04\n",
      "Epoch 288/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0038 - rmse: 0.0608\n",
      "Epoch 288: val_loss did not improve from 0.00308\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0036 - rmse: 0.0593 - val_loss: 0.0036 - val_rmse: 0.0595 - lr: 1.0000e-04\n",
      "Epoch 289/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0574\n",
      "Epoch 289: val_loss improved from 0.00308 to 0.00307, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0036 - rmse: 0.0591 - val_loss: 0.0031 - val_rmse: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 290/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0576\n",
      "Epoch 290: val_loss did not improve from 0.00307\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0034 - rmse: 0.0577 - val_loss: 0.0031 - val_rmse: 0.0547 - lr: 1.0000e-04\n",
      "Epoch 291/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0577\n",
      "Epoch 291: val_loss improved from 0.00307 to 0.00272, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0032 - rmse: 0.0559 - val_loss: 0.0027 - val_rmse: 0.0513 - lr: 1.0000e-04\n",
      "Epoch 292/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0032 - rmse: 0.0557\n",
      "Epoch 292: val_loss did not improve from 0.00272\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0034 - rmse: 0.0572 - val_loss: 0.0032 - val_rmse: 0.0553 - lr: 1.0000e-04\n",
      "Epoch 293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0037 - rmse: 0.0603\n",
      "Epoch 293: val_loss did not improve from 0.00272\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0034 - rmse: 0.0574 - val_loss: 0.0029 - val_rmse: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 294/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0030 - rmse: 0.0542\n",
      "Epoch 294: val_loss did not improve from 0.00272\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0037 - rmse: 0.0597 - val_loss: 0.0031 - val_rmse: 0.0549 - lr: 1.0000e-04\n",
      "Epoch 295/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0035 - rmse: 0.0583\n",
      "Epoch 295: val_loss improved from 0.00272 to 0.00258, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0033 - rmse: 0.0566 - val_loss: 0.0026 - val_rmse: 0.0498 - lr: 1.0000e-04\n",
      "Epoch 296/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0578\n",
      "Epoch 296: val_loss did not improve from 0.00258\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0034 - rmse: 0.0577 - val_loss: 0.0027 - val_rmse: 0.0508 - lr: 1.0000e-04\n",
      "Epoch 297/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0576\n",
      "Epoch 297: val_loss improved from 0.00258 to 0.00249, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0033 - rmse: 0.0568 - val_loss: 0.0025 - val_rmse: 0.0489 - lr: 1.0000e-04\n",
      "Epoch 298/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0468\n",
      "Epoch 298: val_loss did not improve from 0.00249\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0030 - rmse: 0.0539 - val_loss: 0.0026 - val_rmse: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 299/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0028 - rmse: 0.0521\n",
      "Epoch 299: val_loss improved from 0.00249 to 0.00237, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0028 - rmse: 0.0518 - val_loss: 0.0024 - val_rmse: 0.0476 - lr: 1.0000e-04\n",
      "Epoch 300/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0576\n",
      "Epoch 300: val_loss did not improve from 0.00237\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0030 - rmse: 0.0540 - val_loss: 0.0024 - val_rmse: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 301/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0026 - rmse: 0.0500\n",
      "Epoch 301: val_loss improved from 0.00237 to 0.00235, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0029 - rmse: 0.0529 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 302/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0024 - rmse: 0.0475\n",
      "Epoch 302: val_loss improved from 0.00235 to 0.00234, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0027 - rmse: 0.0511 - val_loss: 0.0023 - val_rmse: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 303/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0574\n",
      "Epoch 303: val_loss improved from 0.00234 to 0.00229, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0028 - rmse: 0.0521 - val_loss: 0.0023 - val_rmse: 0.0468 - lr: 1.0000e-04\n",
      "Epoch 304/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0027 - rmse: 0.0507\n",
      "Epoch 304: val_loss did not improve from 0.00229\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0029 - rmse: 0.0532 - val_loss: 0.0023 - val_rmse: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 305/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0572\n",
      "Epoch 305: val_loss did not improve from 0.00229\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0028 - rmse: 0.0518 - val_loss: 0.0023 - val_rmse: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 306/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0028 - rmse: 0.0516\n",
      "Epoch 306: val_loss did not improve from 0.00229\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0030 - rmse: 0.0541 - val_loss: 0.0024 - val_rmse: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 307/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0465\n",
      "Epoch 307: val_loss improved from 0.00229 to 0.00227, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0025 - rmse: 0.0490 - val_loss: 0.0023 - val_rmse: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 308/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0577\n",
      "Epoch 308: val_loss improved from 0.00227 to 0.00222, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0028 - rmse: 0.0520 - val_loss: 0.0022 - val_rmse: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 309/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0029 - rmse: 0.0526\n",
      "Epoch 309: val_loss improved from 0.00222 to 0.00215, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0026 - rmse: 0.0499 - val_loss: 0.0022 - val_rmse: 0.0454 - lr: 1.0000e-04\n",
      "Epoch 310/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0452\n",
      "Epoch 310: val_loss improved from 0.00215 to 0.00210, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0024 - rmse: 0.0484 - val_loss: 0.0021 - val_rmse: 0.0448 - lr: 1.0000e-04\n",
      "Epoch 311/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0024 - rmse: 0.0481\n",
      "Epoch 311: val_loss did not improve from 0.00210\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0026 - rmse: 0.0496 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 312/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0025 - rmse: 0.0494\n",
      "Epoch 312: val_loss improved from 0.00210 to 0.00206, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0025 - rmse: 0.0494 - val_loss: 0.0021 - val_rmse: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 313/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0030 - rmse: 0.0541\n",
      "Epoch 313: val_loss did not improve from 0.00206\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0025 - rmse: 0.0487 - val_loss: 0.0021 - val_rmse: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 314/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0027 - rmse: 0.0508\n",
      "Epoch 314: val_loss improved from 0.00206 to 0.00194, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0025 - rmse: 0.0486 - val_loss: 0.0019 - val_rmse: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 315/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0449\n",
      "Epoch 315: val_loss did not improve from 0.00194\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0024 - rmse: 0.0481 - val_loss: 0.0020 - val_rmse: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 316/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0447\n",
      "Epoch 316: val_loss did not improve from 0.00194\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0023 - rmse: 0.0466 - val_loss: 0.0020 - val_rmse: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 317/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0465\n",
      "Epoch 317: val_loss improved from 0.00194 to 0.00194, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0022 - rmse: 0.0459 - val_loss: 0.0019 - val_rmse: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 318/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0032 - rmse: 0.0555\n",
      "Epoch 318: val_loss improved from 0.00194 to 0.00189, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0023 - rmse: 0.0474 - val_loss: 0.0019 - val_rmse: 0.0424 - lr: 1.0000e-04\n",
      "Epoch 319/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0472\n",
      "Epoch 319: val_loss improved from 0.00189 to 0.00186, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0024 - rmse: 0.0480 - val_loss: 0.0019 - val_rmse: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 320/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0430\n",
      "Epoch 320: val_loss improved from 0.00186 to 0.00183, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0021 - rmse: 0.0448 - val_loss: 0.0018 - val_rmse: 0.0416 - lr: 1.0000e-04\n",
      "Epoch 321/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0022 - rmse: 0.0459\n",
      "Epoch 321: val_loss did not improve from 0.00183\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0022 - rmse: 0.0458 - val_loss: 0.0019 - val_rmse: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 322/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0453\n",
      "Epoch 322: val_loss improved from 0.00183 to 0.00181, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0022 - rmse: 0.0454 - val_loss: 0.0018 - val_rmse: 0.0414 - lr: 1.0000e-04\n",
      "Epoch 323/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0443\n",
      "Epoch 323: val_loss improved from 0.00181 to 0.00177, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0022 - rmse: 0.0457 - val_loss: 0.0018 - val_rmse: 0.0409 - lr: 1.0000e-04\n",
      "Epoch 324/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0470\n",
      "Epoch 324: val_loss did not improve from 0.00177\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0023 - rmse: 0.0471 - val_loss: 0.0018 - val_rmse: 0.0410 - lr: 1.0000e-04\n",
      "Epoch 325/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0424\n",
      "Epoch 325: val_loss improved from 0.00177 to 0.00177, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0023 - rmse: 0.0470 - val_loss: 0.0018 - val_rmse: 0.0409 - lr: 1.0000e-04\n",
      "Epoch 326/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0388\n",
      "Epoch 326: val_loss did not improve from 0.00177\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0024 - rmse: 0.0480 - val_loss: 0.0018 - val_rmse: 0.0419 - lr: 1.0000e-04\n",
      "Epoch 327/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - rmse: 0.0401\n",
      "Epoch 327: val_loss did not improve from 0.00177\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0021 - rmse: 0.0449 - val_loss: 0.0018 - val_rmse: 0.0414 - lr: 1.0000e-04\n",
      "Epoch 328/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0437\n",
      "Epoch 328: val_loss did not improve from 0.00177\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0021 - rmse: 0.0446 - val_loss: 0.0019 - val_rmse: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 329/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0409\n",
      "Epoch 329: val_loss improved from 0.00177 to 0.00174, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0022 - rmse: 0.0454 - val_loss: 0.0017 - val_rmse: 0.0406 - lr: 1.0000e-04\n",
      "Epoch 330/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0465\n",
      "Epoch 330: val_loss improved from 0.00174 to 0.00172, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0022 - rmse: 0.0463 - val_loss: 0.0017 - val_rmse: 0.0403 - lr: 1.0000e-04\n",
      "Epoch 331/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - rmse: 0.0404\n",
      "Epoch 331: val_loss improved from 0.00172 to 0.00166, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0020 - rmse: 0.0432 - val_loss: 0.0017 - val_rmse: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 332/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0436\n",
      "Epoch 332: val_loss did not improve from 0.00166\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0019 - rmse: 0.0427 - val_loss: 0.0017 - val_rmse: 0.0399 - lr: 1.0000e-04\n",
      "Epoch 333/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0418\n",
      "Epoch 333: val_loss improved from 0.00166 to 0.00157, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0020 - rmse: 0.0433 - val_loss: 0.0016 - val_rmse: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 334/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0432\n",
      "Epoch 334: val_loss did not improve from 0.00157\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0021 - rmse: 0.0450 - val_loss: 0.0016 - val_rmse: 0.0390 - lr: 1.0000e-04\n",
      "Epoch 335/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0414\n",
      "Epoch 335: val_loss improved from 0.00157 to 0.00156, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0016 - rmse: 0.0391 - val_loss: 0.0016 - val_rmse: 0.0383 - lr: 1.0000e-04\n",
      "Epoch 336/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - rmse: 0.0401\n",
      "Epoch 336: val_loss did not improve from 0.00156\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0020 - rmse: 0.0435 - val_loss: 0.0016 - val_rmse: 0.0389 - lr: 1.0000e-04\n",
      "Epoch 337/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0432\n",
      "Epoch 337: val_loss improved from 0.00156 to 0.00154, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0017 - rmse: 0.0404 - val_loss: 0.0015 - val_rmse: 0.0381 - lr: 1.0000e-04\n",
      "Epoch 338/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0026 - rmse: 0.0502\n",
      "Epoch 338: val_loss did not improve from 0.00154\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0021 - rmse: 0.0451 - val_loss: 0.0016 - val_rmse: 0.0394 - lr: 1.0000e-04\n",
      "Epoch 339/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0443\n",
      "Epoch 339: val_loss improved from 0.00154 to 0.00152, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0020 - rmse: 0.0438 - val_loss: 0.0015 - val_rmse: 0.0378 - lr: 1.0000e-04\n",
      "Epoch 340/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0392\n",
      "Epoch 340: val_loss improved from 0.00152 to 0.00149, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0017 - rmse: 0.0399 - val_loss: 0.0015 - val_rmse: 0.0374 - lr: 1.0000e-04\n",
      "Epoch 341/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0448\n",
      "Epoch 341: val_loss did not improve from 0.00149\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0019 - rmse: 0.0424 - val_loss: 0.0015 - val_rmse: 0.0375 - lr: 1.0000e-04\n",
      "Epoch 342/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0374\n",
      "Epoch 342: val_loss improved from 0.00149 to 0.00146, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0017 - rmse: 0.0406 - val_loss: 0.0015 - val_rmse: 0.0370 - lr: 1.0000e-04\n",
      "Epoch 343/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0413\n",
      "Epoch 343: val_loss did not improve from 0.00146\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0017 - rmse: 0.0395 - val_loss: 0.0015 - val_rmse: 0.0380 - lr: 1.0000e-04\n",
      "Epoch 344/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0022 - rmse: 0.0458\n",
      "Epoch 344: val_loss did not improve from 0.00146\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0019 - rmse: 0.0421 - val_loss: 0.0015 - val_rmse: 0.0379 - lr: 1.0000e-04\n",
      "Epoch 345/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - rmse: 0.0402\n",
      "Epoch 345: val_loss did not improve from 0.00146\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0017 - rmse: 0.0403 - val_loss: 0.0016 - val_rmse: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 346/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0469\n",
      "Epoch 346: val_loss improved from 0.00146 to 0.00145, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0019 - rmse: 0.0421 - val_loss: 0.0015 - val_rmse: 0.0369 - lr: 1.0000e-04\n",
      "Epoch 347/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0415\n",
      "Epoch 347: val_loss did not improve from 0.00145\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0018 - rmse: 0.0415 - val_loss: 0.0015 - val_rmse: 0.0372 - lr: 1.0000e-04\n",
      "Epoch 348/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0426\n",
      "Epoch 348: val_loss improved from 0.00145 to 0.00135, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0017 - rmse: 0.0402 - val_loss: 0.0014 - val_rmse: 0.0355 - lr: 1.0000e-04\n",
      "Epoch 349/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0446\n",
      "Epoch 349: val_loss did not improve from 0.00135\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0017 - rmse: 0.0400 - val_loss: 0.0014 - val_rmse: 0.0358 - lr: 1.0000e-04\n",
      "Epoch 350/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0368\n",
      "Epoch 350: val_loss improved from 0.00135 to 0.00131, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0016 - rmse: 0.0385 - val_loss: 0.0013 - val_rmse: 0.0349 - lr: 1.0000e-04\n",
      "Epoch 351/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - rmse: 0.0398\n",
      "Epoch 351: val_loss did not improve from 0.00131\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0018 - rmse: 0.0410 - val_loss: 0.0014 - val_rmse: 0.0357 - lr: 1.0000e-04\n",
      "Epoch 352/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0394\n",
      "Epoch 352: val_loss did not improve from 0.00131\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0018 - rmse: 0.0412 - val_loss: 0.0014 - val_rmse: 0.0366 - lr: 1.0000e-04\n",
      "Epoch 353/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0372\n",
      "Epoch 353: val_loss did not improve from 0.00131\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0016 - rmse: 0.0392 - val_loss: 0.0013 - val_rmse: 0.0352 - lr: 1.0000e-04\n",
      "Epoch 354/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0391\n",
      "Epoch 354: val_loss improved from 0.00131 to 0.00129, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0017 - rmse: 0.0402 - val_loss: 0.0013 - val_rmse: 0.0347 - lr: 1.0000e-04\n",
      "Epoch 355/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0371\n",
      "Epoch 355: val_loss improved from 0.00129 to 0.00125, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0014 - rmse: 0.0361 - val_loss: 0.0012 - val_rmse: 0.0340 - lr: 1.0000e-04\n",
      "Epoch 356/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0371\n",
      "Epoch 356: val_loss did not improve from 0.00125\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0017 - rmse: 0.0398 - val_loss: 0.0014 - val_rmse: 0.0366 - lr: 1.0000e-04\n",
      "Epoch 357/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0424\n",
      "Epoch 357: val_loss did not improve from 0.00125\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0016 - rmse: 0.0394 - val_loss: 0.0014 - val_rmse: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 358/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0372\n",
      "Epoch 358: val_loss did not improve from 0.00125\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0016 - rmse: 0.0384 - val_loss: 0.0014 - val_rmse: 0.0367 - lr: 1.0000e-04\n",
      "Epoch 359/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0382\n",
      "Epoch 359: val_loss did not improve from 0.00125\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0015 - rmse: 0.0369 - val_loss: 0.0013 - val_rmse: 0.0341 - lr: 1.0000e-04\n",
      "Epoch 360/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0393\n",
      "Epoch 360: val_loss did not improve from 0.00125\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0016 - rmse: 0.0384 - val_loss: 0.0013 - val_rmse: 0.0345 - lr: 1.0000e-04\n",
      "Epoch 361/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0356\n",
      "Epoch 361: val_loss improved from 0.00125 to 0.00119, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0015 - rmse: 0.0375 - val_loss: 0.0012 - val_rmse: 0.0331 - lr: 1.0000e-04\n",
      "Epoch 362/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0337\n",
      "Epoch 362: val_loss did not improve from 0.00119\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0014 - rmse: 0.0361 - val_loss: 0.0013 - val_rmse: 0.0344 - lr: 1.0000e-04\n",
      "Epoch 363/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0382\n",
      "Epoch 363: val_loss improved from 0.00119 to 0.00119, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0014 - rmse: 0.0363 - val_loss: 0.0012 - val_rmse: 0.0331 - lr: 1.0000e-04\n",
      "Epoch 364/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0362\n",
      "Epoch 364: val_loss improved from 0.00119 to 0.00115, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0014 - rmse: 0.0365 - val_loss: 0.0011 - val_rmse: 0.0325 - lr: 1.0000e-04\n",
      "Epoch 365/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0311\n",
      "Epoch 365: val_loss improved from 0.00115 to 0.00114, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0014 - rmse: 0.0358 - val_loss: 0.0011 - val_rmse: 0.0324 - lr: 1.0000e-04\n",
      "Epoch 366/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0387\n",
      "Epoch 366: val_loss did not improve from 0.00114\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0014 - rmse: 0.0362 - val_loss: 0.0012 - val_rmse: 0.0326 - lr: 1.0000e-04\n",
      "Epoch 367/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0352\n",
      "Epoch 367: val_loss did not improve from 0.00114\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0013 - rmse: 0.0345 - val_loss: 0.0012 - val_rmse: 0.0326 - lr: 1.0000e-04\n",
      "Epoch 368/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0325\n",
      "Epoch 368: val_loss improved from 0.00114 to 0.00113, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0015 - rmse: 0.0375 - val_loss: 0.0011 - val_rmse: 0.0322 - lr: 1.0000e-04\n",
      "Epoch 369/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0312\n",
      "Epoch 369: val_loss did not improve from 0.00113\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0351 - val_loss: 0.0012 - val_rmse: 0.0332 - lr: 1.0000e-04\n",
      "Epoch 370/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0387\n",
      "Epoch 370: val_loss improved from 0.00113 to 0.00110, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0014 - rmse: 0.0355 - val_loss: 0.0011 - val_rmse: 0.0318 - lr: 1.0000e-04\n",
      "Epoch 371/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0323\n",
      "Epoch 371: val_loss improved from 0.00110 to 0.00109, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0014 - rmse: 0.0360 - val_loss: 0.0011 - val_rmse: 0.0316 - lr: 1.0000e-04\n",
      "Epoch 372/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0314\n",
      "Epoch 372: val_loss did not improve from 0.00109\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0345 - val_loss: 0.0011 - val_rmse: 0.0316 - lr: 1.0000e-04\n",
      "Epoch 373/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0324\n",
      "Epoch 373: val_loss improved from 0.00109 to 0.00106, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0013 - rmse: 0.0348 - val_loss: 0.0011 - val_rmse: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 374/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0354\n",
      "Epoch 374: val_loss did not improve from 0.00106\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0014 - rmse: 0.0357 - val_loss: 0.0011 - val_rmse: 0.0314 - lr: 1.0000e-04\n",
      "Epoch 375/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0325\n",
      "Epoch 375: val_loss did not improve from 0.00106\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0013 - rmse: 0.0344 - val_loss: 0.0011 - val_rmse: 0.0317 - lr: 1.0000e-04\n",
      "Epoch 376/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0334\n",
      "Epoch 376: val_loss did not improve from 0.00106\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0343 - val_loss: 0.0011 - val_rmse: 0.0314 - lr: 1.0000e-04\n",
      "Epoch 377/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0317\n",
      "Epoch 377: val_loss did not improve from 0.00106\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0354 - val_loss: 0.0011 - val_rmse: 0.0324 - lr: 1.0000e-04\n",
      "Epoch 378/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0315\n",
      "Epoch 378: val_loss did not improve from 0.00106\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0346 - val_loss: 0.0011 - val_rmse: 0.0315 - lr: 1.0000e-04\n",
      "Epoch 379/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0337\n",
      "Epoch 379: val_loss did not improve from 0.00106\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0012 - rmse: 0.0338 - val_loss: 0.0012 - val_rmse: 0.0338 - lr: 1.0000e-04\n",
      "Epoch 380/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0330\n",
      "Epoch 380: val_loss improved from 0.00106 to 0.00100, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0012 - rmse: 0.0337 - val_loss: 0.0010 - val_rmse: 0.0302 - lr: 1.0000e-04\n",
      "Epoch 381/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0326\n",
      "Epoch 381: val_loss did not improve from 0.00100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0343 - val_loss: 0.0010 - val_rmse: 0.0305 - lr: 1.0000e-04\n",
      "Epoch 382/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - rmse: 0.0307\n",
      "Epoch 382: val_loss improved from 0.00100 to 0.00098, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0012 - rmse: 0.0330 - val_loss: 9.7672e-04 - val_rmse: 0.0297 - lr: 1.0000e-04\n",
      "Epoch 383/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0322\n",
      "Epoch 383: val_loss did not improve from 0.00098\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0012 - rmse: 0.0338 - val_loss: 9.7863e-04 - val_rmse: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 384/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0338\n",
      "Epoch 384: val_loss improved from 0.00098 to 0.00096, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0012 - rmse: 0.0326 - val_loss: 9.6288e-04 - val_rmse: 0.0295 - lr: 1.0000e-04\n",
      "Epoch 385/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5906e-04 - rmse: 0.0294\n",
      "Epoch 385: val_loss improved from 0.00096 to 0.00095, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0011 - rmse: 0.0320 - val_loss: 9.5159e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 386/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0363\n",
      "Epoch 386: val_loss did not improve from 0.00095\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0013 - rmse: 0.0341 - val_loss: 9.7961e-04 - val_rmse: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 387/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9713e-04 - rmse: 0.0301\n",
      "Epoch 387: val_loss did not improve from 0.00095\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0010 - rmse: 0.0306 - val_loss: 9.5591e-04 - val_rmse: 0.0294 - lr: 1.0000e-04\n",
      "Epoch 388/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0318\n",
      "Epoch 388: val_loss did not improve from 0.00095\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0011 - rmse: 0.0323 - val_loss: 0.0010 - val_rmse: 0.0302 - lr: 1.0000e-04\n",
      "Epoch 389/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0327\n",
      "Epoch 389: val_loss improved from 0.00095 to 0.00092, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0011 - rmse: 0.0324 - val_loss: 9.1955e-04 - val_rmse: 0.0288 - lr: 1.0000e-04\n",
      "Epoch 390/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3663e-04 - rmse: 0.0291\n",
      "Epoch 390: val_loss did not improve from 0.00092\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0011 - rmse: 0.0320 - val_loss: 9.6577e-04 - val_rmse: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 391/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8516e-04 - rmse: 0.0282\n",
      "Epoch 391: val_loss did not improve from 0.00092\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0010 - rmse: 0.0307 - val_loss: 9.4590e-04 - val_rmse: 0.0292 - lr: 1.0000e-04\n",
      "Epoch 392/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4421e-04 - rmse: 0.0292\n",
      "Epoch 392: val_loss did not improve from 0.00092\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0011 - rmse: 0.0319 - val_loss: 9.7439e-04 - val_rmse: 0.0297 - lr: 1.0000e-04\n",
      "Epoch 393/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0332\n",
      "Epoch 393: val_loss did not improve from 0.00092\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0011 - rmse: 0.0322 - val_loss: 0.0010 - val_rmse: 0.0302 - lr: 1.0000e-04\n",
      "Epoch 394/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4081e-04 - rmse: 0.0291\n",
      "Epoch 394: val_loss did not improve from 0.00092\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0010 - rmse: 0.0305 - val_loss: 9.6597e-04 - val_rmse: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 395/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0319\n",
      "Epoch 395: val_loss did not improve from 0.00092\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0012 - rmse: 0.0327 - val_loss: 9.8245e-04 - val_rmse: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 396/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0319\n",
      "Epoch 396: val_loss improved from 0.00092 to 0.00090, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0010 - rmse: 0.0303 - val_loss: 8.9687e-04 - val_rmse: 0.0284 - lr: 1.0000e-04\n",
      "Epoch 397/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0323\n",
      "Epoch 397: val_loss did not improve from 0.00090\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0010 - rmse: 0.0302 - val_loss: 9.3715e-04 - val_rmse: 0.0291 - lr: 1.0000e-04\n",
      "Epoch 398/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0321\n",
      "Epoch 398: val_loss improved from 0.00090 to 0.00085, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0010 - rmse: 0.0302 - val_loss: 8.5187e-04 - val_rmse: 0.0276 - lr: 1.0000e-04\n",
      "Epoch 399/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5162e-04 - rmse: 0.0293\n",
      "Epoch 399: val_loss did not improve from 0.00085\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0010 - rmse: 0.0303 - val_loss: 9.2556e-04 - val_rmse: 0.0289 - lr: 1.0000e-04\n",
      "Epoch 400/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5709e-04 - rmse: 0.0277\n",
      "Epoch 400: val_loss improved from 0.00085 to 0.00085, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 9.6947e-04 - rmse: 0.0296 - val_loss: 8.4514e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 401/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - rmse: 0.0307\n",
      "Epoch 401: val_loss improved from 0.00085 to 0.00081, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0010 - rmse: 0.0306 - val_loss: 8.0603e-04 - val_rmse: 0.0267 - lr: 1.0000e-04\n",
      "Epoch 402/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0310\n",
      "Epoch 402: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0011 - rmse: 0.0311 - val_loss: 8.4980e-04 - val_rmse: 0.0275 - lr: 1.0000e-04\n",
      "Epoch 403/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - rmse: 0.0307\n",
      "Epoch 403: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.8607e-04 - rmse: 0.0299 - val_loss: 8.4564e-04 - val_rmse: 0.0275 - lr: 1.0000e-04\n",
      "Epoch 404/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0334\n",
      "Epoch 404: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0010 - rmse: 0.0308 - val_loss: 9.7257e-04 - val_rmse: 0.0297 - lr: 1.0000e-04\n",
      "Epoch 405/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0311\n",
      "Epoch 405: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0011 - rmse: 0.0320 - val_loss: 0.0011 - val_rmse: 0.0314 - lr: 1.0000e-04\n",
      "Epoch 406/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0341\n",
      "Epoch 406: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0011 - rmse: 0.0312 - val_loss: 0.0013 - val_rmse: 0.0347 - lr: 1.0000e-04\n",
      "Epoch 407/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0384\n",
      "Epoch 407: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0013 - rmse: 0.0348 - val_loss: 0.0011 - val_rmse: 0.0325 - lr: 1.0000e-04\n",
      "Epoch 408/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0377\n",
      "Epoch 408: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0351 - val_loss: 8.8538e-04 - val_rmse: 0.0282 - lr: 1.0000e-04\n",
      "Epoch 409/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3183e-04 - rmse: 0.0290\n",
      "Epoch 409: val_loss improved from 0.00081 to 0.00076, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0010 - rmse: 0.0306 - val_loss: 7.6387e-04 - val_rmse: 0.0259 - lr: 1.0000e-04\n",
      "Epoch 410/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0311\n",
      "Epoch 410: val_loss improved from 0.00076 to 0.00075, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0011 - rmse: 0.0324 - val_loss: 7.5348e-04 - val_rmse: 0.0257 - lr: 1.0000e-04\n",
      "Epoch 411/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6902e-04 - rmse: 0.0260\n",
      "Epoch 411: val_loss improved from 0.00075 to 0.00074, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 9.5317e-04 - rmse: 0.0294 - val_loss: 7.4029e-04 - val_rmse: 0.0255 - lr: 1.0000e-04\n",
      "Epoch 412/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9982e-04 - rmse: 0.0266\n",
      "Epoch 412: val_loss improved from 0.00074 to 0.00071, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0010 - rmse: 0.0307 - val_loss: 7.1384e-04 - val_rmse: 0.0250 - lr: 1.0000e-04\n",
      "Epoch 413/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - rmse: 0.0305\n",
      "Epoch 413: val_loss did not improve from 0.00071\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.7916e-04 - rmse: 0.0281 - val_loss: 8.2346e-04 - val_rmse: 0.0271 - lr: 1.0000e-04\n",
      "Epoch 414/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0356\n",
      "Epoch 414: val_loss did not improve from 0.00071\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.5265e-04 - rmse: 0.0293 - val_loss: 8.0786e-04 - val_rmse: 0.0268 - lr: 1.0000e-04\n",
      "Epoch 415/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0314\n",
      "Epoch 415: val_loss did not improve from 0.00071\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.6734e-04 - rmse: 0.0296 - val_loss: 7.8765e-04 - val_rmse: 0.0264 - lr: 1.0000e-04\n",
      "Epoch 416/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1892e-04 - rmse: 0.0270\n",
      "Epoch 416: val_loss improved from 0.00071 to 0.00070, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 8.9692e-04 - rmse: 0.0284 - val_loss: 6.9695e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 417/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5761e-04 - rmse: 0.0277\n",
      "Epoch 417: val_loss did not improve from 0.00070\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.5801e-04 - rmse: 0.0277 - val_loss: 7.5566e-04 - val_rmse: 0.0258 - lr: 1.0000e-04\n",
      "Epoch 418/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4248e-04 - rmse: 0.0292\n",
      "Epoch 418: val_loss did not improve from 0.00070\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.2638e-04 - rmse: 0.0289 - val_loss: 7.5265e-04 - val_rmse: 0.0257 - lr: 1.0000e-04\n",
      "Epoch 419/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3700e-04 - rmse: 0.0254\n",
      "Epoch 419: val_loss did not improve from 0.00070\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.2336e-04 - rmse: 0.0271 - val_loss: 7.3435e-04 - val_rmse: 0.0254 - lr: 1.0000e-04\n",
      "Epoch 420/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1757e-04 - rmse: 0.0287\n",
      "Epoch 420: val_loss did not improve from 0.00070\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.5732e-04 - rmse: 0.0294 - val_loss: 7.0788e-04 - val_rmse: 0.0248 - lr: 1.0000e-04\n",
      "Epoch 421/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7621e-04 - rmse: 0.0280\n",
      "Epoch 421: val_loss improved from 0.00070 to 0.00067, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 9.0833e-04 - rmse: 0.0286 - val_loss: 6.7029e-04 - val_rmse: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 422/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7704e-04 - rmse: 0.0242\n",
      "Epoch 422: val_loss did not improve from 0.00067\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.5405e-04 - rmse: 0.0276 - val_loss: 7.8162e-04 - val_rmse: 0.0263 - lr: 1.0000e-04\n",
      "Epoch 423/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9475e-04 - rmse: 0.0246\n",
      "Epoch 423: val_loss did not improve from 0.00067\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.1715e-04 - rmse: 0.0250 - val_loss: 6.9376e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 424/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.0098e-04 - rmse: 0.0285\n",
      "Epoch 424: val_loss did not improve from 0.00067\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.0698e-04 - rmse: 0.0286 - val_loss: 7.5520e-04 - val_rmse: 0.0258 - lr: 1.0000e-04\n",
      "Epoch 425/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7558e-04 - rmse: 0.0280\n",
      "Epoch 425: val_loss did not improve from 0.00067\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.7455e-04 - rmse: 0.0280 - val_loss: 6.9237e-04 - val_rmse: 0.0245 - lr: 1.0000e-04\n",
      "Epoch 426/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3122e-04 - rmse: 0.0253\n",
      "Epoch 426: val_loss did not improve from 0.00067\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.7452e-04 - rmse: 0.0261 - val_loss: 7.1838e-04 - val_rmse: 0.0251 - lr: 1.0000e-04\n",
      "Epoch 427/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8790e-04 - rmse: 0.0264\n",
      "Epoch 427: val_loss improved from 0.00067 to 0.00064, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 7.3321e-04 - rmse: 0.0253 - val_loss: 6.4485e-04 - val_rmse: 0.0235 - lr: 1.0000e-04\n",
      "Epoch 428/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4320e-04 - rmse: 0.0255\n",
      "Epoch 428: val_loss improved from 0.00064 to 0.00063, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 7.7888e-04 - rmse: 0.0262 - val_loss: 6.2761e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 429/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7267e-04 - rmse: 0.0220\n",
      "Epoch 429: val_loss improved from 0.00063 to 0.00062, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 7.3892e-04 - rmse: 0.0255 - val_loss: 6.1853e-04 - val_rmse: 0.0230 - lr: 1.0000e-04\n",
      "Epoch 430/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3158e-04 - rmse: 0.0272\n",
      "Epoch 430: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.9181e-04 - rmse: 0.0265 - val_loss: 6.4628e-04 - val_rmse: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 431/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8161e-04 - rmse: 0.0263\n",
      "Epoch 431: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.8940e-04 - rmse: 0.0264 - val_loss: 6.5189e-04 - val_rmse: 0.0237 - lr: 1.0000e-04\n",
      "Epoch 432/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2645e-04 - rmse: 0.0252\n",
      "Epoch 432: val_loss improved from 0.00062 to 0.00060, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 6.5484e-04 - rmse: 0.0238 - val_loss: 5.9921e-04 - val_rmse: 0.0226 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5208e-04 - rmse: 0.0257\n",
      "Epoch 433: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2589e-04 - rmse: 0.0252 - val_loss: 6.7146e-04 - val_rmse: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 434/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6478e-04 - rmse: 0.0240\n",
      "Epoch 434: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.1823e-04 - rmse: 0.0251 - val_loss: 6.9739e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 435/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9689e-04 - rmse: 0.0246\n",
      "Epoch 435: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.0568e-04 - rmse: 0.0267 - val_loss: 6.5744e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 436/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5758e-04 - rmse: 0.0258\n",
      "Epoch 436: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.6495e-04 - rmse: 0.0240 - val_loss: 6.1512e-04 - val_rmse: 0.0229 - lr: 1.0000e-04\n",
      "Epoch 437/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9162e-04 - rmse: 0.0265\n",
      "Epoch 437: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2688e-04 - rmse: 0.0252 - val_loss: 6.4016e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 438/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4950e-04 - rmse: 0.0236\n",
      "Epoch 438: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.7248e-04 - rmse: 0.0241 - val_loss: 6.1300e-04 - val_rmse: 0.0229 - lr: 1.0000e-04\n",
      "Epoch 439/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9774e-04 - rmse: 0.0246\n",
      "Epoch 439: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.6626e-04 - rmse: 0.0240 - val_loss: 6.3517e-04 - val_rmse: 0.0233 - lr: 1.0000e-04\n",
      "Epoch 440/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.3741e-04 - rmse: 0.0234\n",
      "Epoch 440: val_loss did not improve from 0.00060\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.8828e-04 - rmse: 0.0245 - val_loss: 6.0972e-04 - val_rmse: 0.0228 - lr: 1.0000e-04\n",
      "Epoch 441/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3916e-04 - rmse: 0.0274\n",
      "Epoch 441: val_loss improved from 0.00060 to 0.00059, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 7.0877e-04 - rmse: 0.0249 - val_loss: 5.9191e-04 - val_rmse: 0.0224 - lr: 1.0000e-04\n",
      "Epoch 442/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.5495e-04 - rmse: 0.0238\n",
      "Epoch 442: val_loss improved from 0.00059 to 0.00059, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 6.5767e-04 - rmse: 0.0238 - val_loss: 5.9038e-04 - val_rmse: 0.0224 - lr: 1.0000e-04\n",
      "Epoch 443/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1205e-04 - rmse: 0.0249\n",
      "Epoch 443: val_loss improved from 0.00059 to 0.00059, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 6.8901e-04 - rmse: 0.0245 - val_loss: 5.8939e-04 - val_rmse: 0.0223 - lr: 1.0000e-04\n",
      "Epoch 444/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1231e-04 - rmse: 0.0249\n",
      "Epoch 444: val_loss did not improve from 0.00059\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.7241e-04 - rmse: 0.0241 - val_loss: 6.4216e-04 - val_rmse: 0.0235 - lr: 1.0000e-04\n",
      "Epoch 445/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7981e-04 - rmse: 0.0243\n",
      "Epoch 445: val_loss improved from 0.00059 to 0.00054, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 6.8958e-04 - rmse: 0.0245 - val_loss: 5.4283e-04 - val_rmse: 0.0213 - lr: 1.0000e-04\n",
      "Epoch 446/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5784e-04 - rmse: 0.0216\n",
      "Epoch 446: val_loss did not improve from 0.00054\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.0844e-04 - rmse: 0.0228 - val_loss: 5.9844e-04 - val_rmse: 0.0225 - lr: 1.0000e-04\n",
      "Epoch 447/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.3561e-04 - rmse: 0.0234\n",
      "Epoch 447: val_loss improved from 0.00054 to 0.00054, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 6.1698e-04 - rmse: 0.0230 - val_loss: 5.3808e-04 - val_rmse: 0.0212 - lr: 1.0000e-04\n",
      "Epoch 448/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1615e-04 - rmse: 0.0206\n",
      "Epoch 448: val_loss improved from 0.00054 to 0.00053, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 6.4872e-04 - rmse: 0.0236 - val_loss: 5.2892e-04 - val_rmse: 0.0210 - lr: 1.0000e-04\n",
      "Epoch 449/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6077e-04 - rmse: 0.0193\n",
      "Epoch 449: val_loss did not improve from 0.00053\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.9085e-04 - rmse: 0.0224 - val_loss: 5.2946e-04 - val_rmse: 0.0210 - lr: 1.0000e-04\n",
      "Epoch 450/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5887e-04 - rmse: 0.0259\n",
      "Epoch 450: val_loss did not improve from 0.00053\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.2645e-04 - rmse: 0.0232 - val_loss: 5.4515e-04 - val_rmse: 0.0213 - lr: 1.0000e-04\n",
      "Epoch 451/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.5147e-04 - rmse: 0.0237\n",
      "Epoch 451: val_loss did not improve from 0.00053\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.5422e-04 - rmse: 0.0238 - val_loss: 5.5951e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 452/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9425e-04 - rmse: 0.0225\n",
      "Epoch 452: val_loss improved from 0.00053 to 0.00052, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 6.0758e-04 - rmse: 0.0228 - val_loss: 5.1724e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 453/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4769e-04 - rmse: 0.0214\n",
      "Epoch 453: val_loss did not improve from 0.00052\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.3903e-04 - rmse: 0.0234 - val_loss: 5.3127e-04 - val_rmse: 0.0210 - lr: 1.0000e-04\n",
      "Epoch 454/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9088e-04 - rmse: 0.0174\n",
      "Epoch 454: val_loss did not improve from 0.00052\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.1295e-04 - rmse: 0.0229 - val_loss: 5.2155e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 455/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8448e-04 - rmse: 0.0199\n",
      "Epoch 455: val_loss improved from 0.00052 to 0.00052, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 4.8656e-04 - rmse: 0.0199 - val_loss: 5.1669e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 456/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4509e-04 - rmse: 0.0236\n",
      "Epoch 456: val_loss did not improve from 0.00052\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.9786e-04 - rmse: 0.0225 - val_loss: 5.4461e-04 - val_rmse: 0.0213 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2298e-04 - rmse: 0.0231\n",
      "Epoch 457: val_loss did not improve from 0.00052\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.8074e-04 - rmse: 0.0222 - val_loss: 5.2671e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 458/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4028e-04 - rmse: 0.0255\n",
      "Epoch 458: val_loss improved from 0.00052 to 0.00051, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 6.2622e-04 - rmse: 0.0232 - val_loss: 5.0503e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 459/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5563e-04 - rmse: 0.0191\n",
      "Epoch 459: val_loss improved from 0.00051 to 0.00050, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 5.2984e-04 - rmse: 0.0210 - val_loss: 5.0366e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 460/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1787e-04 - rmse: 0.0230\n",
      "Epoch 460: val_loss did not improve from 0.00050\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.7035e-04 - rmse: 0.0219 - val_loss: 5.1411e-04 - val_rmse: 0.0206 - lr: 1.0000e-04\n",
      "Epoch 461/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4170e-04 - rmse: 0.0235\n",
      "Epoch 461: val_loss improved from 0.00050 to 0.00049, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 5.6711e-04 - rmse: 0.0219 - val_loss: 4.8732e-04 - val_rmse: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 462/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6058e-04 - rmse: 0.0193\n",
      "Epoch 462: val_loss did not improve from 0.00049\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.1929e-04 - rmse: 0.0207 - val_loss: 5.0583e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 463/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9230e-04 - rmse: 0.0201\n",
      "Epoch 463: val_loss improved from 0.00049 to 0.00047, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 5.5867e-04 - rmse: 0.0217 - val_loss: 4.7389e-04 - val_rmse: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 464/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6897e-04 - rmse: 0.0219\n",
      "Epoch 464: val_loss improved from 0.00047 to 0.00047, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 5.0763e-04 - rmse: 0.0205 - val_loss: 4.6963e-04 - val_rmse: 0.0195 - lr: 1.0000e-04\n",
      "Epoch 465/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4494e-04 - rmse: 0.0189\n",
      "Epoch 465: val_loss did not improve from 0.00047\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.1009e-04 - rmse: 0.0205 - val_loss: 4.9282e-04 - val_rmse: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 466/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8653e-04 - rmse: 0.0244\n",
      "Epoch 466: val_loss improved from 0.00047 to 0.00047, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 5.8615e-04 - rmse: 0.0223 - val_loss: 4.6896e-04 - val_rmse: 0.0195 - lr: 1.0000e-04\n",
      "Epoch 467/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6243e-04 - rmse: 0.0218\n",
      "Epoch 467: val_loss did not improve from 0.00047\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.2432e-04 - rmse: 0.0209 - val_loss: 4.8887e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 468/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1267e-04 - rmse: 0.0206\n",
      "Epoch 468: val_loss did not improve from 0.00047\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.3400e-04 - rmse: 0.0211 - val_loss: 5.2666e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 469/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7430e-04 - rmse: 0.0196\n",
      "Epoch 469: val_loss did not improve from 0.00047\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.4074e-04 - rmse: 0.0212 - val_loss: 4.7658e-04 - val_rmse: 0.0197 - lr: 1.0000e-04\n",
      "Epoch 470/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6465e-04 - rmse: 0.0218\n",
      "Epoch 470: val_loss improved from 0.00047 to 0.00045, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 5.7586e-04 - rmse: 0.0221 - val_loss: 4.4799e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 471/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9153e-04 - rmse: 0.0201\n",
      "Epoch 471: val_loss did not improve from 0.00045\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.8523e-04 - rmse: 0.0199 - val_loss: 5.6037e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 472/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5115e-04 - rmse: 0.0257\n",
      "Epoch 472: val_loss did not improve from 0.00045\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.8932e-04 - rmse: 0.0224 - val_loss: 4.5396e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 473/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5109e-04 - rmse: 0.0190\n",
      "Epoch 473: val_loss improved from 0.00045 to 0.00045, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 5.5362e-04 - rmse: 0.0216 - val_loss: 4.4722e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 474/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3412e-04 - rmse: 0.0211\n",
      "Epoch 474: val_loss did not improve from 0.00045\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.2424e-04 - rmse: 0.0209 - val_loss: 5.7975e-04 - val_rmse: 0.0222 - lr: 1.0000e-04\n",
      "Epoch 475/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.3942e-04 - rmse: 0.0235\n",
      "Epoch 475: val_loss did not improve from 0.00045\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.1124e-04 - rmse: 0.0229 - val_loss: 4.7346e-04 - val_rmse: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 476/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5624e-04 - rmse: 0.0192\n",
      "Epoch 476: val_loss improved from 0.00045 to 0.00043, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 5.0220e-04 - rmse: 0.0203 - val_loss: 4.3256e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 477/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9160e-04 - rmse: 0.0174\n",
      "Epoch 477: val_loss did not improve from 0.00043\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.7008e-04 - rmse: 0.0195 - val_loss: 4.5521e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 478/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7589e-04 - rmse: 0.0242\n",
      "Epoch 478: val_loss did not improve from 0.00043\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.4925e-04 - rmse: 0.0215 - val_loss: 4.8699e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 479/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3248e-04 - rmse: 0.0211\n",
      "Epoch 479: val_loss did not improve from 0.00043\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.0511e-04 - rmse: 0.0204 - val_loss: 5.0149e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1332e-04 - rmse: 0.0180\n",
      "Epoch 480: val_loss did not improve from 0.00043\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.5636e-04 - rmse: 0.0192 - val_loss: 4.5041e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 481/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6615e-04 - rmse: 0.0194\n",
      "Epoch 481: val_loss improved from 0.00043 to 0.00042, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 4.9942e-04 - rmse: 0.0203 - val_loss: 4.2173e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 482/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0863e-04 - rmse: 0.0205\n",
      "Epoch 482: val_loss did not improve from 0.00042\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.8875e-04 - rmse: 0.0200 - val_loss: 4.3688e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 483/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0627e-04 - rmse: 0.0204\n",
      "Epoch 483: val_loss improved from 0.00042 to 0.00040, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.6944e-04 - rmse: 0.0195 - val_loss: 4.0461e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 484/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7199e-04 - rmse: 0.0196\n",
      "Epoch 484: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.3097e-04 - rmse: 0.0185 - val_loss: 4.8057e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 485/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5578e-04 - rmse: 0.0192\n",
      "Epoch 485: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.9537e-04 - rmse: 0.0202 - val_loss: 4.5385e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 486/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0051e-04 - rmse: 0.0177\n",
      "Epoch 486: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.6143e-04 - rmse: 0.0193 - val_loss: 4.2057e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 487/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0555e-04 - rmse: 0.0178\n",
      "Epoch 487: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.0229e-04 - rmse: 0.0177 - val_loss: 4.0692e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 488/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1177e-04 - rmse: 0.0206\n",
      "Epoch 488: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.6026e-04 - rmse: 0.0193 - val_loss: 4.0951e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 489/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8133e-04 - rmse: 0.0171\n",
      "Epoch 489: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.3784e-04 - rmse: 0.0187 - val_loss: 4.1478e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 490/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4946e-04 - rmse: 0.0190\n",
      "Epoch 490: val_loss improved from 0.00040 to 0.00040, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 4.1255e-04 - rmse: 0.0180 - val_loss: 4.0442e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 491/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9492e-04 - rmse: 0.0175\n",
      "Epoch 491: val_loss improved from 0.00040 to 0.00038, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.1593e-04 - rmse: 0.0181 - val_loss: 3.8038e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 492/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3365e-04 - rmse: 0.0186\n",
      "Epoch 492: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.4593e-04 - rmse: 0.0189 - val_loss: 3.8868e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 493/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3925e-04 - rmse: 0.0187\n",
      "Epoch 493: val_loss improved from 0.00038 to 0.00038, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 4.2357e-04 - rmse: 0.0183 - val_loss: 3.7571e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 494/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2267e-04 - rmse: 0.0153\n",
      "Epoch 494: val_loss improved from 0.00038 to 0.00037, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 3.9641e-04 - rmse: 0.0176 - val_loss: 3.7410e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 495/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2389e-04 - rmse: 0.0183\n",
      "Epoch 495: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.1113e-04 - rmse: 0.0180 - val_loss: 3.8039e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 496/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4710e-04 - rmse: 0.0161\n",
      "Epoch 496: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.8802e-04 - rmse: 0.0173 - val_loss: 3.7614e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 497/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7089e-04 - rmse: 0.0168\n",
      "Epoch 497: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.2279e-04 - rmse: 0.0183 - val_loss: 3.8414e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 498/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8063e-04 - rmse: 0.0139\n",
      "Epoch 498: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6597e-04 - rmse: 0.0167 - val_loss: 3.8665e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 499/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4826e-04 - rmse: 0.0190\n",
      "Epoch 499: val_loss improved from 0.00037 to 0.00037, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 4.2717e-04 - rmse: 0.0184 - val_loss: 3.6543e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 500/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9918e-04 - rmse: 0.0176\n",
      "Epoch 500: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6779e-04 - rmse: 0.0167 - val_loss: 3.6960e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 501/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9087e-04 - rmse: 0.0174\n",
      "Epoch 501: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.0401e-04 - rmse: 0.0178 - val_loss: 3.6632e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 502/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3887e-04 - rmse: 0.0158\n",
      "Epoch 502: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.6308e-04 - rmse: 0.0166 - val_loss: 3.6665e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 503/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9029e-04 - rmse: 0.0201\n",
      "Epoch 503: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.5891e-04 - rmse: 0.0193 - val_loss: 3.8625e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 504/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2775e-04 - rmse: 0.0184\n",
      "Epoch 504: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.0028e-04 - rmse: 0.0177 - val_loss: 3.8898e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 505/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5235e-04 - rmse: 0.0191\n",
      "Epoch 505: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.0129e-04 - rmse: 0.0177 - val_loss: 4.2721e-04 - val_rmse: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 506/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3990e-04 - rmse: 0.0213\n",
      "Epoch 506: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.4428e-04 - rmse: 0.0189 - val_loss: 3.7730e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 507/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6431e-04 - rmse: 0.0166\n",
      "Epoch 507: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 3.9213e-04 - rmse: 0.0174 - val_loss: 3.9331e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 508/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6027e-04 - rmse: 0.0193\n",
      "Epoch 508: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.4401e-04 - rmse: 0.0189 - val_loss: 4.8861e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 509/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4168e-04 - rmse: 0.0188\n",
      "Epoch 509: val_loss improved from 0.00037 to 0.00036, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 202ms/step - loss: 4.1638e-04 - rmse: 0.0181 - val_loss: 3.6160e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 510/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7868e-04 - rmse: 0.0171\n",
      "Epoch 510: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9755e-04 - rmse: 0.0176 - val_loss: 3.8690e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 511/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4962e-04 - rmse: 0.0190\n",
      "Epoch 511: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.1000e-04 - rmse: 0.0180 - val_loss: 3.7501e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 512/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1605e-04 - rmse: 0.0181\n",
      "Epoch 512: val_loss improved from 0.00036 to 0.00036, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.0864e-04 - rmse: 0.0179 - val_loss: 3.6073e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 513/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2609e-04 - rmse: 0.0184\n",
      "Epoch 513: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.6363e-04 - rmse: 0.0166 - val_loss: 3.8970e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 514/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0134e-04 - rmse: 0.0203\n",
      "Epoch 514: val_loss improved from 0.00036 to 0.00034, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 4.4276e-04 - rmse: 0.0188 - val_loss: 3.3980e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 515/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0369e-04 - rmse: 0.0147\n",
      "Epoch 515: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.2079e-04 - rmse: 0.0153 - val_loss: 3.8046e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 516/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8954e-04 - rmse: 0.0174\n",
      "Epoch 516: val_loss improved from 0.00034 to 0.00033, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 3.7842e-04 - rmse: 0.0171 - val_loss: 3.2928e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 517/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3377e-04 - rmse: 0.0157\n",
      "Epoch 517: val_loss improved from 0.00033 to 0.00033, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 3.6647e-04 - rmse: 0.0167 - val_loss: 3.2629e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 518/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6971e-04 - rmse: 0.0135\n",
      "Epoch 518: val_loss did not improve from 0.00033\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6603e-04 - rmse: 0.0167 - val_loss: 3.3133e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 519/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8281e-04 - rmse: 0.0140\n",
      "Epoch 519: val_loss did not improve from 0.00033\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.1983e-04 - rmse: 0.0152 - val_loss: 3.3230e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 520/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7418e-04 - rmse: 0.0137\n",
      "Epoch 520: val_loss improved from 0.00033 to 0.00032, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.4971e-04 - rmse: 0.0162 - val_loss: 3.1738e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 521/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1197e-04 - rmse: 0.0150\n",
      "Epoch 521: val_loss did not improve from 0.00032\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.0516e-04 - rmse: 0.0148 - val_loss: 3.1973e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 522/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9926e-04 - rmse: 0.0203\n",
      "Epoch 522: val_loss did not improve from 0.00032\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5845e-04 - rmse: 0.0165 - val_loss: 3.3588e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 523/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1351e-04 - rmse: 0.0181\n",
      "Epoch 523: val_loss did not improve from 0.00032\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.6998e-04 - rmse: 0.0168 - val_loss: 3.2439e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 524/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3790e-04 - rmse: 0.0158\n",
      "Epoch 524: val_loss did not improve from 0.00032\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.2802e-04 - rmse: 0.0155 - val_loss: 3.1895e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 525/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4326e-04 - rmse: 0.0189\n",
      "Epoch 525: val_loss did not improve from 0.00032\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.3620e-04 - rmse: 0.0158 - val_loss: 3.2830e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 526/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1381e-04 - rmse: 0.0151\n",
      "Epoch 526: val_loss improved from 0.00032 to 0.00031, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.4006e-04 - rmse: 0.0159 - val_loss: 3.1394e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 527/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1575e-04 - rmse: 0.0151\n",
      "Epoch 527: val_loss improved from 0.00031 to 0.00031, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.1269e-04 - rmse: 0.0150 - val_loss: 3.0980e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 528/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3909e-04 - rmse: 0.0159\n",
      "Epoch 528: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.3630e-04 - rmse: 0.0158 - val_loss: 3.1909e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 529/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9612e-04 - rmse: 0.0145\n",
      "Epoch 529: val_loss improved from 0.00031 to 0.00030, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 3.1430e-04 - rmse: 0.0151 - val_loss: 2.9860e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 530/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0983e-04 - rmse: 0.0149\n",
      "Epoch 530: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.4326e-04 - rmse: 0.0160 - val_loss: 3.0273e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 531/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8309e-04 - rmse: 0.0140\n",
      "Epoch 531: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9428e-04 - rmse: 0.0144 - val_loss: 2.9927e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 532/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1167e-04 - rmse: 0.0150\n",
      "Epoch 532: val_loss improved from 0.00030 to 0.00030, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 2.9882e-04 - rmse: 0.0146 - val_loss: 2.9674e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 533/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7135e-04 - rmse: 0.0136\n",
      "Epoch 533: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.0931e-04 - rmse: 0.0149 - val_loss: 3.0355e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 534/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1659e-04 - rmse: 0.0152\n",
      "Epoch 534: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9657e-04 - rmse: 0.0145 - val_loss: 3.1505e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 535/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3768e-04 - rmse: 0.0158\n",
      "Epoch 535: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.4052e-04 - rmse: 0.0159 - val_loss: 3.2057e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 536/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1661e-04 - rmse: 0.0152\n",
      "Epoch 536: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.8624e-04 - rmse: 0.0173 - val_loss: 3.2903e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 537/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8793e-04 - rmse: 0.0174\n",
      "Epoch 537: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.5482e-04 - rmse: 0.0164 - val_loss: 4.8438e-04 - val_rmse: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 538/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7821e-04 - rmse: 0.0222\n",
      "Epoch 538: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.4140e-04 - rmse: 0.0188 - val_loss: 3.2346e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 539/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4947e-04 - rmse: 0.0162\n",
      "Epoch 539: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.7179e-04 - rmse: 0.0169 - val_loss: 3.7777e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 540/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4201e-04 - rmse: 0.0189\n",
      "Epoch 540: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.3974e-04 - rmse: 0.0159 - val_loss: 3.4092e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 541/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7958e-04 - rmse: 0.0171\n",
      "Epoch 541: val_loss improved from 0.00030 to 0.00029, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 3.1054e-04 - rmse: 0.0150 - val_loss: 2.8584e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 542/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6567e-04 - rmse: 0.0167\n",
      "Epoch 542: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.2395e-04 - rmse: 0.0154 - val_loss: 2.8719e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 543/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0188e-04 - rmse: 0.0147\n",
      "Epoch 543: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.9571e-04 - rmse: 0.0145 - val_loss: 3.1055e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 544/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0429e-04 - rmse: 0.0148\n",
      "Epoch 544: val_loss improved from 0.00029 to 0.00028, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 3.0650e-04 - rmse: 0.0148 - val_loss: 2.8163e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 545/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0088e-04 - rmse: 0.0146\n",
      "Epoch 545: val_loss improved from 0.00028 to 0.00027, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 3.0009e-04 - rmse: 0.0146 - val_loss: 2.6885e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 546/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5417e-04 - rmse: 0.0130\n",
      "Epoch 546: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.7616e-04 - rmse: 0.0138 - val_loss: 2.8029e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 547/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7874e-04 - rmse: 0.0139\n",
      "Epoch 547: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.1368e-04 - rmse: 0.0151 - val_loss: 2.8148e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 548/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2340e-04 - rmse: 0.0117\n",
      "Epoch 548: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.7516e-04 - rmse: 0.0137 - val_loss: 2.7007e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 549/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5261e-04 - rmse: 0.0129\n",
      "Epoch 549: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.1833e-04 - rmse: 0.0152 - val_loss: 3.2433e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 550/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7812e-04 - rmse: 0.0139\n",
      "Epoch 550: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.9329e-04 - rmse: 0.0144 - val_loss: 2.7805e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 551/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9389e-04 - rmse: 0.0144\n",
      "Epoch 551: val_loss improved from 0.00027 to 0.00027, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 3.0277e-04 - rmse: 0.0147 - val_loss: 2.6527e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 552/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6314e-04 - rmse: 0.0133\n",
      "Epoch 552: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.7502e-04 - rmse: 0.0137 - val_loss: 2.8784e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 553/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9789e-04 - rmse: 0.0146\n",
      "Epoch 553: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9305e-04 - rmse: 0.0144 - val_loss: 2.8316e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 554/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5498e-04 - rmse: 0.0130\n",
      "Epoch 554: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9063e-04 - rmse: 0.0143 - val_loss: 2.8400e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 555/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3821e-04 - rmse: 0.0123\n",
      "Epoch 555: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.7527e-04 - rmse: 0.0138 - val_loss: 2.8432e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 556/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4746e-04 - rmse: 0.0127\n",
      "Epoch 556: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.0604e-04 - rmse: 0.0148 - val_loss: 2.9296e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 557/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1585e-04 - rmse: 0.0152\n",
      "Epoch 557: val_loss improved from 0.00027 to 0.00026, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 2.9373e-04 - rmse: 0.0144 - val_loss: 2.5780e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 558/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6101e-04 - rmse: 0.0132\n",
      "Epoch 558: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7773e-04 - rmse: 0.0139 - val_loss: 3.0292e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 559/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8858e-04 - rmse: 0.0142\n",
      "Epoch 559: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.1882e-04 - rmse: 0.0153 - val_loss: 3.0037e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 560/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8846e-04 - rmse: 0.0142\n",
      "Epoch 560: val_loss improved from 0.00026 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 2.9981e-04 - rmse: 0.0146 - val_loss: 2.5405e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 561/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8076e-04 - rmse: 0.0140\n",
      "Epoch 561: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9926e-04 - rmse: 0.0146 - val_loss: 4.3642e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 562/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8442e-04 - rmse: 0.0173\n",
      "Epoch 562: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.0832e-04 - rmse: 0.0149 - val_loss: 3.0090e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 563/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3355e-04 - rmse: 0.0157\n",
      "Epoch 563: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.1909e-04 - rmse: 0.0153 - val_loss: 2.7265e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 564/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5653e-04 - rmse: 0.0165\n",
      "Epoch 564: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.2715e-04 - rmse: 0.0155 - val_loss: 3.1102e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 565/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7368e-04 - rmse: 0.0137\n",
      "Epoch 565: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.3868e-04 - rmse: 0.0159 - val_loss: 2.7689e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 566/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6118e-04 - rmse: 0.0133\n",
      "Epoch 566: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.2046e-04 - rmse: 0.0153 - val_loss: 3.0861e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 567/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4756e-04 - rmse: 0.0162\n",
      "Epoch 567: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.2455e-04 - rmse: 0.0155 - val_loss: 3.7606e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 568/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6179e-04 - rmse: 0.0166\n",
      "Epoch 568: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.1177e-04 - rmse: 0.0150 - val_loss: 2.7532e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 569/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2737e-04 - rmse: 0.0119\n",
      "Epoch 569: val_loss improved from 0.00025 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 2.6195e-04 - rmse: 0.0133 - val_loss: 2.4776e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 570/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0262e-04 - rmse: 0.0147\n",
      "Epoch 570: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9799e-04 - rmse: 0.0146 - val_loss: 3.5359e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 571/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.8804e-04 - rmse: 0.0174\n",
      "Epoch 571: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.3275e-04 - rmse: 0.0157 - val_loss: 2.4816e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 572/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6293e-04 - rmse: 0.0133\n",
      "Epoch 572: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5792e-04 - rmse: 0.0131 - val_loss: 2.6499e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 573/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7873e-04 - rmse: 0.0139\n",
      "Epoch 573: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7142e-04 - rmse: 0.0136 - val_loss: 2.8898e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 574/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4797e-04 - rmse: 0.0128\n",
      "Epoch 574: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.4519e-04 - rmse: 0.0126 - val_loss: 2.5074e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 575/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6157e-04 - rmse: 0.0133\n",
      "Epoch 575: val_loss improved from 0.00025 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 2.8748e-04 - rmse: 0.0142 - val_loss: 2.4713e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 576/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7469e-04 - rmse: 0.0138\n",
      "Epoch 576: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.5643e-04 - rmse: 0.0131 - val_loss: 2.7966e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 577/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5168e-04 - rmse: 0.0129\n",
      "Epoch 577: val_loss improved from 0.00025 to 0.00024, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 2.6688e-04 - rmse: 0.0135 - val_loss: 2.4305e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 578/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5556e-04 - rmse: 0.0131\n",
      "Epoch 578: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.7459e-04 - rmse: 0.0138 - val_loss: 2.5171e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 579/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2335e-04 - rmse: 0.0118\n",
      "Epoch 579: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5485e-04 - rmse: 0.0130 - val_loss: 2.6542e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 580/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4594e-04 - rmse: 0.0127\n",
      "Epoch 580: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3560e-04 - rmse: 0.0123 - val_loss: 2.5318e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 581/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2922e-04 - rmse: 0.0120\n",
      "Epoch 581: val_loss improved from 0.00024 to 0.00023, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 2.6468e-04 - rmse: 0.0134 - val_loss: 2.2976e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 582/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7872e-04 - rmse: 0.0097\n",
      "Epoch 582: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2638e-04 - rmse: 0.0119 - val_loss: 2.3966e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 583/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6332e-04 - rmse: 0.0134\n",
      "Epoch 583: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.5286e-04 - rmse: 0.0130 - val_loss: 2.3827e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 584/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2631e-04 - rmse: 0.0119\n",
      "Epoch 584: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3724e-04 - rmse: 0.0123 - val_loss: 2.4299e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 585/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2935e-04 - rmse: 0.0120\n",
      "Epoch 585: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2910e-04 - rmse: 0.0120 - val_loss: 2.3092e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 586/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5753e-04 - rmse: 0.0131\n",
      "Epoch 586: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3777e-04 - rmse: 0.0124 - val_loss: 2.3245e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 587/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3019e-04 - rmse: 0.0121\n",
      "Epoch 587: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.1690e-04 - rmse: 0.0115 - val_loss: 2.4464e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 588/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7331e-04 - rmse: 0.0137\n",
      "Epoch 588: val_loss improved from 0.00023 to 0.00022, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 2.4837e-04 - rmse: 0.0128 - val_loss: 2.1768e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 589/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4663e-04 - rmse: 0.0127\n",
      "Epoch 589: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3111e-04 - rmse: 0.0121 - val_loss: 2.2288e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 590/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8802e-04 - rmse: 0.0102\n",
      "Epoch 590: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0656e-04 - rmse: 0.0110 - val_loss: 2.4275e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 591/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5800e-04 - rmse: 0.0132\n",
      "Epoch 591: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5704e-04 - rmse: 0.0131 - val_loss: 2.2835e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 592/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2872e-04 - rmse: 0.0120\n",
      "Epoch 592: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.6766e-04 - rmse: 0.0135 - val_loss: 2.3971e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 593/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4432e-04 - rmse: 0.0126\n",
      "Epoch 593: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.6193e-04 - rmse: 0.0133 - val_loss: 2.2778e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 594/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9356e-04 - rmse: 0.0104\n",
      "Epoch 594: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3145e-04 - rmse: 0.0121 - val_loss: 2.2902e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 595/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2148e-04 - rmse: 0.0117\n",
      "Epoch 595: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.0212e-04 - rmse: 0.0108 - val_loss: 2.1825e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 596/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2118e-04 - rmse: 0.0117\n",
      "Epoch 596: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4576e-04 - rmse: 0.0127 - val_loss: 2.2690e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 597/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0047e-04 - rmse: 0.0108\n",
      "Epoch 597: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.2487e-04 - rmse: 0.0119 - val_loss: 2.3339e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 598/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5433e-04 - rmse: 0.0130\n",
      "Epoch 598: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5317e-04 - rmse: 0.0130 - val_loss: 2.2941e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 599/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1481e-04 - rmse: 0.0114\n",
      "Epoch 599: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2346e-04 - rmse: 0.0118 - val_loss: 2.2967e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 600/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3112e-04 - rmse: 0.0121\n",
      "Epoch 600: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1657e-04 - rmse: 0.0115 - val_loss: 2.3304e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 601/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3486e-04 - rmse: 0.0123\n",
      "Epoch 601: val_loss improved from 0.00022 to 0.00022, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 2.3319e-04 - rmse: 0.0122 - val_loss: 2.1548e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 602/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7456e-04 - rmse: 0.0095\n",
      "Epoch 602: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2218e-04 - rmse: 0.0117 - val_loss: 2.2392e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 603/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4873e-04 - rmse: 0.0128\n",
      "Epoch 603: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3357e-04 - rmse: 0.0122 - val_loss: 2.2819e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 604/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2438e-04 - rmse: 0.0118\n",
      "Epoch 604: val_loss did not improve from 0.00022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3985e-04 - rmse: 0.0125 - val_loss: 2.3010e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 605/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0629e-04 - rmse: 0.0111\n",
      "Epoch 605: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3802e-04 - rmse: 0.0124 - val_loss: 2.3214e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 606/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2731e-04 - rmse: 0.0120\n",
      "Epoch 606: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.2570e-04 - rmse: 0.0119 - val_loss: 2.1763e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 607/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7260e-04 - rmse: 0.0137\n",
      "Epoch 607: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.6429e-04 - rmse: 0.0134 - val_loss: 3.2833e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 608/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5864e-04 - rmse: 0.0166\n",
      "Epoch 608: val_loss improved from 0.00022 to 0.00020, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 2.8154e-04 - rmse: 0.0141 - val_loss: 2.0350e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 609/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0646e-04 - rmse: 0.0111\n",
      "Epoch 609: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.4870e-04 - rmse: 0.0128 - val_loss: 2.5046e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 610/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4729e-04 - rmse: 0.0128\n",
      "Epoch 610: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.2146e-04 - rmse: 0.0117 - val_loss: 3.1124e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 611/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2790e-04 - rmse: 0.0156\n",
      "Epoch 611: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.6608e-04 - rmse: 0.0135 - val_loss: 2.3304e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 612/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5857e-04 - rmse: 0.0086\n",
      "Epoch 612: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.6385e-04 - rmse: 0.0134 - val_loss: 2.6909e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 613/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1878e-04 - rmse: 0.0153\n",
      "Epoch 613: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.6202e-04 - rmse: 0.0133 - val_loss: 2.4998e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 614/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5929e-04 - rmse: 0.0132\n",
      "Epoch 614: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7727e-04 - rmse: 0.0139 - val_loss: 2.5160e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 615/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4157e-04 - rmse: 0.0126\n",
      "Epoch 615: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.4153e-04 - rmse: 0.0126 - val_loss: 2.6251e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 616/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5886e-04 - rmse: 0.0132\n",
      "Epoch 616: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5489e-04 - rmse: 0.0131 - val_loss: 2.1213e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 617/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2714e-04 - rmse: 0.0120\n",
      "Epoch 617: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.5108e-04 - rmse: 0.0129 - val_loss: 2.3581e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 618/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0079e-04 - rmse: 0.0108\n",
      "Epoch 618: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1806e-04 - rmse: 0.0116 - val_loss: 2.0358e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 619/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0778e-04 - rmse: 0.0111\n",
      "Epoch 619: val_loss improved from 0.00020 to 0.00019, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 2.0488e-04 - rmse: 0.0110 - val_loss: 1.9230e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 620/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9850e-04 - rmse: 0.0107\n",
      "Epoch 620: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1225e-04 - rmse: 0.0113 - val_loss: 1.9872e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 621/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9423e-04 - rmse: 0.0105\n",
      "Epoch 621: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0241e-04 - rmse: 0.0109 - val_loss: 2.0229e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 622/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0618e-04 - rmse: 0.0111\n",
      "Epoch 622: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0806e-04 - rmse: 0.0112 - val_loss: 2.0872e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 623/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3714e-04 - rmse: 0.0124\n",
      "Epoch 623: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2256e-04 - rmse: 0.0118 - val_loss: 2.1661e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 624/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5654e-04 - rmse: 0.0132\n",
      "Epoch 624: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1940e-04 - rmse: 0.0117 - val_loss: 2.2846e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 625/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9381e-04 - rmse: 0.0105\n",
      "Epoch 625: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.1691e-04 - rmse: 0.0116 - val_loss: 2.1529e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 626/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9782e-04 - rmse: 0.0107\n",
      "Epoch 626: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0814e-04 - rmse: 0.0112 - val_loss: 2.0475e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 627/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0203e-04 - rmse: 0.0109\n",
      "Epoch 627: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1233e-04 - rmse: 0.0114 - val_loss: 2.0510e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 628/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1865e-04 - rmse: 0.0116\n",
      "Epoch 628: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1227e-04 - rmse: 0.0114 - val_loss: 2.3568e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 629/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4309e-04 - rmse: 0.0126\n",
      "Epoch 629: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2734e-04 - rmse: 0.0120 - val_loss: 2.0285e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 630/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6753e-04 - rmse: 0.0092\n",
      "Epoch 630: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0676e-04 - rmse: 0.0111 - val_loss: 1.9832e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 631/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3937e-04 - rmse: 0.0125\n",
      "Epoch 631: val_loss improved from 0.00019 to 0.00019, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 47ms/step - loss: 2.2666e-04 - rmse: 0.0120 - val_loss: 1.8958e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 632/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1205e-04 - rmse: 0.0114\n",
      "Epoch 632: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0278e-04 - rmse: 0.0109 - val_loss: 2.4300e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 633/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5543e-04 - rmse: 0.0131\n",
      "Epoch 633: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.1091e-04 - rmse: 0.0113 - val_loss: 2.0137e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 634/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9386e-04 - rmse: 0.0105\n",
      "Epoch 634: val_loss improved from 0.00019 to 0.00018, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 1.9196e-04 - rmse: 0.0104 - val_loss: 1.8117e-04 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 635/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9345e-04 - rmse: 0.0105\n",
      "Epoch 635: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0385e-04 - rmse: 0.0110 - val_loss: 2.0470e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 636/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7162e-04 - rmse: 0.0094\n",
      "Epoch 636: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9499e-04 - rmse: 0.0106 - val_loss: 1.9422e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 637/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2530e-04 - rmse: 0.0119\n",
      "Epoch 637: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0003e-04 - rmse: 0.0108 - val_loss: 2.0335e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 638/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1376e-04 - rmse: 0.0114\n",
      "Epoch 638: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0193e-04 - rmse: 0.0109 - val_loss: 1.8305e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 639/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8826e-04 - rmse: 0.0103\n",
      "Epoch 639: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8876e-04 - rmse: 0.0103 - val_loss: 2.1486e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 640/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7688e-04 - rmse: 0.0097\n",
      "Epoch 640: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0254e-04 - rmse: 0.0109 - val_loss: 1.9067e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 641/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8114e-04 - rmse: 0.0099\n",
      "Epoch 641: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8848e-04 - rmse: 0.0103 - val_loss: 1.9139e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 642/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0663e-04 - rmse: 0.0111\n",
      "Epoch 642: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8817e-04 - rmse: 0.0103 - val_loss: 1.8491e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 643/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9694e-04 - rmse: 0.0107\n",
      "Epoch 643: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8761e-04 - rmse: 0.0102 - val_loss: 1.8546e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 644/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5295e-04 - rmse: 0.0084\n",
      "Epoch 644: val_loss improved from 0.00018 to 0.00018, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 1.8419e-04 - rmse: 0.0101 - val_loss: 1.7535e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 645/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0039e-04 - rmse: 0.0108\n",
      "Epoch 645: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8933e-04 - rmse: 0.0103 - val_loss: 1.9386e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 646/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6263e-04 - rmse: 0.0089\n",
      "Epoch 646: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8891e-04 - rmse: 0.0103 - val_loss: 1.8769e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 647/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8988e-04 - rmse: 0.0104\n",
      "Epoch 647: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9521e-04 - rmse: 0.0106 - val_loss: 1.8834e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 648/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0046e-04 - rmse: 0.0109\n",
      "Epoch 648: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9103e-04 - rmse: 0.0104 - val_loss: 2.1239e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 649/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2643e-04 - rmse: 0.0120\n",
      "Epoch 649: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0914e-04 - rmse: 0.0113 - val_loss: 2.0709e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 650/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7963e-04 - rmse: 0.0099\n",
      "Epoch 650: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0431e-04 - rmse: 0.0110 - val_loss: 1.7899e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 651/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9522e-04 - rmse: 0.0106\n",
      "Epoch 651: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9976e-04 - rmse: 0.0108 - val_loss: 1.8802e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 652/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8973e-04 - rmse: 0.0104\n",
      "Epoch 652: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.8300e-04 - rmse: 0.0100 - val_loss: 1.8393e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 653/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7680e-04 - rmse: 0.0097\n",
      "Epoch 653: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7974e-04 - rmse: 0.0099 - val_loss: 1.8545e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 654/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0487e-04 - rmse: 0.0111\n",
      "Epoch 654: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7575e-04 - rmse: 0.0097 - val_loss: 1.9798e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 655/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7885e-04 - rmse: 0.0098\n",
      "Epoch 655: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0298e-04 - rmse: 0.0110 - val_loss: 1.7859e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 656/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9817e-04 - rmse: 0.0108\n",
      "Epoch 656: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9728e-04 - rmse: 0.0107 - val_loss: 2.0212e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 657/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0766e-04 - rmse: 0.0112\n",
      "Epoch 657: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9351e-04 - rmse: 0.0105 - val_loss: 1.7886e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 658/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4660e-04 - rmse: 0.0080\n",
      "Epoch 658: val_loss did not improve from 0.00018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8473e-04 - rmse: 0.0101 - val_loss: 1.9762e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 659/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1859e-04 - rmse: 0.0117\n",
      "Epoch 659: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0371e-04 - rmse: 0.0110 - val_loss: 1.7537e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 660/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9742e-04 - rmse: 0.0107\n",
      "Epoch 660: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7776e-04 - rmse: 0.0098 - val_loss: 1.8322e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 661/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8944e-04 - rmse: 0.0104\n",
      "Epoch 661: val_loss improved from 0.00018 to 0.00017, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 1.7985e-04 - rmse: 0.0099 - val_loss: 1.6749e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 662/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6158e-04 - rmse: 0.0089\n",
      "Epoch 662: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8317e-04 - rmse: 0.0101 - val_loss: 2.0538e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 663/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1988e-04 - rmse: 0.0117\n",
      "Epoch 663: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9974e-04 - rmse: 0.0108 - val_loss: 1.7193e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 664/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9553e-04 - rmse: 0.0107\n",
      "Epoch 664: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8489e-04 - rmse: 0.0101 - val_loss: 2.0283e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 665/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9183e-04 - rmse: 0.0105\n",
      "Epoch 665: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9101e-04 - rmse: 0.0104 - val_loss: 1.7845e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 666/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6929e-04 - rmse: 0.0093\n",
      "Epoch 666: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8528e-04 - rmse: 0.0102 - val_loss: 1.6968e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 667/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7149e-04 - rmse: 0.0095\n",
      "Epoch 667: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7904e-04 - rmse: 0.0099 - val_loss: 1.7104e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 668/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7425e-04 - rmse: 0.0096\n",
      "Epoch 668: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6650e-04 - rmse: 0.0092 - val_loss: 1.7066e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 669/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6304e-04 - rmse: 0.0090\n",
      "Epoch 669: val_loss improved from 0.00017 to 0.00016, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 1.8418e-04 - rmse: 0.0101 - val_loss: 1.6334e-04 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 670/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3736e-04 - rmse: 0.0075\n",
      "Epoch 670: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8327e-04 - rmse: 0.0101 - val_loss: 1.9395e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 671/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2050e-04 - rmse: 0.0118\n",
      "Epoch 671: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9047e-04 - rmse: 0.0104 - val_loss: 1.7242e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 672/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5946e-04 - rmse: 0.0088\n",
      "Epoch 672: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7887e-04 - rmse: 0.0099 - val_loss: 1.9513e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 673/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9612e-04 - rmse: 0.0107\n",
      "Epoch 673: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8658e-04 - rmse: 0.0102 - val_loss: 1.6629e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 674/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6510e-04 - rmse: 0.0091\n",
      "Epoch 674: val_loss improved from 0.00016 to 0.00016, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.6791e-04 - rmse: 0.0093 - val_loss: 1.6062e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 675/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5781e-04 - rmse: 0.0087\n",
      "Epoch 675: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6915e-04 - rmse: 0.0094 - val_loss: 1.6629e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 676/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8339e-04 - rmse: 0.0101\n",
      "Epoch 676: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7770e-04 - rmse: 0.0098 - val_loss: 1.6472e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 677/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4801e-04 - rmse: 0.0082\n",
      "Epoch 677: val_loss improved from 0.00016 to 0.00016, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.6980e-04 - rmse: 0.0094 - val_loss: 1.5748e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 678/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3704e-04 - rmse: 0.0075\n",
      "Epoch 678: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6786e-04 - rmse: 0.0093 - val_loss: 1.6164e-04 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 679/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3332e-04 - rmse: 0.0072\n",
      "Epoch 679: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5866e-04 - rmse: 0.0088 - val_loss: 1.5918e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 680/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5904e-04 - rmse: 0.0088\n",
      "Epoch 680: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7257e-04 - rmse: 0.0095 - val_loss: 1.6695e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 681/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5682e-04 - rmse: 0.0087\n",
      "Epoch 681: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6948e-04 - rmse: 0.0094 - val_loss: 1.5825e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 682/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9170e-04 - rmse: 0.0105\n",
      "Epoch 682: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7023e-04 - rmse: 0.0094 - val_loss: 1.6772e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 683/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8056e-04 - rmse: 0.0100\n",
      "Epoch 683: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7732e-04 - rmse: 0.0098 - val_loss: 1.6145e-04 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 684/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4517e-04 - rmse: 0.0080\n",
      "Epoch 684: val_loss did not improve from 0.00016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6487e-04 - rmse: 0.0091 - val_loss: 1.6064e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 685/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8640e-04 - rmse: 0.0103\n",
      "Epoch 685: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6792e-04 - rmse: 0.0093 - val_loss: 1.5868e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 686/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5291e-04 - rmse: 0.0085\n",
      "Epoch 686: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6439e-04 - rmse: 0.0091 - val_loss: 1.6450e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 687/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6730e-04 - rmse: 0.0093\n",
      "Epoch 687: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6114e-04 - rmse: 0.0089 - val_loss: 1.5826e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 688/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7624e-04 - rmse: 0.0098\n",
      "Epoch 688: val_loss improved from 0.00016 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.6842e-04 - rmse: 0.0093 - val_loss: 1.5353e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 689/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4525e-04 - rmse: 0.0080\n",
      "Epoch 689: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6703e-04 - rmse: 0.0093 - val_loss: 1.6448e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 690/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5471e-04 - rmse: 0.0086\n",
      "Epoch 690: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7256e-04 - rmse: 0.0096 - val_loss: 1.6037e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 691/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5637e-04 - rmse: 0.0087\n",
      "Epoch 691: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7333e-04 - rmse: 0.0096 - val_loss: 1.9788e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 692/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1056e-04 - rmse: 0.0114\n",
      "Epoch 692: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0390e-04 - rmse: 0.0111 - val_loss: 2.2318e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 693/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3506e-04 - rmse: 0.0124\n",
      "Epoch 693: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9751e-04 - rmse: 0.0108 - val_loss: 1.5947e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 694/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6239e-04 - rmse: 0.0090\n",
      "Epoch 694: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7330e-04 - rmse: 0.0096 - val_loss: 1.7657e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 695/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9081e-04 - rmse: 0.0105\n",
      "Epoch 695: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7997e-04 - rmse: 0.0100 - val_loss: 1.6685e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 696/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6895e-04 - rmse: 0.0094\n",
      "Epoch 696: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7029e-04 - rmse: 0.0095 - val_loss: 1.8140e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 697/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8460e-04 - rmse: 0.0102\n",
      "Epoch 697: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6903e-04 - rmse: 0.0094 - val_loss: 1.5791e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 698/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5406e-04 - rmse: 0.0086\n",
      "Epoch 698: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4829e-04 - rmse: 0.0082 - val_loss: 1.5371e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 699/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8214e-04 - rmse: 0.0101\n",
      "Epoch 699: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7143e-04 - rmse: 0.0095 - val_loss: 1.5809e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 700/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4223e-04 - rmse: 0.0078\n",
      "Epoch 700: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4756e-04 - rmse: 0.0082 - val_loss: 1.5696e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 701/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5177e-04 - rmse: 0.0084\n",
      "Epoch 701: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5904e-04 - rmse: 0.0089 - val_loss: 1.5638e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 702/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3860e-04 - rmse: 0.0076\n",
      "Epoch 702: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 1.5056e-04 - rmse: 0.0084 - val_loss: 1.4857e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 703/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4406e-04 - rmse: 0.0080\n",
      "Epoch 703: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4972e-04 - rmse: 0.0083 - val_loss: 1.5674e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 704/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4152e-04 - rmse: 0.0078\n",
      "Epoch 704: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5546e-04 - rmse: 0.0087 - val_loss: 1.5430e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 705/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5435e-04 - rmse: 0.0086\n",
      "Epoch 705: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4730e-04 - rmse: 0.0082 - val_loss: 1.5259e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 706/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7182e-04 - rmse: 0.0096\n",
      "Epoch 706: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.5820e-04 - rmse: 0.0088 - val_loss: 1.4668e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 707/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3922e-04 - rmse: 0.0077\n",
      "Epoch 707: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4168e-04 - rmse: 0.0078 - val_loss: 1.5383e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 708/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7744e-04 - rmse: 0.0098\n",
      "Epoch 708: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5821e-04 - rmse: 0.0088 - val_loss: 1.7057e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 709/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6521e-04 - rmse: 0.0092\n",
      "Epoch 709: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6733e-04 - rmse: 0.0093 - val_loss: 1.5639e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 710/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5772e-04 - rmse: 0.0088\n",
      "Epoch 710: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6563e-04 - rmse: 0.0092 - val_loss: 1.6309e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 711/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7425e-04 - rmse: 0.0097\n",
      "Epoch 711: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7314e-04 - rmse: 0.0096 - val_loss: 1.4764e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 712/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2948e-04 - rmse: 0.0070\n",
      "Epoch 712: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4870e-04 - rmse: 0.0083 - val_loss: 1.5201e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 713/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4768e-04 - rmse: 0.0082\n",
      "Epoch 713: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5572e-04 - rmse: 0.0087 - val_loss: 1.5137e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 714/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7487e-04 - rmse: 0.0097\n",
      "Epoch 714: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7058e-04 - rmse: 0.0095 - val_loss: 1.7174e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 715/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9766e-04 - rmse: 0.0108\n",
      "Epoch 715: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6805e-04 - rmse: 0.0094 - val_loss: 1.6515e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 716/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5270e-04 - rmse: 0.0085\n",
      "Epoch 716: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5171e-04 - rmse: 0.0085 - val_loss: 1.5374e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 717/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5669e-04 - rmse: 0.0088\n",
      "Epoch 717: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6283e-04 - rmse: 0.0091 - val_loss: 1.5545e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 718/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5730e-04 - rmse: 0.0088\n",
      "Epoch 718: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5757e-04 - rmse: 0.0088 - val_loss: 1.5723e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 719/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7180e-04 - rmse: 0.0096\n",
      "Epoch 719: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5525e-04 - rmse: 0.0087 - val_loss: 1.6510e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 720/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7318e-04 - rmse: 0.0097\n",
      "Epoch 720: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5805e-04 - rmse: 0.0088 - val_loss: 1.5466e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 721/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5877e-04 - rmse: 0.0089\n",
      "Epoch 721: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5917e-04 - rmse: 0.0089 - val_loss: 1.6100e-04 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 722/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4536e-04 - rmse: 0.0081\n",
      "Epoch 722: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4317e-04 - rmse: 0.0080 - val_loss: 1.5334e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 723/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6012e-04 - rmse: 0.0090\n",
      "Epoch 723: val_loss improved from 0.00015 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 1.6320e-04 - rmse: 0.0091 - val_loss: 1.4459e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 724/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6518e-04 - rmse: 0.0092\n",
      "Epoch 724: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5421e-04 - rmse: 0.0086 - val_loss: 1.6185e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 725/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4649e-04 - rmse: 0.0082\n",
      "Epoch 725: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.5885e-04 - rmse: 0.0089 - val_loss: 1.3984e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 726/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3929e-04 - rmse: 0.0077\n",
      "Epoch 726: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5428e-04 - rmse: 0.0086 - val_loss: 1.5485e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 727/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6170e-04 - rmse: 0.0091\n",
      "Epoch 727: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6441e-04 - rmse: 0.0092 - val_loss: 1.5867e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 728/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5895e-04 - rmse: 0.0089\n",
      "Epoch 728: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7333e-04 - rmse: 0.0097 - val_loss: 1.6708e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 729/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6592e-04 - rmse: 0.0093\n",
      "Epoch 729: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6310e-04 - rmse: 0.0091 - val_loss: 1.5881e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 730/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5060e-04 - rmse: 0.0084\n",
      "Epoch 730: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.5208e-04 - rmse: 0.0085 - val_loss: 1.4983e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 731/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6372e-04 - rmse: 0.0092\n",
      "Epoch 731: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5221e-04 - rmse: 0.0085 - val_loss: 1.5031e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 732/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7311e-04 - rmse: 0.0097\n",
      "Epoch 732: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5523e-04 - rmse: 0.0087 - val_loss: 1.5865e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 733/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5339e-04 - rmse: 0.0086\n",
      "Epoch 733: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5326e-04 - rmse: 0.0086 - val_loss: 1.4281e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 734/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6341e-04 - rmse: 0.0092\n",
      "Epoch 734: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5620e-04 - rmse: 0.0088 - val_loss: 1.4097e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 735/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4287e-04 - rmse: 0.0080\n",
      "Epoch 735: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3802e-04 - rmse: 0.0077 - val_loss: 1.7227e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 736/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8077e-04 - rmse: 0.0101\n",
      "Epoch 736: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6197e-04 - rmse: 0.0091 - val_loss: 1.7439e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 737/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8166e-04 - rmse: 0.0101\n",
      "Epoch 737: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7549e-04 - rmse: 0.0098 - val_loss: 1.6918e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6203e-04 - rmse: 0.0091\n",
      "Epoch 738: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6276e-04 - rmse: 0.0091 - val_loss: 1.6280e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 739/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8065e-04 - rmse: 0.0101\n",
      "Epoch 739: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5547e-04 - rmse: 0.0087 - val_loss: 1.4941e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 740/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4371e-04 - rmse: 0.0080\n",
      "Epoch 740: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4778e-04 - rmse: 0.0083 - val_loss: 1.4927e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 741/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5532e-04 - rmse: 0.0087\n",
      "Epoch 741: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4457e-04 - rmse: 0.0081 - val_loss: 1.4599e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 742/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5802e-04 - rmse: 0.0089\n",
      "Epoch 742: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5315e-04 - rmse: 0.0086 - val_loss: 1.5016e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 743/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6434e-04 - rmse: 0.0092\n",
      "Epoch 743: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4771e-04 - rmse: 0.0083 - val_loss: 1.6519e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 744/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5042e-04 - rmse: 0.0084\n",
      "Epoch 744: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5216e-04 - rmse: 0.0085 - val_loss: 1.5079e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 745/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5706e-04 - rmse: 0.0088\n",
      "Epoch 745: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6037e-04 - rmse: 0.0090 - val_loss: 1.8050e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 746/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6912e-04 - rmse: 0.0095\n",
      "Epoch 746: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5731e-04 - rmse: 0.0088 - val_loss: 1.6789e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 747/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7575e-04 - rmse: 0.0098\n",
      "Epoch 747: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6573e-04 - rmse: 0.0093 - val_loss: 1.7418e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 748/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7603e-04 - rmse: 0.0098\n",
      "Epoch 748: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9180e-04 - rmse: 0.0106 - val_loss: 1.5335e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 749/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7088e-04 - rmse: 0.0096\n",
      "Epoch 749: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6920e-04 - rmse: 0.0095 - val_loss: 1.7843e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 750/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7764e-04 - rmse: 0.0099\n",
      "Epoch 750: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6232e-04 - rmse: 0.0091 - val_loss: 1.4838e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 751/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5140e-04 - rmse: 0.0085\n",
      "Epoch 751: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5387e-04 - rmse: 0.0087 - val_loss: 1.6563e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 752/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6072e-04 - rmse: 0.0090\n",
      "Epoch 752: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5311e-04 - rmse: 0.0086 - val_loss: 1.4337e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 753/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4000e-04 - rmse: 0.0078\n",
      "Epoch 753: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4978e-04 - rmse: 0.0084 - val_loss: 1.6597e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 754/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6715e-04 - rmse: 0.0094\n",
      "Epoch 754: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.4645e-04 - rmse: 0.0082 - val_loss: 1.3634e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 755/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3765e-04 - rmse: 0.0077\n",
      "Epoch 755: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.4064e-04 - rmse: 0.0079 - val_loss: 1.3543e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 756/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3540e-04 - rmse: 0.0075\n",
      "Epoch 756: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3590e-04 - rmse: 0.0076 - val_loss: 1.4043e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 757/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3718e-04 - rmse: 0.0076\n",
      "Epoch 757: val_loss improved from 0.00014 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 1.3915e-04 - rmse: 0.0078 - val_loss: 1.3317e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 758/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2767e-04 - rmse: 0.0070\n",
      "Epoch 758: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3259e-04 - rmse: 0.0073 - val_loss: 1.3969e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 759/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4565e-04 - rmse: 0.0082\n",
      "Epoch 759: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.3918e-04 - rmse: 0.0078 - val_loss: 1.3098e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 760/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3269e-04 - rmse: 0.0074\n",
      "Epoch 760: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3698e-04 - rmse: 0.0076 - val_loss: 1.4971e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 761/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3148e-04 - rmse: 0.0073\n",
      "Epoch 761: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3706e-04 - rmse: 0.0076 - val_loss: 1.4192e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 762/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4863e-04 - rmse: 0.0084\n",
      "Epoch 762: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4231e-04 - rmse: 0.0080 - val_loss: 1.3664e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 763/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3059e-04 - rmse: 0.0072\n",
      "Epoch 763: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3541e-04 - rmse: 0.0075 - val_loss: 1.3564e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 764/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3149e-04 - rmse: 0.0073\n",
      "Epoch 764: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3531e-04 - rmse: 0.0075 - val_loss: 1.3815e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 765/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2771e-04 - rmse: 0.0070\n",
      "Epoch 765: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3877e-04 - rmse: 0.0078 - val_loss: 1.3656e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 766/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4058e-04 - rmse: 0.0079\n",
      "Epoch 766: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4395e-04 - rmse: 0.0081 - val_loss: 1.4133e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 767/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3568e-04 - rmse: 0.0076\n",
      "Epoch 767: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 1.3768e-04 - rmse: 0.0077 - val_loss: 1.3033e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 768/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3096e-04 - rmse: 0.0073\n",
      "Epoch 768: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3536e-04 - rmse: 0.0076 - val_loss: 1.7363e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 769/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7511e-04 - rmse: 0.0098\n",
      "Epoch 769: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5923e-04 - rmse: 0.0090 - val_loss: 1.4554e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 770/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2238e-04 - rmse: 0.0066\n",
      "Epoch 770: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4457e-04 - rmse: 0.0081 - val_loss: 1.4627e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 771/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4151e-04 - rmse: 0.0080\n",
      "Epoch 771: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4650e-04 - rmse: 0.0083 - val_loss: 1.3035e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 772/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3370e-04 - rmse: 0.0074\n",
      "Epoch 772: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3796e-04 - rmse: 0.0077 - val_loss: 1.4452e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 773/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4573e-04 - rmse: 0.0082\n",
      "Epoch 773: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4554e-04 - rmse: 0.0082 - val_loss: 1.3278e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 774/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3025e-04 - rmse: 0.0072\n",
      "Epoch 774: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3332e-04 - rmse: 0.0074 - val_loss: 1.3551e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 775/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2201e-04 - rmse: 0.0066\n",
      "Epoch 775: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5947e-04 - rmse: 0.0090 - val_loss: 1.3290e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 776/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3004e-04 - rmse: 0.0072\n",
      "Epoch 776: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4601e-04 - rmse: 0.0082 - val_loss: 1.3917e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 777/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2723e-04 - rmse: 0.0070\n",
      "Epoch 777: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.3422e-04 - rmse: 0.0075 - val_loss: 1.2712e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 778/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2131e-04 - rmse: 0.0066\n",
      "Epoch 778: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2811e-04 - rmse: 0.0071 - val_loss: 1.3269e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 779/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3153e-04 - rmse: 0.0073\n",
      "Epoch 779: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3536e-04 - rmse: 0.0076 - val_loss: 1.3681e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 780/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3692e-04 - rmse: 0.0077\n",
      "Epoch 780: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3465e-04 - rmse: 0.0075 - val_loss: 1.3732e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 781/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4631e-04 - rmse: 0.0083\n",
      "Epoch 781: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3112e-04 - rmse: 0.0073 - val_loss: 1.2976e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 782/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3096e-04 - rmse: 0.0073\n",
      "Epoch 782: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2907e-04 - rmse: 0.0072 - val_loss: 1.3568e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 783/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3337e-04 - rmse: 0.0075\n",
      "Epoch 783: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3589e-04 - rmse: 0.0076 - val_loss: 1.3894e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 784/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3129e-04 - rmse: 0.0073\n",
      "Epoch 784: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3733e-04 - rmse: 0.0077 - val_loss: 1.3492e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 785/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2708e-04 - rmse: 0.0070\n",
      "Epoch 785: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3385e-04 - rmse: 0.0075 - val_loss: 1.4424e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 786/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5899e-04 - rmse: 0.0090\n",
      "Epoch 786: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4000e-04 - rmse: 0.0079 - val_loss: 1.3706e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 787/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4763e-04 - rmse: 0.0084\n",
      "Epoch 787: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4399e-04 - rmse: 0.0081 - val_loss: 1.3559e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 788/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2926e-04 - rmse: 0.0072\n",
      "Epoch 788: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4690e-04 - rmse: 0.0083 - val_loss: 1.4953e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 789/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5521e-04 - rmse: 0.0088\n",
      "Epoch 789: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4574e-04 - rmse: 0.0083 - val_loss: 1.4594e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 790/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2701e-04 - rmse: 0.0070\n",
      "Epoch 790: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3224e-04 - rmse: 0.0074 - val_loss: 1.3087e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 791/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3315e-04 - rmse: 0.0075\n",
      "Epoch 791: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3503e-04 - rmse: 0.0076 - val_loss: 1.3467e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 792/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3077e-04 - rmse: 0.0073\n",
      "Epoch 792: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2818e-04 - rmse: 0.0071 - val_loss: 1.2833e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 793/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3099e-04 - rmse: 0.0073\n",
      "Epoch 793: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2518e-04 - rmse: 0.0069 - val_loss: 1.3577e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 794/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2656e-04 - rmse: 0.0070\n",
      "Epoch 794: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3200e-04 - rmse: 0.0074 - val_loss: 1.2907e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 795/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2356e-04 - rmse: 0.0068\n",
      "Epoch 795: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2538e-04 - rmse: 0.0069 - val_loss: 1.4886e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 796/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4095e-04 - rmse: 0.0080\n",
      "Epoch 796: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3572e-04 - rmse: 0.0076 - val_loss: 1.2718e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 797/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1663e-04 - rmse: 0.0063\n",
      "Epoch 797: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2979e-04 - rmse: 0.0072 - val_loss: 1.3204e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 798/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3236e-04 - rmse: 0.0074\n",
      "Epoch 798: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 1.3062e-04 - rmse: 0.0073 - val_loss: 1.2630e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 799/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2762e-04 - rmse: 0.0071\n",
      "Epoch 799: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3125e-04 - rmse: 0.0073 - val_loss: 1.3216e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 800/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3129e-04 - rmse: 0.0074\n",
      "Epoch 800: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2853e-04 - rmse: 0.0072 - val_loss: 1.4632e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 801/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4527e-04 - rmse: 0.0082\n",
      "Epoch 801: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4091e-04 - rmse: 0.0080 - val_loss: 1.6165e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 802/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4596e-04 - rmse: 0.0083\n",
      "Epoch 802: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4388e-04 - rmse: 0.0082 - val_loss: 1.2995e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 803/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2285e-04 - rmse: 0.0068\n",
      "Epoch 803: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3039e-04 - rmse: 0.0073 - val_loss: 1.2916e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 804/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2491e-04 - rmse: 0.0069\n",
      "Epoch 804: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4597e-04 - rmse: 0.0083 - val_loss: 1.5142e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 805/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5232e-04 - rmse: 0.0087\n",
      "Epoch 805: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4296e-04 - rmse: 0.0081 - val_loss: 1.5302e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 806/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7087e-04 - rmse: 0.0097\n",
      "Epoch 806: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4836e-04 - rmse: 0.0084 - val_loss: 1.4969e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 807/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4438e-04 - rmse: 0.0082\n",
      "Epoch 807: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3778e-04 - rmse: 0.0078 - val_loss: 1.2902e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 808/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2884e-04 - rmse: 0.0072\n",
      "Epoch 808: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2856e-04 - rmse: 0.0072 - val_loss: 1.2760e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 809/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2398e-04 - rmse: 0.0069\n",
      "Epoch 809: val_loss improved from 0.00013 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 1.2443e-04 - rmse: 0.0069 - val_loss: 1.2436e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 810/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2661e-04 - rmse: 0.0070\n",
      "Epoch 810: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.2320e-04 - rmse: 0.0068 - val_loss: 1.2321e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 811/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1977e-04 - rmse: 0.0065\n",
      "Epoch 811: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2008e-04 - rmse: 0.0066 - val_loss: 1.2374e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 812/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2166e-04 - rmse: 0.0067\n",
      "Epoch 812: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2397e-04 - rmse: 0.0069 - val_loss: 1.3524e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 813/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2843e-04 - rmse: 0.0072\n",
      "Epoch 813: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2521e-04 - rmse: 0.0070 - val_loss: 1.3540e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 814/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2584e-04 - rmse: 0.0070\n",
      "Epoch 814: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2679e-04 - rmse: 0.0071 - val_loss: 1.3360e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 815/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2344e-04 - rmse: 0.0068\n",
      "Epoch 815: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2481e-04 - rmse: 0.0069 - val_loss: 1.2955e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 816/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2262e-04 - rmse: 0.0068\n",
      "Epoch 816: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3046e-04 - rmse: 0.0073 - val_loss: 1.2503e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 817/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2361e-04 - rmse: 0.0069\n",
      "Epoch 817: val_loss did not improve from 0.00012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3361e-04 - rmse: 0.0075 - val_loss: 1.3129e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 818/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1734e-04 - rmse: 0.0064\n",
      "Epoch 818: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2779e-04 - rmse: 0.0072 - val_loss: 1.3432e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 819/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3763e-04 - rmse: 0.0078\n",
      "Epoch 819: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3736e-04 - rmse: 0.0078 - val_loss: 1.3218e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 820/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3505e-04 - rmse: 0.0076\n",
      "Epoch 820: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2787e-04 - rmse: 0.0072 - val_loss: 1.2686e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 821/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2137e-04 - rmse: 0.0067\n",
      "Epoch 821: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2358e-04 - rmse: 0.0069 - val_loss: 1.2422e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 822/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3927e-04 - rmse: 0.0079\n",
      "Epoch 822: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4132e-04 - rmse: 0.0081 - val_loss: 1.4176e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 823/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4229e-04 - rmse: 0.0081\n",
      "Epoch 823: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3910e-04 - rmse: 0.0079 - val_loss: 1.6147e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 824/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3557e-04 - rmse: 0.0077\n",
      "Epoch 824: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3780e-04 - rmse: 0.0078 - val_loss: 1.5642e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 825/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4610e-04 - rmse: 0.0083\n",
      "Epoch 825: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4370e-04 - rmse: 0.0082 - val_loss: 1.2617e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 826/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1509e-04 - rmse: 0.0062\n",
      "Epoch 826: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3750e-04 - rmse: 0.0078 - val_loss: 1.4730e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 827/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4630e-04 - rmse: 0.0084\n",
      "Epoch 827: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3738e-04 - rmse: 0.0078 - val_loss: 1.3097e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 828/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2916e-04 - rmse: 0.0073\n",
      "Epoch 828: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3125e-04 - rmse: 0.0074 - val_loss: 1.2852e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 829/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1595e-04 - rmse: 0.0063\n",
      "Epoch 829: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3333e-04 - rmse: 0.0076 - val_loss: 1.3945e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 830/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3536e-04 - rmse: 0.0077\n",
      "Epoch 830: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.3656e-04 - rmse: 0.0078 - val_loss: 1.5533e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 831/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5568e-04 - rmse: 0.0089\n",
      "Epoch 831: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4739e-04 - rmse: 0.0084 - val_loss: 1.5144e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 832/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5363e-04 - rmse: 0.0088\n",
      "Epoch 832: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4942e-04 - rmse: 0.0086 - val_loss: 1.2984e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 833/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2500e-04 - rmse: 0.0070\n",
      "Epoch 833: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3142e-04 - rmse: 0.0074 - val_loss: 1.6317e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 834/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6753e-04 - rmse: 0.0096\n",
      "Epoch 834: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5017e-04 - rmse: 0.0086 - val_loss: 1.8067e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 835/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8830e-04 - rmse: 0.0106\n",
      "Epoch 835: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7539e-04 - rmse: 0.0100 - val_loss: 1.5267e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 836/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4400e-04 - rmse: 0.0082\n",
      "Epoch 836: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7976e-04 - rmse: 0.0102 - val_loss: 1.4068e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 837/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5098e-04 - rmse: 0.0087\n",
      "Epoch 837: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9119e-04 - rmse: 0.0107 - val_loss: 2.4494e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 838/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4117e-04 - rmse: 0.0129\n",
      "Epoch 838: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0301e-04 - rmse: 0.0113 - val_loss: 2.8384e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 839/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2940e-04 - rmse: 0.0124\n",
      "Epoch 839: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4945e-04 - rmse: 0.0132 - val_loss: 1.5841e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 840/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6415e-04 - rmse: 0.0094\n",
      "Epoch 840: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5922e-04 - rmse: 0.0135 - val_loss: 1.5472e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 841/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4332e-04 - rmse: 0.0082\n",
      "Epoch 841: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8922e-04 - rmse: 0.0106 - val_loss: 2.3071e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 842/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7081e-04 - rmse: 0.0140\n",
      "Epoch 842: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4177e-04 - rmse: 0.0129 - val_loss: 3.5257e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 843/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6889e-04 - rmse: 0.0171\n",
      "Epoch 843: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9612e-04 - rmse: 0.0148 - val_loss: 1.7557e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 844/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9676e-04 - rmse: 0.0110\n",
      "Epoch 844: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.2865e-04 - rmse: 0.0124 - val_loss: 1.7144e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 845/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5958e-04 - rmse: 0.0092\n",
      "Epoch 845: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9028e-04 - rmse: 0.0107 - val_loss: 2.8055e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 846/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5282e-04 - rmse: 0.0133\n",
      "Epoch 846: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0129e-04 - rmse: 0.0112 - val_loss: 1.5859e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 847/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4538e-04 - rmse: 0.0083\n",
      "Epoch 847: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9429e-04 - rmse: 0.0109 - val_loss: 1.3006e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 848/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3299e-04 - rmse: 0.0076\n",
      "Epoch 848: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8901e-04 - rmse: 0.0106 - val_loss: 2.1823e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 849/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3911e-04 - rmse: 0.0128\n",
      "Epoch 849: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9602e-04 - rmse: 0.0110 - val_loss: 2.1120e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 850/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1314e-04 - rmse: 0.0117\n",
      "Epoch 850: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6704e-04 - rmse: 0.0096 - val_loss: 1.2781e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 851/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1690e-04 - rmse: 0.0064\n",
      "Epoch 851: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4352e-04 - rmse: 0.0082 - val_loss: 2.0666e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 852/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9521e-04 - rmse: 0.0109\n",
      "Epoch 852: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7111e-04 - rmse: 0.0098 - val_loss: 1.7895e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 853/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6815e-04 - rmse: 0.0096\n",
      "Epoch 853: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 1.5241e-04 - rmse: 0.0088 - val_loss: 1.2112e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 854/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1342e-04 - rmse: 0.0062\n",
      "Epoch 854: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5103e-04 - rmse: 0.0087 - val_loss: 1.8211e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 855/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9507e-04 - rmse: 0.0109\n",
      "Epoch 855: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6145e-04 - rmse: 0.0093 - val_loss: 1.6493e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 856/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7780e-04 - rmse: 0.0101\n",
      "Epoch 856: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5095e-04 - rmse: 0.0087 - val_loss: 1.2908e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 857/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2198e-04 - rmse: 0.0068\n",
      "Epoch 857: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3734e-04 - rmse: 0.0079 - val_loss: 1.4384e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 858/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4794e-04 - rmse: 0.0085\n",
      "Epoch 858: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5789e-04 - rmse: 0.0091 - val_loss: 2.0571e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 859/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7660e-04 - rmse: 0.0101\n",
      "Epoch 859: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.7156e-04 - rmse: 0.0098 - val_loss: 1.1721e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 860/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0885e-04 - rmse: 0.0058\n",
      "Epoch 860: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7004e-04 - rmse: 0.0097 - val_loss: 1.9934e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 861/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0339e-04 - rmse: 0.0113\n",
      "Epoch 861: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0820e-04 - rmse: 0.0115 - val_loss: 2.8706e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 862/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7389e-04 - rmse: 0.0141\n",
      "Epoch 862: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.6880e-04 - rmse: 0.0139 - val_loss: 1.3757e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 863/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4446e-04 - rmse: 0.0083\n",
      "Epoch 863: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6074e-04 - rmse: 0.0093 - val_loss: 2.6789e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 864/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4485e-04 - rmse: 0.0130\n",
      "Epoch 864: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9687e-04 - rmse: 0.0110 - val_loss: 1.8450e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 865/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8549e-04 - rmse: 0.0105\n",
      "Epoch 865: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9080e-04 - rmse: 0.0108 - val_loss: 1.8126e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 866/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5428e-04 - rmse: 0.0089\n",
      "Epoch 866: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8845e-04 - rmse: 0.0106 - val_loss: 1.5360e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 867/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5994e-04 - rmse: 0.0092\n",
      "Epoch 867: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7453e-04 - rmse: 0.0100 - val_loss: 1.8498e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 868/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0935e-04 - rmse: 0.0116\n",
      "Epoch 868: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8892e-04 - rmse: 0.0107 - val_loss: 1.2377e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 869/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1358e-04 - rmse: 0.0062\n",
      "Epoch 869: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6345e-04 - rmse: 0.0094 - val_loss: 2.0324e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 870/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9717e-04 - rmse: 0.0111\n",
      "Epoch 870: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6725e-04 - rmse: 0.0096 - val_loss: 1.6300e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 871/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5250e-04 - rmse: 0.0088\n",
      "Epoch 871: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4995e-04 - rmse: 0.0087 - val_loss: 1.3052e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 872/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3823e-04 - rmse: 0.0080\n",
      "Epoch 872: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4212e-04 - rmse: 0.0082 - val_loss: 1.5399e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 873/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6602e-04 - rmse: 0.0095\n",
      "Epoch 873: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4618e-04 - rmse: 0.0084 - val_loss: 1.2655e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 874/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2395e-04 - rmse: 0.0070\n",
      "Epoch 874: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2837e-04 - rmse: 0.0073 - val_loss: 1.2994e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 875/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3258e-04 - rmse: 0.0076\n",
      "Epoch 875: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3308e-04 - rmse: 0.0076 - val_loss: 1.1945e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 876/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1956e-04 - rmse: 0.0067\n",
      "Epoch 876: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2048e-04 - rmse: 0.0068 - val_loss: 1.2934e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 877/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1940e-04 - rmse: 0.0067\n",
      "Epoch 877: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 1.1525e-04 - rmse: 0.0064 - val_loss: 1.1613e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 878/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1732e-04 - rmse: 0.0065\n",
      "Epoch 878: val_loss improved from 0.00012 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.1110e-04 - rmse: 0.0060 - val_loss: 1.1324e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 879/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1412e-04 - rmse: 0.0063\n",
      "Epoch 879: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1102e-04 - rmse: 0.0060 - val_loss: 1.2024e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 880/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2453e-04 - rmse: 0.0071\n",
      "Epoch 880: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1810e-04 - rmse: 0.0066 - val_loss: 1.1792e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 881/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0930e-04 - rmse: 0.0059\n",
      "Epoch 881: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1785e-04 - rmse: 0.0066 - val_loss: 1.2274e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 882/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1703e-04 - rmse: 0.0065\n",
      "Epoch 882: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1595e-04 - rmse: 0.0064 - val_loss: 1.1492e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 883/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0870e-04 - rmse: 0.0058\n",
      "Epoch 883: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2028e-04 - rmse: 0.0068 - val_loss: 1.2007e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 884/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1706e-04 - rmse: 0.0065\n",
      "Epoch 884: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1106e-04 - rmse: 0.0060 - val_loss: 1.1772e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 885/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1079e-04 - rmse: 0.0060\n",
      "Epoch 885: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1660e-04 - rmse: 0.0065 - val_loss: 1.2034e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 886/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1714e-04 - rmse: 0.0065\n",
      "Epoch 886: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1980e-04 - rmse: 0.0067 - val_loss: 1.2744e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 887/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2730e-04 - rmse: 0.0073\n",
      "Epoch 887: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.2698e-04 - rmse: 0.0073 - val_loss: 1.1245e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 888/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0898e-04 - rmse: 0.0059\n",
      "Epoch 888: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1437e-04 - rmse: 0.0063 - val_loss: 1.1608e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 889/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1496e-04 - rmse: 0.0064\n",
      "Epoch 889: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1495e-04 - rmse: 0.0064 - val_loss: 1.1967e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 890/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0554e-04 - rmse: 0.0056\n",
      "Epoch 890: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 1.0815e-04 - rmse: 0.0058 - val_loss: 1.1200e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 891/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0738e-04 - rmse: 0.0058\n",
      "Epoch 891: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1133e-04 - rmse: 0.0061 - val_loss: 1.1805e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 892/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1188e-04 - rmse: 0.0061\n",
      "Epoch 892: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1497e-04 - rmse: 0.0064 - val_loss: 1.2000e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 893/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1514e-04 - rmse: 0.0064\n",
      "Epoch 893: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1419e-04 - rmse: 0.0063 - val_loss: 1.2003e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 894/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2365e-04 - rmse: 0.0070\n",
      "Epoch 894: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1386e-04 - rmse: 0.0063 - val_loss: 1.1744e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 895/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1833e-04 - rmse: 0.0066\n",
      "Epoch 895: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 1.1585e-04 - rmse: 0.0065 - val_loss: 1.1187e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 896/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0838e-04 - rmse: 0.0059\n",
      "Epoch 896: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1153e-04 - rmse: 0.0061 - val_loss: 1.2112e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 897/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1769e-04 - rmse: 0.0066\n",
      "Epoch 897: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1737e-04 - rmse: 0.0066 - val_loss: 1.1788e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 898/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1700e-04 - rmse: 0.0066\n",
      "Epoch 898: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 1.1783e-04 - rmse: 0.0066 - val_loss: 1.1182e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 899/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0860e-04 - rmse: 0.0059\n",
      "Epoch 899: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1386e-04 - rmse: 0.0063 - val_loss: 1.3284e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 900/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1639e-04 - rmse: 0.0065\n",
      "Epoch 900: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1618e-04 - rmse: 0.0065 - val_loss: 1.3406e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 901/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2776e-04 - rmse: 0.0073\n",
      "Epoch 901: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.2079e-04 - rmse: 0.0068 - val_loss: 1.1219e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 902/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0868e-04 - rmse: 0.0059\n",
      "Epoch 902: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0932e-04 - rmse: 0.0059 - val_loss: 1.1542e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 903/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2055e-04 - rmse: 0.0068\n",
      "Epoch 903: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1278e-04 - rmse: 0.0062 - val_loss: 1.1305e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 904/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1325e-04 - rmse: 0.0063\n",
      "Epoch 904: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 1.1166e-04 - rmse: 0.0061 - val_loss: 1.0995e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 905/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0956e-04 - rmse: 0.0060\n",
      "Epoch 905: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0841e-04 - rmse: 0.0059 - val_loss: 1.1667e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 906/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0752e-04 - rmse: 0.0058\n",
      "Epoch 906: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0715e-04 - rmse: 0.0058 - val_loss: 1.1058e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 907/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0548e-04 - rmse: 0.0056\n",
      "Epoch 907: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0791e-04 - rmse: 0.0058 - val_loss: 1.1272e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 908/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0909e-04 - rmse: 0.0059\n",
      "Epoch 908: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1326e-04 - rmse: 0.0063 - val_loss: 1.1599e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 909/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1112e-04 - rmse: 0.0061\n",
      "Epoch 909: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0892e-04 - rmse: 0.0059 - val_loss: 1.1224e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 910/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1155e-04 - rmse: 0.0062\n",
      "Epoch 910: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0661e-04 - rmse: 0.0057 - val_loss: 1.1833e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 911/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1456e-04 - rmse: 0.0064\n",
      "Epoch 911: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1277e-04 - rmse: 0.0063 - val_loss: 1.1360e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 912/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0433e-04 - rmse: 0.0055\n",
      "Epoch 912: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0623e-04 - rmse: 0.0057 - val_loss: 1.1214e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 913/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0778e-04 - rmse: 0.0058\n",
      "Epoch 913: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1191e-04 - rmse: 0.0062 - val_loss: 1.1232e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 914/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0282e-04 - rmse: 0.0054\n",
      "Epoch 914: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0935e-04 - rmse: 0.0060 - val_loss: 1.1216e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 915/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1639e-04 - rmse: 0.0065\n",
      "Epoch 915: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1553e-04 - rmse: 0.0065 - val_loss: 1.1968e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 916/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1597e-04 - rmse: 0.0065\n",
      "Epoch 916: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1884e-04 - rmse: 0.0067 - val_loss: 1.1283e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 917/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1113e-04 - rmse: 0.0061\n",
      "Epoch 917: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0824e-04 - rmse: 0.0059 - val_loss: 1.1175e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 918/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1387e-04 - rmse: 0.0064\n",
      "Epoch 918: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0825e-04 - rmse: 0.0059 - val_loss: 1.1073e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 919/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0686e-04 - rmse: 0.0058\n",
      "Epoch 919: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1153e-04 - rmse: 0.0062 - val_loss: 1.2299e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 920/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2076e-04 - rmse: 0.0069\n",
      "Epoch 920: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2229e-04 - rmse: 0.0070 - val_loss: 1.2599e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 921/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1839e-04 - rmse: 0.0067\n",
      "Epoch 921: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.1237e-04 - rmse: 0.0062 - val_loss: 1.1500e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 922/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1111e-04 - rmse: 0.0061\n",
      "Epoch 922: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1422e-04 - rmse: 0.0064 - val_loss: 1.1619e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 923/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0658e-04 - rmse: 0.0058\n",
      "Epoch 923: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 1.0708e-04 - rmse: 0.0058 - val_loss: 1.0724e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 924/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0513e-04 - rmse: 0.0056\n",
      "Epoch 924: val_loss did not improve from 0.00011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0577e-04 - rmse: 0.0057 - val_loss: 1.0747e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 925/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0402e-04 - rmse: 0.0055\n",
      "Epoch 925: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0323e-04 - rmse: 0.0055 - val_loss: 1.1266e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 926/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1961e-04 - rmse: 0.0068\n",
      "Epoch 926: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0731e-04 - rmse: 0.0058 - val_loss: 1.0815e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 927/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0449e-04 - rmse: 0.0056\n",
      "Epoch 927: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0618e-04 - rmse: 0.0057 - val_loss: 1.0744e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 928/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0173e-04 - rmse: 0.0053\n",
      "Epoch 928: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 1.0271e-04 - rmse: 0.0054 - val_loss: 1.0685e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 929/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8974e-05 - rmse: 0.0051\n",
      "Epoch 929: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0204e-04 - rmse: 0.0054 - val_loss: 1.1165e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 930/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1079e-04 - rmse: 0.0061\n",
      "Epoch 930: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0652e-04 - rmse: 0.0058 - val_loss: 1.1247e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 931/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0897e-04 - rmse: 0.0060\n",
      "Epoch 931: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0836e-04 - rmse: 0.0059 - val_loss: 1.1565e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 932/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1246e-04 - rmse: 0.0063\n",
      "Epoch 932: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0916e-04 - rmse: 0.0060 - val_loss: 1.1109e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 933/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0361e-04 - rmse: 0.0055\n",
      "Epoch 933: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1323e-04 - rmse: 0.0063 - val_loss: 1.0723e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 934/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9697e-05 - rmse: 0.0052\n",
      "Epoch 934: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1328e-04 - rmse: 0.0064 - val_loss: 1.0866e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 935/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0487e-04 - rmse: 0.0057\n",
      "Epoch 935: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0747e-04 - rmse: 0.0059 - val_loss: 1.1386e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 936/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0920e-04 - rmse: 0.0060\n",
      "Epoch 936: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0816e-04 - rmse: 0.0059 - val_loss: 1.2377e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 937/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2124e-04 - rmse: 0.0070\n",
      "Epoch 937: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1286e-04 - rmse: 0.0063 - val_loss: 1.1368e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 938/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1557e-04 - rmse: 0.0065\n",
      "Epoch 938: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 1.1044e-04 - rmse: 0.0061 - val_loss: 1.0659e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 939/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0228e-04 - rmse: 0.0054\n",
      "Epoch 939: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1027e-04 - rmse: 0.0061 - val_loss: 1.0944e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 940/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0274e-04 - rmse: 0.0055\n",
      "Epoch 940: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1262e-04 - rmse: 0.0063 - val_loss: 1.4836e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 941/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3889e-04 - rmse: 0.0081\n",
      "Epoch 941: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2601e-04 - rmse: 0.0073 - val_loss: 1.2742e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 942/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2355e-04 - rmse: 0.0071\n",
      "Epoch 942: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1226e-04 - rmse: 0.0063 - val_loss: 1.1679e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 943/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0854e-04 - rmse: 0.0060\n",
      "Epoch 943: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0681e-04 - rmse: 0.0058 - val_loss: 1.1024e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 944/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0946e-04 - rmse: 0.0061\n",
      "Epoch 944: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 1.0651e-04 - rmse: 0.0058 - val_loss: 1.0572e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 945/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0639e-04 - rmse: 0.0058\n",
      "Epoch 945: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0882e-04 - rmse: 0.0060 - val_loss: 1.0701e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 946/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0474e-04 - rmse: 0.0057\n",
      "Epoch 946: val_loss improved from 0.00011 to 0.00010, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.1699e-04 - rmse: 0.0067 - val_loss: 1.0392e-04 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 947/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9163e-05 - rmse: 0.0052\n",
      "Epoch 947: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.0028e-04 - rmse: 0.0053 - val_loss: 1.0282e-04 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 948/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8533e-05 - rmse: 0.0051\n",
      "Epoch 948: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0389e-04 - rmse: 0.0056 - val_loss: 1.0880e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 949/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0413e-04 - rmse: 0.0056\n",
      "Epoch 949: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0563e-04 - rmse: 0.0058 - val_loss: 1.0897e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1962e-04 - rmse: 0.0069\n",
      "Epoch 950: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2414e-04 - rmse: 0.0072 - val_loss: 1.1129e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 951/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0212e-04 - rmse: 0.0054\n",
      "Epoch 951: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0925e-04 - rmse: 0.0061 - val_loss: 1.1573e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 952/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1498e-04 - rmse: 0.0065\n",
      "Epoch 952: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1969e-04 - rmse: 0.0069 - val_loss: 1.3750e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 953/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2573e-04 - rmse: 0.0073\n",
      "Epoch 953: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1864e-04 - rmse: 0.0068 - val_loss: 1.2862e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 954/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2123e-04 - rmse: 0.0070\n",
      "Epoch 954: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1549e-04 - rmse: 0.0066 - val_loss: 1.1709e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 955/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0731e-04 - rmse: 0.0059\n",
      "Epoch 955: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1995e-04 - rmse: 0.0069 - val_loss: 1.3357e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 956/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2657e-04 - rmse: 0.0074\n",
      "Epoch 956: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2906e-04 - rmse: 0.0075 - val_loss: 1.2055e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 957/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1833e-04 - rmse: 0.0068\n",
      "Epoch 957: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2200e-04 - rmse: 0.0071 - val_loss: 1.1245e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 958/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1029e-04 - rmse: 0.0062\n",
      "Epoch 958: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2651e-04 - rmse: 0.0074 - val_loss: 1.2517e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 959/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1638e-04 - rmse: 0.0066\n",
      "Epoch 959: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1151e-04 - rmse: 0.0063 - val_loss: 1.3499e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 960/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3318e-04 - rmse: 0.0078\n",
      "Epoch 960: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2333e-04 - rmse: 0.0072 - val_loss: 1.4208e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 961/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4116e-04 - rmse: 0.0083\n",
      "Epoch 961: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3569e-04 - rmse: 0.0080 - val_loss: 1.0702e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 962/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0037e-04 - rmse: 0.0053\n",
      "Epoch 962: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1937e-04 - rmse: 0.0069 - val_loss: 1.1781e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 963/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0667e-04 - rmse: 0.0059\n",
      "Epoch 963: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0901e-04 - rmse: 0.0061 - val_loss: 1.4976e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 964/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3340e-04 - rmse: 0.0078\n",
      "Epoch 964: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1775e-04 - rmse: 0.0068 - val_loss: 1.0864e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 965/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0528e-04 - rmse: 0.0058\n",
      "Epoch 965: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1039e-04 - rmse: 0.0062 - val_loss: 1.0757e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 966/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0621e-04 - rmse: 0.0059\n",
      "Epoch 966: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0205e-04 - rmse: 0.0055 - val_loss: 1.2098e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 967/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2283e-04 - rmse: 0.0071\n",
      "Epoch 967: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1670e-04 - rmse: 0.0067 - val_loss: 1.2209e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 968/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2025e-04 - rmse: 0.0070\n",
      "Epoch 968: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1523e-04 - rmse: 0.0066 - val_loss: 1.1579e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 969/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2124e-04 - rmse: 0.0070\n",
      "Epoch 969: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1166e-04 - rmse: 0.0063 - val_loss: 1.1040e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 970/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1628e-04 - rmse: 0.0067\n",
      "Epoch 970: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0908e-04 - rmse: 0.0061 - val_loss: 1.2961e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 971/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2955e-04 - rmse: 0.0076\n",
      "Epoch 971: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1645e-04 - rmse: 0.0067 - val_loss: 1.0887e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 972/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0537e-04 - rmse: 0.0058\n",
      "Epoch 972: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1010e-04 - rmse: 0.0062 - val_loss: 1.0389e-04 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 973/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7429e-05 - rmse: 0.0051\n",
      "Epoch 973: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0453e-04 - rmse: 0.0057 - val_loss: 1.1648e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 974/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1039e-04 - rmse: 0.0062\n",
      "Epoch 974: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0425e-04 - rmse: 0.0057 - val_loss: 1.0763e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 975/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4826e-05 - rmse: 0.0048\n",
      "Epoch 975: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 9.6441e-05 - rmse: 0.0050 - val_loss: 1.0085e-04 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 976/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8154e-05 - rmse: 0.0051\n",
      "Epoch 976: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0464e-04 - rmse: 0.0057 - val_loss: 1.2238e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 977/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2368e-04 - rmse: 0.0072\n",
      "Epoch 977: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1387e-04 - rmse: 0.0065 - val_loss: 1.2026e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 978/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2018e-04 - rmse: 0.0070\n",
      "Epoch 978: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0520e-04 - rmse: 0.0058 - val_loss: 1.1262e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 979/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1170e-04 - rmse: 0.0063\n",
      "Epoch 979: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0907e-04 - rmse: 0.0061 - val_loss: 1.0409e-04 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 980/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0187e-04 - rmse: 0.0055\n",
      "Epoch 980: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0239e-04 - rmse: 0.0056 - val_loss: 1.1920e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 981/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1380e-04 - rmse: 0.0065\n",
      "Epoch 981: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0761e-04 - rmse: 0.0060 - val_loss: 1.0293e-04 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 982/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0287e-04 - rmse: 0.0056\n",
      "Epoch 982: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.0119e-04 - rmse: 0.0054 - val_loss: 1.0043e-04 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 983/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2851e-05 - rmse: 0.0046\n",
      "Epoch 983: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.5523e-05 - rmse: 0.0049 - val_loss: 1.0092e-04 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 984/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6105e-05 - rmse: 0.0050\n",
      "Epoch 984: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.9945e-05 - rmse: 0.0053 - val_loss: 1.0714e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 985/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1014e-04 - rmse: 0.0062\n",
      "Epoch 985: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0546e-04 - rmse: 0.0058 - val_loss: 1.0570e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 986/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0242e-04 - rmse: 0.0056\n",
      "Epoch 986: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0984e-04 - rmse: 0.0062 - val_loss: 1.1238e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 987/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1262e-04 - rmse: 0.0064\n",
      "Epoch 987: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1106e-04 - rmse: 0.0063 - val_loss: 1.1135e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 988/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1129e-04 - rmse: 0.0063\n",
      "Epoch 988: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1136e-04 - rmse: 0.0063 - val_loss: 1.3943e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 989/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2989e-04 - rmse: 0.0077\n",
      "Epoch 989: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3246e-04 - rmse: 0.0078 - val_loss: 1.0765e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 990/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0742e-04 - rmse: 0.0060\n",
      "Epoch 990: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0900e-04 - rmse: 0.0061 - val_loss: 1.1411e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 991/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1067e-04 - rmse: 0.0063\n",
      "Epoch 991: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0644e-04 - rmse: 0.0059 - val_loss: 1.0847e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 992/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1363e-04 - rmse: 0.0065\n",
      "Epoch 992: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.0327e-04 - rmse: 0.0057 - val_loss: 9.7436e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 993/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7509e-05 - rmse: 0.0051\n",
      "Epoch 993: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0068e-04 - rmse: 0.0054 - val_loss: 1.0438e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 994/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1730e-05 - rmse: 0.0045\n",
      "Epoch 994: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0577e-04 - rmse: 0.0059 - val_loss: 1.1007e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 995/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0535e-04 - rmse: 0.0058\n",
      "Epoch 995: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4140e-04 - rmse: 0.0084 - val_loss: 1.1610e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 996/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1216e-04 - rmse: 0.0064\n",
      "Epoch 996: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4269e-04 - rmse: 0.0085 - val_loss: 1.0693e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 997/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1082e-04 - rmse: 0.0063\n",
      "Epoch 997: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4441e-04 - rmse: 0.0086 - val_loss: 2.0344e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 998/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4600e-04 - rmse: 0.0132\n",
      "Epoch 998: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1473e-04 - rmse: 0.0120 - val_loss: 2.0765e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 999/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4499e-04 - rmse: 0.0132\n",
      "Epoch 999: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9549e-04 - rmse: 0.0112 - val_loss: 1.6258e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1000/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5350e-04 - rmse: 0.0091\n",
      "Epoch 1000: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4946e-04 - rmse: 0.0089 - val_loss: 2.1343e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1001/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0589e-04 - rmse: 0.0116\n",
      "Epoch 1001: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7145e-04 - rmse: 0.0100 - val_loss: 1.1513e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 1002/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1209e-04 - rmse: 0.0064\n",
      "Epoch 1002: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1851e-04 - rmse: 0.0069 - val_loss: 1.0069e-04 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1003/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9405e-05 - rmse: 0.0043\n",
      "Epoch 1003: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.6929e-05 - rmse: 0.0051 - val_loss: 1.2554e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 1004/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2059e-04 - rmse: 0.0071\n",
      "Epoch 1004: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1489e-04 - rmse: 0.0066 - val_loss: 1.2072e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1005/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2316e-04 - rmse: 0.0072\n",
      "Epoch 1005: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1477e-04 - rmse: 0.0066 - val_loss: 1.1659e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 1006/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0912e-04 - rmse: 0.0062\n",
      "Epoch 1006: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0435e-04 - rmse: 0.0058 - val_loss: 1.2426e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 1007/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2143e-04 - rmse: 0.0071\n",
      "Epoch 1007: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1301e-04 - rmse: 0.0065 - val_loss: 1.0453e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 1008/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0130e-04 - rmse: 0.0055\n",
      "Epoch 1008: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0710e-04 - rmse: 0.0060 - val_loss: 1.0364e-04 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 1009/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0166e-04 - rmse: 0.0056\n",
      "Epoch 1009: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1291e-04 - rmse: 0.0065 - val_loss: 1.2123e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 1010/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2352e-04 - rmse: 0.0073\n",
      "Epoch 1010: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1567e-04 - rmse: 0.0067 - val_loss: 9.7858e-05 - val_rmse: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 1011/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6477e-05 - rmse: 0.0051\n",
      "Epoch 1011: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0215e-04 - rmse: 0.0056 - val_loss: 9.8281e-05 - val_rmse: 0.0053 - lr: 1.0000e-04\n",
      "Epoch 1012/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4813e-05 - rmse: 0.0049\n",
      "Epoch 1012: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0170e-04 - rmse: 0.0056 - val_loss: 1.0222e-04 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1013/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8053e-05 - rmse: 0.0052\n",
      "Epoch 1013: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.7658e-05 - rmse: 0.0052 - val_loss: 1.0669e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 1014/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9380e-05 - rmse: 0.0054\n",
      "Epoch 1014: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.6195e-05 - rmse: 0.0051 - val_loss: 1.0415e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 1015/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0557e-04 - rmse: 0.0059\n",
      "Epoch 1015: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.8619e-05 - rmse: 0.0053 - val_loss: 1.0832e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 1016/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0642e-04 - rmse: 0.0060\n",
      "Epoch 1016: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0040e-04 - rmse: 0.0055 - val_loss: 1.0414e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 1017/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0174e-04 - rmse: 0.0056\n",
      "Epoch 1017: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 9.8834e-05 - rmse: 0.0053 - val_loss: 9.6171e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 1018/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6683e-05 - rmse: 0.0051\n",
      "Epoch 1018: val_loss did not improve from 0.00010\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.8297e-05 - rmse: 0.0053 - val_loss: 9.7814e-05 - val_rmse: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 1019/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6881e-05 - rmse: 0.0051\n",
      "Epoch 1019: val_loss improved from 0.00010 to 0.00009, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 1.0211e-04 - rmse: 0.0056 - val_loss: 9.3536e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1020/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1522e-05 - rmse: 0.0046\n",
      "Epoch 1020: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 9.4310e-05 - rmse: 0.0049 - val_loss: 9.2020e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1021/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9581e-05 - rmse: 0.0044\n",
      "Epoch 1021: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4845e-05 - rmse: 0.0049 - val_loss: 9.8736e-05 - val_rmse: 0.0053 - lr: 1.0000e-04\n",
      "Epoch 1022/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2042e-05 - rmse: 0.0047\n",
      "Epoch 1022: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4932e-05 - rmse: 0.0050 - val_loss: 1.0265e-04 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 1023/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0522e-04 - rmse: 0.0059\n",
      "Epoch 1023: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0603e-04 - rmse: 0.0060 - val_loss: 1.0120e-04 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1024/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5411e-05 - rmse: 0.0050\n",
      "Epoch 1024: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.9399e-05 - rmse: 0.0054 - val_loss: 9.5729e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1025/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3893e-05 - rmse: 0.0049\n",
      "Epoch 1025: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.3479e-05 - rmse: 0.0048 - val_loss: 9.3115e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1026/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9044e-05 - rmse: 0.0043\n",
      "Epoch 1026: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 8.8567e-05 - rmse: 0.0043 - val_loss: 9.1419e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1027/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9403e-05 - rmse: 0.0044\n",
      "Epoch 1027: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.1180e-05 - rmse: 0.0046 - val_loss: 9.3659e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1028/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7018e-05 - rmse: 0.0041\n",
      "Epoch 1028: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.0564e-05 - rmse: 0.0045 - val_loss: 9.4789e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1029/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5076e-05 - rmse: 0.0050\n",
      "Epoch 1029: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4337e-05 - rmse: 0.0049 - val_loss: 9.7284e-05 - val_rmse: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 1030/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3425e-05 - rmse: 0.0048\n",
      "Epoch 1030: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4085e-05 - rmse: 0.0049 - val_loss: 9.4844e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1031/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3387e-05 - rmse: 0.0048\n",
      "Epoch 1031: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0598e-04 - rmse: 0.0060 - val_loss: 9.3949e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1032/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9529e-05 - rmse: 0.0044\n",
      "Epoch 1032: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0513e-04 - rmse: 0.0059 - val_loss: 1.0317e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 1033/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0039e-04 - rmse: 0.0055\n",
      "Epoch 1033: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0268e-04 - rmse: 0.0057 - val_loss: 1.0015e-04 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1034/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0052e-04 - rmse: 0.0055\n",
      "Epoch 1034: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0075e-04 - rmse: 0.0055 - val_loss: 1.0450e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1035/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0254e-04 - rmse: 0.0057\n",
      "Epoch 1035: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 1.0313e-04 - rmse: 0.0058 - val_loss: 9.1392e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1036/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2091e-05 - rmse: 0.0047\n",
      "Epoch 1036: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4212e-05 - rmse: 0.0049 - val_loss: 1.0180e-04 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1037/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4725e-05 - rmse: 0.0050\n",
      "Epoch 1037: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.3607e-05 - rmse: 0.0049 - val_loss: 9.7662e-05 - val_rmse: 0.0053 - lr: 1.0000e-04\n",
      "Epoch 1038/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1916e-05 - rmse: 0.0047\n",
      "Epoch 1038: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.3425e-05 - rmse: 0.0049 - val_loss: 9.9792e-05 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1039/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4406e-05 - rmse: 0.0050\n",
      "Epoch 1039: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 9.3864e-05 - rmse: 0.0049 - val_loss: 9.4587e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1040/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5931e-05 - rmse: 0.0040\n",
      "Epoch 1040: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.2165e-05 - rmse: 0.0047 - val_loss: 9.3369e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1041/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4588e-05 - rmse: 0.0050\n",
      "Epoch 1041: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.2274e-05 - rmse: 0.0047 - val_loss: 9.6052e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 1042/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1379e-05 - rmse: 0.0046\n",
      "Epoch 1042: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.1035e-05 - rmse: 0.0046 - val_loss: 9.9639e-05 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1043/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8507e-05 - rmse: 0.0054\n",
      "Epoch 1043: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4189e-05 - rmse: 0.0049 - val_loss: 9.5420e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 1044/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4960e-05 - rmse: 0.0050\n",
      "Epoch 1044: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.3947e-05 - rmse: 0.0049 - val_loss: 9.1527e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1045/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8289e-05 - rmse: 0.0043\n",
      "Epoch 1045: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.7651e-05 - rmse: 0.0053 - val_loss: 9.2724e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1046/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3980e-05 - rmse: 0.0049\n",
      "Epoch 1046: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0133e-04 - rmse: 0.0056 - val_loss: 9.8725e-05 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 1047/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0458e-04 - rmse: 0.0059\n",
      "Epoch 1047: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0494e-04 - rmse: 0.0059 - val_loss: 1.1338e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 1048/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1754e-04 - rmse: 0.0069\n",
      "Epoch 1048: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0455e-04 - rmse: 0.0059 - val_loss: 9.3472e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1049/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9077e-05 - rmse: 0.0044\n",
      "Epoch 1049: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.3622e-05 - rmse: 0.0049 - val_loss: 9.6815e-05 - val_rmse: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 1050/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2379e-05 - rmse: 0.0048\n",
      "Epoch 1050: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.3601e-05 - rmse: 0.0049 - val_loss: 1.0384e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1051/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0203e-04 - rmse: 0.0057\n",
      "Epoch 1051: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.9123e-05 - rmse: 0.0054 - val_loss: 1.0435e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1052/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9556e-05 - rmse: 0.0055\n",
      "Epoch 1052: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0087e-04 - rmse: 0.0056 - val_loss: 9.1858e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1053/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6265e-05 - rmse: 0.0052\n",
      "Epoch 1053: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0058e-04 - rmse: 0.0056 - val_loss: 9.6967e-05 - val_rmse: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 1054/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6525e-05 - rmse: 0.0052\n",
      "Epoch 1054: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0245e-04 - rmse: 0.0057 - val_loss: 1.0476e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1055/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0322e-04 - rmse: 0.0058\n",
      "Epoch 1055: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0515e-04 - rmse: 0.0060 - val_loss: 9.9983e-05 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1056/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6700e-05 - rmse: 0.0052\n",
      "Epoch 1056: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0449e-04 - rmse: 0.0059 - val_loss: 1.2319e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 1057/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1649e-04 - rmse: 0.0069\n",
      "Epoch 1057: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1053e-04 - rmse: 0.0064 - val_loss: 1.1418e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 1058/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1570e-04 - rmse: 0.0068\n",
      "Epoch 1058: val_loss did not improve from 0.00009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0388e-04 - rmse: 0.0059 - val_loss: 1.0911e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 1059/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0903e-04 - rmse: 0.0063\n",
      "Epoch 1059: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 9.9758e-05 - rmse: 0.0055 - val_loss: 8.7582e-05 - val_rmse: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 1060/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5059e-05 - rmse: 0.0040\n",
      "Epoch 1060: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.8472e-05 - rmse: 0.0044 - val_loss: 9.0530e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1061/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9334e-05 - rmse: 0.0045\n",
      "Epoch 1061: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.8082e-05 - rmse: 0.0043 - val_loss: 9.4076e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1062/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8040e-05 - rmse: 0.0043\n",
      "Epoch 1062: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.6814e-05 - rmse: 0.0042 - val_loss: 9.0787e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1063/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4081e-05 - rmse: 0.0050\n",
      "Epoch 1063: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.0664e-05 - rmse: 0.0046 - val_loss: 9.0866e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1064/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7911e-05 - rmse: 0.0043\n",
      "Epoch 1064: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.2706e-05 - rmse: 0.0049 - val_loss: 8.9977e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1065/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5312e-05 - rmse: 0.0040\n",
      "Epoch 1065: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.8956e-05 - rmse: 0.0045 - val_loss: 8.7678e-05 - val_rmse: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 1066/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4262e-05 - rmse: 0.0039\n",
      "Epoch 1066: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.5721e-05 - rmse: 0.0052 - val_loss: 9.2311e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1067/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8427e-05 - rmse: 0.0044\n",
      "Epoch 1067: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.7429e-05 - rmse: 0.0043 - val_loss: 8.9850e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1068/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7326e-05 - rmse: 0.0043\n",
      "Epoch 1068: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.7236e-05 - rmse: 0.0043 - val_loss: 9.4330e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1069/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8483e-05 - rmse: 0.0044\n",
      "Epoch 1069: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.0069e-05 - rmse: 0.0046 - val_loss: 8.9268e-05 - val_rmse: 0.0045 - lr: 1.0000e-04\n",
      "Epoch 1070/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5831e-05 - rmse: 0.0041\n",
      "Epoch 1070: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.9210e-05 - rmse: 0.0045 - val_loss: 9.8347e-05 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 1071/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3577e-05 - rmse: 0.0050\n",
      "Epoch 1071: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.8999e-05 - rmse: 0.0045 - val_loss: 9.1988e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1072/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9007e-05 - rmse: 0.0045\n",
      "Epoch 1072: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.5639e-05 - rmse: 0.0041 - val_loss: 8.9429e-05 - val_rmse: 0.0045 - lr: 1.0000e-04\n",
      "Epoch 1073/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7238e-05 - rmse: 0.0043\n",
      "Epoch 1073: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.6742e-05 - rmse: 0.0042 - val_loss: 9.4087e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1074/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7825e-05 - rmse: 0.0044\n",
      "Epoch 1074: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 8.9408e-05 - rmse: 0.0045 - val_loss: 8.8184e-05 - val_rmse: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 1075/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4494e-05 - rmse: 0.0040\n",
      "Epoch 1075: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.7085e-05 - rmse: 0.0043 - val_loss: 9.7230e-05 - val_rmse: 0.0053 - lr: 1.0000e-04\n",
      "Epoch 1076/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3845e-05 - rmse: 0.0050\n",
      "Epoch 1076: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.2366e-05 - rmse: 0.0049 - val_loss: 9.2730e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1077/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7509e-05 - rmse: 0.0043\n",
      "Epoch 1077: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.9928e-05 - rmse: 0.0046 - val_loss: 9.3282e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1078/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9896e-05 - rmse: 0.0046\n",
      "Epoch 1078: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.9186e-05 - rmse: 0.0045 - val_loss: 8.9123e-05 - val_rmse: 0.0045 - lr: 1.0000e-04\n",
      "Epoch 1079/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8122e-05 - rmse: 0.0044\n",
      "Epoch 1079: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.7708e-05 - rmse: 0.0044 - val_loss: 9.0574e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1080/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5546e-05 - rmse: 0.0041\n",
      "Epoch 1080: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.6067e-05 - rmse: 0.0042 - val_loss: 9.3688e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1081/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6599e-05 - rmse: 0.0042\n",
      "Epoch 1081: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.6291e-05 - rmse: 0.0042 - val_loss: 9.0305e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1082/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8094e-05 - rmse: 0.0044\n",
      "Epoch 1082: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.9284e-05 - rmse: 0.0045 - val_loss: 9.4268e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 1083/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9884e-05 - rmse: 0.0046\n",
      "Epoch 1083: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.3085e-05 - rmse: 0.0049 - val_loss: 9.0702e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1084/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6646e-05 - rmse: 0.0042\n",
      "Epoch 1084: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 8.6647e-05 - rmse: 0.0042 - val_loss: 8.7336e-05 - val_rmse: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 1085/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5454e-05 - rmse: 0.0041\n",
      "Epoch 1085: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.6818e-05 - rmse: 0.0043 - val_loss: 8.9778e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1086/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7730e-05 - rmse: 0.0044\n",
      "Epoch 1086: val_loss did not improve from 0.00009\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.7021e-05 - rmse: 0.0043 - val_loss: 8.9506e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1087/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9388e-05 - rmse: 0.0046\n",
      "Epoch 1087: val_loss improved from 0.00009 to 0.00008, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 8.7905e-05 - rmse: 0.0044 - val_loss: 8.4766e-05 - val_rmse: 0.0040 - lr: 1.0000e-04\n",
      "Epoch 1088/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4363e-05 - rmse: 0.0040\n",
      "Epoch 1088: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.8829e-05 - rmse: 0.0045 - val_loss: 1.0266e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 1089/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7051e-05 - rmse: 0.0053\n",
      "Epoch 1089: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.6015e-05 - rmse: 0.0052 - val_loss: 8.8109e-05 - val_rmse: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 1090/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.2019e-05 - rmse: 0.0037\n",
      "Epoch 1090: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0970e-04 - rmse: 0.0064 - val_loss: 1.0294e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1091/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9680e-05 - rmse: 0.0056\n",
      "Epoch 1091: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1833e-04 - rmse: 0.0071 - val_loss: 1.0558e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 1092/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0313e-04 - rmse: 0.0059\n",
      "Epoch 1092: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0551e-04 - rmse: 0.0061 - val_loss: 1.2528e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1093/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2103e-04 - rmse: 0.0073\n",
      "Epoch 1093: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0851e-04 - rmse: 0.0063 - val_loss: 1.1482e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 1094/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0985e-04 - rmse: 0.0064\n",
      "Epoch 1094: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0097e-04 - rmse: 0.0057 - val_loss: 1.2173e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 1095/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1575e-04 - rmse: 0.0069\n",
      "Epoch 1095: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0449e-04 - rmse: 0.0060 - val_loss: 1.2501e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1096/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2538e-04 - rmse: 0.0076\n",
      "Epoch 1096: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0965e-04 - rmse: 0.0064 - val_loss: 1.2472e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1097/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1412e-04 - rmse: 0.0068\n",
      "Epoch 1097: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0634e-04 - rmse: 0.0062 - val_loss: 1.1788e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 1098/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2307e-04 - rmse: 0.0074\n",
      "Epoch 1098: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1064e-04 - rmse: 0.0065 - val_loss: 1.4501e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1099/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5138e-04 - rmse: 0.0091\n",
      "Epoch 1099: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2578e-04 - rmse: 0.0076 - val_loss: 1.2934e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 1100/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2948e-04 - rmse: 0.0078\n",
      "Epoch 1100: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2223e-04 - rmse: 0.0074 - val_loss: 9.3950e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 1101/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2174e-05 - rmse: 0.0049\n",
      "Epoch 1101: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2046e-04 - rmse: 0.0072 - val_loss: 9.7674e-05 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 1102/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0241e-04 - rmse: 0.0059\n",
      "Epoch 1102: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2804e-04 - rmse: 0.0077 - val_loss: 1.4630e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1103/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7010e-04 - rmse: 0.0101\n",
      "Epoch 1103: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3725e-04 - rmse: 0.0083 - val_loss: 1.1514e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 1104/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2698e-04 - rmse: 0.0077\n",
      "Epoch 1104: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0913e-04 - rmse: 0.0064 - val_loss: 8.5484e-05 - val_rmse: 0.0042 - lr: 1.0000e-04\n",
      "Epoch 1105/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3508e-05 - rmse: 0.0039\n",
      "Epoch 1105: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0526e-04 - rmse: 0.0061 - val_loss: 1.0024e-04 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 1106/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6185e-05 - rmse: 0.0053\n",
      "Epoch 1106: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1444e-04 - rmse: 0.0068 - val_loss: 1.0862e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 1107/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0492e-04 - rmse: 0.0061\n",
      "Epoch 1107: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0121e-04 - rmse: 0.0058 - val_loss: 1.3439e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 1108/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2425e-04 - rmse: 0.0075\n",
      "Epoch 1108: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1320e-04 - rmse: 0.0067 - val_loss: 1.2770e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1109/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2290e-04 - rmse: 0.0074\n",
      "Epoch 1109: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1519e-04 - rmse: 0.0069 - val_loss: 8.7648e-05 - val_rmse: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 1110/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6930e-05 - rmse: 0.0044\n",
      "Epoch 1110: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1104e-04 - rmse: 0.0066 - val_loss: 1.0528e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 1111/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1061e-04 - rmse: 0.0065\n",
      "Epoch 1111: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1982e-04 - rmse: 0.0072 - val_loss: 1.0972e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 1112/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2811e-04 - rmse: 0.0078\n",
      "Epoch 1112: val_loss did not improve from 0.00008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1040e-04 - rmse: 0.0065 - val_loss: 9.6781e-05 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 1113/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7616e-05 - rmse: 0.0055\n",
      "Epoch 1113: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0398e-04 - rmse: 0.0060 - val_loss: 9.7525e-05 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1114/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0123e-04 - rmse: 0.0058\n",
      "Epoch 1114: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.5894e-05 - rmse: 0.0053 - val_loss: 1.0258e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1115/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4010e-05 - rmse: 0.0051\n",
      "Epoch 1115: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 9.1221e-05 - rmse: 0.0048 - val_loss: 9.5812e-05 - val_rmse: 0.0053 - lr: 1.0000e-04\n",
      "Epoch 1116/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0196e-04 - rmse: 0.0058\n",
      "Epoch 1116: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.4921e-05 - rmse: 0.0052 - val_loss: 8.8551e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1117/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8379e-05 - rmse: 0.0045\n",
      "Epoch 1117: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.6223e-05 - rmse: 0.0043 - val_loss: 8.7932e-05 - val_rmse: 0.0045 - lr: 1.0000e-04\n",
      "Epoch 1118/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7570e-05 - rmse: 0.0045\n",
      "Epoch 1118: val_loss improved from 0.00008 to 0.00008, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 8.5537e-05 - rmse: 0.0042 - val_loss: 8.2136e-05 - val_rmse: 0.0038 - lr: 1.0000e-04\n",
      "Epoch 1119/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8586e-05 - rmse: 0.0033\n",
      "Epoch 1119: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.0491e-05 - rmse: 0.0036 - val_loss: 8.3887e-05 - val_rmse: 0.0040 - lr: 1.0000e-04\n",
      "Epoch 1120/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.2864e-05 - rmse: 0.0039\n",
      "Epoch 1120: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.2610e-05 - rmse: 0.0039 - val_loss: 1.0088e-04 - val_rmse: 0.0058 - lr: 1.0000e-04\n",
      "Epoch 1121/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0105e-04 - rmse: 0.0058\n",
      "Epoch 1121: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.3803e-05 - rmse: 0.0051 - val_loss: 9.9203e-05 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1122/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4554e-05 - rmse: 0.0052\n",
      "Epoch 1122: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.7933e-05 - rmse: 0.0055 - val_loss: 8.8449e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1123/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7579e-05 - rmse: 0.0045\n",
      "Epoch 1123: val_loss improved from 0.00008 to 0.00008, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 8.8020e-05 - rmse: 0.0045 - val_loss: 8.0956e-05 - val_rmse: 0.0037 - lr: 1.0000e-04\n",
      "Epoch 1124/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7221e-05 - rmse: 0.0031\n",
      "Epoch 1124: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.1103e-05 - rmse: 0.0037 - val_loss: 8.5916e-05 - val_rmse: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 1125/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4452e-05 - rmse: 0.0041\n",
      "Epoch 1125: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.3010e-05 - rmse: 0.0039 - val_loss: 8.5273e-05 - val_rmse: 0.0042 - lr: 1.0000e-04\n",
      "Epoch 1126/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.2347e-05 - rmse: 0.0039\n",
      "Epoch 1126: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.2935e-05 - rmse: 0.0039 - val_loss: 8.2311e-05 - val_rmse: 0.0039 - lr: 1.0000e-04\n",
      "Epoch 1127/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.0547e-05 - rmse: 0.0036\n",
      "Epoch 1127: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.0477e-05 - rmse: 0.0036 - val_loss: 8.1923e-05 - val_rmse: 0.0038 - lr: 1.0000e-04\n",
      "Epoch 1128/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9787e-05 - rmse: 0.0035\n",
      "Epoch 1128: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.9975e-05 - rmse: 0.0035 - val_loss: 8.4044e-05 - val_rmse: 0.0041 - lr: 1.0000e-04\n",
      "Epoch 1129/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9373e-05 - rmse: 0.0035\n",
      "Epoch 1129: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.1535e-05 - rmse: 0.0038 - val_loss: 8.6850e-05 - val_rmse: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 1130/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.2559e-05 - rmse: 0.0039\n",
      "Epoch 1130: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.2651e-05 - rmse: 0.0039 - val_loss: 8.5480e-05 - val_rmse: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 1131/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3142e-05 - rmse: 0.0040\n",
      "Epoch 1131: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.6604e-05 - rmse: 0.0044 - val_loss: 9.3747e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 1132/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3869e-05 - rmse: 0.0051\n",
      "Epoch 1132: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.8715e-05 - rmse: 0.0046 - val_loss: 9.8567e-05 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1133/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4323e-05 - rmse: 0.0052\n",
      "Epoch 1133: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.2741e-05 - rmse: 0.0050 - val_loss: 9.1302e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1134/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7462e-05 - rmse: 0.0045\n",
      "Epoch 1134: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.6737e-05 - rmse: 0.0054 - val_loss: 8.3561e-05 - val_rmse: 0.0040 - lr: 1.0000e-04\n",
      "Epoch 1135/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8892e-05 - rmse: 0.0034\n",
      "Epoch 1135: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.5716e-05 - rmse: 0.0053 - val_loss: 9.0424e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1136/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6543e-05 - rmse: 0.0044\n",
      "Epoch 1136: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.8383e-05 - rmse: 0.0046 - val_loss: 9.0107e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1137/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8162e-05 - rmse: 0.0046\n",
      "Epoch 1137: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.6073e-05 - rmse: 0.0043 - val_loss: 8.1498e-05 - val_rmse: 0.0038 - lr: 1.0000e-04\n",
      "Epoch 1138/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8099e-05 - rmse: 0.0033\n",
      "Epoch 1138: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.4427e-05 - rmse: 0.0042 - val_loss: 8.2502e-05 - val_rmse: 0.0039 - lr: 1.0000e-04\n",
      "Epoch 1139/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8283e-05 - rmse: 0.0033\n",
      "Epoch 1139: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.2439e-05 - rmse: 0.0050 - val_loss: 9.9343e-05 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 1140/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0053e-04 - rmse: 0.0058\n",
      "Epoch 1140: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4213e-05 - rmse: 0.0052 - val_loss: 8.3399e-05 - val_rmse: 0.0040 - lr: 1.0000e-04\n",
      "Epoch 1141/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3325e-05 - rmse: 0.0040\n",
      "Epoch 1141: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.0859e-05 - rmse: 0.0049 - val_loss: 1.0228e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1142/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5356e-05 - rmse: 0.0053\n",
      "Epoch 1142: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1680e-04 - rmse: 0.0071 - val_loss: 1.0556e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 1143/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0620e-04 - rmse: 0.0063\n",
      "Epoch 1143: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0955e-04 - rmse: 0.0065 - val_loss: 1.0161e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1144/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0097e-04 - rmse: 0.0058\n",
      "Epoch 1144: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4825e-05 - rmse: 0.0053 - val_loss: 9.3386e-05 - val_rmse: 0.0051 - lr: 1.0000e-04\n",
      "Epoch 1145/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9451e-05 - rmse: 0.0047\n",
      "Epoch 1145: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.7353e-05 - rmse: 0.0045 - val_loss: 9.0592e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1146/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2261e-05 - rmse: 0.0050\n",
      "Epoch 1146: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.5191e-05 - rmse: 0.0043 - val_loss: 8.2668e-05 - val_rmse: 0.0040 - lr: 1.0000e-04\n",
      "Epoch 1147/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.0359e-05 - rmse: 0.0037\n",
      "Epoch 1147: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 8.6323e-05 - rmse: 0.0044 - val_loss: 8.4790e-05 - val_rmse: 0.0042 - lr: 1.0000e-04\n",
      "Epoch 1148/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1969e-05 - rmse: 0.0039\n",
      "Epoch 1148: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.3580e-05 - rmse: 0.0041 - val_loss: 8.5285e-05 - val_rmse: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 1149/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5272e-05 - rmse: 0.0043\n",
      "Epoch 1149: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.4697e-05 - rmse: 0.0042 - val_loss: 8.1970e-05 - val_rmse: 0.0039 - lr: 1.0000e-04\n",
      "Epoch 1150/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9963e-05 - rmse: 0.0036\n",
      "Epoch 1150: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.6828e-05 - rmse: 0.0045 - val_loss: 9.0476e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1151/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7617e-05 - rmse: 0.0046\n",
      "Epoch 1151: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.3952e-05 - rmse: 0.0041 - val_loss: 8.5763e-05 - val_rmse: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 1152/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6124e-05 - rmse: 0.0044\n",
      "Epoch 1152: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.5329e-05 - rmse: 0.0043 - val_loss: 9.0175e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1153/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7924e-05 - rmse: 0.0046\n",
      "Epoch 1153: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.4811e-05 - rmse: 0.0042 - val_loss: 8.9844e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1154/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6880e-05 - rmse: 0.0045\n",
      "Epoch 1154: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.8461e-05 - rmse: 0.0047 - val_loss: 9.3900e-05 - val_rmse: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 1155/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8268e-05 - rmse: 0.0056\n",
      "Epoch 1155: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.2511e-05 - rmse: 0.0051 - val_loss: 1.0816e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 1156/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0626e-04 - rmse: 0.0063\n",
      "Epoch 1156: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.6715e-05 - rmse: 0.0055 - val_loss: 1.1702e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 1157/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0935e-04 - rmse: 0.0065\n",
      "Epoch 1157: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0749e-04 - rmse: 0.0064 - val_loss: 8.3911e-05 - val_rmse: 0.0042 - lr: 1.0000e-04\n",
      "Epoch 1158/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3662e-05 - rmse: 0.0041\n",
      "Epoch 1158: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.0130e-05 - rmse: 0.0048 - val_loss: 8.9361e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1159/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3013e-05 - rmse: 0.0051\n",
      "Epoch 1159: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0841e-04 - rmse: 0.0065 - val_loss: 1.0295e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 1160/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8694e-05 - rmse: 0.0057\n",
      "Epoch 1160: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0294e-04 - rmse: 0.0060 - val_loss: 9.5256e-05 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 1161/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2910e-05 - rmse: 0.0051\n",
      "Epoch 1161: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.7943e-05 - rmse: 0.0056 - val_loss: 8.6171e-05 - val_rmse: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 1162/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3045e-05 - rmse: 0.0041\n",
      "Epoch 1162: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0107e-04 - rmse: 0.0059 - val_loss: 8.9412e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1163/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8516e-05 - rmse: 0.0047\n",
      "Epoch 1163: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0738e-04 - rmse: 0.0064 - val_loss: 1.0079e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1164/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0934e-04 - rmse: 0.0065\n",
      "Epoch 1164: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0524e-04 - rmse: 0.0062 - val_loss: 1.1822e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 1165/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1304e-04 - rmse: 0.0068\n",
      "Epoch 1165: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0421e-04 - rmse: 0.0061 - val_loss: 1.1345e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 1166/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1569e-04 - rmse: 0.0070\n",
      "Epoch 1166: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1321e-04 - rmse: 0.0068 - val_loss: 1.3025e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 1167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3394e-04 - rmse: 0.0082\n",
      "Epoch 1167: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2288e-04 - rmse: 0.0075 - val_loss: 1.7321e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1168/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6100e-04 - rmse: 0.0097\n",
      "Epoch 1168: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3486e-04 - rmse: 0.0083 - val_loss: 2.5855e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1169/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5295e-04 - rmse: 0.0137\n",
      "Epoch 1169: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0955e-04 - rmse: 0.0120 - val_loss: 1.3544e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1170/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3863e-04 - rmse: 0.0085\n",
      "Epoch 1170: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4207e-04 - rmse: 0.0087 - val_loss: 1.5900e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1171/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5578e-04 - rmse: 0.0095\n",
      "Epoch 1171: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2475e-04 - rmse: 0.0076 - val_loss: 1.2563e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1172/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3576e-04 - rmse: 0.0083\n",
      "Epoch 1172: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1964e-04 - rmse: 0.0073 - val_loss: 1.4029e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1173/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4724e-04 - rmse: 0.0090\n",
      "Epoch 1173: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3991e-04 - rmse: 0.0086 - val_loss: 1.2208e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1174/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1341e-04 - rmse: 0.0069\n",
      "Epoch 1174: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2057e-04 - rmse: 0.0074 - val_loss: 8.1143e-05 - val_rmse: 0.0039 - lr: 1.0000e-04\n",
      "Epoch 1175/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8489e-05 - rmse: 0.0035\n",
      "Epoch 1175: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.8375e-05 - rmse: 0.0047 - val_loss: 9.6291e-05 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1176/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0467e-04 - rmse: 0.0062\n",
      "Epoch 1176: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.2878e-04 - rmse: 0.0079 - val_loss: 1.0600e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 1177/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0488e-04 - rmse: 0.0062\n",
      "Epoch 1177: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0515e-04 - rmse: 0.0062 - val_loss: 1.2803e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 1178/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3794e-04 - rmse: 0.0085\n",
      "Epoch 1178: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1041e-04 - rmse: 0.0067 - val_loss: 9.8041e-05 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1179/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7643e-05 - rmse: 0.0056\n",
      "Epoch 1179: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4031e-04 - rmse: 0.0086 - val_loss: 1.0402e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 1180/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.8358e-05 - rmse: 0.0057\n",
      "Epoch 1180: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4896e-04 - rmse: 0.0091 - val_loss: 1.3501e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1181/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2974e-04 - rmse: 0.0080\n",
      "Epoch 1181: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1678e-04 - rmse: 0.0071 - val_loss: 1.3722e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1182/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3395e-04 - rmse: 0.0082\n",
      "Epoch 1182: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1114e-04 - rmse: 0.0067 - val_loss: 9.6953e-05 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1183/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0103e-04 - rmse: 0.0059\n",
      "Epoch 1183: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0633e-04 - rmse: 0.0063 - val_loss: 8.5619e-05 - val_rmse: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 1184/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7111e-05 - rmse: 0.0046\n",
      "Epoch 1184: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.9328e-05 - rmse: 0.0058 - val_loss: 1.0705e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 1185/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0581e-04 - rmse: 0.0063\n",
      "Epoch 1185: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.6982e-05 - rmse: 0.0056 - val_loss: 1.0577e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 1186/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7257e-05 - rmse: 0.0056\n",
      "Epoch 1186: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2269e-04 - rmse: 0.0075 - val_loss: 9.8119e-05 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 1187/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3999e-05 - rmse: 0.0053\n",
      "Epoch 1187: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0618e-04 - rmse: 0.0063 - val_loss: 1.0144e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 1188/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9143e-05 - rmse: 0.0058\n",
      "Epoch 1188: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.0305e-04 - rmse: 0.0061 - val_loss: 1.1821e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 1189/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1099e-04 - rmse: 0.0067\n",
      "Epoch 1189: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1304e-04 - rmse: 0.0069 - val_loss: 1.2365e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 1190/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1500e-04 - rmse: 0.0070\n",
      "Epoch 1190: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0033e-04 - rmse: 0.0059 - val_loss: 1.0467e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 1191/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0097e-04 - rmse: 0.0059\n",
      "Epoch 1191: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.7487e-05 - rmse: 0.0056 - val_loss: 9.7296e-05 - val_rmse: 0.0056 - lr: 1.0000e-04\n",
      "Epoch 1192/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6616e-05 - rmse: 0.0055\n",
      "Epoch 1192: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.2059e-05 - rmse: 0.0051 - val_loss: 9.0943e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1193/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1605e-05 - rmse: 0.0051\n",
      "Epoch 1193: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.9209e-05 - rmse: 0.0058 - val_loss: 8.7893e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1194/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5234e-05 - rmse: 0.0044\n",
      "Epoch 1194: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.8179e-05 - rmse: 0.0047 - val_loss: 8.5731e-05 - val_rmse: 0.0045 - lr: 1.0000e-04\n",
      "Epoch 1195/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9405e-05 - rmse: 0.0049\n",
      "Epoch 1195: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.4670e-05 - rmse: 0.0044 - val_loss: 8.2916e-05 - val_rmse: 0.0041 - lr: 1.0000e-04\n",
      "Epoch 1196/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9029e-05 - rmse: 0.0036\n",
      "Epoch 1196: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.0426e-05 - rmse: 0.0038 - val_loss: 8.9337e-05 - val_rmse: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 1197/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1263e-05 - rmse: 0.0051\n",
      "Epoch 1197: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.5219e-05 - rmse: 0.0044 - val_loss: 9.9974e-05 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1198/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7349e-05 - rmse: 0.0047\n",
      "Epoch 1198: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.1345e-05 - rmse: 0.0051 - val_loss: 8.6448e-05 - val_rmse: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 1199/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.2420e-05 - rmse: 0.0041\n",
      "Epoch 1199: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.6385e-05 - rmse: 0.0055 - val_loss: 1.2138e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1200/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2201e-04 - rmse: 0.0075\n",
      "Epoch 1200: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0296e-04 - rmse: 0.0061 - val_loss: 9.8534e-05 - val_rmse: 0.0057 - lr: 1.0000e-04\n",
      "Epoch 1201/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1554e-05 - rmse: 0.0051\n",
      "Epoch 1201: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4979e-05 - rmse: 0.0054 - val_loss: 1.0822e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 1202/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0778e-04 - rmse: 0.0065\n",
      "Epoch 1202: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4780e-05 - rmse: 0.0054 - val_loss: 1.0330e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 1203/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0128e-04 - rmse: 0.0060\n",
      "Epoch 1203: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.1597e-05 - rmse: 0.0051 - val_loss: 1.0466e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 1204/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0343e-04 - rmse: 0.0062\n",
      "Epoch 1204: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4563e-05 - rmse: 0.0054 - val_loss: 1.2124e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1205/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1837e-04 - rmse: 0.0073\n",
      "Epoch 1205: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0723e-04 - rmse: 0.0065 - val_loss: 1.0425e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 1206/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1011e-04 - rmse: 0.0067\n",
      "Epoch 1206: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3381e-04 - rmse: 0.0083 - val_loss: 9.2828e-05 - val_rmse: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 1207/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.0093e-05 - rmse: 0.0050\n",
      "Epoch 1207: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0797e-04 - rmse: 0.0065 - val_loss: 8.1137e-05 - val_rmse: 0.0040 - lr: 1.0000e-04\n",
      "Epoch 1208/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8720e-05 - rmse: 0.0036\n",
      "Epoch 1208: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.7537e-05 - rmse: 0.0057 - val_loss: 8.2924e-05 - val_rmse: 0.0042 - lr: 1.0000e-04\n",
      "Epoch 1209/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.0123e-05 - rmse: 0.0038\n",
      "Epoch 1209: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.9041e-05 - rmse: 0.0058 - val_loss: 1.0180e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 1210/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4822e-05 - rmse: 0.0054\n",
      "Epoch 1210: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.8070e-05 - rmse: 0.0048 - val_loss: 9.4365e-05 - val_rmse: 0.0054 - lr: 1.0000e-04\n",
      "Epoch 1211/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3372e-05 - rmse: 0.0053\n",
      "Epoch 1211: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.9482e-05 - rmse: 0.0049 - val_loss: 1.1505e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 1212/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1229e-04 - rmse: 0.0069\n",
      "Epoch 1212: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.4777e-05 - rmse: 0.0054 - val_loss: 1.0819e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 1213/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0135e-04 - rmse: 0.0060\n",
      "Epoch 1213: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.8572e-05 - rmse: 0.0048 - val_loss: 9.5370e-05 - val_rmse: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 1214/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5878e-05 - rmse: 0.0055\n",
      "Epoch 1214: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.7688e-05 - rmse: 0.0047 - val_loss: 1.0006e-04 - val_rmse: 0.0059 - lr: 1.0000e-04\n",
      "Epoch 1215/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0316e-04 - rmse: 0.0062\n",
      "Epoch 1215: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.5623e-05 - rmse: 0.0055 - val_loss: 8.8093e-05 - val_rmse: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 1216/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3332e-05 - rmse: 0.0043\n",
      "Epoch 1216: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.3263e-05 - rmse: 0.0042 - val_loss: 8.7505e-05 - val_rmse: 0.0047 - lr: 1.0000e-04\n",
      "Epoch 1217/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5645e-05 - rmse: 0.0045\n",
      "Epoch 1217: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.7976e-05 - rmse: 0.0048 - val_loss: 9.0528e-05 - val_rmse: 0.0050 - lr: 1.0000e-04\n",
      "Epoch 1218/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.7866e-05 - rmse: 0.0048\n",
      "Epoch 1218: val_loss did not improve from 0.00008\n",
      "\n",
      "Epoch 1218: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.8725e-05 - rmse: 0.0049 - val_loss: 8.2192e-05 - val_rmse: 0.0041 - lr: 1.0000e-04\n",
      "Epoch 1219/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9814e-05 - rmse: 0.0038\n",
      "Epoch 1219: val_loss improved from 0.00008 to 0.00008, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 7.8860e-05 - rmse: 0.0037 - val_loss: 7.7136e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1220/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6423e-05 - rmse: 0.0034\n",
      "Epoch 1220: val_loss improved from 0.00008 to 0.00008, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 7.5939e-05 - rmse: 0.0033 - val_loss: 7.5256e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1221/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5199e-05 - rmse: 0.0032\n",
      "Epoch 1221: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.5243e-05 - rmse: 0.0032 - val_loss: 7.7309e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1222/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4369e-05 - rmse: 0.0030\n",
      "Epoch 1222: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3707e-05 - rmse: 0.0029 - val_loss: 7.7297e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1223/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5478e-05 - rmse: 0.0032\n",
      "Epoch 1223: val_loss did not improve from 0.00008\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4321e-05 - rmse: 0.0030 - val_loss: 7.5913e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1224/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1988e-05 - rmse: 0.0026\n",
      "Epoch 1224: val_loss improved from 0.00008 to 0.00008, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 7.3421e-05 - rmse: 0.0029 - val_loss: 7.5156e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1225/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2987e-05 - rmse: 0.0028\n",
      "Epoch 1225: val_loss improved from 0.00008 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 7.4075e-05 - rmse: 0.0030 - val_loss: 7.4724e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1226/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1586e-05 - rmse: 0.0026\n",
      "Epoch 1226: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4717e-05 - rmse: 0.0031 - val_loss: 7.7565e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1227/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4202e-05 - rmse: 0.0030\n",
      "Epoch 1227: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.4649e-05 - rmse: 0.0031 - val_loss: 7.7252e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1228/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5083e-05 - rmse: 0.0032\n",
      "Epoch 1228: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4150e-05 - rmse: 0.0030 - val_loss: 7.6026e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1229/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6445e-05 - rmse: 0.0034\n",
      "Epoch 1229: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4659e-05 - rmse: 0.0031 - val_loss: 7.5182e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1230/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1671e-05 - rmse: 0.0026\n",
      "Epoch 1230: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3385e-05 - rmse: 0.0029 - val_loss: 7.6272e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1231/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3640e-05 - rmse: 0.0029\n",
      "Epoch 1231: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.5232e-05 - rmse: 0.0032 - val_loss: 7.6222e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1232/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3265e-05 - rmse: 0.0029\n",
      "Epoch 1232: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3555e-05 - rmse: 0.0029 - val_loss: 7.5196e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1233/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5425e-05 - rmse: 0.0032\n",
      "Epoch 1233: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 7.4437e-05 - rmse: 0.0031 - val_loss: 7.4707e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1234/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3289e-05 - rmse: 0.0029\n",
      "Epoch 1234: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3881e-05 - rmse: 0.0030 - val_loss: 7.7487e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1235/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7985e-05 - rmse: 0.0036\n",
      "Epoch 1235: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.5075e-05 - rmse: 0.0032 - val_loss: 7.5616e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1236/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4619e-05 - rmse: 0.0031\n",
      "Epoch 1236: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.4012e-05 - rmse: 0.0030 - val_loss: 7.6420e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1237/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4738e-05 - rmse: 0.0031\n",
      "Epoch 1237: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.4512e-05 - rmse: 0.0031 - val_loss: 7.6661e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1238/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3389e-05 - rmse: 0.0029\n",
      "Epoch 1238: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2904e-05 - rmse: 0.0028 - val_loss: 7.5077e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1239/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4868e-05 - rmse: 0.0032\n",
      "Epoch 1239: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4021e-05 - rmse: 0.0030 - val_loss: 7.5704e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1240/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3689e-05 - rmse: 0.0030\n",
      "Epoch 1240: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3793e-05 - rmse: 0.0030 - val_loss: 7.5566e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1241/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3521e-05 - rmse: 0.0029\n",
      "Epoch 1241: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3713e-05 - rmse: 0.0030 - val_loss: 7.5185e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1242/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5864e-05 - rmse: 0.0033\n",
      "Epoch 1242: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4096e-05 - rmse: 0.0030 - val_loss: 7.5823e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1243/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3496e-05 - rmse: 0.0029\n",
      "Epoch 1243: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3882e-05 - rmse: 0.0030 - val_loss: 7.7124e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1244/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4353e-05 - rmse: 0.0031\n",
      "Epoch 1244: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 7.4498e-05 - rmse: 0.0031 - val_loss: 7.4411e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1245/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3045e-05 - rmse: 0.0029\n",
      "Epoch 1245: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3038e-05 - rmse: 0.0029 - val_loss: 7.4798e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1246/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2351e-05 - rmse: 0.0027\n",
      "Epoch 1246: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2961e-05 - rmse: 0.0029 - val_loss: 7.5824e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1247/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3802e-05 - rmse: 0.0030\n",
      "Epoch 1247: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3554e-05 - rmse: 0.0030 - val_loss: 7.6985e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1248/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5000e-05 - rmse: 0.0032\n",
      "Epoch 1248: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 7.3821e-05 - rmse: 0.0030 - val_loss: 7.4391e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1249/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0826e-05 - rmse: 0.0025\n",
      "Epoch 1249: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3306e-05 - rmse: 0.0029 - val_loss: 7.5078e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1250/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1036e-05 - rmse: 0.0025\n",
      "Epoch 1250: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2893e-05 - rmse: 0.0028 - val_loss: 7.5672e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1251/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3414e-05 - rmse: 0.0029\n",
      "Epoch 1251: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4653e-05 - rmse: 0.0031 - val_loss: 7.4704e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1252/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1544e-05 - rmse: 0.0026\n",
      "Epoch 1252: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3123e-05 - rmse: 0.0029 - val_loss: 7.4507e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1253/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2735e-05 - rmse: 0.0028\n",
      "Epoch 1253: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2915e-05 - rmse: 0.0029 - val_loss: 7.4635e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1254/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2543e-05 - rmse: 0.0028\n",
      "Epoch 1254: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3053e-05 - rmse: 0.0029 - val_loss: 7.5264e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1255/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2557e-05 - rmse: 0.0028\n",
      "Epoch 1255: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2639e-05 - rmse: 0.0028 - val_loss: 7.4702e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1256/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3598e-05 - rmse: 0.0030\n",
      "Epoch 1256: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2662e-05 - rmse: 0.0028 - val_loss: 7.4608e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1257/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3253e-05 - rmse: 0.0029\n",
      "Epoch 1257: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2481e-05 - rmse: 0.0028 - val_loss: 7.4860e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1258/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2382e-05 - rmse: 0.0028\n",
      "Epoch 1258: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 7.2663e-05 - rmse: 0.0028 - val_loss: 7.4321e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1259/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2726e-05 - rmse: 0.0028\n",
      "Epoch 1259: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3321e-05 - rmse: 0.0029 - val_loss: 7.4553e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1260/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1420e-05 - rmse: 0.0026\n",
      "Epoch 1260: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.1634e-05 - rmse: 0.0026 - val_loss: 7.5363e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1261/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3313e-05 - rmse: 0.0029\n",
      "Epoch 1261: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 7.2869e-05 - rmse: 0.0029 - val_loss: 7.4238e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1262/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1115e-05 - rmse: 0.0025\n",
      "Epoch 1262: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 7.2212e-05 - rmse: 0.0028 - val_loss: 7.3734e-05 - val_rmse: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 1263/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2931e-05 - rmse: 0.0029\n",
      "Epoch 1263: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2518e-05 - rmse: 0.0028 - val_loss: 7.4329e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1264/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1505e-05 - rmse: 0.0026\n",
      "Epoch 1264: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2903e-05 - rmse: 0.0029 - val_loss: 7.4722e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1265/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2265e-05 - rmse: 0.0028\n",
      "Epoch 1265: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2307e-05 - rmse: 0.0028 - val_loss: 7.4192e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1266/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3079e-05 - rmse: 0.0029\n",
      "Epoch 1266: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2255e-05 - rmse: 0.0028 - val_loss: 7.4709e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1267/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4125e-05 - rmse: 0.0031\n",
      "Epoch 1267: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2939e-05 - rmse: 0.0029 - val_loss: 7.5386e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1268/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3632e-05 - rmse: 0.0030\n",
      "Epoch 1268: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3280e-05 - rmse: 0.0030 - val_loss: 7.5264e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1269/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3515e-05 - rmse: 0.0030\n",
      "Epoch 1269: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2759e-05 - rmse: 0.0029 - val_loss: 7.6033e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1270/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5154e-05 - rmse: 0.0033\n",
      "Epoch 1270: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3291e-05 - rmse: 0.0030 - val_loss: 7.4304e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1271/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1781e-05 - rmse: 0.0027\n",
      "Epoch 1271: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2503e-05 - rmse: 0.0028 - val_loss: 7.4227e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1272/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1950e-05 - rmse: 0.0027\n",
      "Epoch 1272: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3353e-05 - rmse: 0.0030 - val_loss: 7.4575e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1273/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4511e-05 - rmse: 0.0032\n",
      "Epoch 1273: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2670e-05 - rmse: 0.0029 - val_loss: 7.4165e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1274/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3204e-05 - rmse: 0.0029\n",
      "Epoch 1274: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3939e-05 - rmse: 0.0031 - val_loss: 7.5726e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1275/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4524e-05 - rmse: 0.0032\n",
      "Epoch 1275: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4784e-05 - rmse: 0.0032 - val_loss: 7.4589e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1276/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3848e-05 - rmse: 0.0031\n",
      "Epoch 1276: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.4074e-05 - rmse: 0.0031 - val_loss: 7.5996e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1277/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4634e-05 - rmse: 0.0032\n",
      "Epoch 1277: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3066e-05 - rmse: 0.0029 - val_loss: 7.3773e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1278/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2254e-05 - rmse: 0.0028\n",
      "Epoch 1278: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2385e-05 - rmse: 0.0028 - val_loss: 7.4196e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1279/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2193e-05 - rmse: 0.0028\n",
      "Epoch 1279: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2932e-05 - rmse: 0.0029 - val_loss: 7.5124e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1280/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1533e-05 - rmse: 0.0027\n",
      "Epoch 1280: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2770e-05 - rmse: 0.0029 - val_loss: 7.4432e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1281/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2309e-05 - rmse: 0.0028\n",
      "Epoch 1281: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3455e-05 - rmse: 0.0030 - val_loss: 7.4229e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1282/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2186e-05 - rmse: 0.0028\n",
      "Epoch 1282: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2039e-05 - rmse: 0.0028 - val_loss: 7.4656e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1283/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3151e-05 - rmse: 0.0030\n",
      "Epoch 1283: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 7.3197e-05 - rmse: 0.0030 - val_loss: 7.3683e-05 - val_rmse: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 1284/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1857e-05 - rmse: 0.0027\n",
      "Epoch 1284: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2206e-05 - rmse: 0.0028 - val_loss: 7.3802e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1285/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1547e-05 - rmse: 0.0027\n",
      "Epoch 1285: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 7.2088e-05 - rmse: 0.0028 - val_loss: 7.3671e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1286/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3173e-05 - rmse: 0.0030\n",
      "Epoch 1286: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2718e-05 - rmse: 0.0029 - val_loss: 7.3875e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1287/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2008e-05 - rmse: 0.0028\n",
      "Epoch 1287: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2531e-05 - rmse: 0.0029 - val_loss: 7.6322e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1288/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3277e-05 - rmse: 0.0030\n",
      "Epoch 1288: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3239e-05 - rmse: 0.0030 - val_loss: 7.4188e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1289/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2544e-05 - rmse: 0.0029\n",
      "Epoch 1289: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3126e-05 - rmse: 0.0030 - val_loss: 7.4723e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1290/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4262e-05 - rmse: 0.0032\n",
      "Epoch 1290: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3467e-05 - rmse: 0.0030 - val_loss: 7.7104e-05 - val_rmse: 0.0036 - lr: 5.0000e-05\n",
      "Epoch 1291/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5458e-05 - rmse: 0.0033\n",
      "Epoch 1291: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3945e-05 - rmse: 0.0031 - val_loss: 7.7775e-05 - val_rmse: 0.0037 - lr: 5.0000e-05\n",
      "Epoch 1292/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7707e-05 - rmse: 0.0037\n",
      "Epoch 1292: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.5785e-05 - rmse: 0.0034 - val_loss: 7.4013e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1293/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1720e-05 - rmse: 0.0027\n",
      "Epoch 1293: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2572e-05 - rmse: 0.0029 - val_loss: 7.4384e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1294/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5678e-05 - rmse: 0.0034\n",
      "Epoch 1294: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3177e-05 - rmse: 0.0030 - val_loss: 7.4617e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1295/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3992e-05 - rmse: 0.0031\n",
      "Epoch 1295: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.1717e-05 - rmse: 0.0027 - val_loss: 7.4397e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1296/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1962e-05 - rmse: 0.0028\n",
      "Epoch 1296: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 7.2430e-05 - rmse: 0.0029 - val_loss: 7.3190e-05 - val_rmse: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 1297/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9967e-05 - rmse: 0.0024\n",
      "Epoch 1297: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 7.1712e-05 - rmse: 0.0027 - val_loss: 7.2858e-05 - val_rmse: 0.0029 - lr: 5.0000e-05\n",
      "Epoch 1298/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0641e-05 - rmse: 0.0025\n",
      "Epoch 1298: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2242e-05 - rmse: 0.0028 - val_loss: 7.3204e-05 - val_rmse: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 1299/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0038e-05 - rmse: 0.0024\n",
      "Epoch 1299: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.3479e-05 - rmse: 0.0030 - val_loss: 7.4850e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1300/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3259e-05 - rmse: 0.0030\n",
      "Epoch 1300: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3464e-05 - rmse: 0.0030 - val_loss: 7.8392e-05 - val_rmse: 0.0038 - lr: 5.0000e-05\n",
      "Epoch 1301/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4793e-05 - rmse: 0.0033\n",
      "Epoch 1301: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.5280e-05 - rmse: 0.0033 - val_loss: 7.5526e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1302/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7825e-05 - rmse: 0.0037\n",
      "Epoch 1302: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.7087e-05 - rmse: 0.0036 - val_loss: 7.7123e-05 - val_rmse: 0.0036 - lr: 5.0000e-05\n",
      "Epoch 1303/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6035e-05 - rmse: 0.0034\n",
      "Epoch 1303: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.6800e-05 - rmse: 0.0036 - val_loss: 7.9604e-05 - val_rmse: 0.0039 - lr: 5.0000e-05\n",
      "Epoch 1304/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7180e-05 - rmse: 0.0036\n",
      "Epoch 1304: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.5295e-05 - rmse: 0.0033 - val_loss: 7.4727e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1305/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4470e-05 - rmse: 0.0032\n",
      "Epoch 1305: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.5948e-05 - rmse: 0.0034 - val_loss: 7.5115e-05 - val_rmse: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 1306/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3465e-05 - rmse: 0.0031\n",
      "Epoch 1306: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4271e-05 - rmse: 0.0032 - val_loss: 7.7772e-05 - val_rmse: 0.0037 - lr: 5.0000e-05\n",
      "Epoch 1307/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5633e-05 - rmse: 0.0034\n",
      "Epoch 1307: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4862e-05 - rmse: 0.0033 - val_loss: 7.3297e-05 - val_rmse: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 1308/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0277e-05 - rmse: 0.0025\n",
      "Epoch 1308: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2757e-05 - rmse: 0.0029 - val_loss: 7.3760e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1309/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3340e-05 - rmse: 0.0030\n",
      "Epoch 1309: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.1673e-05 - rmse: 0.0028 - val_loss: 7.4610e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1310/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3270e-05 - rmse: 0.0030\n",
      "Epoch 1310: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 7.1744e-05 - rmse: 0.0028 - val_loss: 7.2985e-05 - val_rmse: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 1311/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1709e-05 - rmse: 0.0028\n",
      "Epoch 1311: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2209e-05 - rmse: 0.0029 - val_loss: 7.2983e-05 - val_rmse: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 1312/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1741e-05 - rmse: 0.0028\n",
      "Epoch 1312: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2061e-05 - rmse: 0.0028 - val_loss: 7.3542e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1313/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9921e-05 - rmse: 0.0024\n",
      "Epoch 1313: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2043e-05 - rmse: 0.0028 - val_loss: 7.6518e-05 - val_rmse: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 1314/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4021e-05 - rmse: 0.0032\n",
      "Epoch 1314: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\20221230\\Case13_WithParameters20221230unsteadyPrediction_MLP_Case13_WithParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 7.3085e-05 - rmse: 0.0030 - val_loss: 7.2653e-05 - val_rmse: 0.0029 - lr: 5.0000e-05\n",
      "Epoch 1315/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1904e-05 - rmse: 0.0028\n",
      "Epoch 1315: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.2653e-05 - rmse: 0.0029 - val_loss: 7.5667e-05 - val_rmse: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 1316/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6937e-05 - rmse: 0.0036\n",
      "Epoch 1316: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3908e-05 - rmse: 0.0031 - val_loss: 7.3629e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1317/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4346e-05 - rmse: 0.0032\n",
      "Epoch 1317: val_loss did not improve from 0.00007\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2299e-05 - rmse: 0.0029 - val_loss: 7.4145e-05 - val_rmse: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 1318/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1508e-05 - rmse: 0.0027Restoring model weights from the end of the best epoch: 1118.\n",
      "\n",
      "Epoch 1318: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 1318: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 7.2175e-05 - rmse: 0.0029 - val_loss: 7.3538e-05 - val_rmse: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 1318: early stopping\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size,\n",
    "                    validation_data=[x_val, y_val],\n",
    "                    steps_per_epoch = STEP_SIZE_TRAIN, validation_steps=VALIDATION_STEPS,\n",
    "                    epochs=10000, shuffle=True, callbacks=[es, ckpt, rp])\n",
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79204ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:01:30.217070\n"
     ]
    }
   ],
   "source": [
    "time = end - start\n",
    "print(\"Training time:\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fbce220",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = \"D:\\\\VAWT_data\\\\flap_unsteady\\\\result\\\\\"+\"20221230_MLP_Case13_WithParameters\\\\test\"+str(test_rate)+\"_val\"+str(val_rate)+\"_\"+str(n_layers)+\"layers_\"+ str(n_units) +\"units_CmPrediction\"\n",
    "if not os.path.exists(storage_dir):\n",
    "    os.makedirs(storage_dir)\n",
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77cc60d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = history.history\n",
    "plt.plot(hist['loss'], lw=2, label='Training loss')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation loss')\n",
    "plt.title('Training loss (mean squared error)\\nMLP, optimal settings, $C_m$ prediction', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "#plt.ylim([5e-5, 1e-1])\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"TrainingLoss_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e42b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2)\n",
    "plt.plot(hist['val_rmse'], lw=2)\n",
    "plt.title('Root Mean Squared Error, optimal settings\\n$C_m$ prediction', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "983b76cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2, label='Training RMSE')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation RMSE')\n",
    "plt.title('Root Mean Squared Error\\nMLP, optimal settings\\n$C_m$ prediction', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"RMSE_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c76ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 4ms/step - loss: 8.1095e-05 - rmse: 0.0037\n"
     ]
    }
   ],
   "source": [
    "train_results = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b1d836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 8.2136e-05 - rmse: 0.0038\n"
     ]
    }
   ],
   "source": [
    "val_results = model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abc70d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 8.5337e-05 - rmse: 0.0042\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "745feda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "decoded_train_ = model.predict(x_train)\n",
    "decoded_val_ = model.predict(x_val)\n",
    "decoded_test_ = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51faee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_index(cm, y):\n",
    "    return np.unique(np.where(np.isin(cm, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e8e16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_absolute(y_pred, y_true):\n",
    "    return np.abs(y_pred - y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aee7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize \n",
    "def denormalize(y):\n",
    "    return y*(np.max(cm)-np.min(cm))+np.min(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "946bb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_error(y_pred, y_real):\n",
    "    return np.sqrt(np.sum((y_pred - y_real)**2) / np.sum(y_real**2))\n",
    "\n",
    "def mape(y_pred, y_real):\n",
    "    return 100/len(y_real)*np.sum(np.abs((y_real-y_pred)/y_real))\n",
    "\n",
    "def smape(y_pred, y_real):\n",
    "    return 100*np.sum(np.abs(y_pred-y_real))/np.sum(y_real+y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb958632",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = define_index(y, y_train)\n",
    "index_val = define_index(y, y_val)\n",
    "index_test = define_index(y, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31fb219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008050168675626563\n",
      "0.3589081454872352\n"
     ]
    }
   ],
   "source": [
    "l2_error_train = l2_error(decoded_train_, y_train)\n",
    "mape_train = smape(decoded_train_, y_train)\n",
    "print(l2_error_train)\n",
    "print(mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c63ffb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007944121792075747\n",
      "0.3596195382616708\n"
     ]
    }
   ],
   "source": [
    "l2_error_val = l2_error(decoded_val_, y_val)\n",
    "mape_val= smape(decoded_val_, y_val)\n",
    "print(l2_error_val)\n",
    "print(mape_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3770434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00868330137461422\n",
      "0.40005691293986567\n"
     ]
    }
   ],
   "source": [
    "l2_error_test = l2_error(decoded_test_, y_test)\n",
    "mape_test= smape(decoded_test_, y_test)\n",
    "print(l2_error_test)\n",
    "print(mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "945ad132",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = denormalize(y_train)\n",
    "y_val = denormalize(y_val)\n",
    "y_test = denormalize(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93ca4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_train = denormalize(decoded_train_)\n",
    "decoded_val = denormalize(decoded_val_)\n",
    "decoded_test = denormalize(decoded_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55d01da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221230\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "os.chdir(model_directory)\n",
    "model_name = \"20221230unsteadyValidation_MLP_Case13_WithParameters_val_\"+str(val_rate)+\"_test\"+str(test_rate)+ \"_\" + str(n_layers) +\"layers_\"+str(n_units)+\"units_CmPrediction.h5\"\n",
    "model.save(model_name, overwrite=True, include_optimizer=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c38a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train_abs = error_absolute(decoded_train, y_train)\n",
    "error_val_abs = error_absolute(decoded_val, y_val)\n",
    "error_test_abs = error_absolute(decoded_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e21d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9785cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_train_list = []\n",
    "for i in range(0, len(x_train)):\n",
    "    l2_error_train_data = l2_error(decoded_train[i], y_train[i])\n",
    "    l2_error_train_list.append(l2_error_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "053a1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_error_val_list = []\n",
    "for i in range(0, len(x_val)):\n",
    "    l2_error_val_data = l2_error(decoded_val[i], y_val[i])\n",
    "    l2_error_val_list.append(l2_error_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72b6a696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_test_list = []\n",
    "for i in range(0, len(x_test)):\n",
    "    l2_error_test_data = l2_error(decoded_test[i], y_test[i])\n",
    "    l2_error_test_list.append(l2_error_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e5afbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_train_list = []\n",
    "for i in range(0, len(x_train)):\n",
    "    mape_train_data = smape(decoded_train[i], y_train[i])\n",
    "    mape_train_list.append(mape_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fa71fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_val_list = []\n",
    "for i in range(0, len(x_val)):\n",
    "    mape_val_data = smape(decoded_val[i], y_val[i])\n",
    "    mape_val_list.append(mape_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2f0fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_test_list = []\n",
    "for i in range(0, len(x_test)):\n",
    "    mape_test_data = smape(decoded_test[i], y_test[i])\n",
    "    mape_test_list.append(mape_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f75a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plot(y_pred, y_real, dataset=\"train\"):\n",
    "    dictionary_name = {\"train\":\"training\", \"val\":\"validation\", \"test\":\"test\"}\n",
    "    dictionary_data = {\"train\":l2_error_train_list, \"val\":l2_error_val_list, \"test\":l2_error_test_list}\n",
    "    dictionary_error = {\"train\":l2_error_train, \"val\":l2_error_val, \"test\":l2_error_test}\n",
    "    plot_title = '$L_2$ error norm distribution - MLP, unsteady, '+ dictionary_name.get(dataset) +'.\\nValidation rate {0}, test rate {1}, {2} layers, {3} units ($C_m$)'.format(\n",
    "        val_rate, test_rate, n_layers, n_units)\n",
    "    plt.plot(np.linspace(1,y_real.shape[0],y_real.shape[0]),\n",
    "             dictionary_error.get(dataset)*np.ones(y_real.shape[0],), 'k', lw=2.5)\n",
    "    plt.scatter(np.linspace(1, y_real.shape[0], y_real.shape[0]), dictionary_data.get(dataset), c='b')\n",
    "    plt.xlabel('Index', fontsize=15)\n",
    "    plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "    plt.yscale('log')\n",
    "    plt.title(plot_title, fontsize=15)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd795141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAAE1CAYAAAB0j+DkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABY50lEQVR4nO2de5geRZXwf2cmGTQJggwYlZAMGkQRXDWsivppIisiiCgioiMKollRV939XIWN3y7uGnU1soiKGDBETSQqwQuI4gqDdxfIKooXFIFAQLnFoCECgZzvj+pmenr6UtXd77z9znt+z9PPTPfbXXWqurpOnVM3UVUMwzAMo18Z6LYAhmEYhtFNTBEahmEYfY0pQsMwDKOvMUVoGIZh9DWmCA3DMIy+xhShYRiG0deYIjQMwzD6GlOEhmEYRl/TWkUoIh8Xkdu6LYcRhojsLyIqIouj89UiclXA88eIyPEB908IPzS+KrI0GUeTiMipUd7/Luf366LfT009c6dHmPFxq4isF5HHdyAJjRFajhqKc0LZbzjsxtNTtRy3tfzXYUa3BSjgAOAX3RbCqM1/AA8PuP8YYHdgdYfCDyFPlk7GWZd7gb1F5EBVTTYQ/hZYEP0eyt3AodH/j8Ol/1IRebKq3lNX4A4RWo7aTifSU7Uct7n8V6LNinB/4PPdilxEBoFBVb3f53qdMDtNt+IFUNXfdyLcRJo6En4R3YgzgHuA/wWOBZKt9mOBy4BFFcJ8QFV/Ev3/ExG5Cfg+cBjw5RqyGg0T8q1XLcctL/+VaKVrVEQeCwzToEUoIs8Vke+KyDYRuUtEzhaRnRO/rxaRq0TkZSLyS1zL+Zl516NnjhGRX4jIfSJys4gsF5EZZWHmyBff+0IR+bmI3CMiPxCRJ2fcWynexPXDReRXUV58Q0R2E5GFIjIWxXuViDzFM1/fEslwj4hcCDwmK12J8yeLyLdEZHP0zK9F5K3xvcArgOcnXHGn+qQpQ66XichvROTeKB/3S/1+uYicn7q2OIpzfx9ZQt5JKg2l77gm64BjRESieAVnUaxrKPwN0d+RkIfK8jw698qjquUo+r2sLjhIRL4uzg18j4j8TERGM9JTVvYPF5EdIrJ36vre0fWXeuZbbnoKvovSNGR8m755X+m56N63JfLsqyJysHTInRxCWy3CA6K/jShCEXkOcCnwVeBonJL9EPDI6DxmBPgw8O/AbcANeddF5BDgi8DngH8GnoJzGQwDb/YIM4v5wEeA5cBfgRXAl0Rkf41WR28g3vnRtfcCs4CPAyuj+8+OnvkgsE6c6yt3VXYRORL4JHAWLm+fD6wqSB/A14HfAK8F7gP2BR4R/fYfkXy7Am+Jrm3ySFOaBcBpwP/D5eP7gEtEZB9V9XUNlsnyEAHvBDzecQNcAHwKeC7Ocvs/wB7AV6K46zIS/f1jA2Fl4ZNHlcqRZ12wAPghrlzfCzwHOFdEdqjqeVE4PmX/W8CtwOuBUxPXjwfuAC72zI8q38Vzy9KQQ9Xy6VN3vRxX35wJfC2S8TPlyZ8CVLV1B/Au4EFgVkPhfR8YS117AaDA/tH56uj8qan78q7/JCPMd0dyzyt6NkfG1cADwD6Jay+Lnn9iE/Em4nh84tqHo3tfl7h2WHTtSSUyXwF8M3Xt7OjZxYk4r4r+3z367YCCMM8HLs/Jn7w0XZVx37MT1xZE6X5z4trlwPmpsBanykSRLMk4S99JyDuuUc5PBe6M/v8a8Mno/zOBr0b/3wmcmvVMUZi4RvMM4AnAGPBn4DGB8vnkeWke1SxHpXVB6jeJ0v1p4LKQsh9dez9OMUkivBuBFYF5F/RdeKYhXY69ymeN564EvpGS7cx0nnXjaKVrFGcRXq+q29I/iMheInJp5Ar5pYh8OHYBZSEis4CDcK2TGfEB/ADYzsQ+k1tU9WcZwUy4Ls4P/3Qm9498EeduPsgjzCxuVNXkiL9fRX/nNRjvjTrRx39d9PeyjGt75gkayfI0XIWb5IK8Z4DNwM3AWSLyKhF5VMG9Wfjm5e2q+qP4RFU34tx5zwiMr5TAdwIl7zgjfEmW2yg+H9YBR4vITjhLp45bdBj3rWwHrsUNmHmVqv6hRphFlOVRpXLkWxeIyCNF5AwR2ch4upfiGgGhZX8VriG2ODpfEp2f6yOzJ5O+i7I0FBBUPn2fi/LsqThLPkn6vCu0WRHmuUUfAN6jqk/CFcZnAkcVhPVIYBDX8tieOO4DZgJ7Je7Nm66Rvr579Gz6eny+m0eYWWxJnccd3g9rMN68OLZkXHsY+eyBa2XenrqePn8IVd0BHIJzqa0C/igi3xeRpxXEk8Q3L7NkuJ1UH05DhLwTKH/HaZ7PxHJ7qadcXwfm4FxVs4ELPZ/L4m7gb4EDcRXbiKp+s0Z4ZWxJnU/IoxrlyLcuWA28CufqOwSX9lWMvyPvsq+q1+Ms4ROiSycAV6jqL0tkDSHru1hNcRry2JI696kLfJ6L8+yO1H3p867Quj7CqOXwJHI+3KgV+ofo//tF5OdMVGZptuBM71PJ9snfmgw+J4z09TtxH1C6JTo3+rvZI8wqdCveLO7ANUrSshS2zlX1N8ArRGQmru/qP4FviMi8qIIrfNxTtiwZHgUkK597gaHUPWml5UPIO6nCBlwlFvMXn4dU9R4RuQj4R+DLWm+awwOamIpRg6byvGo52kJJXSAiDwMOB96mqmfFP4hI0mgILfvnAGeLyCm4Rvv/LUhaFSZ8F55pmGriPNsjdT193hXaaBHug2tFlA6UEZFhnC/6krx7ogrgJ8C+qnpVxnFr3rMFYT6Iq6BemfrpGGAH8OPQMNscb4EsPwOOTP1UZJ0nn9+uqpfhBrU8BjcQAFxLsqz1WcajROTZ8YmIzMe5L69I3LMJeGLquRemzktl6fQ7UdW/pMrrtQGPfwrXoDyr7MYpwifPgwgpR551wU44q/G++DlxI0pfmggntOxfEMmzDlfnVnFTh3wXpWmYagryrGsyJWmdRcj4iNF5IvKy1G9Xq+oNAFHfx/nA6ar665Iw342bALwjeuYvuFFOhwPLVPW3FeT8N9xIxHNxBfsA3Oius1U1c3RhQ3Qr3iw+AFwgIp/CjUh8PuMTrychbkrGClz/2fU4V9V7cO81tpx+AxwZvftNwK0VGit3Ap8XkXjU6L/j3FarE/d8BThRRP4L+Aau7+ZFqXB8ZWnTO3kIVb0c55YrY0hEjs64/l2feKKh72PAkijOPHzy3Ce+OuWotC4QkSuBfxWRP+MaMyfj3MPxqFQIKPuqeq+IrAXeCpynqltS6VlMef55fxeqerdnGqaaOM8+gXPdPweX7+BkREReh3PhPj7q258S2mgRxorwv3AFLHkcAA+5T9cCP1XVj5YFqKo/AJ6HM8M/j2slvxvX4V5pGTdV/TZukvKBUXjvBD4KvK1KeG2PN0eWrwD/AByBG0L+NODEgkf+iMvvZcA3cX01v2Ziq/BM4Nu4j+FKXAd/KBtx0xhOxSmmPwMv0sTUCVX9BvAvuIEkX8ENYHhnKhwvWdr0TiqyM26wT/rwnd84K/qb2z8M3nnuQ+Vy5FkXvAY30vNzwMeA9dH/ybSElv2vRn+zphf55F/od1GahqkmyrO347x4X8W5/N8V/fzn6O8AzprNHQDZCeIhvT2FiJyDy6w3aC8mwDCmESLyPuB5qrqk27K0FRH5MG7wyt7pPsx+zj8ReS+uQbObqv61W3K00TVaSDQh9kTgGuCn0cyJVap6RlcFM4z+5dm4PjojhYjsC+wHnAS8L2cgT1/kn4jsAZyCcwNvww1yeg/wmW4qQehRi9AwDKMXEJHLcVO8vg4cp11Y77ctiMguwHm4+by74Eb/fwH4f6q6vauymSI0DMMw+pk2DpYxDMMwjCnDFKFhGIbR15giNAzDMPoaU4SGYRhGX2OK0DAMw+hrTBEahmEYfU1tRSgiF4lI7gLZIvIJEflTtDaoT3irReSqvPOcZ/YXEY3W7PNGRI4RkePLZGgzeWmoGeZ+0Z6P20TkVhH596J98ETklSLydRG5RUS2isgGEXl1xbgbT89UhN3peELfSfTMQhH5tIhcLSIPRnPaqsZ/fPSNpY83ezzbM99TKD5lPyTvxO2ReLKI/E5E7hORTdHarFOVnnT922hZFsfVIvL6jN9misg/isgVInK3iPw1ys9/FJH0riVl8XxSRD7je38TK8ucB6wRkSen99iKPtSjgQtU9b7Mp8v5D+DhNWXM4xjcfnKrpzDOpslLQyVE5JHAd3Abax4JPB63buYA8N6cx/4Jt67hP+IWvD4M+IKI7K6qHw8UodH0TGHYHYun4jsBt1boYbgdF4IqkgJegFvIPOb6hsLtVULKvk/enQscDLwPt9D2XriVaaaKdN3X9DdzDG6R9C8kLybK+OOBjwP/Gv30YuBDwC3AlwLi+QjwGxH5oKpeV3ZzE4rwa7jlco4F/l/qtyW4fdnOqxp4ajf1KaEbcSaJGhCDXVqF4s24D+EoVf0z8N8i8gjgVBH5cHQtzRGqemfi/DIReSyukghVhK2kB98JwIWq+jUAETkfV6HV5UpV3dpAOF2hA+8xpOwX5p2IHIqrR/9GVX+Vd18nmYK67+3A55MryYhbJ/MC4LHAs6K9JmO+JSKfB+4KiURVbxSRH+CWtivf/1FVax+47VB+m3H9HNxK8YPR+UG4pYZuBe7B7U81mnpmNXBV3nl07S241eLvwa0e/0Lc5pSLE/cUxhWFq6nj1II4j8HtkXhfFPdyYEZazkiWn0dx/gB4skf+xc++DLd57HbcOnyV0xD9/lzcVjrbcAXpbGDnElm+B6xLXZsfhX1EQJn4Z+CewHJUKz04C+hbuM1w78HtSPBWn7Cn+zvBbTl0eY1v/PgovjkVnl3NxG+6LA8Px23Ls3cqnL2j6y8NKBN57zG3rNQ90mXfN+9wFs8lFeK7HDg/dW1xFOf+GXmRW0cl31VRWa6Sf8DCKIyn5ZStI5vI/0S4J+H0z0DZvU0tun0ecIyILFLVDeD8vcDLgbXqNmUEt+3KD3Ebhd6L24/qXBHZoapeVqOIHAl8Mgrjq7h9wLK2NimL6z9wlcmuOMUKbp+vrDgPwSn7z+EK+VOi54dxrfWY+TiTfDnOBbIC+JKI7K/RmylgBPgwbu+823DuludWTYO4xckvjfLo6EjWD+HcEll7z8U8EbgseUFVbxKRbdFvF5akI+bZOFdeCHXT83WcO+m1uAbLvozvv+b9vhOMML3eSRP8XtyG2L8HTlPVT1cIo+zb/BZOSb4et5VWzPG4nc4vhqD8HGHye/wu+WWlLnllvyzvngl8Xdx+fa/Deey+hdtpPngD8RxC6qiib6boW8vjYJzSvDp1/Z+AX2vkvWiQH+E8kgdkxDmRhjTvTsCfgI8krr0Ep+UPynlGcC/608BlWS2SnPMrgG+mwjqblEXoGVdmKzkjzp8AY6l73g08CMxLPPMAsE/inpdFcj2xJP9WR/c9teCe0DR8P0PmF5BqJWY8tx14Z8b1TcAHPMvDwbiW+/EVylKl9ODcfgocEBp2n7yTuhbhi3D9kYfg+m0+F8n9j555eVXOb3l5+H6cwpLEfTcCK0LyM+s9+pSVGvk0qez75h1OofwFZ6Udhtu2aSPwP3E+5MR5Of4WYWEdlX5XWeWmav4BK3Hu4eS1BVFYyzrwLmZE6X1T2b2NTJ9QNxDmKzirMN5QMX6JP4nvE5FHisgZIrIR93Fvx20w+QSfeCL//tNw/ZJJLsi4t1ZcqTifjtuoNMkXcYMVDkpcu1FVf5c4j1uF8zyiukVVf5aKu1IaRGRWJNeXolFoM0RkBu4D2w4sKpEly3qVnOvpuEdwHeFfU9XVZff74JmezTiX9Vki8ioReVQDUU+Ld9IEqnqJqr5fVb+tqt9U1dfhXHnvFZGgesQzD1fhKsnF0fmS6PzcKIyQ/Ey/x06UldyyH5B3Eh1HqurFqvpF4Djcbg0vaEJG6tVRMVXz79G4AUVJ4o3YrwmI3wtVfQDYEsVbSJPzCM/DmdEHicjDcKPbztNINUesxinIj+BaR3+LK/AP84xjD5yWT+/knLWzc924YnYHZjJ5J/v4fLfEtS2pe+IOeZ840+FD9TQ8Erdx8ZmMVzTbcS3OmbiRaHn8CecKSbMLk9M3ARHZDbdj+E04l0lTlKZH3T5vh+D6BFYBfxSR74vI02rE2/PvpMOcjyv/I4HPraYkD1X1epylc0J06QTgCh0fmR6SnxPeYyfKSoWyn5V3fwJ+oarJgSE/wNUjTY0c3ZI6D6mjgFr59zDc+0myS/Q361trgvvwSFuTG/NehkvMscBjgJ1JjBaNlOPhOH/3WYnrIcr4Dpypm26BTDhvKK6YO3EfWDrOudHfzRXCzGJCy75mGrZE4Z1K1J+Soqi/4Te4fqekLHsBs6PfMola6Bfhhukfrqr3eMjpyxY80qNutNkrov7p/wP8J/ANEZmn2RuiltHT72QK8bZKA/PwHOBsETkFOIqJo/+24J+fk+RrsqzULPtJ2X6N62aaFAXO3ZrHvUyeHrNb1o1NUTH/NjPZOouNmMeWxSkicZ/qPrjv4V9w9fJROCPpcJ044hRcA7K0jm7MIlQ3IObLwCuB1+A6P3+euGUnXAvuoRaBiOwMvDQwjp/hrM0kR6XOfeO6n5LWQhTnBly6khyDK5w/9hC9CpXTEH2IPwH2VdWrMo6iSvebwIuiuGJehetY/27WA5FL6su4AvpiVc2y0H2pnR5V3a6ql+F2/X4M49ZU6fsuoWfeyRTxClxDcWPAMyH1wAW4vFyHq6vWxT/UzM+HKCgrXtQo+1l5dxHwFBFJTnN5Hs7CLRrssYlUQwk3MrQJCr+ZwPy7FjfyN8mPgT8zbvlPQESemzh9Kq7MH4zTMR/HWdDPwr2Do1LP7gHMAn5bIBPQrEUIzgJ8G2606L8mf1DVu0XkSuBfReTPOCVyMnA3YaO1PgBcICKfwvVLPh84tGJcvwGOFJGX4QrTrTkf0L8Bl4jIubiP8QDciKqzVbVs5GElGkjDu4FLRWQHzg3zF5zr+nBcx3Re4TgLN9fnAhH5T+BxuFb3aRrNVxOR1+FcIo9X1Y0499RhwDuA3UTkWYnwfhr1ISNu5Z8xYImqXp4Tf6X04D7WFbi+2+txrrP3AFer6uaSsL1o8zuBye8lslQOi37eE3iEiMSjKS9W1W3Rc4speS8ish43UO3nOEX2quh4e4gFFVIPqOq9IrIWeCuum2VLKrhK+SkiT6GkrHiWVfAo+wF5txL3ni8UkQ/gvGr/CXxHVX9QIMNXgBPFrUDzDVx/6osK7g9hUlnGdReVfWtZ/BD33vdQ1TsAVHWriLwH+JSIfA34PM7z93ic8fEI4DmRx2AhcLCqqogo8BNV/WYU9gCTLb8DcRb3j0pTmR49U+fAmfA3RJEvzPh9Ic6Feg/Ol/5u3Ad9Z+Ke1ZTPI3xb9FK24dwihzB5HqFPXLvjCtFmyucRvgo3j/D+KO7MeYSpZ0aicF9Skm+Tnq2bhui3Z+KGX/85CuNXuJbbLiXy7BfF+1fgDzilP5j4/fgorpHo/EYmzzfS5D3RfYdF1/YriLtSenAuks/jPsx7cf0X5wHzfcLu9XeS817i8tfEe/kArkW/LZJhA3CcZ70wIS998jBx799Fsv1dTtiF+Zn1Hj3LSmme+Jb9kLyL8ubiKC1/iuR/pEcen4IbwPIXYA3OwlYy5hEW1VEZ72pSWfbJvxwZh3BzPSelHefl+z6wNTp+hWsAPiP6/UnA/yTufzvwvsT5JcCzU2F+jNSo4rwjHppsGB1HRN4HPE9Vl3RbFmOcNr8XEfkwrhG6t1br560ab2vzpJcRkY/hjKTDA597NfB8VX1zdH4ubnTuV6PzW4EnaLRyj7jR/huBk1V1TVn4tvuEMZU8G9daN9pF696LiOwrIi/HrQ7y8alUghGty5NpwkeAxSISNI0N+Bvc+JCYp8XnIvJo3Eo+yeXrXomzvtfhgVmEhmG0DnE7ZTwTt4LJcdqdNV6NDiAixwJ/UNWODfSKLMhbVPV7XvebIjQMwzD6GXONGoZhGH1N09Mneobdd99dR0ZGgp+75557mD17dvMCdYhekreXZAWTt5P0kqzQW/LWkXXDhg13quoeDYvUdfpWEY6MjHDVVeGbZl9++eUsXry4eYE6RC/J20uygsnbSXpJVugteevIGq0PO+3oO9eoiBwhIivvvvvubotiGIZhtIC+U4SqeqGqLt1ll13KbzYMwzCmPX2nCM0iNAzDMJL0nSI0i9AwDMNI0neK0CxCwzAMI0nfKUKzCA3DqMratTAyAgMD7u/atd2WyGiCvp0+YRiGEcLatbB0KWzb5s43bnTnAHvu2T25jPr0nUVorlHDMKqwbNm4EozZts1dN3qbvlOE5ho1DKMKN90Udt3oHfpOERqGYVRh/vyw60bvYIrQMAzDg+XLYdasiddmzXLXjd6m7xSh9REahlGF0VFYuRIWLAAR93flSnfd6G36ThFaH6FhGFUZHYUbb4QdO9xfU4LTg75ThIZhGIaRxBShYRiG0deYIjQMwzD6GlOEhmEYRl/Td4rQRo0ahmEYSfpOEdqoUcMwDCNJ3ylCwzAMw0hiitAwDMPoa0wRGoZhGH2NKULDMAyjr5kWilBEHicinxGR87sti2EYhtFbtFYRisgqEbldRK5JXT9URK4VketE5GQAVb1eVU/sjqSGYRhGL9NaRQisBg5NXhCRQeCTwIuB/YBXi8h+Uy+aYRiGMV0QVe22DLmIyAhwkaruH50fBJyqqi+Kzk8BUNUPRufnq+rRBeEtBZYCzJ07d9G6deuCZdq6dStz5swJfq5b9JK8vSQrmLydpJdkhd6St46sS5Ys2aCqBzYsUvdR1dYewAhwTeL8aOCcxPlxwCeAYeAs4PfAKT5hL1q0SKswNjZW6blu0Uvy9pKsqiZvJ+klWVV7S946sgJXaQt0Q9PHjC7o3jpIxjVV1buAN3sFIHIEcMTChQsbFcwwDMPoTdrcR5jFJmCvxPk84NYuyWIYhmFMA3pNEV4J7CMie4vIEHAs8PWQANTWGjUMwzAStFYRish5wI+BfUVkk4icqKoPAG8DLgF+DXxJVX8ZGK7tPmEYhmE8RGv7CFX11TnXLwYurhHuhcCFBx544JuqhmEYhmFMH1prEXYKswgNwzCMJH2nCK2P0DAMw0jSd4rQMAzDMJL0nSI016hhGIaRpO8UoblGDcMwjCR9pwgNwzAMI0nfKUJzjRqGYRhJ+k4RmmvUMAzDSNJ3itAwDMMwkpgiNAzDMPqavlOE1kdoGIZhJOk7RWh9hIZhTBfWroWRERgYcH/Xru22RL1JaxfdNgzDMPJZuxaWLoVt29z5xo3uHGB0tHty9SJ9ZxEahmFMB5YtG1eCMdu2uetGGKYIDcMwepCbbgq7buTTd4rQBssYhjEdmD8/7LqRT98pQhssYxjGdGD5cpg1a+K1WbPcdSOMvlOEhmEY04HRUVi5EhYsABH3d+VKGyhTBRs1ahiG0aOMjpriawKzCA3DMIy+xhShYRiG0deYIjQMwzD6mmmhCEVktoh8VkTOFpEp85jb8kaGYRi9T2sVoYisEpHbReSa1PVDReRaEblORE6OLh8FnK+qbwJeOhXyxcsbbdwIquPLG5kyNAzD6C1aqwiB1cChyQsiMgh8EngxsB/wahHZD5gH3Bzd9uBUCGfLGxmGYUwPRFW7LUMuIjICXKSq+0fnBwGnquqLovNTols3AX9S1YtEZJ2qHpsT3lJgKcDcuXMXrVu3LlimrVu3MmfOHDZsyL9n0aLgYDtGLG8v0EuygsnbSXpJVugteevIumTJkg2qemDDInWdSvMIRWRfYE/gYenfVPXiukIVsCfjlh84BfhM4AzgEyJyOHBh3sOqulJE/gAcsfPOOy9avHhxsACXX345ixcv5vjjnTs0zYIFcOONwcF2jFjeXqCXZAWTt5P0kqzQW/L2kqxTRZAiFJEDgPOAJwGScYsCgw3IlStCVpyqeg9wgk8AqnohcOGBBx74pjqCLF8+cQsUsOWNDMMwepFQi3AVsB14CXAdcH/jEhWzCdgrcT4PuDUkABE5Ajhi4cKFtQSJV3NYtsyt9j5/vlOCtsqDYRhGbxGqCJ8EvEJVL+mEMB5cCewjInsDtwDHAq8JCaApixBseSPDMIzpQOio0SuAKdnkQ0TOA34M7Csim0TkRFV9AHgbcAnwa+BLqvrLwHBtGybDMKYlNre5GqEW4VLgPBHZBowBW9I3qOq29LUqqOqrc65fDFQekNOkRWgYhtEW4rnN8biFeG4zmOeqjFCL8E7gRuBzuNGbf8k4Wo1ZhIZhTEdsbnN1QhXhGuAFwArgzcAbMo5W0+mNec01YRjtoN++xZtuCrtujBPqGl0CvElVv9AJYXodc00YRjvox29x/vzsuc3zp2RUR28TahHeCDTSB9gtOukaNdeEYbSDfvwWly93c5mT2NxmP0IV4T8Dy6Klz3qSTrpGzTVhNE2/ufeaoh+/xdFRWLnSrW4l4v6uXDl9LeAmCXWNvg83feK3InIj2aNGn1FfrN7EXBNGk/Sje68p+vVbtLnN1Qi1CK/BTV1YC/wQ+GXG0Wo66Ro114TRJP3o3msK+xaNELwtQhGZCZwD3Kiqt3ROpM7SyXmEtuya0ST96N5rCvsWjRBCLMIHgcuAJ3ZIlmnB6KjbfWLHDvfXPjyjKnluvOnu3muK5Le4fLlTir3S12p9w1OLtyJU1R3A74C5nRPHMIwYc+81Q9zXunEjqI73tXZLuZQpubbJ2w+E9hEuA/412o6pJ7GVZYxewUYBNkOb+lp9lFxdec2aDCdUEb4XGAZ+JiI3iciVInJF8uiAjI3S6ZVlDKNJilztVuH50aa+Vh8lV0desyarETp94proMAyji9jUCn/aNJXCR8nVkbdI0Vq5yCfIIlTVE8qOTgnaC1gLfTJTnSf98g7a5O5rO23qa/UZAFVH3jZZv71EqGsUABF5rIi8QkTeJCJHichjmxas1zCXxGTWroUTTpiYJyeckD04YGQENmyYrLxCFFvRO5gOCjKZhiyLAcorvOmQD6G0qa/VR8nVkddGGldEVb0PYBA4E9gO7Egc24FPAgMh4XXjAI4AVi5cuFCrMDY2lnl9wQJVV/1OPAYHVdes8Q9/zRoXloj7G/JsiLxTwfBwdp4MD4/fs2aN6qxZ7vqKFWMK7nzNmom/pZ/Pype8dzB79uRw4jjqMJV5m5cX6WPBgvww1q8f60g+NEG63K9fP1b4extkThJSFjqZlqxykn7HdcotcJW2oB5v+ghVIu8H7sWtOTof2Cn6+8/AX4F/73aCfI9FixblvetC8gqRSH7l5FvZ+BTipuSdCooq7Jik8ooVYVyh5ym2vHwpegehSsOHqczborzwLStnnDHWkXyoS1a5P+20sYfS0onvIhl3E0qpm99ZmrI0mSKsrwhvAt6V89u7gJu6nSDfo2lFWFZR+VQ2eWHUqajargiTyiupCEXKFVs6X3yURfIQqZe+qczborwoq8TjijGZv03mQ12y3tuKFWMPvd9OfBeqzSrYNinCMkwRTj5C+wgfBfw857efR7/3JVm+/yQ+ndXTraN7eLj8el7fxcCAq5qKSOdL6OCHXuo3yZN1wYLiVYyS/aa+YU91P2JZuc/7vShNPuQNOHrHO+qFa/QeoYrwt8CxOb8dC1xbT5zeJe7gHhzM/n3+/PIKphsd3Z2s9D72MRgamnhtaMhdj8lrQDz4YHn46XwZHc1XviITz3tthZaqIwmzKvuiMLox6Kus3Of9LlJPrjwFe9dd/TGIyEgQYj4Cx+AGx3wHeDPwcuDvo/MHgVd228T1PZp2jcbkuVtOOqncDTMVfYTJ/oPhYdWhIZ3kJovdTlPVB5N03Q0OaqYbzLc/rCj/mx6gMNXusCr9WXmu57x33Ck3ZBE+fYR5ruE6chW50kPDNddobx/hD8AhwI+B+yKleB/wI+CFXUsEPA74DHC+7zOdUoSq2RWWbwXTiVGjyfhDBpRM9YjCsbGxQvl882WqRhhOVeVXJz1Fg5GyyMv/Tvcjlo0a7UQ/75o1zYVrirC3jzrKZwDXJ1hrygSwCrgduCZ1/VCcq/U64GTPsFqhCLPoVgWTNWQ+5ChqGVetoPOeGxsb64pFUpUmKr+yPKzrJSianpJFW/I/nbedkitvio9ZhNlMV0VYaUJ95FLdoaq3q9uVog6rI6X3ECIyiJuX+GJgP+DVIrKfiBwgIheljp4YoDNV/X/pPr+bby7uIyoj7kdJh/uWt1TrSyrrg5rqVUC6OcF8KhZgTk7OhvLJ2W1ahSVJp+T62MfamV5jiqmiPYEnAC8ADksfFcMbIWERAgcBlyTOTwFO8QintRZhJ+dCFcWRN2Q+xCLMCrdqn01Rv8wZZ4w9NJF+KlybJ500OR0h7yRZFkL6QuN7fKyRIlfx8HB2fEUWtw9tmLyeJauP9dykh6KuvJ2Iu4qsZW7nEJimFqG4tPkhIvsBX8RZaZJxi6pqzrjJwnBHgItUdf/o/GjgUFV9Y3R+HPBMVX1bzvPDwHLghcA5qvrBnPuWAksB5s6du2jdunWhorJ161bmzJlTeM/mzXDLLXD//ePXhoZgl13g7rvd9aEh2HNP2G23YBFy+cUvJsYJMG/eVjZtypdXxI10feCByb8NDDgLIp2WMhYtyv9tw4b83+bN28qtt85hwYJm8yX5PuJ8B7jhhuz7h4bgAI+NxuKysHmzs+Z2JHwjcd7F6ci6p4g4D7PeaRZxfJAvy9BQedltCz7fWRKfd9BJ0vJmlbm0HKEy+5azZLy77OJGwSaf2WuvrcyYMadSvixZsmSDqh4Y/mTLCdGawPdx/XZHAvsAC9JHFW3MZIvwlTiFFp8fB3y8Cc1Ph5ZYiylaCqvTg0+yrIcyi3DOnHGZ8lqbIQNs6liEsaxNDiDKs8TzrLG0JVxEXBZ8+q9CJvsnn/NdWi1+rkiW6dyPVZTuqbBw094BH+9PaL9n2f2+npvkYgWhME0twrCbYSvwksaFaMg1GnI05Rr1dXeFVrJVyFuho0yeoaFieXwrcRE/105exR7LmhxAFOJSDhmt63OUNVzisuAzEMq3MZEVZ9HoxnR8RfHEFWCVsjfV7tJQRVi2xGFWHjeZpqS8vgoudABd2f2+ZX3FirHKg/RMEaoCXA0c3bgQkxXhDOB6YG9gKIr3yQ3F1ZhFGNJaD61kfUh/yFl9Xr59hGWjQ33TWSRf2vLMU4Q+llSW1ZhV4VVVgj75EpcFn74+n0pqYCB8NGc6Ph+LW8SVFV+mon87TVMWYd681OHhZtOUlNdXwTVtEfo2tswirK8I/w74X+BxjQkA5wF/wO1gsQk4Mbp+GG4lm98Dy5pOeBMWYR1ro6ySLcO34vdVhGUtxKRSy6tcki6aLOVQtoDAihVjk+6pW6mEVISh+RLP0Zw5c/JzaSvbtzGRVxGfdFLxc2U7dqTLgo/1Xpa3IeU31PoKVYRNNYTy0lQmfxWLMEvmmTPzB0HllYG4URNiEYY0hJKYIlQFuBK4Dbg/UlJXpI9uJ8gjDY1ZhKG7HdRRPvFHUebuS1f8TViEWXIVuZuKKqCi/r941GiSIgWXzJe6rrEy5Z5F0bzHgYFx+Q4+OD98n/jK8jS91VcyXUVlwfed150DW8WirLJwRdbqQaGN1aw0lZX3BQsmup2ruvPLVnrK8zzE79+3sZXV4PTFFKEqwLllR7cT5Ht00iKcPXv8t6IKMNQdOXPm5A+lrOJvoo8wT76sFnJZxVNmYfnkQ1aYRXnsO7WhSmVdtzHkkz9FeTpzpn//bros+CqyuhZhleerLmWY1b8aMlgqS6Y8+ZNehay9NH3KXOj4gqJvPo637N68QWk+mCKcJkcdi3DNGme1JFugWW6xdCVcpZKt6naN44w/qjJFWNQv5ZsnyY/ZR7488io/H8utqHKomhYf911d97hP/hQp27JGTNJarmoR1u0jDLEosyysLEKUa9rqir8N3zmkPo2drLwtWt+2zviCom/Z5xvJGpTmiynCaXaEWoRxwU0W+Fmz3PQDn8o4tJKtYmkk+xfij8HHIiwa0BJqRZW5KOu6w3zzJe1KrjMBOS9P1q8fK3XLlpUNn/wpU7ZlCi0eRJUuu51sJPjIX9RnVrYcXBV3bVF5raJ0ixRhWflvugEVcphFaIqwskUYF9wqK7VUKXChH0pW/4KPvFmtYp+dMopkzKqkhofrDZAI6e+JK8MqlkzeM1l5Eu+QUKUyivt18vqA09eKrId44EuRokp7M+p4AULxfQ95btwQd2XRtxb6TLLMZX0nSVdmaF981cbT8LC/VyTuR0/XDdZHaIrwoSPUIsxzL4VUzHlUqfySx9BQfv9Ckbx5H6PvwJGyvrnQSrfINRriRorlbLKyzMqT2H1XpXWfNWovz2I56ST3W92pAN2cUO8zsCVZntIjXNNkTRWK8yKvvBW9jyx5y6zHLAvWty7wGfCW57r1bXyl5+MWDUrzxRThNDtCFWGeRegzDD90UEyWOzW0os2qUNJDs6uEl5UnIektIq+iLpK1qJ+nTFEnK524cgvN29gayysDIqr77Tcez+Bg/hy+Igu7qK/Zd+DHVG8blczfuG8ub3RkkYWVTkeeEswqA0nyGhKDgxPlLhtFnOyDB9c3F2oRljXuyvbQ9F24IwvbfaKGIgRmAs8BHtttoWsluKJrNK+PMK6g8j6csvlavsqkquKKK+ssyyzE+slKS91BFGnyPtCi/qAil2Bons2a5Sq1kLxNVmxVpmD4pDNdgRZZUun8SefvSScVK+U6fYHx83UGgaQ9L1lzT30ahll5XnR/XblXrBjzGtEN41Zrmbs/qw81OeinLL68BpcpwnqKcAC3Ce/B3Ra6iaPK9Imifpaij7OIoudCRpllWaYzZ6qefnq+IizqD8uTK+vjTLeO4/tCK9G8D7TqnnGdGJmXPJK7qKvWn29XprhDGzTJfFqzRvWznx0rrDDrNmyKGgMhRzywp0pai/Lcxx1Z9VixYkyHhye7f/PKbjzat8gLkXZt5k2+z3t+eDj7PZkirKEIXR5wDfCabgvdxNH0NkxV3YRFBTmrNZx1f1wBJ91Ps2frQx9o+sNJ90XG96bD86lgipRNqHUY52265ZtVSfnOfQwZZBN6pLezqesq9rV2ivqqispSnvsudg0WeQjKLMQ1a/zmuPoeZ5wxlhlPSDfBVExZiI+89TuLyl5Zt0qy3BSVrbLGdNZo56qYIlQl2nXiN8AB3Ra87tG0IixqsRVVIj67IGTFlffczJkTFVtR38WsWW7Vk5APPtRlm9cqzcvbsv6fooouJJ+qVk7JuNevH5tkAVRdUSTdgAmVOenyLKqs835T9cv3dN9Vcl5ek0dsEYYu2JAlb1oZhoQRNwJ85U1TZ2qNr7ehzCrMG+1cBVOEqvESa3cADwI3Red9u8RamjIfflalWPah5E069m3Z1t2YNy1Lut/B50P3Xddw/fqx4IojbzWROpXz8HCxQoobEOm8TSqJuBKFqbFMfJfWKipjIW7HuvIWLUSRJWuyP77s2fRRVUGFzPv76EezlUtVb0Q8PzkuQ3l918PD4flhi25PPgYI4xrgIuBzwKXR+S9TR6tR1QtVdekuu+xS6fnNm2FkxG1oO2PGxL/LlsHy5W4TzDlzJm+oum2buwdg7VoXjmpxfPPnT762bJkLa6pRhc9+1skekyVfmrPOmvhMTJwHAwPu7803l+dHmjhP47BE4Ljj3GakVbnrLrjnnvzfH3wQLr00W5aLL3ZlYNYsdx+4zVSXLh3Pg068v23b3AbLVVB1+bZxo//9oQwPu0PEbSR77rnjGwn7sG0bvP717v9HPCIs7ptumnwtr9wODo7LuHIljI666/E7zWJ42JW9+N4ky5fDzJlh8gJs3er+xmUoa0PnoSH3d/v28PCz8qSv6bYm7tZRdbDMRz865tWKLBvp6NOCr7PkU17Luokj2ZqsOoovKw/qyNrJQTEheVs0NaXuZOpOyFt0NCVnelHwmKIdNfJkbWpHiSoDg7Lc9vH56aePZfa/503Ib+KI46tSDswirGcRAiAijxWRV4jIm0TkKBF5bMP6uZW84x2uKBURWyh5Lc758/0sguFhePjDnXUzMjJuTaxd6yyoKsyYUe25NMnW5OhoeZ6kn4FmraLBwe5YyFnMn5/f2o6v+1jReYhUf7ZbPPjgRIs45uKLw8MKtXxnzXJWWZrRUWfxLVgw2QJMeip2390dAwPu93RZj88feMB5ElSdZf3a17ojtrJ9vpFQNm+uXpay8qSfCapSRWRQRM4ENgJfBj4NnA9sFJFPikjFKro38HW33XRTtisl/ih93BJ//evED+u449wH+9rXjrtL0syePe4uSTI8DGvWwJve5Cd/GemPz8fFlX6mKddM0gXZbUTc+y1qBEGxmw2cKy3daBkacu/w859373MqaLLyTnYLxFQtAw8+6O9uTLrO04yOjr+vm25yDd05c8YVmKr7BuPvsC3lLGb+/PKylMWMGdlu3H4mVHG9D3gD8C+4XeUfHv39l+j6qc2J1rvsttt4izNZaYn4WZVZFk7RM4ODrpLcuhVWrRpXTHHLec4c+OEPnTx5xBXtmjXFH1ZWC7vsY8x6Jk9ZxH1JRQwMTGzFh/Q1DQ/DSSeFPeODCLz5ze69H3ZY9j133eUq5LQ1ktV/tnr1RGtl1Sr33Ogo3Hmne0/x771CWvFVtWbiPPJtECQbkmnvytKlE5VeUd9wm4j7dJctc32nvlbyrFmw116dla0nCfGj4kaKvivnt3cBN3Xb1+uRhsqjRoeH/fpZ4ikDnZ7QneynSFJlDcSyFXLidOVNBwmdWJ+VN8lh3XnTOmbMGA8vpA8mZNFw3+O007IXKygKt87KO3n4pqMT/cVZ6SsasZvMq6Lvo6iPsGw6StnqQHEYTc4vnYq8LUpP1tSd9BH31dqE+gy9EHQz3AsckvPbIcC93U6Q71F1sIxPgQ8djt7EkRyqH7oqflw5+UzKzquUsp4v2+Imb6Jv3lzCOXP84ytapzEZf508rzrpO2/wSFV8G1zJZcs6VQZ9Jviny03WlKP0ottF5ajKO62zhm+saPMWCQ89sjbcDn1H8bJtZevyqpoizDrCboafA6tyflsFXN3tBPkeVSfUn376mNdHptq90YHJo+wD9W0dl62jWfa8z76MY2NjhaNQk0tYFe3EELJWZt28zcKnAZQ3/7HqOp9F1nG8sENy2bKiZeuqNuCydjso+kay0lt378SQuZBV05n0+Ph+Z3nfU3qEaTIvQuUrm/MYf6umCOsrwmOAHcB3gDcDLwf+Pjp/EHhltxPke1RVhL6u0U4u7RVy+FqEVcOPKz8fpT97dvFuEZ3Y8b2sEq2zxmSeRehroaWnodRdwDxZ5rIm8yeXsMuahJ1c/7KpaQohjcEiCytvuH9agYTIWjWdSYUfxxmiCH3Xni3yehTlf1lZMkVYUxG6fOAQ4Me4Bbh3RH9/BLyw24kJOaoqwjPOGPOugH389p0+QvZJK/q9rkVYVimpug+0E1Z00ZypvLlsO+1UrCR9FjQvW90mq0INkT0dX5kijSvAvLgGBrJdlr7lPcuyqbrCT7rc+q6wFLpiTN5Sd0WyZzVgQhRh3rzGLG9A+nrR3MssizzP81KVvleEpLZhwo04fRQw0O1EVDmqKsL168e8P760+ye9J1vyf1/LJLT/MbSlWlaBFFUoVZV+nKa83RHqHmUt8KyticoWSx4a8nPfFQ1ASlaIndq5IrnWa1wBlr3jdDrKlFlyW6Gs9XZnzAh/Zz4Woe834LPmbxa+/d5r1oQ1kLMaTSHegKq7scSYIqynCFu7DRPwMuBs4Gt5g3nSR51Ft33dMU1tv5NV2EMHSPgeZRuSFvVhVV1JI27pFq2FGa+9mPVb/H/eaMEqq2j4LHIc4r4ra0R0ci/DOJ54kXDfd+IjX1LGTo3CrLLCUsgAm/S7SltgeYonaQX7KMK4wZAm1BtQ141uirCGInR50Pw2TNEgm9uBa1LXDwWuBa4DTvYM65HAZ3zubXL3ibKCXKZAQt1PMT79kFnKpUzZNbHZrq+LLF5ybnAwXxEWWRtlW/+UyZ73boosrLy8LWr4FLm+mtjGymdgymmnZeevT94VjcaM0x0SdtkR522RIisr+6ENoCplLN7dwafBGarQq5QnH0wR1leER9LwNkzA84CnJxUhMAj8HngcMARcDewHHIBb9Dt5PCrx3EeBp/vE26QizKvM4u1x8hSLTz9SMqxQV8ns2ZMrv7y+y3SllwyzKO4i0m7hIgWcpVjiIyu8IuXqs39e3nuLK6W83UPKpqaEVkp5lbnvFAufxlCcF1WH+MeWUVEXgGozm/Im8zbP1ZwsU0VKytcjU/YufOX1zUvfeKt4MtJ5lFUeTRFOPsSlzQ8RuRK3ksxuwC3AbcCEAFT1Gd4Bjoc7AlykqvtH5wcBp6rqi6LzU6KwP5jzvAAfAv5bVb9TEM9SYCnA3LlzF61bty5UVLZu3cqcOXMmXd+82e2e8MADfuHMmOGWbArI/oeW3XrgAbcSzC67wN13T97lAsZX0RgY2MrNN8/h/vvdM3vu6Va+2bwZbrmFSdfjtGzcOHHF+4EBt6JHfE8ZWWGUMW/eVjZtmpy3ixZlh3/DDflhZT2T5he/yM67JAMDLg1xHiXjzJM3JK82bMj/be+9x99R+t3vuac798njoSEXRp68ddl7b5fWorSEMm/eVubOdbImy2oIcT7llfM0deQPydt02Sz73oq+1TR5YQ0Pw5/+5MrPvHlb+eMf57DXXv7fc8ySJUs2qOqBYU/1ACFaEzi37KiijXHKNWkRHg2ckzg/DvhEwfNvBzYAZwFv9omzaYuwDVMlsvpFqrT+ytLiYyFWyY+sVnXWXoZl/aNN9KvlpTeZriIrwHe4f5E1X2TtlK3gkrwvjrNTq5+UvfMqI4HjqSm+feFZ1vvBB2dP18lbbCF0hGuVCfVl5QImTn0J2ey56B0kj1jWeLpMCPS7RSgiM4FnADeq6i0N6OBk2CNMtAhfCbxIVd8YnR8HPENV/6GBuI4Ajli4cOGbfve73wU/f/TRR3PnnXc+dH7bbfDb34ZZPp1kp51cC/2GG+C++2Dhwi3MmLErc+f6h/Hd75bfIwKPeYxbn/G++8bjjePxCSPN4x63heuv3/Wh88c+FvbZZ/J9P/mJizOPvOdCw0kyMABPeIL7/9prXZWSljfN858/8fy228afnQqS7+S222D79i38/ve7Nh7Hs57l/s/6FgYG4NGPnlhOhofHz7M8IwMD8KQnbWH33Xf1fkczZrj1Nn3fZ5JYxltvDX82pqwsxPE84QnkfoshdUky35P4fHdO1sXA6SxYADfeWP5MjIj0t0VIB0eNMtkiPAi4JHF+CnBKQ3HV2qH+b/7mbxSwww477Ojh4/lKZCGGwDS1CL13qFPVHSLyOyDAtqjMlcA+IrI3ri/yWOA1TQSsqhcCFx544IGVNiVauHAhu+6660PnoZbPTju5fQa3bKkSezjplmrSsslqvef95stOO7m/VVrmaVl33dVtR5W2OH2thCc+cXLr+3e/m9jyT8bhw047jd9bZgWkLYAqVnIRM2a4d5T1DrOsji1btkwou2mK8jXLA5C07OL3A/nlqswrcdttcN11rh/Lx8JKMmOGf/98J6jiHYi9NnHe/eY3YXFm5auPVelkfSoQtrfjtCZEa9KZUaPnAX8AtgObgBOj64cBv8WNHl3WYHy1LMJ0n1vopN6ie+JJ9j7h+R55Ixt9p3xUibPq6jC+66JWWcJMNX9VjrgfsmhkZPx+QuSNn4nzs8n3Gvd1hYzuzevf9ilz6f6kvGk2vpO9s+brJcML6c/0mUZTdpTtWJE8skbH+vQXF+V11pJ3vrKErkaTljUEpqlFGHazs9TuwK0relN0fkXy6HaCfI+6g2WKJo9nVVI+BToO1+ejGBzMn2Re9oGK+M9d8pUn+XxTy2oVVSplSisrLXnD++O8j9ObJX9WZetbWR98cLNL7VUZRJEsu1XfbVKZVWkkxZW1z9JoeeU2TnvVZdx22in7+uzZ1ZVRUVlINt7qhO9z+K5Gk5Q1dIrGdFWEoRvzXoObu/c54NLo/Jepo9WIyBEisvLuu++uHEZyQ09wRSreIDXeLPY5z3EuN1/i3a9HR/02Hd2xo/omovPnl++iHhPLM3v2+LWBATj44OxNYVXd39Bds32JN3e9+OLxuPJIpyVvh/Hk9ayNbxcsgEc8Inz4fsyll07eaDmUePNlVTe44eKLJ4eZtQt8EcuWwfbt/vcnN9atsrv8xo3uu3nHO8I2ngb3DnbscGk/80z3Nz7fvNkv/tmz812/27aFbfYL7p3E5WOPPSZviB3XBaOj4XldhW3b3Ca98cbDST72scnfZNaG2X1LtzVxt446FqHPBNjQFnPSKvEJo8i9WdRSLXIvFq2XmXVvXpzxajHJic+zZ4fLWpTHIRvxlrl5s/I+TVZ8U70Za9L9VbYLQfyOk5ZTvN9jUZp88r5K+Q49QrZhakKW9PZdSavTp5wnt7jyLT+hR8gKVFlTRHxlLYJpahFWe8it8nIc8C/Ao6NrC4Gdu50g36OOIiwqqDGhBT9rvlxRGCed5Apy2Wof8fqSWatM+C7TFFrRZPUHhayLOjiY7U5MLp5ctvalzxJmRXnvkwdNKMKQvqlkJVe2ikuWK/e00yau1lL0Xou2ywp5p1WPOG+T/Z55Kx7VlSVvY1yf5fbieJObHoesJ+p7xGXa9/6y7c6qYopQFWAO8CXGt196kGhJs+j6im4nyCMNtQbLrF+fv1VQlRZzvNtBFkVhxAU7q6+sqUIfE6LUsyqCoonWccUWrzWa1+lftpxWunL0ycOivE+TVQml+1p8+i27ecT5m0xTVr/V4OBECz5vEE68Rmzet1B3ybK04i3bQ7HMgisqM3nX89IwNBTW7+bbR5glf3KhjDqD6WK5TBHWV4QrcSM7l+DWAN2RUITHk1o4u81H0/sRxm6SmCJrLWQdSZ8VVIosuyYUYV2lHlL5QbgyTV9LPl93e6MkeaNGk+++zcpwxYqxzMFQyXRlrWhT5JosGomb10jzqcxXrBjzev/J78Dn3qxnq7yvtFJLKsK8slU2Qje9p2NW+a4zQjaWyxTh5CN0sMxRwHtUdSyyBpNsBBYEhtdz5A2YUHUd4gMDbo3P+FoWO3a4DvQyRkddZ3seGzeOx5UcPDA66jrMR0bc+okjI9kd6L4sX+43+OXBB+Gzn50Y19q12YNqisga9JE3OCMrj5ODBnwHBcX5Fb+/vAEHeQOEYnnPPBM+//mwQRdZzJpVP4wsBgYmpjEeHKTqBuPce+/kMl40COfii7Ovf+lLriwk34+Iey9ZAzfSDA1NPC8anJP+zbe8zpwJhx3m8iOU7dvz5+DllblkXsf5nRyQFQ+sGR113/GCBZPL9/btsPPO488ND0/Oq7zvLU8uA4K0JnAPcGj0/yATLcKXAlu6rdk90lDLNXr66WNera+QOVVllPUHFfXflPVd+BLSP1F1UEVRq7qKm81np410fhXdk5ahzArw3X0jK/9it3eVlr9P/qbT6NPPli4/VeZGZs1TzbIa0wN7QixCX9kGB+vPP4xlL/rOfPvi04RMcSqakxkfsYvbLMIMvRB0M1wOfCH6P60IPwdc3O0E+R5VXaO+ijAueE3s7RdSwajmLwxddVuXrHDLKoeYELdT1ka3RRW1j5stDqOoIvJZXCCrYsnK27y4fPOuEyMz48FFeYsr+MYVqjh9KvG8PPOd81i0cHTV/Bsc9HPfphfdFpncNRAyOttXfp/vOM8Nm9XICMEUoSrAc4F7ge8Af49zj74X+DzwV+Bvu50g36OqIgwZKZieRpA1GbjOB5E+YvJWxa/SL1bFqmnCIsyrfLOGhJdN5SijrOWdl4a0FVBU6fk0CNL9zE30NSanm+RNUg+JK0RxVq3EVbMVYbpiL1tJp46y9ilXWWXBt7/SV5nVaUjnxR3v7FEFU4TxA/Ac4Pu4JdF2RMrwh8Bzup2YkKPpwTI+hb1qwfb5oJOVaFMWYZWKpM4w+7xGRpncZaMX0/emlWlZZZWnJNKjXIvCCW3MFIXnuxRf8l0MD9e3CONyVpQn8ZHncpwzx68RmFSEvt9NXkOp6vSfdCMwnrpTNJUm3fgty8cyfN2qWfcVlduqmCJMPwgPBx4LzOp2IgLlrj19Imt+m89Iu7qtw7IPOsuVV6ePsEoFktevlt5jLe3mmjkzXxH6uNPq9AWWPevbsi6yLH0aBKENp6JRiOn5d1n5m3Qr+jZYfBSn71JiRWUyqQjz4kqOvi7LK9/RvFXdlsl5jz75GK9B2wR5ac9bgtEswgYVYa8fdSbUF7U8iyauF30YIRSFk3bBlq0iUeS69anM6lTi6fzKs7Z9wy5zOxc1RMreX1Z8vgM6sgaI+FbAPhaBbx9onkWY9U6y5m369BGGLhyf1whMKsKispjceLjo3fp6JYrKZzrP04uE+04NgWqb4uYR0mAdGrI+wqyj6wJ062hyh/oifD/CkI/Cp+DHHfdF8lbtQymK09fdmEWWtd2kZV1nTmGWdXvGGWNeCrPsnvhd+brBQkmPbMxTJmmSK9hkzRHNkze0EZVFcnH7sn7pooZbctGGsqNocFSZKzYuCyFpD+2qyCMkzuFhGzWadXRdgG4dnVaEoX0TIa5LXwUmUtz6a2pUYlZaimQqylsfZVAl7KL0+lZIeVZAlvUfapU2NcK4KN0+WwXlpTUpT9U0hryzuCz4lvOifA39/qqUk7heCP2mmiA0TlOEpggfOjqpCKtaWiEtRJ8OeSjuDwiVD5xrxacF6jt4pUre+ixxl/dsk6Pw8kYKFhFqLfmuQlREnO4iReg7bzNPYSfd0r7L4cVHljvyjDPGgiy5vHdbFEbWXn5F76isEZfM6/RzeeHVebfJxrZvuRocNEWYdXRdgClPcMMb82ZR1dKqMr2hLL68EWKhbpz0x+ujhH2VThyez6r4eWn1rVTquB/zhsyHvLcqZaOKZZjV91s04jmtzOtO3RgYmDjKskghZSky32lKZf3ORenIy9MqC2GkR7mm875q4y3vvWYpv/i8rIyZIjRF+NDRSYuwaiVStc+g6IPPswirKuu0jFUHoCRlDxnhWpS3naYJi7CK5dJUHOvXZ7sbQ/phQ47kyMiiVXKyXJu+W3KVNRCKLNu8vAuduK9aXi8Upd2XkJHHRd+lKcKGFSHwcuDtwL6p62/rdsLKjk4qwqJKJHYZZi0rVcdNkrfAcV4fYZFCyWtx5g0YyBv84UMyr3zmPPr233Ri4IlPH6FvOGnZyiq5JqzOuFEU2kCJjyo7rJfJFL+7PGu7rML3yes8V2VoH24RZfVC3f7pojCyyklRg8cUYYOKEPhQtOTaGcBNwDsTv/1vtxNWdkx1H2HooIoqZIWZJ6/Ph+krY57rJ73yR5nryncF/ybWDq1K1kjBpigaIdlEP2ToROr0+6qyBVCZTJA9sKxIEYa+yyJ3YjqsqiOLy+qFOoOPfPIw5Ps1RdisIvwFMCP6fxi4DPhIdP7Tbies7KiiCNescZWfj/Lq1FD4UPIKfZPKoqilWrb8WMiebknZ01MZknncROvbhya2uErTxHspswirEuryT1pRZZZWWR9h3sCWEHzKRdWy49tA9vEE5DWay/Lbt5yYImxWEf4ydT4EfBn4DHB1txNWdoQqwqyRd01ZGZ2kbB5hE8q6rIIssiZCdvlOy55XgdSZLxhCJxShav33ktcf99nPjtWSq0iZpUeJpvvUfCv8uJykvQPpxk58f0g+lXUH+CwkkEfVslCmeH1c5ll50wlZVdUU4aQH3cLbT09dGwDOAR7sdsLKjlBFGBfYrB0S2kzVQh9SydQZWJEcheozarQszgULumcRtsULkJf+9K7voVRx7yWvp9frzJMl9rwkK/qs+EIt56IRx8nzmTP95EzL28RI5HSjraxvtcr7NEXYrCKcBzw657dnT2ki4EnAWcD5wEk+z4QqwrzVOZq2MpqmSqEPrWTKWq1FK4MklVOIrEWt+6qt+lCqLAzdNGX9rmlFWLZCiq/LP/leQwbchLjvmm7sFA2cqdpoquspKktHJ7wbpggbVISTAoIR4CUVnlsF3A5ck7p+KHAtcB1wsmdYA8BnfO41izCfKq3QpFsrXfHlhQUTwwqRtcwKDW3VV8FnYehOlg+ffte0IkzLVEVZ+T5TJ0/GxsYKlUBVBZFW+nnlx1fR1K0XyvKyE+XKFGFnFeGRVVyiwPOApycVYbTp7++Bx0V9j1cD+wEHABeljkdFz7wU+BHwGp9429ZH2Cm3WpVC7zNRvmgEaXp9St8h6SGyllmhcfiddFX6LAzdSY9BUb5m5U3WAgBVKlrfZ+rkSScswjppyaMJT1HRt98JT4MpwsmHuLTVR0SOBC5Q1cEKz44AF6nq/tH5QcCpqvqi6PwUAFX9oEdY31DVw3N+WwosBZg7d+6idevWBcm5eTM88MBWbr55DkNDsOeesNtuQUHkhrtxI+zYMX5tYAAWLKgf/tatW5kzZ07QM7/4Bdx/f/E9Q0NwwAETr+WlY3gY7rqrPH2hsm7eDLfcUi5rUZxFYcbvGCZf2223ifLm5VlWPjXFhg35v+29N9xww8Rr8+ZtZdOmORNkKgpj0aLweJPP1MmTrVu3snnzHO64Y+L1+B1CM99M3W8vTmOctzFNvvesMlmnXqhSJ8QsWbJkg6oeWD32ltKURqWiRRg9O8JEi/Bo4JzE+XHAJwqeX4ybz/hp4K0lcXV8ibVQOulWa6qPMOsIcYX5WLxNj7wLzdOsdBftNdntPsIqUxLSMnXSIqyTJ+vXj2WuV5pcqKEpL0qdcHpxNLlZhBl6ofQG+CPwbeCjwPHAIuBhGfc1qQhfmaEIP95kwqdqGyYfOulWqztqtEipNDUZuQlZfRc5L5IlVKF2e9TomjXly4ElZcpaAKCTfYTp+EPy5PTTxwqVfJuoM2q0G5ginHwMeBiNXwZmAq/DDWy5AviLiFwrIueLyL+JyFGRMmuKTcBeifN5wK1NBCwiR4jIyrvvvruJ4Bph/vyw61PB6CjceCOsWQOzZmXfs20bLFs2ft6tdIyOwsqVzp0l4v4OD4fLctNN/nFm3Rvn2Y4d7u/oqH94VRgdhUc8YvL1++8ffy9JmQ44YLJMWXm3cmWx7CHPpPMEYGTEuR9HRmDt2uw4Hngg+/pdd+XLVYe1a/3kyroXXN7u2AHLl7u89wmnU/IZFQjRmsBjgEOA/wusBjYA24Ad0dGURTgDuB7Ym/HBMk9usgXQJouwk261JuRdsybfMkpaWHXT0WTeNjnPrMwi7Ob8wRArvFMLAPgS8k6Klljrplx594YsaN5J+XwwizBDB9UOwE1Z2BfnznxfhefPA/4AbMdZgidG1w8DfosbPbqssQS3sI9QtV2jRrMI6RcKSUeZ664OVWTxcbHGiwDkbR47lX1EIX183VaEIbJOpWvUZ3WXuBzlzYk944ziUa6dlC8UU4QdUIS9erTJIuwkTcmbpyTSC2vXCbPqbg5NsmZN8QIASaukbIj/VMnrq4i7XXZDrNf168cm9X/OnNmZslEkl2/jaMWK4nmPnZKvCqYIJx8+fYTTijb2EfYCcb9Quu/trrtg6dJqfRbLlrl+xiTpfsepZnQUPvvZ/H5RGB++D/n9iiH9jXWo0sfXLUL6kHfbDc49d2K6zj23M+kqkiurjGYxNNS5PvI2jiGYbvSdIlTVC1V16S677NJtUR6iVzrCR0cha/pRVeXVbSWSR57SB6cgly8fPy+qpNauhd13dxW5iPu/E+92qgfpVGX58skNjHR+JpmqdBXJ5VMWZ81yc/tC09eEfEYz9J0ibBtr1zqLauNG5/DYuLG6hTUVNKm82tzSHR2FO+90o2aLrK28Suqww+CEEyaOcrzrLnjDG+Atb+mNhk/TtNV6LZIrrywODk68d7fdOpe+onB7pRHderrtm53qg5YNlul0H1Ob5W1jH2EIRaNGy9ax7Mbgmm73EYbQFll9+2C7IW/VgVrWR2h9hGjLXKNtdQ/m0aSbJt3SHRpqh4UQSpYLr+j9ufbYON3uFzXyaasVC+3sY+9V+k4Rto02uwezaLpiKJvw3auEvr+2NnyM9vbB9lojus30nSJs26jRNnWE+/Y3tLViaBPLl8PMmf73t7XhY7SXXmtEt5m+U4Rtc422xfXSa4N22s7oqBvunxx5OjwMJ53UnoaP0du0qRHd6/SdImwjbbCwrL+heeKRp/FQhjvvhDPPbEfDx+h92tKIng7M6LYARjuw/oapY3TUKiujGawsNUPfWYRt6yNsC9bfYBhGv9J3irBtfYRtwfobDMPoV/pOERrZWH+DYUxP0qPBN2/utkTtwxSh8RBtGLRTRJ3NU230q5GkX8pH1mjwjRunb3qrYorQ6AlCpndk3XvCCW7R6+le8Rnl9NNUoazR4Dt22GjwNH2nCG2wTG8SMr0j697t292i19O94jPK6aepQjYa3I++U4Q2WKY3CfmgfT7y6VrxGeVMpXLotgvWRoP70XeK0OhNQj5o34/cWsUT6XalPVXUVQ6++dQGF2zWaPCBARsNnsYUoQG0vxIMmd6RdW8W1ioepw2V9lRRZ6pQXj5ljcRsgws2azT4ggXtGwjXbUwRGj1RCYZM70jfOzzstnhKYnMkJ9KGSnuqqDNVKC+fbrll8r1t6Z9Ljwbfbbepjb8XMEVo9EwlGDK9I3nvnXfCqlU2R7KItlTaU0XVqUJ5+XH//ZOvWf9c7zBtFKGIzBaRDSLykm7L0mv0QyXY9jmS3cYqbT/y8iPtcQBbramX6LoiFJFVInK7iFyTun6oiFwrIteJyMkeQb0H+FJnpJzeWCVoWKXtR14+7bnn5HtttabeoeuKEFgNHJq8ICKDwCeBFwP7Aa8Wkf1E5AARuSh1PEpE/g74FXDbVAs/HbBK0LBK24+8fMrrdzNPRG/Q9W2YVPV7IjKSuvwM4DpVvR5ARNYBR6rqB4FJrk8RWQLMxinNv4rIxaq6o7OSTx/ij3PZMucOnT/fKUH7aPsL29LHj6x8uvzyrohiNISoardlIFKEF6nq/tH50cChqvrG6Pw44Jmq+raScI4H7lTVi3J+XwosBZg7d+6idevWBcu6detW5syZE/xct+gleXtJViiXd/NmN5rw/vtdH9Kee3Z3xF4v5W8vyQq9JW8dWZcsWbJBVQ9sWKTuo6pdP4AR4JrE+SuBcxLnxwEfbyiuI4CVCxcu1CqMjY1Veq5b9JK8vSSrarG8a9aozpoV703vjlmz3PVu0Uv520uyqjYj75o1qgsWqIq4v50qK3VkBa7SFuiMpo829BFmsQnYK3E+D7i1iYDVllgzpoBemZJitINemMs7nWmrIrwS2EdE9haRIeBY4OtNBGyLbjdD21ei6Tb9MCXFaA5rOHWXritCETkP+DGwr4hsEpETVfUB4G3AJcCvgS+p6i+biM8swvpY67Ucm5JihGANp+7SdUWoqq9W1ceo6kxVnaeqn4muX6yqT1DVx6tqYwP5zSKsj7Vey7EpKUYI1nDqLl1XhFONWYT1sdZrOTYvzwjBGk7dpe8UoVmE9bHWqx82mdrwxRpO3aXvFKFZhPWx1qthNI81nLpH3ylCswjr083Wq41WNQyjafpOEZpF2AzdaL3aaFXDMDpB3ynCXsMsoHFstKrhg30zRihdX3R7qhGRI4AjFi5c2G1RSoktoLjyjy0g6M/+AxutapRh34xRhb6zCHvJNWoW0ERstKpRhn0zRhX6ThH2EmYBTcRGqxpl2DdjVMEUYYsxC2giNtfKKMO+GaMKfacIe2n6hFlAk7G5VkYR9s0YVeg7RdhLfYRmARlGGPbNGFXoO0XYa5gFZLSNtk9PsG/GCKXvpk8YhlEdm55gTEfMIjQMwxubnmBMR0wRGobhjU1PMKYjfacIe2nUqGG0DZueYExH+k4R9tKoUcNoGzY9wZiO9J0iNAyjOjY9wZiO2KhRwzCCGB01xWdML8wiNAzDMPoaU4SGYRhGX2OK0DAMw+hrTBEahmEYfY0pQsMwDKOvEVXttgxdQUTuADZWeHR34M6GxekkvSRvL8kKJm8n6SVZobfkrSPrAlXdo0lh2kDfKsKqiMhVqnpgt+XwpZfk7SVZweTtJL0kK/SWvL0k61RhrlHDMAyjrzFFaBiGYfQ1pgjDWdltAQLpJXl7SVYweTtJL8kKvSVvL8k6JVgfoWEYhtHXmEVoGIZh9DWmCAMQkUNF5FoRuU5ETm6BPKtE5HYRuSZxbTcR+W8R+V3095GJ306JZL9WRF7UBXn3EpExEfm1iPxSRN7RVplF5GEicoWIXB3J+r62ypqIf1BEfioiF/WArDeKyC9E5GciclUPyLuriJwvIr+Jyu9BbZRXRPaN8jQ+/iwi72yjrK1CVe3wOIBB4PfA44Ah4Gpgvy7L9Dzg6cA1iWsfBk6O/j8Z+M/o//0imXcC9o7SMjjF8j4GeHr0/87AbyO5WiczIMCc6P+ZwP8Az2qjrAmZ/wn4AnBRD5SFG4HdU9faLO9ngTdG/w8Bu7ZZ3kiOQeCPwIK2y9rtwyxCf54BXKeq16vq/cA64MhuCqSq3wM2py4fiftoif6+LHF9narep6o3ANfh0jRlqOofVPV/o///Avwa2LONMqtja3Q6Mzq0jbICiMg84HDgnMTlVspaQCvlFZFH4BqdnwFQ1ftVdUtb5U1wMPB7Vd1I+2XtKqYI/dkTuDlxvim61jbmquofwCke4FHR9VbJLyIjwNNwllYrZY5cjT8Dbgf+W1VbKytwOvBuYEfiWltlBdeo+LaIbBCRpdG1tsr7OOAO4NzI9XyOiMxusbwxxwLnRf+3XdauYorQH8m41ktDblsjv4jMAdYD71TVPxfdmnFtymRW1QdV9anAPOAZIrJ/we1dk1VEXgLcrqobfB/JuDbVZeE5qvp04MXAW0XkeQX3dlveGbguiE+p6tOAe3DuxTy6LS8iMgS8FPhy2a0Z13qpXmsEU4T+bAL2SpzPA27tkixF3CYijwGI/t4eXW+F/CIyE6cE16rqBdHlVsscucEuBw6lnbI+B3ipiNyIc9m/QETWtFRWAFT11ujv7cBXcO64tsq7CdgUeQQAzscpxrbKC66B8b+qelt03mZZu44pQn+uBPYRkb2j1taxwNe7LFMWXwdeH/3/euBrievHishOIrI3sA9wxVQKJiKC62f5taqelvipdTKLyB4ismv0/8OBvwN+00ZZVfUUVZ2nqiO4cnmZqr62jbICiMhsEdk5/h84BLimrfKq6h+Bm0Vk3+jSwcCv2ipvxKsZd4vGMrVV1u7T7dE6vXQAh+FGOv4eWNYCec4D/gBsx7XsTgSGgUuB30V/d0vcvyyS/VrgxV2Q97k4t8vPgZ9Fx2FtlBl4CvDTSNZrgH+NrrdO1pTcixkfNdpKWXF9bldHxy/jb6mt8kbxPxW4KioPXwUe2VZ5gVnAXcAuiWutlLUth60sYxiGYfQ15ho1DMMw+hpThIZhGEZfY4rQMAzD6GtMERqGYRh9jSlCwzAMo68xRWgYDSAip4rInQ2Es7+IqIgsri+VYRg+mCI0DMMw+hpThIZhGEZfY4rQMBpGRBbH7k0R+bKIbBWR60XkLRn3vkVEbhaRe0TkQtyejel7BkTk5Gjz1PtE5Lci8vrE768UkR0icnDi2ki0Kev7O5ZQw5gmmCI0jM5xNm4ZsZfjFu3+pIg8tNebiBwJfBK4CDgK+AWwKiOcjwPvBVbi9hz8CrAq2nUCVf0y8MXo2iOiNV1XATcA/96RlBnGNGJGtwUwjGnMear6fgARuRw4Aqfw4kWNlwHfUtWTovNLRGQP4I1xACKyEDgJOEFV441VvxPtIPBvOCUK8Fbcmqj/hVO+zwX+Vt0m0oZhFGAWoWF0jm/H/6jqdtyCx/PAbfqL25j4a6lnLkidH4zbbPcrIjIjPnALJz81CgdV3Qy8CXgD8BHgfap6dfNJMozph1mEhtE5tqTO7wceFv2/B+77uz11T/p8d2AQuDsnjsfgdh4BuAy4DbfTwNnh4hpGf2KK0DC6wx3AA8CjUtfT55uj+56DswzTJBXnh3BK84/A6cBrmhDUMKY7pggNowuo6oMi8jPgSOCsxE9HpW69DKfcdlHV/84LL5qA/w/AMcCfcf2N61V1fYNiG8a0xBShYXSPDwAXiMincCNBnw8cmrxBVa8VkbOAdSLyYdzmsA8Dngw8QVXfKCJzgHOBL6rq+QAi8mngUyLyPVW9Y+qSZBi9hw2WMYwuoapfwVlxR+B2PX8acGLGrW8F/gN4HXAxsBo3jeJ70e8fxSnHtyWeeRewlYnWpmEYGdgO9YZhGEZfYxahYRiG0deYIjQMwzD6GlOEhmEYRl9jitAwDMPoa0wRGoZhGH2NKULDMAyjrzFFaBiGYfQ1pggNwzCMvsYUoWEYhtHX/H8fyvBC3VhEgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_plot(decoded_train, y_train, dataset=\"train\")\n",
    "saveName = \"trainingErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d10cf447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_plot(decoded_val, y_val, dataset=\"val\")\n",
    "saveName = \"validationErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "342bbd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_plot(decoded_test, y_test, dataset=\"test\")\n",
    "saveName = \"testErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8978795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "714cd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tTrain = t[index_train]+10\n",
    "tVal = t[index_val]+10\n",
    "tTest = t[index_test]+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fac613c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58d8635f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0004165], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4a78a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008099452476439383"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_error(predicted, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1d9be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "iTrain=[]\n",
    "iVal=[]\n",
    "iTest=[]\n",
    "for i, index in enumerate(index_train):\n",
    "    iTrain.append(y[index])\n",
    "for k , index in enumerate(index_val):\n",
    "    iVal.append(y[index])\n",
    "for j, index in enumerate(index_test):\n",
    "    iTest.append(y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ab1b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iTrain = np.array(iTrain)\n",
    "iVal = np.array(iVal)\n",
    "iTest = np.array(iTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87c41222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cm_trainTestSplit_Plot(i, Cm, cm, tTrain, tVal, tTest, iTrain, iVal, iTest):\n",
    "    \n",
    "    title_0_Cm = 'Gurney flap not attached (NACA0018)\\n$C_m$ prediction, $L_2$ error=%.4f' % l2_error_Cm    \n",
    "    title_n_Cm = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_m$ prediction, $L_2$ error=%.4f'%(l2_error_Cm)\n",
    "    \n",
    "    if i==0:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    else:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    \n",
    "    # CD graph plot\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm), 'k--', label='Predicted value')\n",
    "    plt.scatter(tTrain, denormalize(iTrain), color='b', label='Training set')\n",
    "    plt.scatter(tVal, denormalize(iVal), color='g', label='Validation set')\n",
    "    plt.scatter(tTest,denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylim([-0.05, 0.22])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "893cf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0] *= 5\n",
    "x[:,0] += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf177275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 error of Cm: 0.0081\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = [0.03]\n",
    "beta = [90]\n",
    "for i in range(0,1):\n",
    "    #Index from each dataset\n",
    "    iTrain_ = []\n",
    "    iVal_ = []\n",
    "    iTest_ = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    t_train = []\n",
    "    t_val = []\n",
    "    t_test = []\n",
    "    \n",
    "    predictedValue = predicted[t_len*i:t_len*(i+1),:]\n",
    "    y_corres = y[t_len*i:t_len*(i+1),:]\n",
    "    \n",
    "    l2_error_Cm = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    print('L2 error of Cm: {0:0.4f}'.format(l2_error_Cm))\n",
    "    \n",
    "    cm_ = predictedValue#denormalize(predictedValue)\n",
    "    Cm = y_corres#denormalize(y_corres)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        iTrain_.append(predicted[index])\n",
    "    for jj, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        iVal_.append(predicted[index])    \n",
    "    for kk, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & (index_test>=i*t_len))]):\n",
    "        iTest_.append(predicted[index])\n",
    "        \n",
    "#     iTrain = denormalize(np.array(iTrain))\n",
    "#     iTest = denormalize(np.array(iTest))\n",
    "#     iVal = denormalize(np.array(iVal))\n",
    "    iTrain_ = np.array(iTrain_)\n",
    "    iVal_ = np.array(iVal_)\n",
    "    iTest_ = np.array(iTest_)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        t_train.append(t[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        t_val.append(t[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & ((index_test>=i*t_len)))]):\n",
    "        t_test.append(t[index])\n",
    "        \n",
    "    tTrain = np.array(t_train)\n",
    "    tVal = np.array(t_val)\n",
    "    tTest = np.array(t_test)\n",
    "        \n",
    "    Cm_trainTestSplit_Plot(i, Cm, cm_, tTrain, tVal, tTest, iTrain_, iVal_, iTest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7eb11de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cm_trainTestSplit_Plot2(i, Cm, cm, tTrain, tVal, tTest, iTrain, iVal, iTest):\n",
    "    \n",
    "    title_0_Cm = 'Gurney flap not attached (NACA0018)\\n$C_m$ prediction, $L_2$ error=%.4f' % l2_error_Cm    \n",
    "    title_n_Cm = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_m$ prediction, $L_2$ error=%.4f'%(l2_error_Cm)\n",
    "    \n",
    "    if i==0:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    else:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    \n",
    "    # CD graph plot\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm), 'b--', label='Predicted value')\n",
    "#     plt.scatter(tTrain, denormalize(iTrain), color='b', label='Training set')\n",
    "#     plt.scatter(tVal, denormalize(iVal), color='g', label='Validation set')\n",
    "#     plt.scatter(tTest,denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b2cc294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 error of Cm: 0.0081\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    #Index from each dataset\n",
    "    iTrain_ = []\n",
    "    iVal_ = []\n",
    "    iTest_ = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    t_train = []\n",
    "    t_val = []\n",
    "    t_test = []\n",
    "    \n",
    "    predictedValue = predicted[t_len*i:t_len*(i+1),:]\n",
    "    y_corres = y[t_len*i:t_len*(i+1),:]\n",
    "    \n",
    "    l2_error_Cm = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    print('L2 error of Cm: {0:0.4f}'.format(l2_error_Cm))\n",
    "    \n",
    "    cm_ = predictedValue#denormalize(predictedValue)\n",
    "    Cm = y_corres#denormalize(y_corres)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        iTrain_.append(predicted[index])\n",
    "    for jj, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        iVal_.append(predicted[index])    \n",
    "    for kk, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & (index_test>=i*t_len))]):\n",
    "        iTest_.append(predicted[index])\n",
    "        \n",
    "#     iTrain = denormalize(np.array(iTrain))\n",
    "#     iTest = denormalize(np.array(iTest))\n",
    "#     iVal = denormalize(np.array(iVal))\n",
    "    iTrain_ = np.array(iTrain_)\n",
    "    iVal_ = np.array(iVal_)\n",
    "    iTest_ = np.array(iTest_)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        t_train.append(t[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        t_val.append(t[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & ((index_test>=i*t_len)))]):\n",
    "        t_test.append(t[index])\n",
    "        \n",
    "    tTrain = np.array(t_train)\n",
    "    tVal = np.array(t_val)\n",
    "    tTest = np.array(t_test)\n",
    "        \n",
    "    Cm_trainTestSplit_Plot2(i, Cm, cm_, tTrain, tVal, tTest, iTrain_, iVal_, iTest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b420f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 error of Cm: 0.0081\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    #Index from each dataset\n",
    "    iTrain_ = []\n",
    "    iVal_ = []\n",
    "    iTest_ = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    t_train = []\n",
    "    t_val = []\n",
    "    t_test = []\n",
    "    title_n_Cm = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_m$ prediction, $L_2$ error=%.4f'%(l2_error_Cm)\n",
    "    \n",
    "    title_Cm = title_n_Cm\n",
    "    savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "\n",
    "    predictedValue = predicted[t_len*i:t_len*(i+1),:]\n",
    "    y_corres = y[t_len*i:t_len*(i+1),:]\n",
    "    \n",
    "    l2_error_Cm = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    print('L2 error of Cm: {0:0.4f}'.format(l2_error_Cm))\n",
    "    \n",
    "    cm_ = predictedValue#denormalize(predictedValue)\n",
    "    Cm = y_corres#denormalize(y_corres)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        iTrain_.append(predicted[index])\n",
    "    for jj, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        iVal_.append(predicted[index])    \n",
    "    for kk, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & (index_test>=i*t_len))]):\n",
    "        iTest_.append(predicted[index])\n",
    "        \n",
    "#     iTrain = denormalize(np.array(iTrain))\n",
    "#     iTest = denormalize(np.array(iTest))\n",
    "#     iVal = denormalize(np.array(iVal))\n",
    "    iTrain_ = np.array(iTrain_)\n",
    "    iVal_ = np.array(iVal_)\n",
    "    iTest_ = np.array(iTest_)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        t_train.append(t[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        t_val.append(t[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & ((index_test>=i*t_len)))]):\n",
    "        t_test.append(t[index])\n",
    "        \n",
    "    tTrain = np.array(t_train)\n",
    "    tVal = np.array(t_val)\n",
    "    tTest = np.array(t_test)\n",
    "        \n",
    "#     Cm_trainTestSplit_Plot2(i, Cm, cm_, tTrain, tVal, tTest, iTrain_, iVal_, iTest_)\n",
    "\n",
    "    # CD graph plot\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm_), 'k--', label='Predicted value')\n",
    "    #plt.scatter(tTrain, iTrain, color='b', label='Training set')\n",
    "    #plt.scatter(tVal, iVal, color='g', label='Validation set')\n",
    "    plt.scatter(tTest, denormalize(iTest_), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper right')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    #plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm_), 'k--', label='Predicted value')\n",
    "    plt.scatter(tTrain, denormalize(iTrain_), color='b', label='Training set')\n",
    "    #plt.scatter(tVal, iVal, color='g', label='Validation set')\n",
    "#     plt.scatter(tTest, denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper right')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    #plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm_), 'k--', label='Predicted value')\n",
    "    #plt.scatter(tTrain, iTrain, color='b', label='Training set')\n",
    "    plt.scatter(tVal, denormalize(iVal_), color='g', label='Validation set')\n",
    "#     plt.scatter(tTest, denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper right')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    #plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f20d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

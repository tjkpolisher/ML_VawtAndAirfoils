{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc91fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the steady-state simulation - Case 1: MLP with Optimal settings\n",
    "## Optimal Settings are like below:\n",
    "# 1. Train/Validation/Test dataset ratio = 0.7/0.2/0.1\n",
    "# 2. Cd scaling -> replaced as normalization for both Cl and Cd\n",
    "# 3. Seperate the ML models into the two models, a model only for Cl and the other for Cd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy import interpolate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1130c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining parameters and hyperparameters of the model\n",
    "\n",
    "n_units=256 # Number of units in the hidden layer of the MLP network\n",
    "n_layers=5\n",
    "input_size = 110 + 3 # Size of input for the network (110 coefficients and 3 other parameters, time, h, beta)\n",
    "lr = 1e-04 # Learning rate of the network\n",
    "test_rate = 0.1 # Defines the ratio of test dataset\n",
    "val_rate = 0.2 # Defines the ratio of validation dataset\n",
    "n_data = 16 # Number of txt files from which the aerodynamic coefficients are extracted\n",
    "batch_size = 200 # Mini-batch size\n",
    "l2_regularizer=1e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d84b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing working directory\n",
    "\n",
    "main_directory = 'D:\\\\VAWT_data\\\\flap_steady\\\\flap_steady'\n",
    "os.chdir(main_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic parameters\n",
    "\n",
    "c = 1 # Chord length\n",
    "h = np.array([0.01, 0.02, 0.03]) * c # Height of the Gurney flaps\n",
    "thickness = 0.02 * h # Thickness of the Gurney flaps\n",
    "beta = np.linspace(30, 90, 5).reshape((5,1))\n",
    "\n",
    "h = h[-1]\n",
    "beta = beta[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18aaa915",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h.reshape((-1,1))\n",
    "thickness = thickness.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9745480",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_interval = 0.001\n",
    "t_len = int((11-10) / t_interval)\n",
    "\n",
    "n_beta = len(beta)# Number of the Gurney flap inclination\n",
    "n_h = len(h) # Number of the height of the Gurney flaps\n",
    "n_cases = n_data * t_len # Total number of cases(Number of geometries * Number of angles of attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d7f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Input dataset\n",
    "# Defining time as input\n",
    "\n",
    "main_directory = 'D:\\\\VAWT_data'\n",
    "cm_dir = main_directory + \"\\\\blade_1_cm_data\"\n",
    "cm_list = os.listdir(cm_dir)\n",
    "os.chdir(cm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5014fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_target = [file for file in cm_list if file.endswith('.csv')]\n",
    "cm_target = sorted(cm_target, key=lambda s: int(re.search(r'\\d+',s).group()))\n",
    "cm_target = [cm_target[-8],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d3bbcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case15cm_blade1.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62d3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create input and output data\n",
    "### This function is the main framework where data are reordered with respect to the shape the NNs require.\n",
    "### Each input features are made by calling the corresponding functions, which generate the data,\n",
    "### e.g., time, cm, h, beta, coordinates of airfoil and Gurney flaps, etc.\n",
    "def genereate_input_output(cm_target, n_beta, t_len, normalize:bool=False):\n",
    "    \n",
    "    input_time_cm = time_and_cm(cm_target)\n",
    "    t = input_time_cm[:,0].reshape((-1, 1))\n",
    "    cm = input_time_cm[:,1].reshape((-1, 1))\n",
    "    \n",
    "    hh = generate_h(n_beta, t_len, normalize)\n",
    "    bb = generate_beta(n_beta, t_len, normalize)\n",
    "#     total_coords = generate_coordinates(n_cases)\n",
    "    \n",
    "    # Concatenate data for input dataset\n",
    "    #x = np.hstack((t, hh, bb, total_coords))\n",
    "    #x = np.hstack((t, total_coords))\n",
    "    x = t\n",
    "    #x = np.hstack((t, hh, bb))\n",
    "    \n",
    "    # Generating output dataset (depending on whether the data be normalized or not)\n",
    "    if normalize==True:\n",
    "        y = (cm-np.min(cm))/(np.max(cm)-np.min(cm))\n",
    "    else:\n",
    "        y = cm\n",
    "    print(\"Dimension - x: \", x.shape)\n",
    "    print(\"Dimension - y: \", y.shape)\n",
    "    \n",
    "    return x, y, t, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa96208",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating time for input, and Cm (moment coefficient) for output\n",
    "def time_and_cm(cm_target):\n",
    "    cm_df = pd.DataFrame()\n",
    "    for i, file in enumerate(cm_target):\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        data = pd.read_csv(file, header=None)\n",
    "        df = pd.concat([df, data], axis=0)\n",
    "        \n",
    "        time = df.iloc[:,0].values\n",
    "        cm = df.iloc[:,1].values\n",
    "        \n",
    "        time_beUsed = time[np.where(np.logical_and(time>=10, time<11))]\n",
    "        cm_beUsed = cm[np.where(np.logical_and(time>=10, time<11))]\n",
    "        \n",
    "        # Handle the time that is duplicated because of digits\n",
    "        # Also, outliers are regulated at the second conditional statement.\n",
    "        time_beUsed = handler_time(time_beUsed)\n",
    "        cm_beUsed = handler_cm(cm_beUsed)\n",
    "        \n",
    "        linear_func = interpolate.interp1d(time_beUsed, cm_beUsed,\n",
    "                                           bounds_error=False,kind='quadratic',\n",
    "                                           fill_value='extrapolate')\n",
    "        time_interp = np.arange(10, 11, t_interval).reshape((-1,1))\n",
    "        cm_interp=linear_func(time_interp).reshape((-1,1))\n",
    "        \n",
    "        cm_df = pd.concat([cm_df, pd.DataFrame(np.hstack((time_interp, cm_interp)))], axis=0)\n",
    "    \n",
    "    input_time_cm = cm_df.iloc[:,:].values\n",
    "    print(\"Dimension - time and Cm: \", input_time_cm.shape)\n",
    "    return input_time_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c96f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling duplicated time value\n",
    "def handler_time(time_beUsed):\n",
    "    for i in range(len(time_beUsed)):\n",
    "        if time_beUsed[i]==time_beUsed[i-1]:\n",
    "            time_beUsed[i] += 0.0005\n",
    "            \n",
    "    return time_beUsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d715ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outlier, (if there are)\n",
    "def handler_cm(cm_beUsed):\n",
    "    period = int(len(cm_beUsed) / 5)\n",
    "    for i in range(len(cm_beUsed)):\n",
    "        if np.abs(cm_beUsed[i]-cm_beUsed[i-1])>0.3:\n",
    "            cm_beUsed[i-1] = cm_beUsed[i-1 + period]\n",
    "            \n",
    "    return cm_beUsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "937cc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining beta, the Gurney flap inclination\n",
    "## In case of mere NACA0018, the bb in those indexes are considered as zero.\n",
    "def generate_beta(n_beta=n_beta, t_len=t_len, normalize=True):\n",
    "\n",
    "#     beta_0 = np.zeros((t_len,1)) # Values for sheer NACA0018\n",
    "    b_ = np.ones((t_len,1)) # Template for the inclination for a single h and single beta\n",
    "    bb_imp = np.zeros((t_len*n_beta,1))\n",
    "\n",
    "    for j in range(n_beta):\n",
    "        b_imp = b_ * beta[j]\n",
    "        bb_imp[t_len*j:t_len*(j+1),:] = b_imp[:,:]\n",
    "\n",
    "    bb_imp = bb_imp.reshape((-1,1))\n",
    "    bb = bb_imp\n",
    "    if normalize==True:\n",
    "        bb = bb / np.max(beta)\n",
    "    \n",
    "    print(\"Dimension - inclination(beta): \", bb.shape)\n",
    "\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6302058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the Gurney flap height\n",
    "## In case of mere NACA0018, the hh in those indexes are considered as zero.\n",
    "def generate_h(n_beta=n_beta, t_len=t_len, normalize:bool=True):\n",
    "    #hh = np.concatenate((np.zeros(t_len), h[0]*np.ones(n_beta*t_len), h[1]*np.ones(n_beta*t_len), h[2]*np.ones(n_beta*t_len)))\n",
    "    hh = h[0]*np.ones(n_beta*t_len)\n",
    "    hh = hh.reshape((-1,1))\n",
    "    \n",
    "    if normalize==True:\n",
    "        hh = hh / np.max(h)\n",
    "    \n",
    "    print(\"Dimension - heights of Gurney flaps: \", hh.shape)\n",
    "    return hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2d882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generates coordinate data of NACA0018 airfoil and Gurney flaps\n",
    "def generate_coordinates(n_cases):\n",
    "    origin_coord = \"D:\\\\AirfoilClCdCoordinates_out\\\\AirfoilClCdCoordinates_out\\\\airfoil15\"\n",
    "\n",
    "    csv_file_name = origin_coord + '\\\\airfoilOut15.txt'\n",
    "    data = pd.read_csv(csv_file_name, header=None)\n",
    "    \n",
    "    baseline_coord_high = data.iloc[0,:]\n",
    "    baseline_coord_low = data.iloc[1,:]\n",
    "    baseline_coord = np.hstack((np.flip(baseline_coord_high), baseline_coord_low)).reshape((1,-1))\n",
    "    airfoil_coord = np.repeat(baseline_coord, n_cases, axis=0)\n",
    "    print(\"Dimension - airfoil coordinates: \", airfoil_coord.shape)\n",
    "    \n",
    "    flap_coords= coord_with_flaps(n_cases)\n",
    "    total_coords = np.hstack((airfoil_coord, flap_coords))\n",
    "    \n",
    "    print(\"Dimension - total coordinates: \", total_coords.shape)\n",
    "    \n",
    "    return total_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a25f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data of Gurney flap coordinates\n",
    "def coord_with_flaps(n_cases):\n",
    "    flap_left = np.zeros((15,5))\n",
    "    flap_right = np.zeros((15,5))\n",
    "\n",
    "    for i in range(n_h):\n",
    "        # Defining coordinates of the flaps with respect to beta=90 degree.\n",
    "        yLeft = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "        yRight = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "        xLeft = 0.5*np.ones((5,1)) - 0.02*h[i]\n",
    "        xRight = 0.5*np.ones((5,1))\n",
    "\n",
    "        for j in range(n_beta):\n",
    "            betaValue = beta[j]\n",
    "\n",
    "            # Rotating transformation\n",
    "            rotateTransf = np.array([[np.cos(90-betaValue), -np.sin(90-betaValue)],\n",
    "                                     [np.sin(90-betaValue), np.cos(90-betaValue)]])\n",
    "            rotateTransf = rotateTransf.reshape((2,2))\n",
    "\n",
    "            LeftImp = np.hstack((xLeft-0.5, yLeft))\n",
    "            RightImp = np.hstack((xRight-0.5, yRight))\n",
    "\n",
    "            rotatedFlapLeft = rotateTransf @ LeftImp.T # shape: 2*5 (x-coordinates on first row, y-coordinates on second row)\n",
    "            rotatedFlapRight = rotateTransf @ RightImp.T\n",
    "\n",
    "            # All we need is the y-coordinates of the flaps\n",
    "            flap_left[5*i+j,:] = rotatedFlapLeft[1,:]\n",
    "            flap_right[5*i+j,:] = rotatedFlapRight[1,:]\n",
    "    \n",
    "    flap_coords = np.hstack((flap_left, np.flip(flap_right, axis=1)))\n",
    "    flap_coords2 = np.zeros((n_cases, 10))\n",
    "    \n",
    "    for i in range(t_len, n_cases):\n",
    "        flap_coords2[i,:] = flap_coords[i%15,:]\n",
    "    print(\"Dimension - coord with flaps: \", flap_coords2.shape)\n",
    "    \n",
    "    return flap_coords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc16d509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension - time and Cm:  (1000, 2)\n",
      "Dimension - heights of Gurney flaps:  (1000, 1)\n",
      "Dimension - inclination(beta):  (1000, 1)\n",
      "Dimension - x:  (1000, 1)\n",
      "Dimension - y:  (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generating x, y and cm (for denormalizing)\n",
    "x, y, t, cm = genereate_input_output(cm_target, n_beta, t_len, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "766ff9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0] -= 10\n",
    "x[:,0] /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7444ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(x, y, cm, test_rate, random_state=1, **kwargs):\n",
    "    if kwargs.get('validation')==True:\n",
    "        val_rate = kwargs.get('val_rate')\n",
    "        x_all, x_test, y_all, y_test, cm_all, cm_test = train_test_split(x, y, cm, test_size=test_rate, random_state=kwargs.get('random_state'))\n",
    "        x_train, x_val, y_train, y_val, cm_train, cm_val = train_test_split(x_all, y_all, cm_all, test_size=val_rate/(1-test_rate),\n",
    "                                                                            random_state=kwargs.get('random_state'))\n",
    "        return x_train, x_val, x_test, y_train, y_val, y_test, cm_train, cm_val, cm_test\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test, cm_train, cm_test = train_test_split(x, y, cm, test_size=test_rate, random_state=kwargs.get('random_state'))\n",
    "        return x_train, x_test, y_train, y_test, cm_train, cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec562b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test, cm_train, cm_val, cm_test = dataset_split(x, y, cm,\n",
    "                                                                                          test_rate, val_rate=val_rate,\n",
    "                                                                                          validation=True, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37290049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp_model(num_layer:int = 1): # This function can only generate model with at least 3 hidden layers\n",
    "    input_data = tf.keras.Input(shape=1)\n",
    "\n",
    "    # The first hidden layer\n",
    "    x_fc = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc1',\n",
    "                                #kernel_initializer='he_uniform',\n",
    "                                kernel_regularizer=regularizers.l2(l2_regularizer))(input_data)\n",
    "    \n",
    "    # The other hidden layers, which will be placed between the first hidden layer and the last hidden layer.\n",
    "    # The number of layers that the user desires is input of this function.\n",
    "    for i in range(0, num_layer-2):\n",
    "        x_fc = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc%d' % (i+2),\n",
    "                                     #kernel_initializer='he_uniform',\n",
    "                                     kernel_regularizer=regularizers.l2(l2_regularizer))(x_fc)\n",
    "    \n",
    "    # The last hidden layer\n",
    "    x_fc_final = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc%d' % num_layer,\n",
    "                                       #kernel_initializer='he_uniform',\n",
    "                                       kernel_regularizer=regularizers.l2(l2_regularizer))(x_fc)\n",
    "\n",
    "    # The output layer\n",
    "    output_data = tf.keras.layers.Dense(units=1, activation='linear', name='outputLayer')(x_fc_final)\n",
    "    \n",
    "    # MLP(FC layer)-based\n",
    "    model = tf.keras.Model(input_data, output_data)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3c482f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 256)               512       \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " fc3 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " fc4 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " fc5 (Dense)                 (None, 256)               65792     \n",
      "                                                                 \n",
      " outputLayer (Dense)         (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 263,937\n",
      "Trainable params: 263,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_mlp_model(num_layer=n_layers)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30f3abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss = tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "360fbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221230\\\\Case13_WithoutParameters\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b6c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = model_directory + \"20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_\"+str(val_rate) + \"_test\"+str(test_rate)+ \"_\" +str(n_layers)+\"layers_\"+ str(n_units) +\"units_checkpoint.h5\"\n",
    "\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(ckpt_name, monitor=\"val_loss\", mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200, min_delta=1e-05,\n",
    "                                      restore_best_weights=True, verbose=1)\n",
    "rp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=100, factor=0.5,\n",
    "                                          min_delta = 1e-05, min_lr=1e-05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4cc904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = len(x_train)//batch_size\n",
    "VALIDATION_STEPS = len(x_val)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17d6b26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/3 [=========>....................] - ETA: 2s - loss: 0.1792 - rmse: 0.4232\n",
      "Epoch 1: val_loss improved from inf to 0.18935, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 2s 116ms/step - loss: 0.2076 - rmse: 0.4555 - val_loss: 0.1894 - val_rmse: 0.4350 - lr: 1.0000e-04\n",
      "Epoch 2/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2511 - rmse: 0.5010\n",
      "Epoch 2: val_loss improved from 0.18935 to 0.18269, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2132 - rmse: 0.4616 - val_loss: 0.1827 - val_rmse: 0.4273 - lr: 1.0000e-04\n",
      "Epoch 3/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1932 - rmse: 0.4394\n",
      "Epoch 3: val_loss improved from 0.18269 to 0.17565, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.2089 - rmse: 0.4569 - val_loss: 0.1757 - val_rmse: 0.4190 - lr: 1.0000e-04\n",
      "Epoch 4/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1992 - rmse: 0.4462\n",
      "Epoch 4: val_loss improved from 0.17565 to 0.16805, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.1877 - rmse: 0.4331 - val_loss: 0.1681 - val_rmse: 0.4098 - lr: 1.0000e-04\n",
      "Epoch 5/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1730 - rmse: 0.4158\n",
      "Epoch 5: val_loss improved from 0.16805 to 0.15971, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.1832 - rmse: 0.4279 - val_loss: 0.1597 - val_rmse: 0.3995 - lr: 1.0000e-04\n",
      "Epoch 6/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1878 - rmse: 0.4332\n",
      "Epoch 6: val_loss improved from 0.15971 to 0.15049, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.1699 - rmse: 0.4121 - val_loss: 0.1505 - val_rmse: 0.3878 - lr: 1.0000e-04\n",
      "Epoch 7/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1766 - rmse: 0.4201\n",
      "Epoch 7: val_loss improved from 0.15049 to 0.14037, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.1725 - rmse: 0.4152 - val_loss: 0.1404 - val_rmse: 0.3745 - lr: 1.0000e-04\n",
      "Epoch 8/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1586 - rmse: 0.3981\n",
      "Epoch 8: val_loss improved from 0.14037 to 0.12946, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.1553 - rmse: 0.3939 - val_loss: 0.1295 - val_rmse: 0.3597 - lr: 1.0000e-04\n",
      "Epoch 9/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1470 - rmse: 0.3833\n",
      "Epoch 9: val_loss improved from 0.12946 to 0.11793, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1471 - rmse: 0.3834 - val_loss: 0.1179 - val_rmse: 0.3433 - lr: 1.0000e-04\n",
      "Epoch 10/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1077 - rmse: 0.3281\n",
      "Epoch 10: val_loss improved from 0.11793 to 0.10618, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.1123 - rmse: 0.3350 - val_loss: 0.1062 - val_rmse: 0.3257 - lr: 1.0000e-04\n",
      "Epoch 11/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1542 - rmse: 0.3926\n",
      "Epoch 11: val_loss improved from 0.10618 to 0.09450, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.1251 - rmse: 0.3535 - val_loss: 0.0945 - val_rmse: 0.3072 - lr: 1.0000e-04\n",
      "Epoch 12/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1115 - rmse: 0.3338\n",
      "Epoch 12: val_loss improved from 0.09450 to 0.08365, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.1087 - rmse: 0.3296 - val_loss: 0.0836 - val_rmse: 0.2890 - lr: 1.0000e-04\n",
      "Epoch 13/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1017 - rmse: 0.3188\n",
      "Epoch 13: val_loss improved from 0.08365 to 0.07452, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0943 - rmse: 0.3069 - val_loss: 0.0745 - val_rmse: 0.2728 - lr: 1.0000e-04\n",
      "Epoch 14/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0785 - rmse: 0.2800\n",
      "Epoch 14: val_loss improved from 0.07452 to 0.06827, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0746 - rmse: 0.2729 - val_loss: 0.0683 - val_rmse: 0.2611 - lr: 1.0000e-04\n",
      "Epoch 15/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0886 - rmse: 0.2976\n",
      "Epoch 15: val_loss improved from 0.06827 to 0.06556, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0807 - rmse: 0.2840 - val_loss: 0.0656 - val_rmse: 0.2558 - lr: 1.0000e-04\n",
      "Epoch 16/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0700 - rmse: 0.2644\n",
      "Epoch 16: val_loss did not improve from 0.06556\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0748 - rmse: 0.2734 - val_loss: 0.0663 - val_rmse: 0.2573 - lr: 1.0000e-04\n",
      "Epoch 17/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0732 - rmse: 0.2704\n",
      "Epoch 17: val_loss did not improve from 0.06556\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0763 - rmse: 0.2760 - val_loss: 0.0688 - val_rmse: 0.2622 - lr: 1.0000e-04\n",
      "Epoch 18/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0602 - rmse: 0.2452\n",
      "Epoch 18: val_loss did not improve from 0.06556\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0699 - rmse: 0.2642 - val_loss: 0.0699 - val_rmse: 0.2641 - lr: 1.0000e-04\n",
      "Epoch 19/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0784 - rmse: 0.2799\n",
      "Epoch 19: val_loss did not improve from 0.06556\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0793 - rmse: 0.2814 - val_loss: 0.0689 - val_rmse: 0.2623 - lr: 1.0000e-04\n",
      "Epoch 20/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0778 - rmse: 0.2788\n",
      "Epoch 20: val_loss did not improve from 0.06556\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0718 - rmse: 0.2677 - val_loss: 0.0669 - val_rmse: 0.2585 - lr: 1.0000e-04\n",
      "Epoch 21/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0671 - rmse: 0.2589\n",
      "Epoch 21: val_loss improved from 0.06556 to 0.06507, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0715 - rmse: 0.2672 - val_loss: 0.0651 - val_rmse: 0.2549 - lr: 1.0000e-04\n",
      "Epoch 22/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0780 - rmse: 0.2792\n",
      "Epoch 22: val_loss improved from 0.06507 to 0.06414, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0714 - rmse: 0.2671 - val_loss: 0.0641 - val_rmse: 0.2531 - lr: 1.0000e-04\n",
      "Epoch 23/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0804 - rmse: 0.2834\n",
      "Epoch 23: val_loss improved from 0.06414 to 0.06371, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0734 - rmse: 0.2707 - val_loss: 0.0637 - val_rmse: 0.2522 - lr: 1.0000e-04\n",
      "Epoch 24/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0696 - rmse: 0.2637\n",
      "Epoch 24: val_loss improved from 0.06371 to 0.06347, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0700 - rmse: 0.2643 - val_loss: 0.0635 - val_rmse: 0.2517 - lr: 1.0000e-04\n",
      "Epoch 25/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0725 - rmse: 0.2690\n",
      "Epoch 25: val_loss improved from 0.06347 to 0.06327, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0694 - rmse: 0.2632 - val_loss: 0.0633 - val_rmse: 0.2513 - lr: 1.0000e-04\n",
      "Epoch 26/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0794 - rmse: 0.2817\n",
      "Epoch 26: val_loss improved from 0.06327 to 0.06308, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0734 - rmse: 0.2706 - val_loss: 0.0631 - val_rmse: 0.2510 - lr: 1.0000e-04\n",
      "Epoch 27/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0654 - rmse: 0.2555\n",
      "Epoch 27: val_loss improved from 0.06308 to 0.06298, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0666 - rmse: 0.2579 - val_loss: 0.0630 - val_rmse: 0.2507 - lr: 1.0000e-04\n",
      "Epoch 28/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0797 - rmse: 0.2822\n",
      "Epoch 28: val_loss improved from 0.06298 to 0.06298, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0721 - rmse: 0.2684 - val_loss: 0.0630 - val_rmse: 0.2507 - lr: 1.0000e-04\n",
      "Epoch 29/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0519 - rmse: 0.2277\n",
      "Epoch 29: val_loss did not improve from 0.06298\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0673 - rmse: 0.2592 - val_loss: 0.0630 - val_rmse: 0.2509 - lr: 1.0000e-04\n",
      "Epoch 30/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0819 - rmse: 0.2859\n",
      "Epoch 30: val_loss did not improve from 0.06298\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0771 - rmse: 0.2775 - val_loss: 0.0631 - val_rmse: 0.2510 - lr: 1.0000e-04\n",
      "Epoch 31/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0679 - rmse: 0.2604\n",
      "Epoch 31: val_loss improved from 0.06298 to 0.06287, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0645 - rmse: 0.2537 - val_loss: 0.0629 - val_rmse: 0.2505 - lr: 1.0000e-04\n",
      "Epoch 32/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0682 - rmse: 0.2609\n",
      "Epoch 32: val_loss improved from 0.06287 to 0.06244, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0679 - rmse: 0.2604 - val_loss: 0.0624 - val_rmse: 0.2497 - lr: 1.0000e-04\n",
      "Epoch 33/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0621 - rmse: 0.2490\n",
      "Epoch 33: val_loss improved from 0.06244 to 0.06201, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0673 - rmse: 0.2592 - val_loss: 0.0620 - val_rmse: 0.2488 - lr: 1.0000e-04\n",
      "Epoch 34/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0747 - rmse: 0.2731\n",
      "Epoch 34: val_loss improved from 0.06201 to 0.06173, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0674 - rmse: 0.2594 - val_loss: 0.0617 - val_rmse: 0.2483 - lr: 1.0000e-04\n",
      "Epoch 35/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0722 - rmse: 0.2686\n",
      "Epoch 35: val_loss improved from 0.06173 to 0.06150, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0701 - rmse: 0.2645 - val_loss: 0.0615 - val_rmse: 0.2478 - lr: 1.0000e-04\n",
      "Epoch 36/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0740 - rmse: 0.2718\n",
      "Epoch 36: val_loss improved from 0.06150 to 0.06132, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0673 - rmse: 0.2593 - val_loss: 0.0613 - val_rmse: 0.2474 - lr: 1.0000e-04\n",
      "Epoch 37/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0760 - rmse: 0.2754\n",
      "Epoch 37: val_loss improved from 0.06132 to 0.06121, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0673 - rmse: 0.2592 - val_loss: 0.0612 - val_rmse: 0.2472 - lr: 1.0000e-04\n",
      "Epoch 38/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0670 - rmse: 0.2586\n",
      "Epoch 38: val_loss improved from 0.06121 to 0.06104, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0679 - rmse: 0.2604 - val_loss: 0.0610 - val_rmse: 0.2469 - lr: 1.0000e-04\n",
      "Epoch 39/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0723 - rmse: 0.2687\n",
      "Epoch 39: val_loss improved from 0.06104 to 0.06085, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0658 - rmse: 0.2563 - val_loss: 0.0608 - val_rmse: 0.2465 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0692 - rmse: 0.2628\n",
      "Epoch 40: val_loss improved from 0.06085 to 0.06061, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0663 - rmse: 0.2573 - val_loss: 0.0606 - val_rmse: 0.2460 - lr: 1.0000e-04\n",
      "Epoch 41/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0570 - rmse: 0.2385\n",
      "Epoch 41: val_loss improved from 0.06061 to 0.06026, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0631 - rmse: 0.2511 - val_loss: 0.0603 - val_rmse: 0.2453 - lr: 1.0000e-04\n",
      "Epoch 42/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0836 - rmse: 0.2890\n",
      "Epoch 42: val_loss improved from 0.06026 to 0.06017, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0717 - rmse: 0.2676 - val_loss: 0.0602 - val_rmse: 0.2451 - lr: 1.0000e-04\n",
      "Epoch 43/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0626 - rmse: 0.2500\n",
      "Epoch 43: val_loss improved from 0.06017 to 0.05999, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0662 - rmse: 0.2571 - val_loss: 0.0600 - val_rmse: 0.2447 - lr: 1.0000e-04\n",
      "Epoch 44/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0561 - rmse: 0.2365\n",
      "Epoch 44: val_loss improved from 0.05999 to 0.05970, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0620 - rmse: 0.2488 - val_loss: 0.0597 - val_rmse: 0.2441 - lr: 1.0000e-04\n",
      "Epoch 45/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0661 - rmse: 0.2569\n",
      "Epoch 45: val_loss improved from 0.05970 to 0.05951, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0669 - rmse: 0.2584 - val_loss: 0.0595 - val_rmse: 0.2437 - lr: 1.0000e-04\n",
      "Epoch 46/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0521 - rmse: 0.2281\n",
      "Epoch 46: val_loss improved from 0.05951 to 0.05920, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0629 - rmse: 0.2505 - val_loss: 0.0592 - val_rmse: 0.2431 - lr: 1.0000e-04\n",
      "Epoch 47/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0657 - rmse: 0.2562\n",
      "Epoch 47: val_loss improved from 0.05920 to 0.05891, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0634 - rmse: 0.2515 - val_loss: 0.0589 - val_rmse: 0.2425 - lr: 1.0000e-04\n",
      "Epoch 48/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0620 - rmse: 0.2487\n",
      "Epoch 48: val_loss improved from 0.05891 to 0.05864, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0635 - rmse: 0.2518 - val_loss: 0.0586 - val_rmse: 0.2420 - lr: 1.0000e-04\n",
      "Epoch 49/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0632 - rmse: 0.2512\n",
      "Epoch 49: val_loss improved from 0.05864 to 0.05836, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0633 - rmse: 0.2514 - val_loss: 0.0584 - val_rmse: 0.2414 - lr: 1.0000e-04\n",
      "Epoch 50/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0636 - rmse: 0.2519\n",
      "Epoch 50: val_loss improved from 0.05836 to 0.05823, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0656 - rmse: 0.2559 - val_loss: 0.0582 - val_rmse: 0.2411 - lr: 1.0000e-04\n",
      "Epoch 51/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0573 - rmse: 0.2391\n",
      "Epoch 51: val_loss improved from 0.05823 to 0.05791, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0585 - rmse: 0.2417 - val_loss: 0.0579 - val_rmse: 0.2404 - lr: 1.0000e-04\n",
      "Epoch 52/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0651 - rmse: 0.2550\n",
      "Epoch 52: val_loss improved from 0.05791 to 0.05757, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0639 - rmse: 0.2525 - val_loss: 0.0576 - val_rmse: 0.2397 - lr: 1.0000e-04\n",
      "Epoch 53/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0578 - rmse: 0.2401\n",
      "Epoch 53: val_loss improved from 0.05757 to 0.05736, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0628 - rmse: 0.2504 - val_loss: 0.0574 - val_rmse: 0.2393 - lr: 1.0000e-04\n",
      "Epoch 54/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0554 - rmse: 0.2352\n",
      "Epoch 54: val_loss improved from 0.05736 to 0.05716, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0628 - rmse: 0.2504 - val_loss: 0.0572 - val_rmse: 0.2389 - lr: 1.0000e-04\n",
      "Epoch 55/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0538 - rmse: 0.2318\n",
      "Epoch 55: val_loss improved from 0.05716 to 0.05683, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0553 - rmse: 0.2349 - val_loss: 0.0568 - val_rmse: 0.2382 - lr: 1.0000e-04\n",
      "Epoch 56/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0727 - rmse: 0.2694\n",
      "Epoch 56: val_loss improved from 0.05683 to 0.05655, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0637 - rmse: 0.2522 - val_loss: 0.0565 - val_rmse: 0.2376 - lr: 1.0000e-04\n",
      "Epoch 57/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0644 - rmse: 0.2536\n",
      "Epoch 57: val_loss improved from 0.05655 to 0.05634, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0599 - rmse: 0.2445 - val_loss: 0.0563 - val_rmse: 0.2371 - lr: 1.0000e-04\n",
      "Epoch 58/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0614 - rmse: 0.2476\n",
      "Epoch 58: val_loss improved from 0.05634 to 0.05602, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0597 - rmse: 0.2441 - val_loss: 0.0560 - val_rmse: 0.2365 - lr: 1.0000e-04\n",
      "Epoch 59/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0629 - rmse: 0.2505\n",
      "Epoch 59: val_loss improved from 0.05602 to 0.05570, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0562 - rmse: 0.2369 - val_loss: 0.0557 - val_rmse: 0.2358 - lr: 1.0000e-04\n",
      "Epoch 60/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0640 - rmse: 0.2527\n",
      "Epoch 60: val_loss improved from 0.05570 to 0.05551, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0622 - rmse: 0.2492 - val_loss: 0.0555 - val_rmse: 0.2354 - lr: 1.0000e-04\n",
      "Epoch 61/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0589 - rmse: 0.2426\n",
      "Epoch 61: val_loss improved from 0.05551 to 0.05524, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0557 - rmse: 0.2358 - val_loss: 0.0552 - val_rmse: 0.2348 - lr: 1.0000e-04\n",
      "Epoch 62/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0742 - rmse: 0.2722\n",
      "Epoch 62: val_loss improved from 0.05524 to 0.05517, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0621 - rmse: 0.2489 - val_loss: 0.0552 - val_rmse: 0.2347 - lr: 1.0000e-04\n",
      "Epoch 63/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0531 - rmse: 0.2302\n",
      "Epoch 63: val_loss improved from 0.05517 to 0.05477, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0547 - rmse: 0.2336 - val_loss: 0.0548 - val_rmse: 0.2338 - lr: 1.0000e-04\n",
      "Epoch 64/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0559 - rmse: 0.2363\n",
      "Epoch 64: val_loss improved from 0.05477 to 0.05442, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0593 - rmse: 0.2434 - val_loss: 0.0544 - val_rmse: 0.2331 - lr: 1.0000e-04\n",
      "Epoch 65/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0539 - rmse: 0.2320\n",
      "Epoch 65: val_loss improved from 0.05442 to 0.05414, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0558 - rmse: 0.2360 - val_loss: 0.0541 - val_rmse: 0.2325 - lr: 1.0000e-04\n",
      "Epoch 66/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0623 - rmse: 0.2495\n",
      "Epoch 66: val_loss improved from 0.05414 to 0.05400, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0563 - rmse: 0.2370 - val_loss: 0.0540 - val_rmse: 0.2322 - lr: 1.0000e-04\n",
      "Epoch 67/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0594 - rmse: 0.2435\n",
      "Epoch 67: val_loss improved from 0.05400 to 0.05376, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0604 - rmse: 0.2455 - val_loss: 0.0538 - val_rmse: 0.2316 - lr: 1.0000e-04\n",
      "Epoch 68/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0598 - rmse: 0.2443\n",
      "Epoch 68: val_loss improved from 0.05376 to 0.05349, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0524 - rmse: 0.2287 - val_loss: 0.0535 - val_rmse: 0.2311 - lr: 1.0000e-04\n",
      "Epoch 69/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0598 - rmse: 0.2443\n",
      "Epoch 69: val_loss improved from 0.05349 to 0.05335, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0548 - rmse: 0.2338 - val_loss: 0.0534 - val_rmse: 0.2308 - lr: 1.0000e-04\n",
      "Epoch 70/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0572 - rmse: 0.2389\n",
      "Epoch 70: val_loss improved from 0.05335 to 0.05298, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0563 - rmse: 0.2370 - val_loss: 0.0530 - val_rmse: 0.2299 - lr: 1.0000e-04\n",
      "Epoch 71/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0504 - rmse: 0.2242\n",
      "Epoch 71: val_loss improved from 0.05298 to 0.05232, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0542 - rmse: 0.2327 - val_loss: 0.0523 - val_rmse: 0.2285 - lr: 1.0000e-04\n",
      "Epoch 72/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0505 - rmse: 0.2244\n",
      "Epoch 72: val_loss improved from 0.05232 to 0.05176, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0526 - rmse: 0.2291 - val_loss: 0.0518 - val_rmse: 0.2273 - lr: 1.0000e-04\n",
      "Epoch 73/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0562 - rmse: 0.2370\n",
      "Epoch 73: val_loss improved from 0.05176 to 0.05129, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0543 - rmse: 0.2329 - val_loss: 0.0513 - val_rmse: 0.2263 - lr: 1.0000e-04\n",
      "Epoch 74/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0460 - rmse: 0.2142\n",
      "Epoch 74: val_loss improved from 0.05129 to 0.05087, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0527 - rmse: 0.2294 - val_loss: 0.0509 - val_rmse: 0.2253 - lr: 1.0000e-04\n",
      "Epoch 75/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0500 - rmse: 0.2235\n",
      "Epoch 75: val_loss improved from 0.05087 to 0.05026, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0487 - rmse: 0.2204 - val_loss: 0.0503 - val_rmse: 0.2240 - lr: 1.0000e-04\n",
      "Epoch 76/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0571 - rmse: 0.2388\n",
      "Epoch 76: val_loss improved from 0.05026 to 0.04965, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0534 - rmse: 0.2309 - val_loss: 0.0496 - val_rmse: 0.2226 - lr: 1.0000e-04\n",
      "Epoch 77/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0553 - rmse: 0.2349\n",
      "Epoch 77: val_loss improved from 0.04965 to 0.04892, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0504 - rmse: 0.2242 - val_loss: 0.0489 - val_rmse: 0.2210 - lr: 1.0000e-04\n",
      "Epoch 78/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0533 - rmse: 0.2306\n",
      "Epoch 78: val_loss improved from 0.04892 to 0.04829, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0500 - rmse: 0.2233 - val_loss: 0.0483 - val_rmse: 0.2195 - lr: 1.0000e-04\n",
      "Epoch 79/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0491 - rmse: 0.2213\n",
      "Epoch 79: val_loss improved from 0.04829 to 0.04803, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0506 - rmse: 0.2248 - val_loss: 0.0480 - val_rmse: 0.2189 - lr: 1.0000e-04\n",
      "Epoch 80/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0482 - rmse: 0.2193\n",
      "Epoch 80: val_loss improved from 0.04803 to 0.04744, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0486 - rmse: 0.2202 - val_loss: 0.0474 - val_rmse: 0.2176 - lr: 1.0000e-04\n",
      "Epoch 81/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0454 - rmse: 0.2128\n",
      "Epoch 81: val_loss improved from 0.04744 to 0.04613, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0466 - rmse: 0.2157 - val_loss: 0.0461 - val_rmse: 0.2145 - lr: 1.0000e-04\n",
      "Epoch 82/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0559 - rmse: 0.2363\n",
      "Epoch 82: val_loss improved from 0.04613 to 0.04523, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0436 - rmse: 0.2086 - val_loss: 0.0452 - val_rmse: 0.2124 - lr: 1.0000e-04\n",
      "Epoch 83/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0585 - rmse: 0.2417\n",
      "Epoch 83: val_loss improved from 0.04523 to 0.04475, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0524 - rmse: 0.2286 - val_loss: 0.0448 - val_rmse: 0.2113 - lr: 1.0000e-04\n",
      "Epoch 84/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0444 - rmse: 0.2104\n",
      "Epoch 84: val_loss improved from 0.04475 to 0.04408, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0446 - rmse: 0.2110 - val_loss: 0.0441 - val_rmse: 0.2097 - lr: 1.0000e-04\n",
      "Epoch 85/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0414 - rmse: 0.2031\n",
      "Epoch 85: val_loss improved from 0.04408 to 0.04279, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0442 - rmse: 0.2099 - val_loss: 0.0428 - val_rmse: 0.2066 - lr: 1.0000e-04\n",
      "Epoch 86/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0454 - rmse: 0.2128\n",
      "Epoch 86: val_loss improved from 0.04279 to 0.04165, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0441 - rmse: 0.2099 - val_loss: 0.0417 - val_rmse: 0.2038 - lr: 1.0000e-04\n",
      "Epoch 87/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0399 - rmse: 0.1996\n",
      "Epoch 87: val_loss improved from 0.04165 to 0.04052, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0406 - rmse: 0.2013 - val_loss: 0.0405 - val_rmse: 0.2011 - lr: 1.0000e-04\n",
      "Epoch 88/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0438 - rmse: 0.2091\n",
      "Epoch 88: val_loss improved from 0.04052 to 0.03956, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0424 - rmse: 0.2056 - val_loss: 0.0396 - val_rmse: 0.1986 - lr: 1.0000e-04\n",
      "Epoch 89/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0406 - rmse: 0.2013\n",
      "Epoch 89: val_loss improved from 0.03956 to 0.03837, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0397 - rmse: 0.1991 - val_loss: 0.0384 - val_rmse: 0.1956 - lr: 1.0000e-04\n",
      "Epoch 90/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0412 - rmse: 0.2026\n",
      "Epoch 90: val_loss improved from 0.03837 to 0.03775, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0395 - rmse: 0.1986 - val_loss: 0.0377 - val_rmse: 0.1940 - lr: 1.0000e-04\n",
      "Epoch 91/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0374 - rmse: 0.1931\n",
      "Epoch 91: val_loss improved from 0.03775 to 0.03623, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0368 - rmse: 0.1915 - val_loss: 0.0362 - val_rmse: 0.1901 - lr: 1.0000e-04\n",
      "Epoch 92/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0377 - rmse: 0.1939\n",
      "Epoch 92: val_loss improved from 0.03623 to 0.03463, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0371 - rmse: 0.1923 - val_loss: 0.0346 - val_rmse: 0.1858 - lr: 1.0000e-04\n",
      "Epoch 93/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0337 - rmse: 0.1832\n",
      "Epoch 93: val_loss improved from 0.03463 to 0.03346, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0358 - rmse: 0.1891 - val_loss: 0.0335 - val_rmse: 0.1826 - lr: 1.0000e-04\n",
      "Epoch 94/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0320 - rmse: 0.1785\n",
      "Epoch 94: val_loss improved from 0.03346 to 0.03228, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0339 - rmse: 0.1837 - val_loss: 0.0323 - val_rmse: 0.1794 - lr: 1.0000e-04\n",
      "Epoch 95/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0338 - rmse: 0.1837\n",
      "Epoch 95: val_loss improved from 0.03228 to 0.03122, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0327 - rmse: 0.1807 - val_loss: 0.0312 - val_rmse: 0.1764 - lr: 1.0000e-04\n",
      "Epoch 96/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0314 - rmse: 0.1769\n",
      "Epoch 96: val_loss improved from 0.03122 to 0.02981, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0312 - rmse: 0.1763 - val_loss: 0.0298 - val_rmse: 0.1724 - lr: 1.0000e-04\n",
      "Epoch 97/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0280 - rmse: 0.1672\n",
      "Epoch 97: val_loss improved from 0.02981 to 0.02871, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0303 - rmse: 0.1737 - val_loss: 0.0287 - val_rmse: 0.1691 - lr: 1.0000e-04\n",
      "Epoch 98/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0293 - rmse: 0.1710\n",
      "Epoch 98: val_loss improved from 0.02871 to 0.02748, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0306 - rmse: 0.1745 - val_loss: 0.0275 - val_rmse: 0.1655 - lr: 1.0000e-04\n",
      "Epoch 99/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0244 - rmse: 0.1558\n",
      "Epoch 99: val_loss improved from 0.02748 to 0.02630, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0272 - rmse: 0.1646 - val_loss: 0.0263 - val_rmse: 0.1619 - lr: 1.0000e-04\n",
      "Epoch 100/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0230 - rmse: 0.1514\n",
      "Epoch 100: val_loss improved from 0.02630 to 0.02519, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0257 - rmse: 0.1600 - val_loss: 0.0252 - val_rmse: 0.1584 - lr: 1.0000e-04\n",
      "Epoch 101/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0243 - rmse: 0.1555\n",
      "Epoch 101: val_loss improved from 0.02519 to 0.02404, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0253 - rmse: 0.1589 - val_loss: 0.0240 - val_rmse: 0.1547 - lr: 1.0000e-04\n",
      "Epoch 102/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0259 - rmse: 0.1605\n",
      "Epoch 102: val_loss improved from 0.02404 to 0.02308, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0239 - rmse: 0.1543 - val_loss: 0.0231 - val_rmse: 0.1516 - lr: 1.0000e-04\n",
      "Epoch 103/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0241 - rmse: 0.1550\n",
      "Epoch 103: val_loss improved from 0.02308 to 0.02237, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0244 - rmse: 0.1558 - val_loss: 0.0224 - val_rmse: 0.1492 - lr: 1.0000e-04\n",
      "Epoch 104/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0189 - rmse: 0.1372\n",
      "Epoch 104: val_loss improved from 0.02237 to 0.02108, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0222 - rmse: 0.1487 - val_loss: 0.0211 - val_rmse: 0.1448 - lr: 1.0000e-04\n",
      "Epoch 105/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0217 - rmse: 0.1471\n",
      "Epoch 105: val_loss improved from 0.02108 to 0.02017, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0213 - rmse: 0.1456 - val_loss: 0.0202 - val_rmse: 0.1417 - lr: 1.0000e-04\n",
      "Epoch 106/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0229 - rmse: 0.1509\n",
      "Epoch 106: val_loss improved from 0.02017 to 0.01953, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0225 - rmse: 0.1498 - val_loss: 0.0195 - val_rmse: 0.1394 - lr: 1.0000e-04\n",
      "Epoch 107/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0186 - rmse: 0.1359\n",
      "Epoch 107: val_loss improved from 0.01953 to 0.01890, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0185 - rmse: 0.1357 - val_loss: 0.0189 - val_rmse: 0.1371 - lr: 1.0000e-04\n",
      "Epoch 108/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0191 - rmse: 0.1377\n",
      "Epoch 108: val_loss improved from 0.01890 to 0.01763, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0183 - rmse: 0.1349 - val_loss: 0.0176 - val_rmse: 0.1324 - lr: 1.0000e-04\n",
      "Epoch 109/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0156 - rmse: 0.1245\n",
      "Epoch 109: val_loss improved from 0.01763 to 0.01681, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0179 - rmse: 0.1335 - val_loss: 0.0168 - val_rmse: 0.1293 - lr: 1.0000e-04\n",
      "Epoch 110/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0187 - rmse: 0.1364\n",
      "Epoch 110: val_loss improved from 0.01681 to 0.01615, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0168 - rmse: 0.1293 - val_loss: 0.0161 - val_rmse: 0.1267 - lr: 1.0000e-04\n",
      "Epoch 111/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0172 - rmse: 0.1308\n",
      "Epoch 111: val_loss improved from 0.01615 to 0.01560, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0167 - rmse: 0.1290 - val_loss: 0.0156 - val_rmse: 0.1245 - lr: 1.0000e-04\n",
      "Epoch 112/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0172 - rmse: 0.1309\n",
      "Epoch 112: val_loss improved from 0.01560 to 0.01476, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0157 - rmse: 0.1249 - val_loss: 0.0148 - val_rmse: 0.1211 - lr: 1.0000e-04\n",
      "Epoch 113/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0136 - rmse: 0.1163\n",
      "Epoch 113: val_loss improved from 0.01476 to 0.01415, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0152 - rmse: 0.1228 - val_loss: 0.0142 - val_rmse: 0.1185 - lr: 1.0000e-04\n",
      "Epoch 114/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0130 - rmse: 0.1136\n",
      "Epoch 114: val_loss improved from 0.01415 to 0.01341, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0141 - rmse: 0.1182 - val_loss: 0.0134 - val_rmse: 0.1154 - lr: 1.0000e-04\n",
      "Epoch 115/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0131 - rmse: 0.1140\n",
      "Epoch 115: val_loss improved from 0.01341 to 0.01277, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0130 - rmse: 0.1135 - val_loss: 0.0128 - val_rmse: 0.1125 - lr: 1.0000e-04\n",
      "Epoch 116/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0141 - rmse: 0.1182\n",
      "Epoch 116: val_loss improved from 0.01277 to 0.01227, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0134 - rmse: 0.1153 - val_loss: 0.0123 - val_rmse: 0.1103 - lr: 1.0000e-04\n",
      "Epoch 117/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0132 - rmse: 0.1146\n",
      "Epoch 117: val_loss improved from 0.01227 to 0.01169, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0119 - rmse: 0.1088 - val_loss: 0.0117 - val_rmse: 0.1077 - lr: 1.0000e-04\n",
      "Epoch 118/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0140 - rmse: 0.1181\n",
      "Epoch 118: val_loss improved from 0.01169 to 0.01115, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0124 - rmse: 0.1110 - val_loss: 0.0111 - val_rmse: 0.1051 - lr: 1.0000e-04\n",
      "Epoch 119/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0102 - rmse: 0.1003\n",
      "Epoch 119: val_loss improved from 0.01115 to 0.01070, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0105 - rmse: 0.1022 - val_loss: 0.0107 - val_rmse: 0.1029 - lr: 1.0000e-04\n",
      "Epoch 120/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0092 - rmse: 0.0953\n",
      "Epoch 120: val_loss improved from 0.01070 to 0.01011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0110 - rmse: 0.1042 - val_loss: 0.0101 - val_rmse: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 121/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0094 - rmse: 0.0963\n",
      "Epoch 121: val_loss improved from 0.01011 to 0.00965, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0099 - rmse: 0.0992 - val_loss: 0.0096 - val_rmse: 0.0977 - lr: 1.0000e-04\n",
      "Epoch 122/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0110 - rmse: 0.1045\n",
      "Epoch 122: val_loss improved from 0.00965 to 0.00927, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0094 - rmse: 0.0964 - val_loss: 0.0093 - val_rmse: 0.0958 - lr: 1.0000e-04\n",
      "Epoch 123/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0096 - rmse: 0.0977\n",
      "Epoch 123: val_loss improved from 0.00927 to 0.00866, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0097 - rmse: 0.0978 - val_loss: 0.0087 - val_rmse: 0.0925 - lr: 1.0000e-04\n",
      "Epoch 124/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0085 - rmse: 0.0915\n",
      "Epoch 124: val_loss improved from 0.00866 to 0.00827, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0086 - rmse: 0.0920 - val_loss: 0.0083 - val_rmse: 0.0904 - lr: 1.0000e-04\n",
      "Epoch 125/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0076 - rmse: 0.0866\n",
      "Epoch 125: val_loss improved from 0.00827 to 0.00794, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0083 - rmse: 0.0907 - val_loss: 0.0079 - val_rmse: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 126/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0071 - rmse: 0.0834\n",
      "Epoch 126: val_loss improved from 0.00794 to 0.00754, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0077 - rmse: 0.0872 - val_loss: 0.0075 - val_rmse: 0.0862 - lr: 1.0000e-04\n",
      "Epoch 127/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0073 - rmse: 0.0848\n",
      "Epoch 127: val_loss improved from 0.00754 to 0.00713, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0074 - rmse: 0.0853 - val_loss: 0.0071 - val_rmse: 0.0838 - lr: 1.0000e-04\n",
      "Epoch 128/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0074 - rmse: 0.0856\n",
      "Epoch 128: val_loss improved from 0.00713 to 0.00688, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0071 - rmse: 0.0839 - val_loss: 0.0069 - val_rmse: 0.0823 - lr: 1.0000e-04\n",
      "Epoch 129/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0069 - rmse: 0.0825\n",
      "Epoch 129: val_loss improved from 0.00688 to 0.00652, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0068 - rmse: 0.0820 - val_loss: 0.0065 - val_rmse: 0.0801 - lr: 1.0000e-04\n",
      "Epoch 130/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0059 - rmse: 0.0760\n",
      "Epoch 130: val_loss improved from 0.00652 to 0.00620, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0061 - rmse: 0.0775 - val_loss: 0.0062 - val_rmse: 0.0781 - lr: 1.0000e-04\n",
      "Epoch 131/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0066 - rmse: 0.0808\n",
      "Epoch 131: val_loss improved from 0.00620 to 0.00603, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0067 - rmse: 0.0812 - val_loss: 0.0060 - val_rmse: 0.0770 - lr: 1.0000e-04\n",
      "Epoch 132/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0059 - rmse: 0.0762\n",
      "Epoch 132: val_loss improved from 0.00603 to 0.00564, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0055 - rmse: 0.0737 - val_loss: 0.0056 - val_rmse: 0.0744 - lr: 1.0000e-04\n",
      "Epoch 133/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0057 - rmse: 0.0750\n",
      "Epoch 133: val_loss improved from 0.00564 to 0.00538, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0056 - rmse: 0.0738 - val_loss: 0.0054 - val_rmse: 0.0726 - lr: 1.0000e-04\n",
      "Epoch 134/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0051 - rmse: 0.0710\n",
      "Epoch 134: val_loss improved from 0.00538 to 0.00515, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0052 - rmse: 0.0714 - val_loss: 0.0052 - val_rmse: 0.0711 - lr: 1.0000e-04\n",
      "Epoch 135/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0056 - rmse: 0.0739\n",
      "Epoch 135: val_loss improved from 0.00515 to 0.00497, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0051 - rmse: 0.0705 - val_loss: 0.0050 - val_rmse: 0.0698 - lr: 1.0000e-04\n",
      "Epoch 136/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0044 - rmse: 0.0655\n",
      "Epoch 136: val_loss improved from 0.00497 to 0.00471, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0049 - rmse: 0.0690 - val_loss: 0.0047 - val_rmse: 0.0679 - lr: 1.0000e-04\n",
      "Epoch 137/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0051 - rmse: 0.0709\n",
      "Epoch 137: val_loss improved from 0.00471 to 0.00453, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0044 - rmse: 0.0659 - val_loss: 0.0045 - val_rmse: 0.0665 - lr: 1.0000e-04\n",
      "Epoch 138/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0056 - rmse: 0.0738\n",
      "Epoch 138: val_loss improved from 0.00453 to 0.00447, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0046 - rmse: 0.0674 - val_loss: 0.0045 - val_rmse: 0.0661 - lr: 1.0000e-04\n",
      "Epoch 139/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0042 - rmse: 0.0641\n",
      "Epoch 139: val_loss improved from 0.00447 to 0.00420, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0042 - rmse: 0.0641 - val_loss: 0.0042 - val_rmse: 0.0640 - lr: 1.0000e-04\n",
      "Epoch 140/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0042 - rmse: 0.0639\n",
      "Epoch 140: val_loss improved from 0.00420 to 0.00408, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0041 - rmse: 0.0634 - val_loss: 0.0041 - val_rmse: 0.0631 - lr: 1.0000e-04\n",
      "Epoch 141/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0035 - rmse: 0.0586\n",
      "Epoch 141: val_loss improved from 0.00408 to 0.00392, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0040 - rmse: 0.0623 - val_loss: 0.0039 - val_rmse: 0.0618 - lr: 1.0000e-04\n",
      "Epoch 142/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0033 - rmse: 0.0566\n",
      "Epoch 142: val_loss improved from 0.00392 to 0.00387, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0036 - rmse: 0.0595 - val_loss: 0.0039 - val_rmse: 0.0614 - lr: 1.0000e-04\n",
      "Epoch 143/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0037 - rmse: 0.0603\n",
      "Epoch 143: val_loss improved from 0.00387 to 0.00371, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0038 - rmse: 0.0607 - val_loss: 0.0037 - val_rmse: 0.0600 - lr: 1.0000e-04\n",
      "Epoch 144/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0571\n",
      "Epoch 144: val_loss improved from 0.00371 to 0.00359, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0035 - rmse: 0.0582 - val_loss: 0.0036 - val_rmse: 0.0590 - lr: 1.0000e-04\n",
      "Epoch 145/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0035 - rmse: 0.0583\n",
      "Epoch 145: val_loss improved from 0.00359 to 0.00347, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0035 - val_rmse: 0.0581 - lr: 1.0000e-04\n",
      "Epoch 146/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0041 - rmse: 0.0629\n",
      "Epoch 146: val_loss improved from 0.00347 to 0.00340, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0034 - rmse: 0.0577 - val_loss: 0.0034 - val_rmse: 0.0574 - lr: 1.0000e-04\n",
      "Epoch 147/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0035 - rmse: 0.0585\n",
      "Epoch 147: val_loss improved from 0.00340 to 0.00333, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0033 - rmse: 0.0562 - val_loss: 0.0033 - val_rmse: 0.0568 - lr: 1.0000e-04\n",
      "Epoch 148/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0028 - rmse: 0.0516\n",
      "Epoch 148: val_loss improved from 0.00333 to 0.00321, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0032 - rmse: 0.0559 - val_loss: 0.0032 - val_rmse: 0.0558 - lr: 1.0000e-04\n",
      "Epoch 149/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0577\n",
      "Epoch 149: val_loss improved from 0.00321 to 0.00314, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0031 - rmse: 0.0548 - val_loss: 0.0031 - val_rmse: 0.0551 - lr: 1.0000e-04\n",
      "Epoch 150/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0028 - rmse: 0.0523\n",
      "Epoch 150: val_loss improved from 0.00314 to 0.00307, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0028 - rmse: 0.0521 - val_loss: 0.0031 - val_rmse: 0.0544 - lr: 1.0000e-04\n",
      "Epoch 151/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0033 - rmse: 0.0567\n",
      "Epoch 151: val_loss improved from 0.00307 to 0.00301, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0031 - rmse: 0.0545 - val_loss: 0.0030 - val_rmse: 0.0539 - lr: 1.0000e-04\n",
      "Epoch 152/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0030 - rmse: 0.0537\n",
      "Epoch 152: val_loss improved from 0.00301 to 0.00297, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0029 - rmse: 0.0526 - val_loss: 0.0030 - val_rmse: 0.0536 - lr: 1.0000e-04\n",
      "Epoch 153/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0030 - rmse: 0.0541\n",
      "Epoch 153: val_loss improved from 0.00297 to 0.00292, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0027 - rmse: 0.0513 - val_loss: 0.0029 - val_rmse: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 154/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0034 - rmse: 0.0571\n",
      "Epoch 154: val_loss did not improve from 0.00292\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0029 - rmse: 0.0528 - val_loss: 0.0029 - val_rmse: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 155/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0028 - rmse: 0.0524\n",
      "Epoch 155: val_loss improved from 0.00292 to 0.00289, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0028 - rmse: 0.0523 - val_loss: 0.0029 - val_rmse: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 156/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0026 - rmse: 0.0502\n",
      "Epoch 156: val_loss improved from 0.00289 to 0.00273, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0026 - rmse: 0.0500 - val_loss: 0.0027 - val_rmse: 0.0513 - lr: 1.0000e-04\n",
      "Epoch 157/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0022 - rmse: 0.0453\n",
      "Epoch 157: val_loss improved from 0.00273 to 0.00270, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0026 - rmse: 0.0496 - val_loss: 0.0027 - val_rmse: 0.0509 - lr: 1.0000e-04\n",
      "Epoch 158/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0028 - rmse: 0.0522\n",
      "Epoch 158: val_loss did not improve from 0.00270\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0027 - rmse: 0.0513 - val_loss: 0.0027 - val_rmse: 0.0510 - lr: 1.0000e-04\n",
      "Epoch 159/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0452\n",
      "Epoch 159: val_loss improved from 0.00270 to 0.00262, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0025 - rmse: 0.0488 - val_loss: 0.0026 - val_rmse: 0.0502 - lr: 1.0000e-04\n",
      "Epoch 160/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0025 - rmse: 0.0494\n",
      "Epoch 160: val_loss improved from 0.00262 to 0.00258, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0024 - rmse: 0.0480 - val_loss: 0.0026 - val_rmse: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 161/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0022 - rmse: 0.0456\n",
      "Epoch 161: val_loss did not improve from 0.00258\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0025 - rmse: 0.0486 - val_loss: 0.0026 - val_rmse: 0.0498 - lr: 1.0000e-04\n",
      "Epoch 162/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0022 - rmse: 0.0461\n",
      "Epoch 162: val_loss improved from 0.00258 to 0.00249, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0025 - rmse: 0.0485 - val_loss: 0.0025 - val_rmse: 0.0488 - lr: 1.0000e-04\n",
      "Epoch 163/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0024 - rmse: 0.0475\n",
      "Epoch 163: val_loss improved from 0.00249 to 0.00246, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0024 - rmse: 0.0477 - val_loss: 0.0025 - val_rmse: 0.0486 - lr: 1.0000e-04\n",
      "Epoch 164/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0469\n",
      "Epoch 164: val_loss improved from 0.00246 to 0.00242, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0022 - rmse: 0.0461 - val_loss: 0.0024 - val_rmse: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 165/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0024 - rmse: 0.0481\n",
      "Epoch 165: val_loss improved from 0.00242 to 0.00238, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0022 - rmse: 0.0461 - val_loss: 0.0024 - val_rmse: 0.0477 - lr: 1.0000e-04\n",
      "Epoch 166/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0026 - rmse: 0.0495\n",
      "Epoch 166: val_loss did not improve from 0.00238\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0025 - rmse: 0.0486 - val_loss: 0.0024 - val_rmse: 0.0477 - lr: 1.0000e-04\n",
      "Epoch 167/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - rmse: 0.0398\n",
      "Epoch 167: val_loss improved from 0.00238 to 0.00237, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0021 - rmse: 0.0444 - val_loss: 0.0024 - val_rmse: 0.0476 - lr: 1.0000e-04\n",
      "Epoch 168/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0465\n",
      "Epoch 168: val_loss improved from 0.00237 to 0.00230, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0022 - rmse: 0.0457 - val_loss: 0.0023 - val_rmse: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 169/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0026 - rmse: 0.0504\n",
      "Epoch 169: val_loss did not improve from 0.00230\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0022 - rmse: 0.0458 - val_loss: 0.0023 - val_rmse: 0.0471 - lr: 1.0000e-04\n",
      "Epoch 170/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0426\n",
      "Epoch 170: val_loss improved from 0.00230 to 0.00226, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0019 - rmse: 0.0421 - val_loss: 0.0023 - val_rmse: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 171/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0024 - rmse: 0.0480\n",
      "Epoch 171: val_loss improved from 0.00226 to 0.00221, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0023 - rmse: 0.0468 - val_loss: 0.0022 - val_rmse: 0.0459 - lr: 1.0000e-04\n",
      "Epoch 172/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0473\n",
      "Epoch 172: val_loss improved from 0.00221 to 0.00220, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0021 - rmse: 0.0447 - val_loss: 0.0022 - val_rmse: 0.0458 - lr: 1.0000e-04\n",
      "Epoch 173/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0021 - rmse: 0.0444\n",
      "Epoch 173: val_loss improved from 0.00220 to 0.00212, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0021 - rmse: 0.0446 - val_loss: 0.0021 - val_rmse: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 174/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0417\n",
      "Epoch 174: val_loss did not improve from 0.00212\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0020 - rmse: 0.0437 - val_loss: 0.0022 - val_rmse: 0.0458 - lr: 1.0000e-04\n",
      "Epoch 175/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0024 - rmse: 0.0476\n",
      "Epoch 175: val_loss did not improve from 0.00212\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0022 - rmse: 0.0453 - val_loss: 0.0021 - val_rmse: 0.0451 - lr: 1.0000e-04\n",
      "Epoch 176/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0407\n",
      "Epoch 176: val_loss did not improve from 0.00212\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0019 - rmse: 0.0424 - val_loss: 0.0021 - val_rmse: 0.0450 - lr: 1.0000e-04\n",
      "Epoch 177/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0439\n",
      "Epoch 177: val_loss improved from 0.00212 to 0.00207, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0020 - rmse: 0.0437 - val_loss: 0.0021 - val_rmse: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 178/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0382\n",
      "Epoch 178: val_loss improved from 0.00207 to 0.00205, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0018 - rmse: 0.0418 - val_loss: 0.0021 - val_rmse: 0.0442 - lr: 1.0000e-04\n",
      "Epoch 179/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0433\n",
      "Epoch 179: val_loss improved from 0.00205 to 0.00201, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0018 - rmse: 0.0411 - val_loss: 0.0020 - val_rmse: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 180/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0428\n",
      "Epoch 180: val_loss improved from 0.00201 to 0.00199, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0020 - rmse: 0.0433 - val_loss: 0.0020 - val_rmse: 0.0434 - lr: 1.0000e-04\n",
      "Epoch 181/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0393\n",
      "Epoch 181: val_loss improved from 0.00199 to 0.00198, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0019 - rmse: 0.0420 - val_loss: 0.0020 - val_rmse: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 182/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0434\n",
      "Epoch 182: val_loss improved from 0.00198 to 0.00198, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0019 - rmse: 0.0427 - val_loss: 0.0020 - val_rmse: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 183/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0390\n",
      "Epoch 183: val_loss did not improve from 0.00198\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0018 - rmse: 0.0417 - val_loss: 0.0020 - val_rmse: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 184/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0410\n",
      "Epoch 184: val_loss improved from 0.00198 to 0.00195, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0018 - rmse: 0.0415 - val_loss: 0.0019 - val_rmse: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 185/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0434\n",
      "Epoch 185: val_loss improved from 0.00195 to 0.00195, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0018 - rmse: 0.0416 - val_loss: 0.0019 - val_rmse: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 186/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0414\n",
      "Epoch 186: val_loss improved from 0.00195 to 0.00187, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0020 - rmse: 0.0433 - val_loss: 0.0019 - val_rmse: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 187/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0380\n",
      "Epoch 187: val_loss did not improve from 0.00187\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0017 - rmse: 0.0398 - val_loss: 0.0019 - val_rmse: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 188/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0415\n",
      "Epoch 188: val_loss did not improve from 0.00187\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0017 - rmse: 0.0400 - val_loss: 0.0020 - val_rmse: 0.0440 - lr: 1.0000e-04\n",
      "Epoch 189/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0418\n",
      "Epoch 189: val_loss did not improve from 0.00187\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0017 - rmse: 0.0405 - val_loss: 0.0019 - val_rmse: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 190/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0419\n",
      "Epoch 190: val_loss improved from 0.00187 to 0.00187, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0015 - rmse: 0.0378 - val_loss: 0.0019 - val_rmse: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 191/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0023 - rmse: 0.0470\n",
      "Epoch 191: val_loss improved from 0.00187 to 0.00178, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0019 - rmse: 0.0421 - val_loss: 0.0018 - val_rmse: 0.0410 - lr: 1.0000e-04\n",
      "Epoch 192/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0411\n",
      "Epoch 192: val_loss improved from 0.00178 to 0.00175, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0017 - rmse: 0.0404 - val_loss: 0.0017 - val_rmse: 0.0405 - lr: 1.0000e-04\n",
      "Epoch 193/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0018 - rmse: 0.0417\n",
      "Epoch 193: val_loss did not improve from 0.00175\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0017 - rmse: 0.0396 - val_loss: 0.0018 - val_rmse: 0.0407 - lr: 1.0000e-04\n",
      "Epoch 194/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0370\n",
      "Epoch 194: val_loss improved from 0.00175 to 0.00172, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0015 - rmse: 0.0377 - val_loss: 0.0017 - val_rmse: 0.0403 - lr: 1.0000e-04\n",
      "Epoch 195/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0019 - rmse: 0.0423\n",
      "Epoch 195: val_loss improved from 0.00172 to 0.00170, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0017 - rmse: 0.0405 - val_loss: 0.0017 - val_rmse: 0.0400 - lr: 1.0000e-04\n",
      "Epoch 196/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0354\n",
      "Epoch 196: val_loss improved from 0.00170 to 0.00166, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0015 - rmse: 0.0368 - val_loss: 0.0017 - val_rmse: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 197/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0373\n",
      "Epoch 197: val_loss did not improve from 0.00166\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0015 - rmse: 0.0378 - val_loss: 0.0017 - val_rmse: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 198/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0017 - rmse: 0.0394\n",
      "Epoch 198: val_loss improved from 0.00166 to 0.00164, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0015 - rmse: 0.0377 - val_loss: 0.0016 - val_rmse: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 199/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0386\n",
      "Epoch 199: val_loss improved from 0.00164 to 0.00163, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0015 - rmse: 0.0376 - val_loss: 0.0016 - val_rmse: 0.0391 - lr: 1.0000e-04\n",
      "Epoch 200/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0372\n",
      "Epoch 200: val_loss improved from 0.00163 to 0.00162, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0015 - rmse: 0.0376 - val_loss: 0.0016 - val_rmse: 0.0390 - lr: 1.0000e-04\n",
      "Epoch 201/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0351\n",
      "Epoch 201: val_loss improved from 0.00162 to 0.00158, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0014 - rmse: 0.0359 - val_loss: 0.0016 - val_rmse: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 202/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0020 - rmse: 0.0437\n",
      "Epoch 202: val_loss improved from 0.00158 to 0.00156, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0014 - rmse: 0.0366 - val_loss: 0.0016 - val_rmse: 0.0382 - lr: 1.0000e-04\n",
      "Epoch 203/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0016 - rmse: 0.0391\n",
      "Epoch 203: val_loss did not improve from 0.00156\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0017 - rmse: 0.0396 - val_loss: 0.0016 - val_rmse: 0.0383 - lr: 1.0000e-04\n",
      "Epoch 204/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0360\n",
      "Epoch 204: val_loss improved from 0.00156 to 0.00153, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0013 - rmse: 0.0350 - val_loss: 0.0015 - val_rmse: 0.0378 - lr: 1.0000e-04\n",
      "Epoch 205/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0330\n",
      "Epoch 205: val_loss improved from 0.00153 to 0.00152, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0014 - rmse: 0.0365 - val_loss: 0.0015 - val_rmse: 0.0377 - lr: 1.0000e-04\n",
      "Epoch 206/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0014 - rmse: 0.0357\n",
      "Epoch 206: val_loss did not improve from 0.00152\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0014 - rmse: 0.0357 - val_loss: 0.0015 - val_rmse: 0.0377 - lr: 1.0000e-04\n",
      "Epoch 207/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0373\n",
      "Epoch 207: val_loss improved from 0.00152 to 0.00149, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0013 - rmse: 0.0350 - val_loss: 0.0015 - val_rmse: 0.0372 - lr: 1.0000e-04\n",
      "Epoch 208/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0342\n",
      "Epoch 208: val_loss improved from 0.00149 to 0.00146, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0014 - rmse: 0.0364 - val_loss: 0.0015 - val_rmse: 0.0368 - lr: 1.0000e-04\n",
      "Epoch 209/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0340\n",
      "Epoch 209: val_loss improved from 0.00146 to 0.00144, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0013 - rmse: 0.0351 - val_loss: 0.0014 - val_rmse: 0.0366 - lr: 1.0000e-04\n",
      "Epoch 210/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0367\n",
      "Epoch 210: val_loss did not improve from 0.00144\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0014 - rmse: 0.0367 - val_loss: 0.0015 - val_rmse: 0.0368 - lr: 1.0000e-04\n",
      "Epoch 211/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0322\n",
      "Epoch 211: val_loss improved from 0.00144 to 0.00143, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0013 - rmse: 0.0348 - val_loss: 0.0014 - val_rmse: 0.0364 - lr: 1.0000e-04\n",
      "Epoch 212/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0351\n",
      "Epoch 212: val_loss did not improve from 0.00143\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0344 - val_loss: 0.0014 - val_rmse: 0.0365 - lr: 1.0000e-04\n",
      "Epoch 213/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0316\n",
      "Epoch 213: val_loss improved from 0.00143 to 0.00140, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0014 - rmse: 0.0357 - val_loss: 0.0014 - val_rmse: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 214/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5926e-04 - rmse: 0.0293\n",
      "Epoch 214: val_loss did not improve from 0.00140\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0344 - val_loss: 0.0014 - val_rmse: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 215/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0342\n",
      "Epoch 215: val_loss did not improve from 0.00140\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0351 - val_loss: 0.0015 - val_rmse: 0.0371 - lr: 1.0000e-04\n",
      "Epoch 216/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0339\n",
      "Epoch 216: val_loss did not improve from 0.00140\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0012 - rmse: 0.0338 - val_loss: 0.0014 - val_rmse: 0.0362 - lr: 1.0000e-04\n",
      "Epoch 217/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0332\n",
      "Epoch 217: val_loss did not improve from 0.00140\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0013 - rmse: 0.0351 - val_loss: 0.0014 - val_rmse: 0.0366 - lr: 1.0000e-04\n",
      "Epoch 218/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.5171e-04 - rmse: 0.0292\n",
      "Epoch 218: val_loss improved from 0.00140 to 0.00140, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0011 - rmse: 0.0310 - val_loss: 0.0014 - val_rmse: 0.0360 - lr: 1.0000e-04\n",
      "Epoch 219/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0015 - rmse: 0.0376\n",
      "Epoch 219: val_loss improved from 0.00140 to 0.00136, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0013 - rmse: 0.0343 - val_loss: 0.0014 - val_rmse: 0.0354 - lr: 1.0000e-04\n",
      "Epoch 220/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0348\n",
      "Epoch 220: val_loss improved from 0.00136 to 0.00131, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0013 - rmse: 0.0343 - val_loss: 0.0013 - val_rmse: 0.0348 - lr: 1.0000e-04\n",
      "Epoch 221/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - rmse: 0.0303\n",
      "Epoch 221: val_loss did not improve from 0.00131\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0012 - rmse: 0.0327 - val_loss: 0.0013 - val_rmse: 0.0352 - lr: 1.0000e-04\n",
      "Epoch 222/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0337\n",
      "Epoch 222: val_loss improved from 0.00131 to 0.00130, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0011 - rmse: 0.0322 - val_loss: 0.0013 - val_rmse: 0.0346 - lr: 1.0000e-04\n",
      "Epoch 223/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6401e-04 - rmse: 0.0294\n",
      "Epoch 223: val_loss improved from 0.00130 to 0.00130, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0012 - rmse: 0.0329 - val_loss: 0.0013 - val_rmse: 0.0346 - lr: 1.0000e-04\n",
      "Epoch 224/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0350\n",
      "Epoch 224: val_loss improved from 0.00130 to 0.00129, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0012 - rmse: 0.0326 - val_loss: 0.0013 - val_rmse: 0.0344 - lr: 1.0000e-04\n",
      "Epoch 225/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0317\n",
      "Epoch 225: val_loss improved from 0.00129 to 0.00127, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0012 - rmse: 0.0331 - val_loss: 0.0013 - val_rmse: 0.0342 - lr: 1.0000e-04\n",
      "Epoch 226/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.0465e-04 - rmse: 0.0265\n",
      "Epoch 226: val_loss did not improve from 0.00127\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0010 - rmse: 0.0306 - val_loss: 0.0013 - val_rmse: 0.0344 - lr: 1.0000e-04\n",
      "Epoch 227/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0333\n",
      "Epoch 227: val_loss improved from 0.00127 to 0.00124, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0010 - rmse: 0.0304 - val_loss: 0.0012 - val_rmse: 0.0338 - lr: 1.0000e-04\n",
      "Epoch 228/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0316\n",
      "Epoch 228: val_loss improved from 0.00124 to 0.00122, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0012 - rmse: 0.0336 - val_loss: 0.0012 - val_rmse: 0.0334 - lr: 1.0000e-04\n",
      "Epoch 229/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0330\n",
      "Epoch 229: val_loss did not improve from 0.00122\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0011 - rmse: 0.0311 - val_loss: 0.0012 - val_rmse: 0.0335 - lr: 1.0000e-04\n",
      "Epoch 230/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0329\n",
      "Epoch 230: val_loss improved from 0.00122 to 0.00121, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0010 - rmse: 0.0305 - val_loss: 0.0012 - val_rmse: 0.0333 - lr: 1.0000e-04\n",
      "Epoch 231/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0321\n",
      "Epoch 231: val_loss improved from 0.00121 to 0.00121, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0011 - rmse: 0.0319 - val_loss: 0.0012 - val_rmse: 0.0332 - lr: 1.0000e-04\n",
      "Epoch 232/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0330\n",
      "Epoch 232: val_loss improved from 0.00121 to 0.00118, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0011 - rmse: 0.0312 - val_loss: 0.0012 - val_rmse: 0.0329 - lr: 1.0000e-04\n",
      "Epoch 233/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9053e-04 - rmse: 0.0281\n",
      "Epoch 233: val_loss improved from 0.00118 to 0.00117, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 9.9762e-04 - rmse: 0.0299 - val_loss: 0.0012 - val_rmse: 0.0327 - lr: 1.0000e-04\n",
      "Epoch 234/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0013 - rmse: 0.0344\n",
      "Epoch 234: val_loss did not improve from 0.00117\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0011 - rmse: 0.0323 - val_loss: 0.0012 - val_rmse: 0.0330 - lr: 1.0000e-04\n",
      "Epoch 235/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1728e-04 - rmse: 0.0268\n",
      "Epoch 235: val_loss improved from 0.00117 to 0.00113, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 8.6276e-04 - rmse: 0.0276 - val_loss: 0.0011 - val_rmse: 0.0321 - lr: 1.0000e-04\n",
      "Epoch 236/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0318\n",
      "Epoch 236: val_loss improved from 0.00113 to 0.00111, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0011 - rmse: 0.0318 - val_loss: 0.0011 - val_rmse: 0.0318 - lr: 1.0000e-04\n",
      "Epoch 237/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - rmse: 0.0307\n",
      "Epoch 237: val_loss did not improve from 0.00111\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0010 - rmse: 0.0306 - val_loss: 0.0011 - val_rmse: 0.0319 - lr: 1.0000e-04\n",
      "Epoch 238/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1658e-04 - rmse: 0.0267\n",
      "Epoch 238: val_loss did not improve from 0.00111\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 9.8191e-04 - rmse: 0.0297 - val_loss: 0.0011 - val_rmse: 0.0322 - lr: 1.0000e-04\n",
      "Epoch 239/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7435e-04 - rmse: 0.0296\n",
      "Epoch 239: val_loss did not improve from 0.00111\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.8052e-04 - rmse: 0.0297 - val_loss: 0.0011 - val_rmse: 0.0321 - lr: 1.0000e-04\n",
      "Epoch 240/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0012 - rmse: 0.0335\n",
      "Epoch 240: val_loss improved from 0.00111 to 0.00110, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0010 - rmse: 0.0300 - val_loss: 0.0011 - val_rmse: 0.0316 - lr: 1.0000e-04\n",
      "Epoch 241/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0313\n",
      "Epoch 241: val_loss improved from 0.00110 to 0.00108, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 9.4870e-04 - rmse: 0.0291 - val_loss: 0.0011 - val_rmse: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 242/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0311\n",
      "Epoch 242: val_loss did not improve from 0.00108\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0010 - rmse: 0.0305 - val_loss: 0.0011 - val_rmse: 0.0313 - lr: 1.0000e-04\n",
      "Epoch 243/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4101e-04 - rmse: 0.0290\n",
      "Epoch 243: val_loss improved from 0.00108 to 0.00105, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 8.6602e-04 - rmse: 0.0277 - val_loss: 0.0011 - val_rmse: 0.0309 - lr: 1.0000e-04\n",
      "Epoch 244/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0010 - rmse: 0.0302\n",
      "Epoch 244: val_loss did not improve from 0.00105\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0010 - rmse: 0.0305 - val_loss: 0.0011 - val_rmse: 0.0314 - lr: 1.0000e-04\n",
      "Epoch 245/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1840e-04 - rmse: 0.0286\n",
      "Epoch 245: val_loss did not improve from 0.00105\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.6690e-04 - rmse: 0.0294 - val_loss: 0.0011 - val_rmse: 0.0311 - lr: 1.0000e-04\n",
      "Epoch 246/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8744e-04 - rmse: 0.0262\n",
      "Epoch 246: val_loss improved from 0.00105 to 0.00104, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 8.7042e-04 - rmse: 0.0277 - val_loss: 0.0010 - val_rmse: 0.0307 - lr: 1.0000e-04\n",
      "Epoch 247/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7779e-04 - rmse: 0.0296\n",
      "Epoch 247: val_loss did not improve from 0.00104\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.0417e-04 - rmse: 0.0283 - val_loss: 0.0010 - val_rmse: 0.0307 - lr: 1.0000e-04\n",
      "Epoch 248/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4568e-04 - rmse: 0.0273\n",
      "Epoch 248: val_loss improved from 0.00104 to 0.00104, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 9.5748e-04 - rmse: 0.0293 - val_loss: 0.0010 - val_rmse: 0.0307 - lr: 1.0000e-04\n",
      "Epoch 249/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9490e-04 - rmse: 0.0299\n",
      "Epoch 249: val_loss improved from 0.00104 to 0.00102, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 9.1083e-04 - rmse: 0.0285 - val_loss: 0.0010 - val_rmse: 0.0304 - lr: 1.0000e-04\n",
      "Epoch 250/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.9514e-04 - rmse: 0.0282\n",
      "Epoch 250: val_loss improved from 0.00102 to 0.00102, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 8.4942e-04 - rmse: 0.0274 - val_loss: 0.0010 - val_rmse: 0.0303 - lr: 1.0000e-04\n",
      "Epoch 251/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.0246e-04 - rmse: 0.0283\n",
      "Epoch 251: val_loss improved from 0.00102 to 0.00101, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 50ms/step - loss: 9.6199e-04 - rmse: 0.0293 - val_loss: 0.0010 - val_rmse: 0.0301 - lr: 1.0000e-04\n",
      "Epoch 252/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.8698e-04 - rmse: 0.0221\n",
      "Epoch 252: val_loss improved from 0.00101 to 0.00098, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 8.4283e-04 - rmse: 0.0272 - val_loss: 9.8427e-04 - val_rmse: 0.0297 - lr: 1.0000e-04\n",
      "Epoch 253/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4709e-04 - rmse: 0.0273\n",
      "Epoch 253: val_loss improved from 0.00098 to 0.00098, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 8.2798e-04 - rmse: 0.0270 - val_loss: 9.8049e-04 - val_rmse: 0.0297 - lr: 1.0000e-04\n",
      "Epoch 254/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0011 - rmse: 0.0310\n",
      "Epoch 254: val_loss did not improve from 0.00098\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.6759e-04 - rmse: 0.0277 - val_loss: 0.0010 - val_rmse: 0.0301 - lr: 1.0000e-04\n",
      "Epoch 255/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6826e-04 - rmse: 0.0295\n",
      "Epoch 255: val_loss did not improve from 0.00098\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.5904e-04 - rmse: 0.0293 - val_loss: 0.0010 - val_rmse: 0.0308 - lr: 1.0000e-04\n",
      "Epoch 256/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.8385e-04 - rmse: 0.0261\n",
      "Epoch 256: val_loss improved from 0.00098 to 0.00097, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 8.6367e-04 - rmse: 0.0276 - val_loss: 9.7468e-04 - val_rmse: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 257/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4265e-04 - rmse: 0.0253\n",
      "Epoch 257: val_loss did not improve from 0.00097\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.4040e-04 - rmse: 0.0272 - val_loss: 9.8647e-04 - val_rmse: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 258/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1384e-04 - rmse: 0.0285\n",
      "Epoch 258: val_loss did not improve from 0.00097\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 9.3970e-04 - rmse: 0.0290 - val_loss: 9.7742e-04 - val_rmse: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 259/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8813e-04 - rmse: 0.0242\n",
      "Epoch 259: val_loss improved from 0.00097 to 0.00097, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 7.6763e-04 - rmse: 0.0258 - val_loss: 9.6926e-04 - val_rmse: 0.0295 - lr: 1.0000e-04\n",
      "Epoch 260/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6608e-04 - rmse: 0.0294\n",
      "Epoch 260: val_loss improved from 0.00097 to 0.00093, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 8.3908e-04 - rmse: 0.0272 - val_loss: 9.2576e-04 - val_rmse: 0.0287 - lr: 1.0000e-04\n",
      "Epoch 261/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4181e-04 - rmse: 0.0272\n",
      "Epoch 261: val_loss improved from 0.00093 to 0.00091, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 8.4127e-04 - rmse: 0.0272 - val_loss: 9.1312e-04 - val_rmse: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 262/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.6242e-04 - rmse: 0.0276\n",
      "Epoch 262: val_loss improved from 0.00091 to 0.00090, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 7.9959e-04 - rmse: 0.0264 - val_loss: 9.0202e-04 - val_rmse: 0.0283 - lr: 1.0000e-04\n",
      "Epoch 263/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6112e-04 - rmse: 0.0237\n",
      "Epoch 263: val_loss did not improve from 0.00090\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 8.4679e-04 - rmse: 0.0273 - val_loss: 9.4668e-04 - val_rmse: 0.0291 - lr: 1.0000e-04\n",
      "Epoch 264/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1622e-04 - rmse: 0.0268\n",
      "Epoch 264: val_loss improved from 0.00090 to 0.00090, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 8.3335e-04 - rmse: 0.0271 - val_loss: 8.9970e-04 - val_rmse: 0.0283 - lr: 1.0000e-04\n",
      "Epoch 265/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5903e-04 - rmse: 0.0257\n",
      "Epoch 265: val_loss improved from 0.00090 to 0.00089, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 8.5002e-04 - rmse: 0.0274 - val_loss: 8.9416e-04 - val_rmse: 0.0282 - lr: 1.0000e-04\n",
      "Epoch 266/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9356e-04 - rmse: 0.0198\n",
      "Epoch 266: val_loss improved from 0.00089 to 0.00087, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 8.0024e-04 - rmse: 0.0265 - val_loss: 8.6934e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 267/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.3457e-04 - rmse: 0.0289\n",
      "Epoch 267: val_loss did not improve from 0.00087\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.3668e-04 - rmse: 0.0271 - val_loss: 8.7026e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 268/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6407e-04 - rmse: 0.0237\n",
      "Epoch 268: val_loss did not improve from 0.00087\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.9053e-04 - rmse: 0.0263 - val_loss: 8.7994e-04 - val_rmse: 0.0279 - lr: 1.0000e-04\n",
      "Epoch 269/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6240e-04 - rmse: 0.0237\n",
      "Epoch 269: val_loss improved from 0.00087 to 0.00086, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 8.1833e-04 - rmse: 0.0268 - val_loss: 8.5617e-04 - val_rmse: 0.0275 - lr: 1.0000e-04\n",
      "Epoch 270/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7093e-04 - rmse: 0.0239\n",
      "Epoch 270: val_loss did not improve from 0.00086\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.6657e-04 - rmse: 0.0277 - val_loss: 8.6469e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 271/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6784e-04 - rmse: 0.0258\n",
      "Epoch 271: val_loss did not improve from 0.00086\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 7.0282e-04 - rmse: 0.0245 - val_loss: 9.1571e-04 - val_rmse: 0.0286 - lr: 1.0000e-04\n",
      "Epoch 272/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8184e-04 - rmse: 0.0280\n",
      "Epoch 272: val_loss improved from 0.00086 to 0.00084, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 8.8980e-04 - rmse: 0.0281 - val_loss: 8.4202e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1608e-04 - rmse: 0.0248\n",
      "Epoch 273: val_loss did not improve from 0.00084\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.2651e-04 - rmse: 0.0270 - val_loss: 8.6092e-04 - val_rmse: 0.0276 - lr: 1.0000e-04\n",
      "Epoch 274/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4472e-04 - rmse: 0.0211\n",
      "Epoch 274: val_loss improved from 0.00084 to 0.00083, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 7.6979e-04 - rmse: 0.0259 - val_loss: 8.3479e-04 - val_rmse: 0.0271 - lr: 1.0000e-04\n",
      "Epoch 275/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2716e-04 - rmse: 0.0288\n",
      "Epoch 275: val_loss did not improve from 0.00083\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 8.5506e-04 - rmse: 0.0275 - val_loss: 8.5142e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 276/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0222e-04 - rmse: 0.0224\n",
      "Epoch 276: val_loss did not improve from 0.00083\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4507e-04 - rmse: 0.0254 - val_loss: 8.5117e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 277/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3759e-04 - rmse: 0.0272\n",
      "Epoch 277: val_loss did not improve from 0.00083\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.9048e-04 - rmse: 0.0263 - val_loss: 8.5091e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 278/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2362e-04 - rmse: 0.0206\n",
      "Epoch 278: val_loss improved from 0.00083 to 0.00081, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 6.6334e-04 - rmse: 0.0237 - val_loss: 8.0548e-04 - val_rmse: 0.0266 - lr: 1.0000e-04\n",
      "Epoch 279/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6627e-04 - rmse: 0.0258\n",
      "Epoch 279: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 8.1007e-04 - rmse: 0.0266 - val_loss: 8.1139e-04 - val_rmse: 0.0267 - lr: 1.0000e-04\n",
      "Epoch 280/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1473e-04 - rmse: 0.0267\n",
      "Epoch 280: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.7911e-04 - rmse: 0.0241 - val_loss: 8.1254e-04 - val_rmse: 0.0267 - lr: 1.0000e-04\n",
      "Epoch 281/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1152e-04 - rmse: 0.0226\n",
      "Epoch 281: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.9145e-04 - rmse: 0.0243 - val_loss: 8.5281e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 282/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1033e-04 - rmse: 0.0267\n",
      "Epoch 282: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.7407e-04 - rmse: 0.0260 - val_loss: 8.2595e-04 - val_rmse: 0.0269 - lr: 1.0000e-04\n",
      "Epoch 283/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4195e-04 - rmse: 0.0233\n",
      "Epoch 283: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.5316e-04 - rmse: 0.0235 - val_loss: 8.7087e-04 - val_rmse: 0.0278 - lr: 1.0000e-04\n",
      "Epoch 284/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5266e-04 - rmse: 0.0274\n",
      "Epoch 284: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.4798e-04 - rmse: 0.0255 - val_loss: 9.1073e-04 - val_rmse: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 285/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9209e-04 - rmse: 0.0263\n",
      "Epoch 285: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.5456e-04 - rmse: 0.0256 - val_loss: 8.7889e-04 - val_rmse: 0.0279 - lr: 1.0000e-04\n",
      "Epoch 286/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1687e-04 - rmse: 0.0268\n",
      "Epoch 286: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 7.3522e-04 - rmse: 0.0252 - val_loss: 8.2476e-04 - val_rmse: 0.0269 - lr: 1.0000e-04\n",
      "Epoch 287/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7132e-04 - rmse: 0.0217\n",
      "Epoch 287: val_loss did not improve from 0.00081\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.0023e-04 - rmse: 0.0245 - val_loss: 8.5175e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 288/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8147e-04 - rmse: 0.0241\n",
      "Epoch 288: val_loss improved from 0.00081 to 0.00078, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 6.7235e-04 - rmse: 0.0239 - val_loss: 7.7926e-04 - val_rmse: 0.0261 - lr: 1.0000e-04\n",
      "Epoch 289/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.5002e-04 - rmse: 0.0235\n",
      "Epoch 289: val_loss did not improve from 0.00078\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.2017e-04 - rmse: 0.0249 - val_loss: 8.1995e-04 - val_rmse: 0.0268 - lr: 1.0000e-04\n",
      "Epoch 290/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.6033e-04 - rmse: 0.0215\n",
      "Epoch 290: val_loss improved from 0.00078 to 0.00077, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 6.4812e-04 - rmse: 0.0234 - val_loss: 7.6562e-04 - val_rmse: 0.0258 - lr: 1.0000e-04\n",
      "Epoch 291/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9636e-04 - rmse: 0.0223\n",
      "Epoch 291: val_loss did not improve from 0.00077\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.2300e-04 - rmse: 0.0229 - val_loss: 8.0620e-04 - val_rmse: 0.0266 - lr: 1.0000e-04\n",
      "Epoch 292/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1039e-04 - rmse: 0.0267\n",
      "Epoch 292: val_loss improved from 0.00077 to 0.00073, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 7.0987e-04 - rmse: 0.0247 - val_loss: 7.3105e-04 - val_rmse: 0.0251 - lr: 1.0000e-04\n",
      "Epoch 293/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0299e-04 - rmse: 0.0224\n",
      "Epoch 293: val_loss did not improve from 0.00073\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 6.5908e-04 - rmse: 0.0237 - val_loss: 7.6266e-04 - val_rmse: 0.0258 - lr: 1.0000e-04\n",
      "Epoch 294/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2512e-04 - rmse: 0.0229\n",
      "Epoch 294: val_loss did not improve from 0.00073\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.5426e-04 - rmse: 0.0236 - val_loss: 7.3848e-04 - val_rmse: 0.0253 - lr: 1.0000e-04\n",
      "Epoch 295/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5874e-04 - rmse: 0.0257\n",
      "Epoch 295: val_loss did not improve from 0.00073\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.3688e-04 - rmse: 0.0232 - val_loss: 7.7607e-04 - val_rmse: 0.0260 - lr: 1.0000e-04\n",
      "Epoch 296/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0557e-04 - rmse: 0.0246\n",
      "Epoch 296: val_loss improved from 0.00073 to 0.00072, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 6.8683e-04 - rmse: 0.0242 - val_loss: 7.2208e-04 - val_rmse: 0.0250 - lr: 1.0000e-04\n",
      "Epoch 297/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1723e-04 - rmse: 0.0228\n",
      "Epoch 297: val_loss improved from 0.00072 to 0.00071, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 45ms/step - loss: 6.8805e-04 - rmse: 0.0243 - val_loss: 7.1471e-04 - val_rmse: 0.0248 - lr: 1.0000e-04\n",
      "Epoch 298/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.3078e-04 - rmse: 0.0231\n",
      "Epoch 298: val_loss improved from 0.00071 to 0.00069, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 6.7213e-04 - rmse: 0.0239 - val_loss: 6.9358e-04 - val_rmse: 0.0244 - lr: 1.0000e-04\n",
      "Epoch 299/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.3326e-04 - rmse: 0.0231\n",
      "Epoch 299: val_loss improved from 0.00069 to 0.00069, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 5.2974e-04 - rmse: 0.0207 - val_loss: 6.8837e-04 - val_rmse: 0.0243 - lr: 1.0000e-04\n",
      "Epoch 300/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.5618e-04 - rmse: 0.0236\n",
      "Epoch 300: val_loss did not improve from 0.00069\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.8038e-04 - rmse: 0.0241 - val_loss: 7.0238e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 301/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5859e-04 - rmse: 0.0214\n",
      "Epoch 301: val_loss improved from 0.00069 to 0.00067, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 5.8375e-04 - rmse: 0.0220 - val_loss: 6.7284e-04 - val_rmse: 0.0239 - lr: 1.0000e-04\n",
      "Epoch 302/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.0261e-04 - rmse: 0.0265\n",
      "Epoch 302: val_loss did not improve from 0.00067\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 6.8526e-04 - rmse: 0.0242 - val_loss: 6.9688e-04 - val_rmse: 0.0244 - lr: 1.0000e-04\n",
      "Epoch 303/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5269e-04 - rmse: 0.0213\n",
      "Epoch 303: val_loss did not improve from 0.00067\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.2687e-04 - rmse: 0.0207 - val_loss: 6.7921e-04 - val_rmse: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 304/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2313e-04 - rmse: 0.0229\n",
      "Epoch 304: val_loss improved from 0.00067 to 0.00067, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 6.2002e-04 - rmse: 0.0228 - val_loss: 6.6864e-04 - val_rmse: 0.0239 - lr: 1.0000e-04\n",
      "Epoch 305/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7101e-04 - rmse: 0.0217\n",
      "Epoch 305: val_loss improved from 0.00067 to 0.00066, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 6.1649e-04 - rmse: 0.0227 - val_loss: 6.5817e-04 - val_rmse: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 306/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4196e-04 - rmse: 0.0185\n",
      "Epoch 306: val_loss improved from 0.00066 to 0.00065, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 5.2789e-04 - rmse: 0.0207 - val_loss: 6.4680e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 307/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8663e-04 - rmse: 0.0242\n",
      "Epoch 307: val_loss did not improve from 0.00065\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.7449e-04 - rmse: 0.0218 - val_loss: 6.4895e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 308/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7103e-04 - rmse: 0.0217\n",
      "Epoch 308: val_loss did not improve from 0.00065\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.1534e-04 - rmse: 0.0227 - val_loss: 6.7421e-04 - val_rmse: 0.0240 - lr: 1.0000e-04\n",
      "Epoch 309/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1006e-04 - rmse: 0.0226\n",
      "Epoch 309: val_loss improved from 0.00065 to 0.00064, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 5.6726e-04 - rmse: 0.0216 - val_loss: 6.4459e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 310/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9395e-04 - rmse: 0.0264\n",
      "Epoch 310: val_loss did not improve from 0.00064\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 7.0789e-04 - rmse: 0.0247 - val_loss: 6.6641e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 311/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8792e-04 - rmse: 0.0197\n",
      "Epoch 311: val_loss did not improve from 0.00064\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 5.4276e-04 - rmse: 0.0211 - val_loss: 6.5012e-04 - val_rmse: 0.0235 - lr: 1.0000e-04\n",
      "Epoch 312/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7494e-04 - rmse: 0.0194\n",
      "Epoch 312: val_loss improved from 0.00064 to 0.00064, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 5.6360e-04 - rmse: 0.0216 - val_loss: 6.4209e-04 - val_rmse: 0.0233 - lr: 1.0000e-04\n",
      "Epoch 313/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.0590e-04 - rmse: 0.0202\n",
      "Epoch 313: val_loss improved from 0.00064 to 0.00062, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 4.9804e-04 - rmse: 0.0200 - val_loss: 6.1631e-04 - val_rmse: 0.0227 - lr: 1.0000e-04\n",
      "Epoch 314/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.2187e-04 - rmse: 0.0287\n",
      "Epoch 314: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 5.8577e-04 - rmse: 0.0221 - val_loss: 6.1752e-04 - val_rmse: 0.0228 - lr: 1.0000e-04\n",
      "Epoch 315/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4108e-04 - rmse: 0.0210\n",
      "Epoch 315: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 6.2575e-04 - rmse: 0.0230 - val_loss: 6.5511e-04 - val_rmse: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 316/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7962e-04 - rmse: 0.0219\n",
      "Epoch 316: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 5.0174e-04 - rmse: 0.0201 - val_loss: 6.4651e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 317/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3577e-04 - rmse: 0.0209\n",
      "Epoch 317: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.9671e-04 - rmse: 0.0223 - val_loss: 6.8460e-04 - val_rmse: 0.0242 - lr: 1.0000e-04\n",
      "Epoch 318/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4044e-04 - rmse: 0.0185\n",
      "Epoch 318: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.1403e-04 - rmse: 0.0204 - val_loss: 6.9141e-04 - val_rmse: 0.0243 - lr: 1.0000e-04\n",
      "Epoch 319/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.2614e-04 - rmse: 0.0250\n",
      "Epoch 319: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.7515e-04 - rmse: 0.0240 - val_loss: 8.4177e-04 - val_rmse: 0.0273 - lr: 1.0000e-04\n",
      "Epoch 320/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6684e-04 - rmse: 0.0238\n",
      "Epoch 320: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.9722e-04 - rmse: 0.0223 - val_loss: 7.5380e-04 - val_rmse: 0.0256 - lr: 1.0000e-04\n",
      "Epoch 321/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.8610e-04 - rmse: 0.0221\n",
      "Epoch 321: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.6128e-04 - rmse: 0.0237 - val_loss: 8.6075e-04 - val_rmse: 0.0276 - lr: 1.0000e-04\n",
      "Epoch 322/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6918e-04 - rmse: 0.0239\n",
      "Epoch 322: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.9702e-04 - rmse: 0.0223 - val_loss: 6.9640e-04 - val_rmse: 0.0244 - lr: 1.0000e-04\n",
      "Epoch 323/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4902e-04 - rmse: 0.0212\n",
      "Epoch 323: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.3692e-04 - rmse: 0.0232 - val_loss: 8.0067e-04 - val_rmse: 0.0265 - lr: 1.0000e-04\n",
      "Epoch 324/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8273e-04 - rmse: 0.0242\n",
      "Epoch 324: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.7685e-04 - rmse: 0.0219 - val_loss: 7.6571e-04 - val_rmse: 0.0258 - lr: 1.0000e-04\n",
      "Epoch 325/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3133e-04 - rmse: 0.0252\n",
      "Epoch 325: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.2975e-04 - rmse: 0.0230 - val_loss: 8.0004e-04 - val_rmse: 0.0265 - lr: 1.0000e-04\n",
      "Epoch 326/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.8092e-04 - rmse: 0.0241\n",
      "Epoch 326: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.4835e-04 - rmse: 0.0212 - val_loss: 7.1932e-04 - val_rmse: 0.0249 - lr: 1.0000e-04\n",
      "Epoch 327/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.7183e-04 - rmse: 0.0259\n",
      "Epoch 327: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.9614e-04 - rmse: 0.0244 - val_loss: 7.3069e-04 - val_rmse: 0.0251 - lr: 1.0000e-04\n",
      "Epoch 328/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.6029e-04 - rmse: 0.0237\n",
      "Epoch 328: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.4683e-04 - rmse: 0.0212 - val_loss: 6.3476e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 329/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2687e-04 - rmse: 0.0230\n",
      "Epoch 329: val_loss did not improve from 0.00062\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.4107e-04 - rmse: 0.0210 - val_loss: 6.3762e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 330/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3195e-04 - rmse: 0.0208\n",
      "Epoch 330: val_loss improved from 0.00062 to 0.00058, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 5.4355e-04 - rmse: 0.0211 - val_loss: 5.8375e-04 - val_rmse: 0.0220 - lr: 1.0000e-04\n",
      "Epoch 331/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.6431e-04 - rmse: 0.0191\n",
      "Epoch 331: val_loss improved from 0.00058 to 0.00056, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 4.0838e-04 - rmse: 0.0176 - val_loss: 5.5603e-04 - val_rmse: 0.0214 - lr: 1.0000e-04\n",
      "Epoch 332/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9809e-04 - rmse: 0.0224\n",
      "Epoch 332: val_loss did not improve from 0.00056\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.6829e-04 - rmse: 0.0217 - val_loss: 5.5657e-04 - val_rmse: 0.0214 - lr: 1.0000e-04\n",
      "Epoch 333/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4470e-04 - rmse: 0.0211\n",
      "Epoch 333: val_loss did not improve from 0.00056\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.1681e-04 - rmse: 0.0205 - val_loss: 5.6482e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 334/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5063e-04 - rmse: 0.0188\n",
      "Epoch 334: val_loss did not improve from 0.00056\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.2093e-04 - rmse: 0.0206 - val_loss: 5.7508e-04 - val_rmse: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 335/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.3997e-04 - rmse: 0.0210\n",
      "Epoch 335: val_loss improved from 0.00056 to 0.00055, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 5.1013e-04 - rmse: 0.0203 - val_loss: 5.5432e-04 - val_rmse: 0.0214 - lr: 1.0000e-04\n",
      "Epoch 336/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4812e-04 - rmse: 0.0212\n",
      "Epoch 336: val_loss did not improve from 0.00055\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 5.0178e-04 - rmse: 0.0201 - val_loss: 5.8359e-04 - val_rmse: 0.0220 - lr: 1.0000e-04\n",
      "Epoch 337/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3633e-04 - rmse: 0.0184\n",
      "Epoch 337: val_loss improved from 0.00055 to 0.00053, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 5.4380e-04 - rmse: 0.0211 - val_loss: 5.2627e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 338/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6914e-04 - rmse: 0.0131\n",
      "Epoch 338: val_loss did not improve from 0.00053\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.7331e-04 - rmse: 0.0194 - val_loss: 5.4567e-04 - val_rmse: 0.0212 - lr: 1.0000e-04\n",
      "Epoch 339/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7444e-04 - rmse: 0.0194\n",
      "Epoch 339: val_loss did not improve from 0.00053\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 5.1000e-04 - rmse: 0.0203 - val_loss: 5.3224e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 340/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5144e-04 - rmse: 0.0188\n",
      "Epoch 340: val_loss improved from 0.00053 to 0.00053, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.5168e-04 - rmse: 0.0188 - val_loss: 5.2550e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 341/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8586e-04 - rmse: 0.0197\n",
      "Epoch 341: val_loss improved from 0.00053 to 0.00052, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 4.8068e-04 - rmse: 0.0196 - val_loss: 5.1770e-04 - val_rmse: 0.0205 - lr: 1.0000e-04\n",
      "Epoch 342/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2207e-04 - rmse: 0.0206\n",
      "Epoch 342: val_loss did not improve from 0.00052\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 4.6577e-04 - rmse: 0.0192 - val_loss: 5.2288e-04 - val_rmse: 0.0206 - lr: 1.0000e-04\n",
      "Epoch 343/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4645e-04 - rmse: 0.0212\n",
      "Epoch 343: val_loss improved from 0.00052 to 0.00050, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.6464e-04 - rmse: 0.0191 - val_loss: 5.0097e-04 - val_rmse: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 344/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4577e-04 - rmse: 0.0212\n",
      "Epoch 344: val_loss did not improve from 0.00050\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 4.9385e-04 - rmse: 0.0199 - val_loss: 5.0850e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 345/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1200e-04 - rmse: 0.0203\n",
      "Epoch 345: val_loss improved from 0.00050 to 0.00050, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.3300e-04 - rmse: 0.0183 - val_loss: 5.0094e-04 - val_rmse: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 346/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.7395e-04 - rmse: 0.0240\n",
      "Epoch 346: val_loss did not improve from 0.00050\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 5.8194e-04 - rmse: 0.0220 - val_loss: 5.1563e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 347/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7745e-04 - rmse: 0.0167\n",
      "Epoch 347: val_loss improved from 0.00050 to 0.00049, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.6555e-04 - rmse: 0.0192 - val_loss: 4.8706e-04 - val_rmse: 0.0197 - lr: 1.0000e-04\n",
      "Epoch 348/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7984e-04 - rmse: 0.0168\n",
      "Epoch 348: val_loss did not improve from 0.00049\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.5957e-04 - rmse: 0.0190 - val_loss: 5.1552e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 349/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5132e-04 - rmse: 0.0159\n",
      "Epoch 349: val_loss did not improve from 0.00049\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.4808e-04 - rmse: 0.0187 - val_loss: 4.9056e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 350/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3377e-04 - rmse: 0.0183\n",
      "Epoch 350: val_loss did not improve from 0.00049\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.2737e-04 - rmse: 0.0182 - val_loss: 4.9257e-04 - val_rmse: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 351/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8582e-04 - rmse: 0.0197\n",
      "Epoch 351: val_loss improved from 0.00049 to 0.00049, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 4.4143e-04 - rmse: 0.0185 - val_loss: 4.8587e-04 - val_rmse: 0.0197 - lr: 1.0000e-04\n",
      "Epoch 352/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.9037e-04 - rmse: 0.0198\n",
      "Epoch 352: val_loss did not improve from 0.00049\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.4967e-04 - rmse: 0.0188 - val_loss: 5.3690e-04 - val_rmse: 0.0210 - lr: 1.0000e-04\n",
      "Epoch 353/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.4614e-04 - rmse: 0.0187\n",
      "Epoch 353: val_loss improved from 0.00049 to 0.00049, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 4.6289e-04 - rmse: 0.0191 - val_loss: 4.8560e-04 - val_rmse: 0.0197 - lr: 1.0000e-04\n",
      "Epoch 354/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5102e-04 - rmse: 0.0159\n",
      "Epoch 354: val_loss did not improve from 0.00049\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.1551e-04 - rmse: 0.0178 - val_loss: 5.0668e-04 - val_rmse: 0.0202 - lr: 1.0000e-04\n",
      "Epoch 355/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8249e-04 - rmse: 0.0196\n",
      "Epoch 355: val_loss improved from 0.00049 to 0.00048, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 4.4806e-04 - rmse: 0.0187 - val_loss: 4.8045e-04 - val_rmse: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 356/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.5242e-04 - rmse: 0.0213\n",
      "Epoch 356: val_loss did not improve from 0.00048\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.5717e-04 - rmse: 0.0190 - val_loss: 5.0013e-04 - val_rmse: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 357/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.8184e-04 - rmse: 0.0196\n",
      "Epoch 357: val_loss improved from 0.00048 to 0.00046, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 4.5783e-04 - rmse: 0.0190 - val_loss: 4.6272e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 358/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6503e-04 - rmse: 0.0129\n",
      "Epoch 358: val_loss improved from 0.00046 to 0.00046, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 4.1575e-04 - rmse: 0.0178 - val_loss: 4.6210e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 359/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1110e-04 - rmse: 0.0177\n",
      "Epoch 359: val_loss did not improve from 0.00046\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.7496e-04 - rmse: 0.0167 - val_loss: 4.6283e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 360/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7690e-04 - rmse: 0.0195\n",
      "Epoch 360: val_loss did not improve from 0.00046\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.5601e-04 - rmse: 0.0189 - val_loss: 4.9590e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 361/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1806e-04 - rmse: 0.0205\n",
      "Epoch 361: val_loss improved from 0.00046 to 0.00045, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 4.5620e-04 - rmse: 0.0189 - val_loss: 4.5316e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 362/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6357e-04 - rmse: 0.0129\n",
      "Epoch 362: val_loss did not improve from 0.00045\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 3.9251e-04 - rmse: 0.0172 - val_loss: 4.6743e-04 - val_rmse: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 363/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0388e-04 - rmse: 0.0175\n",
      "Epoch 363: val_loss improved from 0.00045 to 0.00045, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 4.1290e-04 - rmse: 0.0178 - val_loss: 4.5175e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 364/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3004e-04 - rmse: 0.0152\n",
      "Epoch 364: val_loss did not improve from 0.00045\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.1332e-04 - rmse: 0.0178 - val_loss: 4.5295e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 365/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7399e-04 - rmse: 0.0166\n",
      "Epoch 365: val_loss improved from 0.00045 to 0.00044, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 4.1195e-04 - rmse: 0.0177 - val_loss: 4.3670e-04 - val_rmse: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2283e-04 - rmse: 0.0206\n",
      "Epoch 366: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.2199e-04 - rmse: 0.0180 - val_loss: 4.3914e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 367/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1765e-04 - rmse: 0.0205\n",
      "Epoch 367: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.2181e-04 - rmse: 0.0180 - val_loss: 4.3935e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 368/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.1809e-04 - rmse: 0.0205\n",
      "Epoch 368: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.5681e-04 - rmse: 0.0190 - val_loss: 4.5458e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 369/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6275e-04 - rmse: 0.0163\n",
      "Epoch 369: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 4.2474e-04 - rmse: 0.0181 - val_loss: 4.4291e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 370/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3886e-04 - rmse: 0.0185\n",
      "Epoch 370: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9640e-04 - rmse: 0.0173 - val_loss: 4.4489e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 371/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.4369e-04 - rmse: 0.0211\n",
      "Epoch 371: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.8052e-04 - rmse: 0.0196 - val_loss: 4.5049e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 372/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4165e-04 - rmse: 0.0156\n",
      "Epoch 372: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6565e-04 - rmse: 0.0164 - val_loss: 4.4154e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 373/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7843e-04 - rmse: 0.0195\n",
      "Epoch 373: val_loss did not improve from 0.00044\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.2695e-04 - rmse: 0.0182 - val_loss: 4.5137e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 374/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9296e-04 - rmse: 0.0140\n",
      "Epoch 374: val_loss improved from 0.00044 to 0.00042, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 4.0209e-04 - rmse: 0.0175 - val_loss: 4.1982e-04 - val_rmse: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 375/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0265e-04 - rmse: 0.0143\n",
      "Epoch 375: val_loss improved from 0.00042 to 0.00042, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 3.4907e-04 - rmse: 0.0159 - val_loss: 4.1538e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 376/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7804e-04 - rmse: 0.0135\n",
      "Epoch 376: val_loss did not improve from 0.00042\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9006e-04 - rmse: 0.0171 - val_loss: 4.2183e-04 - val_rmse: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 377/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0755e-04 - rmse: 0.0176\n",
      "Epoch 377: val_loss did not improve from 0.00042\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9314e-04 - rmse: 0.0172 - val_loss: 4.1621e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 378/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2251e-04 - rmse: 0.0150\n",
      "Epoch 378: val_loss did not improve from 0.00042\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.2297e-04 - rmse: 0.0181 - val_loss: 4.2931e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 379/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6108e-04 - rmse: 0.0162\n",
      "Epoch 379: val_loss did not improve from 0.00042\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.2060e-04 - rmse: 0.0180 - val_loss: 4.1718e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 380/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1022e-04 - rmse: 0.0146\n",
      "Epoch 380: val_loss improved from 0.00042 to 0.00041, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 3.6441e-04 - rmse: 0.0164 - val_loss: 4.0523e-04 - val_rmse: 0.0176 - lr: 1.0000e-04\n",
      "Epoch 381/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5132e-04 - rmse: 0.0159\n",
      "Epoch 381: val_loss did not improve from 0.00041\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.8601e-04 - rmse: 0.0170 - val_loss: 4.1627e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 382/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7784e-04 - rmse: 0.0168\n",
      "Epoch 382: val_loss improved from 0.00041 to 0.00040, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 3.7517e-04 - rmse: 0.0167 - val_loss: 3.9801e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 383/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2397e-04 - rmse: 0.0151\n",
      "Epoch 383: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 3.5083e-04 - rmse: 0.0159 - val_loss: 4.1336e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 384/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5963e-04 - rmse: 0.0190\n",
      "Epoch 384: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.9816e-04 - rmse: 0.0174 - val_loss: 3.9975e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 385/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.2789e-04 - rmse: 0.0182\n",
      "Epoch 385: val_loss did not improve from 0.00040\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9525e-04 - rmse: 0.0173 - val_loss: 4.0311e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 386/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2243e-04 - rmse: 0.0112\n",
      "Epoch 386: val_loss improved from 0.00040 to 0.00039, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 3.0874e-04 - rmse: 0.0146 - val_loss: 3.9349e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 387/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1109e-04 - rmse: 0.0146\n",
      "Epoch 387: val_loss did not improve from 0.00039\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.0087e-04 - rmse: 0.0174 - val_loss: 4.4254e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 388/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7545e-04 - rmse: 0.0167\n",
      "Epoch 388: val_loss improved from 0.00039 to 0.00039, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 3.9818e-04 - rmse: 0.0174 - val_loss: 3.8765e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 389/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5179e-04 - rmse: 0.0160\n",
      "Epoch 389: val_loss did not improve from 0.00039\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5236e-04 - rmse: 0.0160 - val_loss: 3.8872e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 390/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.2738e-04 - rmse: 0.0208\n",
      "Epoch 390: val_loss did not improve from 0.00039\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9666e-04 - rmse: 0.0173 - val_loss: 3.8783e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9329e-04 - rmse: 0.0172\n",
      "Epoch 391: val_loss did not improve from 0.00039\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9590e-04 - rmse: 0.0173 - val_loss: 3.8993e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 392/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2004e-04 - rmse: 0.0149\n",
      "Epoch 392: val_loss improved from 0.00039 to 0.00038, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 3.3268e-04 - rmse: 0.0154 - val_loss: 3.7855e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 393/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7565e-04 - rmse: 0.0195\n",
      "Epoch 393: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.5744e-04 - rmse: 0.0162 - val_loss: 3.7911e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 394/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9376e-04 - rmse: 0.0172\n",
      "Epoch 394: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 4.0032e-04 - rmse: 0.0174 - val_loss: 3.8075e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 395/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3457e-04 - rmse: 0.0154\n",
      "Epoch 395: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.4163e-04 - rmse: 0.0157 - val_loss: 3.8444e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 396/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7104e-04 - rmse: 0.0166\n",
      "Epoch 396: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.8201e-04 - rmse: 0.0169 - val_loss: 4.2569e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 397/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7628e-04 - rmse: 0.0167\n",
      "Epoch 397: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.8472e-04 - rmse: 0.0170 - val_loss: 3.8953e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 398/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8300e-04 - rmse: 0.0137\n",
      "Epoch 398: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6557e-04 - rmse: 0.0164 - val_loss: 4.0864e-04 - val_rmse: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 399/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6287e-04 - rmse: 0.0163\n",
      "Epoch 399: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5506e-04 - rmse: 0.0161 - val_loss: 4.1246e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 400/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5596e-04 - rmse: 0.0190\n",
      "Epoch 400: val_loss did not improve from 0.00038\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 3.6846e-04 - rmse: 0.0165 - val_loss: 4.1397e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 401/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1878e-04 - rmse: 0.0149\n",
      "Epoch 401: val_loss improved from 0.00038 to 0.00037, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 3.7190e-04 - rmse: 0.0166 - val_loss: 3.7301e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 402/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0397e-04 - rmse: 0.0144\n",
      "Epoch 402: val_loss did not improve from 0.00037\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.5372e-04 - rmse: 0.0160 - val_loss: 3.7904e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 403/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1385e-04 - rmse: 0.0148\n",
      "Epoch 403: val_loss improved from 0.00037 to 0.00036, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.4550e-04 - rmse: 0.0158 - val_loss: 3.5773e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 404/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4957e-04 - rmse: 0.0159\n",
      "Epoch 404: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5041e-04 - rmse: 0.0159 - val_loss: 3.5808e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 405/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3690e-04 - rmse: 0.0119\n",
      "Epoch 405: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6462e-04 - rmse: 0.0164 - val_loss: 4.0089e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 406/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.7058e-04 - rmse: 0.0193\n",
      "Epoch 406: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.9242e-04 - rmse: 0.0172 - val_loss: 4.0282e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 407/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5412e-04 - rmse: 0.0189\n",
      "Epoch 407: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6995e-04 - rmse: 0.0165 - val_loss: 4.0238e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 408/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5450e-04 - rmse: 0.0161\n",
      "Epoch 408: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.8148e-04 - rmse: 0.0169 - val_loss: 3.7254e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 409/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.3437e-04 - rmse: 0.0184\n",
      "Epoch 409: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 3.4854e-04 - rmse: 0.0159 - val_loss: 3.7439e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 410/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1622e-04 - rmse: 0.0148\n",
      "Epoch 410: val_loss did not improve from 0.00036\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.0210e-04 - rmse: 0.0144 - val_loss: 3.5872e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 411/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9762e-04 - rmse: 0.0174\n",
      "Epoch 411: val_loss improved from 0.00036 to 0.00036, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.5468e-04 - rmse: 0.0161 - val_loss: 3.5756e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 412/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0279e-04 - rmse: 0.0144\n",
      "Epoch 412: val_loss improved from 0.00036 to 0.00035, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.4171e-04 - rmse: 0.0157 - val_loss: 3.5376e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 413/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1805e-04 - rmse: 0.0149\n",
      "Epoch 413: val_loss improved from 0.00035 to 0.00035, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.4350e-04 - rmse: 0.0157 - val_loss: 3.4908e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 414/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6331e-04 - rmse: 0.0129\n",
      "Epoch 414: val_loss improved from 0.00035 to 0.00035, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 3.2189e-04 - rmse: 0.0150 - val_loss: 3.4525e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2932e-04 - rmse: 0.0153\n",
      "Epoch 415: val_loss did not improve from 0.00035\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.0334e-04 - rmse: 0.0144 - val_loss: 3.5405e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 416/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1043e-04 - rmse: 0.0177\n",
      "Epoch 416: val_loss improved from 0.00035 to 0.00034, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.7068e-04 - rmse: 0.0166 - val_loss: 3.4128e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 417/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5330e-04 - rmse: 0.0125\n",
      "Epoch 417: val_loss improved from 0.00034 to 0.00034, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 3.1815e-04 - rmse: 0.0149 - val_loss: 3.4007e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 418/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.5273e-04 - rmse: 0.0160\n",
      "Epoch 418: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 3.3284e-04 - rmse: 0.0154 - val_loss: 3.4409e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 419/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9692e-04 - rmse: 0.0142\n",
      "Epoch 419: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 3.0933e-04 - rmse: 0.0146 - val_loss: 3.5275e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 420/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0320e-04 - rmse: 0.0175\n",
      "Epoch 420: val_loss improved from 0.00034 to 0.00034, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 3.4809e-04 - rmse: 0.0159 - val_loss: 3.3824e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 421/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1725e-04 - rmse: 0.0179\n",
      "Epoch 421: val_loss improved from 0.00034 to 0.00034, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.2938e-04 - rmse: 0.0153 - val_loss: 3.3775e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 422/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8972e-04 - rmse: 0.0139\n",
      "Epoch 422: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.1672e-04 - rmse: 0.0149 - val_loss: 3.4951e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 423/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6634e-04 - rmse: 0.0131\n",
      "Epoch 423: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5147e-04 - rmse: 0.0160 - val_loss: 3.8205e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 424/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2286e-04 - rmse: 0.0151\n",
      "Epoch 424: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5942e-04 - rmse: 0.0162 - val_loss: 3.5156e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 425/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5811e-04 - rmse: 0.0190\n",
      "Epoch 425: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5119e-04 - rmse: 0.0160 - val_loss: 3.6080e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 426/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9604e-04 - rmse: 0.0142\n",
      "Epoch 426: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.2488e-04 - rmse: 0.0151 - val_loss: 3.4218e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 427/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.9278e-04 - rmse: 0.0172\n",
      "Epoch 427: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.8861e-04 - rmse: 0.0171 - val_loss: 3.6336e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 428/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2054e-04 - rmse: 0.0150\n",
      "Epoch 428: val_loss did not improve from 0.00034\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.6828e-04 - rmse: 0.0165 - val_loss: 3.7344e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 429/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9380e-04 - rmse: 0.0141\n",
      "Epoch 429: val_loss improved from 0.00034 to 0.00033, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 3.5792e-04 - rmse: 0.0162 - val_loss: 3.3219e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 430/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4259e-04 - rmse: 0.0121\n",
      "Epoch 430: val_loss improved from 0.00033 to 0.00033, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 3.3505e-04 - rmse: 0.0155 - val_loss: 3.2811e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 431/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7051e-04 - rmse: 0.0132\n",
      "Epoch 431: val_loss did not improve from 0.00033\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7326e-04 - rmse: 0.0133 - val_loss: 3.2987e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 432/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7203e-04 - rmse: 0.0166\n",
      "Epoch 432: val_loss did not improve from 0.00033\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 3.3409e-04 - rmse: 0.0155 - val_loss: 3.6224e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 433/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7216e-04 - rmse: 0.0166\n",
      "Epoch 433: val_loss did not improve from 0.00033\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.1973e-04 - rmse: 0.0150 - val_loss: 3.3615e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 434/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6175e-04 - rmse: 0.0163\n",
      "Epoch 434: val_loss did not improve from 0.00033\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.1232e-04 - rmse: 0.0147 - val_loss: 3.4168e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 435/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4145e-04 - rmse: 0.0157\n",
      "Epoch 435: val_loss improved from 0.00033 to 0.00032, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.1823e-04 - rmse: 0.0149 - val_loss: 3.1822e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 436/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1044e-04 - rmse: 0.0147\n",
      "Epoch 436: val_loss improved from 0.00032 to 0.00031, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 3.2649e-04 - rmse: 0.0152 - val_loss: 3.1172e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 437/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7371e-04 - rmse: 0.0134\n",
      "Epoch 437: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 3.2763e-04 - rmse: 0.0152 - val_loss: 3.2068e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 438/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0702e-04 - rmse: 0.0106\n",
      "Epoch 438: val_loss did not improve from 0.00031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 21ms/step - loss: 2.8422e-04 - rmse: 0.0138 - val_loss: 3.1462e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 439/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8158e-04 - rmse: 0.0137\n",
      "Epoch 439: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.8819e-04 - rmse: 0.0139 - val_loss: 3.3365e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 440/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3136e-04 - rmse: 0.0154\n",
      "Epoch 440: val_loss improved from 0.00031 to 0.00031, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 3.1585e-04 - rmse: 0.0149 - val_loss: 3.0914e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 441/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6999e-04 - rmse: 0.0132\n",
      "Epoch 441: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.8760e-04 - rmse: 0.0139 - val_loss: 3.1038e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 442/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0203e-04 - rmse: 0.0144\n",
      "Epoch 442: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.8516e-04 - rmse: 0.0138 - val_loss: 3.1064e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 443/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1855e-04 - rmse: 0.0150\n",
      "Epoch 443: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.1625e-04 - rmse: 0.0149 - val_loss: 3.1877e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 444/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5492e-04 - rmse: 0.0126\n",
      "Epoch 444: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9477e-04 - rmse: 0.0141 - val_loss: 3.2049e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 445/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7086e-04 - rmse: 0.0133\n",
      "Epoch 445: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7297e-04 - rmse: 0.0133 - val_loss: 3.2173e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 446/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.5139e-04 - rmse: 0.0189\n",
      "Epoch 446: val_loss did not improve from 0.00031\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.5706e-04 - rmse: 0.0162 - val_loss: 3.1935e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 447/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4695e-04 - rmse: 0.0123\n",
      "Epoch 447: val_loss improved from 0.00031 to 0.00030, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 2.7277e-04 - rmse: 0.0133 - val_loss: 3.0363e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 448/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4377e-04 - rmse: 0.0122\n",
      "Epoch 448: val_loss improved from 0.00030 to 0.00030, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 3.1219e-04 - rmse: 0.0147 - val_loss: 3.0059e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 449/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.3358e-04 - rmse: 0.0155\n",
      "Epoch 449: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9390e-04 - rmse: 0.0141 - val_loss: 3.0641e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 450/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4698e-04 - rmse: 0.0123\n",
      "Epoch 450: val_loss improved from 0.00030 to 0.00030, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.8996e-04 - rmse: 0.0140 - val_loss: 2.9704e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 451/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7585e-04 - rmse: 0.0135\n",
      "Epoch 451: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.8529e-04 - rmse: 0.0138 - val_loss: 3.1593e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 452/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5520e-04 - rmse: 0.0127\n",
      "Epoch 452: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.8360e-04 - rmse: 0.0137 - val_loss: 3.0391e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 453/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1148e-04 - rmse: 0.0147\n",
      "Epoch 453: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9305e-04 - rmse: 0.0141 - val_loss: 2.9986e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 454/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9057e-04 - rmse: 0.0140\n",
      "Epoch 454: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.7680e-04 - rmse: 0.0135 - val_loss: 3.0828e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 455/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0551e-04 - rmse: 0.0145\n",
      "Epoch 455: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.9500e-04 - rmse: 0.0142 - val_loss: 3.2010e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 456/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8449e-04 - rmse: 0.0138\n",
      "Epoch 456: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9605e-04 - rmse: 0.0142 - val_loss: 3.4113e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 457/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2538e-04 - rmse: 0.0152\n",
      "Epoch 457: val_loss did not improve from 0.00030\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.0578e-04 - rmse: 0.0145 - val_loss: 2.9799e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 458/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0229e-04 - rmse: 0.0144\n",
      "Epoch 458: val_loss improved from 0.00030 to 0.00030, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.8769e-04 - rmse: 0.0139 - val_loss: 2.9537e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 459/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6248e-04 - rmse: 0.0130\n",
      "Epoch 459: val_loss improved from 0.00030 to 0.00030, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.9193e-04 - rmse: 0.0141 - val_loss: 2.9514e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 460/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7900e-04 - rmse: 0.0136\n",
      "Epoch 460: val_loss improved from 0.00030 to 0.00029, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.7279e-04 - rmse: 0.0134 - val_loss: 2.8522e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 461/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8703e-04 - rmse: 0.0096\n",
      "Epoch 461: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7641e-04 - rmse: 0.0135 - val_loss: 2.9872e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 462/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6162e-04 - rmse: 0.0129\n",
      "Epoch 462: val_loss did not improve from 0.00029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4809e-04 - rmse: 0.0124 - val_loss: 2.9439e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 463/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.4512e-04 - rmse: 0.0158\n",
      "Epoch 463: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9285e-04 - rmse: 0.0141 - val_loss: 3.0058e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 464/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6017e-04 - rmse: 0.0129\n",
      "Epoch 464: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 3.0653e-04 - rmse: 0.0146 - val_loss: 2.9310e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 465/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0955e-04 - rmse: 0.0147\n",
      "Epoch 465: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.9781e-04 - rmse: 0.0143 - val_loss: 2.9973e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 466/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1350e-04 - rmse: 0.0148\n",
      "Epoch 466: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.8611e-04 - rmse: 0.0139 - val_loss: 3.0824e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 467/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2059e-04 - rmse: 0.0150\n",
      "Epoch 467: val_loss did not improve from 0.00029\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7504e-04 - rmse: 0.0134 - val_loss: 2.8916e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 468/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7670e-04 - rmse: 0.0135\n",
      "Epoch 468: val_loss improved from 0.00029 to 0.00028, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.6470e-04 - rmse: 0.0131 - val_loss: 2.8313e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 469/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2078e-04 - rmse: 0.0113\n",
      "Epoch 469: val_loss improved from 0.00028 to 0.00028, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.3992e-04 - rmse: 0.0121 - val_loss: 2.7661e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 470/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.1617e-04 - rmse: 0.0179\n",
      "Epoch 470: val_loss improved from 0.00028 to 0.00027, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.9622e-04 - rmse: 0.0142 - val_loss: 2.7246e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 471/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5876e-04 - rmse: 0.0128\n",
      "Epoch 471: val_loss improved from 0.00027 to 0.00027, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.2919e-04 - rmse: 0.0116 - val_loss: 2.7078e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 472/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6472e-04 - rmse: 0.0131\n",
      "Epoch 472: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.7772e-04 - rmse: 0.0136 - val_loss: 2.7165e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 473/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.9033e-04 - rmse: 0.0140\n",
      "Epoch 473: val_loss improved from 0.00027 to 0.00027, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 2.5832e-04 - rmse: 0.0128 - val_loss: 2.6658e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 474/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2697e-04 - rmse: 0.0115\n",
      "Epoch 474: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5030e-04 - rmse: 0.0125 - val_loss: 2.7435e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 475/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7618e-04 - rmse: 0.0135\n",
      "Epoch 475: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.6388e-04 - rmse: 0.0130 - val_loss: 2.6895e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 476/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7727e-04 - rmse: 0.0135\n",
      "Epoch 476: val_loss did not improve from 0.00027\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4360e-04 - rmse: 0.0122 - val_loss: 2.6682e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 477/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.1068e-04 - rmse: 0.0147\n",
      "Epoch 477: val_loss improved from 0.00027 to 0.00026, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 2.6591e-04 - rmse: 0.0131 - val_loss: 2.6296e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 478/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8049e-04 - rmse: 0.0093\n",
      "Epoch 478: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5774e-04 - rmse: 0.0128 - val_loss: 2.6969e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 479/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4928e-04 - rmse: 0.0125\n",
      "Epoch 479: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5296e-04 - rmse: 0.0126 - val_loss: 2.6366e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 480/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2660e-04 - rmse: 0.0115\n",
      "Epoch 480: val_loss improved from 0.00026 to 0.00026, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.4211e-04 - rmse: 0.0122 - val_loss: 2.6171e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 481/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9308e-04 - rmse: 0.0100\n",
      "Epoch 481: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.5711e-04 - rmse: 0.0128 - val_loss: 2.9236e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 482/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5652e-04 - rmse: 0.0128\n",
      "Epoch 482: val_loss improved from 0.00026 to 0.00026, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 2.5586e-04 - rmse: 0.0127 - val_loss: 2.6102e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 483/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1939e-04 - rmse: 0.0112\n",
      "Epoch 483: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5028e-04 - rmse: 0.0125 - val_loss: 2.6111e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 484/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5942e-04 - rmse: 0.0129\n",
      "Epoch 484: val_loss improved from 0.00026 to 0.00026, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.3366e-04 - rmse: 0.0118 - val_loss: 2.5668e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 485/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0419e-04 - rmse: 0.0145\n",
      "Epoch 485: val_loss improved from 0.00026 to 0.00026, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.5379e-04 - rmse: 0.0127 - val_loss: 2.5654e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 486/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8801e-04 - rmse: 0.0097\n",
      "Epoch 486: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4802e-04 - rmse: 0.0124 - val_loss: 2.6247e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 487/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2855e-04 - rmse: 0.0116\n",
      "Epoch 487: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3457e-04 - rmse: 0.0119 - val_loss: 2.6770e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 488/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2991e-04 - rmse: 0.0117\n",
      "Epoch 488: val_loss did not improve from 0.00026\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.6770e-04 - rmse: 0.0132 - val_loss: 2.9814e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 489/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4747e-04 - rmse: 0.0124\n",
      "Epoch 489: val_loss improved from 0.00026 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.6843e-04 - rmse: 0.0132 - val_loss: 2.5242e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 490/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4888e-04 - rmse: 0.0125\n",
      "Epoch 490: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.5671e-04 - rmse: 0.0128 - val_loss: 2.5415e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 491/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0536e-04 - rmse: 0.0106\n",
      "Epoch 491: val_loss improved from 0.00025 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 2.2495e-04 - rmse: 0.0115 - val_loss: 2.5027e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 492/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0344e-04 - rmse: 0.0105\n",
      "Epoch 492: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4567e-04 - rmse: 0.0123 - val_loss: 2.6078e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 493/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3135e-04 - rmse: 0.0117\n",
      "Epoch 493: val_loss improved from 0.00025 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 2.5415e-04 - rmse: 0.0127 - val_loss: 2.4996e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 494/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4608e-04 - rmse: 0.0073\n",
      "Epoch 494: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3251e-04 - rmse: 0.0118 - val_loss: 2.6113e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 495/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2436e-04 - rmse: 0.0114\n",
      "Epoch 495: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1395e-04 - rmse: 0.0110 - val_loss: 2.5442e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 496/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6618e-04 - rmse: 0.0132\n",
      "Epoch 496: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.6745e-04 - rmse: 0.0132 - val_loss: 2.5312e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 497/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2372e-04 - rmse: 0.0114\n",
      "Epoch 497: val_loss improved from 0.00025 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 2.4685e-04 - rmse: 0.0124 - val_loss: 2.4565e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 498/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9291e-04 - rmse: 0.0100\n",
      "Epoch 498: val_loss improved from 0.00025 to 0.00025, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.1858e-04 - rmse: 0.0112 - val_loss: 2.4545e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 499/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2084e-04 - rmse: 0.0113\n",
      "Epoch 499: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3180e-04 - rmse: 0.0118 - val_loss: 2.4758e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 500/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3316e-04 - rmse: 0.0118\n",
      "Epoch 500: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4598e-04 - rmse: 0.0124 - val_loss: 2.4900e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 501/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9384e-04 - rmse: 0.0100\n",
      "Epoch 501: val_loss did not improve from 0.00025\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1659e-04 - rmse: 0.0111 - val_loss: 2.5072e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 502/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2573e-04 - rmse: 0.0153\n",
      "Epoch 502: val_loss improved from 0.00025 to 0.00024, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 2.6616e-04 - rmse: 0.0132 - val_loss: 2.4332e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 503/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7768e-04 - rmse: 0.0092\n",
      "Epoch 503: val_loss improved from 0.00024 to 0.00024, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 2.0807e-04 - rmse: 0.0107 - val_loss: 2.4310e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 504/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8942e-04 - rmse: 0.0098\n",
      "Epoch 504: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3342e-04 - rmse: 0.0119 - val_loss: 2.5289e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 505/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2840e-04 - rmse: 0.0116\n",
      "Epoch 505: val_loss improved from 0.00024 to 0.00024, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 2.3691e-04 - rmse: 0.0120 - val_loss: 2.4177e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 506/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1721e-04 - rmse: 0.0111\n",
      "Epoch 506: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2508e-04 - rmse: 0.0115 - val_loss: 2.4842e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 507/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1990e-04 - rmse: 0.0113\n",
      "Epoch 507: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3353e-04 - rmse: 0.0119 - val_loss: 2.5190e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 508/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5154e-04 - rmse: 0.0126\n",
      "Epoch 508: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.3285e-04 - rmse: 0.0118 - val_loss: 2.4801e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 509/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8433e-04 - rmse: 0.0096\n",
      "Epoch 509: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5134e-04 - rmse: 0.0126 - val_loss: 2.7662e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 510/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3066e-04 - rmse: 0.0117\n",
      "Epoch 510: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.5253e-04 - rmse: 0.0126 - val_loss: 2.4931e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 511/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3841e-04 - rmse: 0.0121\n",
      "Epoch 511: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4030e-04 - rmse: 0.0121 - val_loss: 2.5138e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 512/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3071e-04 - rmse: 0.0117\n",
      "Epoch 512: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1904e-04 - rmse: 0.0112 - val_loss: 2.5190e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 513/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6000e-04 - rmse: 0.0129\n",
      "Epoch 513: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 2.3410e-04 - rmse: 0.0119 - val_loss: 2.4881e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 514/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1563e-04 - rmse: 0.0111\n",
      "Epoch 514: val_loss improved from 0.00024 to 0.00024, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 2.2229e-04 - rmse: 0.0114 - val_loss: 2.4077e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 515/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.5842e-04 - rmse: 0.0129\n",
      "Epoch 515: val_loss improved from 0.00024 to 0.00024, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 2.4577e-04 - rmse: 0.0124 - val_loss: 2.3664e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 516/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3541e-04 - rmse: 0.0120\n",
      "Epoch 516: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4591e-04 - rmse: 0.0124 - val_loss: 2.5590e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 517/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9826e-04 - rmse: 0.0103\n",
      "Epoch 517: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.8597e-04 - rmse: 0.0139 - val_loss: 2.8137e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 518/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3060e-04 - rmse: 0.0118\n",
      "Epoch 518: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.5573e-04 - rmse: 0.0128 - val_loss: 2.5266e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 519/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1656e-04 - rmse: 0.0111\n",
      "Epoch 519: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.1728e-04 - rmse: 0.0112 - val_loss: 2.5722e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 520/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0218e-04 - rmse: 0.0145\n",
      "Epoch 520: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4354e-04 - rmse: 0.0123 - val_loss: 2.4464e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 521/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8485e-04 - rmse: 0.0139\n",
      "Epoch 521: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4767e-04 - rmse: 0.0125 - val_loss: 2.4848e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 522/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7548e-04 - rmse: 0.0091\n",
      "Epoch 522: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3404e-04 - rmse: 0.0119 - val_loss: 2.6108e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 523/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3504e-04 - rmse: 0.0119\n",
      "Epoch 523: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3223e-04 - rmse: 0.0118 - val_loss: 2.5490e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 524/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3635e-04 - rmse: 0.0120\n",
      "Epoch 524: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4628e-04 - rmse: 0.0124 - val_loss: 2.8754e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 525/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4169e-04 - rmse: 0.0122\n",
      "Epoch 525: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.4822e-04 - rmse: 0.0125 - val_loss: 2.6358e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 526/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6728e-04 - rmse: 0.0132\n",
      "Epoch 526: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4552e-04 - rmse: 0.0124 - val_loss: 2.5091e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 527/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3191e-04 - rmse: 0.0118\n",
      "Epoch 527: val_loss did not improve from 0.00024\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.1173e-04 - rmse: 0.0109 - val_loss: 2.3978e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 528/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4397e-04 - rmse: 0.0123\n",
      "Epoch 528: val_loss improved from 0.00024 to 0.00023, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.5039e-04 - rmse: 0.0126 - val_loss: 2.2766e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 529/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8847e-04 - rmse: 0.0098\n",
      "Epoch 529: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3405e-04 - rmse: 0.0119 - val_loss: 2.3684e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 530/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8247e-04 - rmse: 0.0095\n",
      "Epoch 530: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.1937e-04 - rmse: 0.0113 - val_loss: 2.4222e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 531/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4921e-04 - rmse: 0.0125\n",
      "Epoch 531: val_loss did not improve from 0.00023\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.3235e-04 - rmse: 0.0118 - val_loss: 2.4120e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 532/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1627e-04 - rmse: 0.0111\n",
      "Epoch 532: val_loss improved from 0.00023 to 0.00023, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 2.1295e-04 - rmse: 0.0110 - val_loss: 2.2517e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 533/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0760e-04 - rmse: 0.0108\n",
      "Epoch 533: val_loss improved from 0.00023 to 0.00022, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 44ms/step - loss: 2.1156e-04 - rmse: 0.0109 - val_loss: 2.1826e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 534/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3716e-04 - rmse: 0.0120\n",
      "Epoch 534: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 2.3197e-04 - rmse: 0.0118 - val_loss: 2.2267e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 535/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9434e-04 - rmse: 0.0101\n",
      "Epoch 535: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.9218e-04 - rmse: 0.0100 - val_loss: 2.2722e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 536/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2328e-04 - rmse: 0.0115\n",
      "Epoch 536: val_loss did not improve from 0.00022\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 2.1723e-04 - rmse: 0.0112 - val_loss: 2.3079e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 537/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9594e-04 - rmse: 0.0102\n",
      "Epoch 537: val_loss improved from 0.00022 to 0.00022, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.1386e-04 - rmse: 0.0110 - val_loss: 2.1583e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 538/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7452e-04 - rmse: 0.0091\n",
      "Epoch 538: val_loss improved from 0.00022 to 0.00021, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 2.1472e-04 - rmse: 0.0111 - val_loss: 2.1390e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 539/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9975e-04 - rmse: 0.0104\n",
      "Epoch 539: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.8914e-04 - rmse: 0.0099 - val_loss: 2.1865e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 540/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1990e-04 - rmse: 0.0113\n",
      "Epoch 540: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 2.1310e-04 - rmse: 0.0110 - val_loss: 2.2663e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 541/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8173e-04 - rmse: 0.0095\n",
      "Epoch 541: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9723e-04 - rmse: 0.0103 - val_loss: 2.4012e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 542/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0692e-04 - rmse: 0.0147\n",
      "Epoch 542: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.5839e-04 - rmse: 0.0129 - val_loss: 2.6043e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 543/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4248e-04 - rmse: 0.0123\n",
      "Epoch 543: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.4709e-04 - rmse: 0.0125 - val_loss: 2.4008e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 544/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0063e-04 - rmse: 0.0104\n",
      "Epoch 544: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0705e-04 - rmse: 0.0107 - val_loss: 2.2752e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 545/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8382e-04 - rmse: 0.0096\n",
      "Epoch 545: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0138e-04 - rmse: 0.0105 - val_loss: 2.1898e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 546/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6770e-04 - rmse: 0.0133\n",
      "Epoch 546: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0465e-04 - rmse: 0.0106 - val_loss: 2.2375e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 547/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1283e-04 - rmse: 0.0110\n",
      "Epoch 547: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.1752e-04 - rmse: 0.0112 - val_loss: 2.1663e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 548/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0419e-04 - rmse: 0.0106\n",
      "Epoch 548: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9494e-04 - rmse: 0.0102 - val_loss: 2.1466e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 549/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1885e-04 - rmse: 0.0113\n",
      "Epoch 549: val_loss improved from 0.00021 to 0.00021, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 2.0590e-04 - rmse: 0.0107 - val_loss: 2.0898e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 550/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9475e-04 - rmse: 0.0102\n",
      "Epoch 550: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9489e-04 - rmse: 0.0102 - val_loss: 2.0937e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 551/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1112e-04 - rmse: 0.0109\n",
      "Epoch 551: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2404e-04 - rmse: 0.0115 - val_loss: 2.1139e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 552/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6866e-04 - rmse: 0.0088\n",
      "Epoch 552: val_loss improved from 0.00021 to 0.00021, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.7986e-04 - rmse: 0.0094 - val_loss: 2.0570e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 553/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6872e-04 - rmse: 0.0088\n",
      "Epoch 553: val_loss did not improve from 0.00021\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0033e-04 - rmse: 0.0104 - val_loss: 2.0735e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 554/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4513e-04 - rmse: 0.0073\n",
      "Epoch 554: val_loss improved from 0.00021 to 0.00020, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.8118e-04 - rmse: 0.0095 - val_loss: 2.0257e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 555/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6949e-04 - rmse: 0.0088\n",
      "Epoch 555: val_loss improved from 0.00020 to 0.00020, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.9053e-04 - rmse: 0.0100 - val_loss: 2.0255e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 556/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3365e-04 - rmse: 0.0119\n",
      "Epoch 556: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.9998e-04 - rmse: 0.0104 - val_loss: 2.0330e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 557/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8121e-04 - rmse: 0.0095\n",
      "Epoch 557: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.9933e-04 - rmse: 0.0104 - val_loss: 2.0261e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 558/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7205e-04 - rmse: 0.0090\n",
      "Epoch 558: val_loss improved from 0.00020 to 0.00020, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.7454e-04 - rmse: 0.0091 - val_loss: 1.9888e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 559/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9234e-04 - rmse: 0.0101\n",
      "Epoch 559: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0588e-04 - rmse: 0.0107 - val_loss: 2.2027e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 560/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1058e-04 - rmse: 0.0109\n",
      "Epoch 560: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9852e-04 - rmse: 0.0104 - val_loss: 2.0570e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 561/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8283e-04 - rmse: 0.0096\n",
      "Epoch 561: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8720e-04 - rmse: 0.0098 - val_loss: 2.0424e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 562/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0394e-04 - rmse: 0.0106\n",
      "Epoch 562: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9064e-04 - rmse: 0.0100 - val_loss: 2.0115e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 563/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9340e-04 - rmse: 0.0101\n",
      "Epoch 563: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9995e-04 - rmse: 0.0104 - val_loss: 2.0779e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 564/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0375e-04 - rmse: 0.0106\n",
      "Epoch 564: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0336e-04 - rmse: 0.0106 - val_loss: 2.0405e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 565/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3505e-04 - rmse: 0.0120\n",
      "Epoch 565: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0081e-04 - rmse: 0.0105 - val_loss: 2.0991e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 566/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6649e-04 - rmse: 0.0087\n",
      "Epoch 566: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9595e-04 - rmse: 0.0102 - val_loss: 2.1292e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 567/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9167e-04 - rmse: 0.0100\n",
      "Epoch 567: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.9466e-04 - rmse: 0.0102 - val_loss: 2.0099e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 568/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7672e-04 - rmse: 0.0093\n",
      "Epoch 568: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9820e-04 - rmse: 0.0104 - val_loss: 2.0068e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 569/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8398e-04 - rmse: 0.0097\n",
      "Epoch 569: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0325e-04 - rmse: 0.0106 - val_loss: 2.4260e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 570/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6536e-04 - rmse: 0.0132\n",
      "Epoch 570: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.3200e-04 - rmse: 0.0119 - val_loss: 2.6648e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 571/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.6097e-04 - rmse: 0.0130\n",
      "Epoch 571: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.6272e-04 - rmse: 0.0131 - val_loss: 2.5632e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 572/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3148e-04 - rmse: 0.0119\n",
      "Epoch 572: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2257e-04 - rmse: 0.0115 - val_loss: 2.2690e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 573/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8680e-04 - rmse: 0.0098\n",
      "Epoch 573: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.2278e-04 - rmse: 0.0115 - val_loss: 2.0659e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 574/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9398e-04 - rmse: 0.0102\n",
      "Epoch 574: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0740e-04 - rmse: 0.0108 - val_loss: 2.4514e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 575/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3982e-04 - rmse: 0.0122\n",
      "Epoch 575: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.4053e-04 - rmse: 0.0122 - val_loss: 2.2616e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 576/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9012e-04 - rmse: 0.0100\n",
      "Epoch 576: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.8375e-04 - rmse: 0.0097 - val_loss: 2.5127e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 577/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1206e-04 - rmse: 0.0110\n",
      "Epoch 577: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.0673e-04 - rmse: 0.0108 - val_loss: 2.1288e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 578/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7704e-04 - rmse: 0.0093\n",
      "Epoch 578: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7914e-04 - rmse: 0.0094 - val_loss: 2.0118e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 579/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.3260e-04 - rmse: 0.0119\n",
      "Epoch 579: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 2.0230e-04 - rmse: 0.0106 - val_loss: 1.9937e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 580/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9411e-04 - rmse: 0.0102\n",
      "Epoch 580: val_loss did not improve from 0.00020\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9246e-04 - rmse: 0.0101 - val_loss: 2.1160e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 581/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4024e-04 - rmse: 0.0122\n",
      "Epoch 581: val_loss improved from 0.00020 to 0.00019, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 2.1435e-04 - rmse: 0.0111 - val_loss: 1.9384e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 582/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4608e-04 - rmse: 0.0075\n",
      "Epoch 582: val_loss improved from 0.00019 to 0.00019, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.9976e-04 - rmse: 0.0105 - val_loss: 1.9343e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 583/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7190e-04 - rmse: 0.0090\n",
      "Epoch 583: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9609e-04 - rmse: 0.0103 - val_loss: 2.1745e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 584/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1083e-04 - rmse: 0.0110\n",
      "Epoch 584: val_loss did not improve from 0.00019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0882e-04 - rmse: 0.0109 - val_loss: 2.3013e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 585/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1450e-04 - rmse: 0.0111\n",
      "Epoch 585: val_loss improved from 0.00019 to 0.00019, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 2.0409e-04 - rmse: 0.0107 - val_loss: 1.8783e-04 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 586/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8136e-04 - rmse: 0.0095\n",
      "Epoch 586: val_loss improved from 0.00019 to 0.00019, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 1.9021e-04 - rmse: 0.0100 - val_loss: 1.8715e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 587/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6179e-04 - rmse: 0.0085\n",
      "Epoch 587: val_loss improved from 0.00019 to 0.00019, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.8168e-04 - rmse: 0.0096 - val_loss: 1.8512e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 588/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8365e-04 - rmse: 0.0097\n",
      "Epoch 588: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7874e-04 - rmse: 0.0094 - val_loss: 1.8563e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 589/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2856e-04 - rmse: 0.0118\n",
      "Epoch 589: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9699e-04 - rmse: 0.0103 - val_loss: 1.8799e-04 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 590/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6570e-04 - rmse: 0.0087\n",
      "Epoch 590: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.9619e-04 - rmse: 0.0103 - val_loss: 2.0715e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 591/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0815e-04 - rmse: 0.0109\n",
      "Epoch 591: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9886e-04 - rmse: 0.0104 - val_loss: 2.1263e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 592/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8765e-04 - rmse: 0.0099\n",
      "Epoch 592: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0549e-04 - rmse: 0.0107 - val_loss: 2.2120e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 593/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0122e-04 - rmse: 0.0105\n",
      "Epoch 593: val_loss did not improve from 0.00019\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.0111e-04 - rmse: 0.0105 - val_loss: 1.9193e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 594/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0756e-04 - rmse: 0.0108\n",
      "Epoch 594: val_loss improved from 0.00019 to 0.00018, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 2.0301e-04 - rmse: 0.0106 - val_loss: 1.8362e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 595/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6089e-04 - rmse: 0.0084\n",
      "Epoch 595: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5345e-04 - rmse: 0.0080 - val_loss: 1.8398e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 596/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8594e-04 - rmse: 0.0098\n",
      "Epoch 596: val_loss improved from 0.00018 to 0.00018, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.9066e-04 - rmse: 0.0100 - val_loss: 1.8282e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 597/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6840e-04 - rmse: 0.0089\n",
      "Epoch 597: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7497e-04 - rmse: 0.0092 - val_loss: 1.9797e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 598/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9307e-04 - rmse: 0.0102\n",
      "Epoch 598: val_loss improved from 0.00018 to 0.00018, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.8370e-04 - rmse: 0.0097 - val_loss: 1.7978e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 599/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7438e-04 - rmse: 0.0092\n",
      "Epoch 599: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6194e-04 - rmse: 0.0085 - val_loss: 2.0341e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 600/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7157e-04 - rmse: 0.0090\n",
      "Epoch 600: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9950e-04 - rmse: 0.0105 - val_loss: 2.2599e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 601/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9771e-04 - rmse: 0.0104\n",
      "Epoch 601: val_loss improved from 0.00018 to 0.00018, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 2.1355e-04 - rmse: 0.0111 - val_loss: 1.7792e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 602/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5708e-04 - rmse: 0.0082\n",
      "Epoch 602: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.8024e-04 - rmse: 0.0095 - val_loss: 1.8500e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 603/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9344e-04 - rmse: 0.0102\n",
      "Epoch 603: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7895e-04 - rmse: 0.0094 - val_loss: 1.7800e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 604/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9549e-04 - rmse: 0.0103\n",
      "Epoch 604: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8086e-04 - rmse: 0.0095 - val_loss: 1.8999e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 605/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8473e-04 - rmse: 0.0097\n",
      "Epoch 605: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8402e-04 - rmse: 0.0097 - val_loss: 1.8408e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 606/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3548e-04 - rmse: 0.0068\n",
      "Epoch 606: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6578e-04 - rmse: 0.0087 - val_loss: 1.7861e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 607/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6522e-04 - rmse: 0.0087\n",
      "Epoch 607: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6670e-04 - rmse: 0.0088 - val_loss: 1.7940e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 608/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0473e-04 - rmse: 0.0107\n",
      "Epoch 608: val_loss did not improve from 0.00018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7913e-04 - rmse: 0.0095 - val_loss: 1.8826e-04 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 609/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0236e-04 - rmse: 0.0106\n",
      "Epoch 609: val_loss did not improve from 0.00018\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8094e-04 - rmse: 0.0096 - val_loss: 1.7903e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 610/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4727e-04 - rmse: 0.0076\n",
      "Epoch 610: val_loss improved from 0.00018 to 0.00017, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.5859e-04 - rmse: 0.0083 - val_loss: 1.7173e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 611/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7065e-04 - rmse: 0.0090\n",
      "Epoch 611: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9387e-04 - rmse: 0.0102 - val_loss: 1.8230e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 612/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7756e-04 - rmse: 0.0094\n",
      "Epoch 612: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7004e-04 - rmse: 0.0090 - val_loss: 1.8760e-04 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 613/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7306e-04 - rmse: 0.0091\n",
      "Epoch 613: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8003e-04 - rmse: 0.0095 - val_loss: 1.8892e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 614/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5541e-04 - rmse: 0.0081\n",
      "Epoch 614: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5856e-04 - rmse: 0.0083 - val_loss: 1.8429e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 615/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9005e-04 - rmse: 0.0100\n",
      "Epoch 615: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.8612e-04 - rmse: 0.0098 - val_loss: 1.8759e-04 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 616/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8560e-04 - rmse: 0.0098\n",
      "Epoch 616: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6790e-04 - rmse: 0.0089 - val_loss: 1.8118e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 617/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6478e-04 - rmse: 0.0087\n",
      "Epoch 617: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7714e-04 - rmse: 0.0094 - val_loss: 2.0842e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 618/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9823e-04 - rmse: 0.0104\n",
      "Epoch 618: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6743e-04 - rmse: 0.0088 - val_loss: 2.0941e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 619/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7410e-04 - rmse: 0.0136\n",
      "Epoch 619: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2126e-04 - rmse: 0.0115 - val_loss: 1.9266e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 620/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8395e-04 - rmse: 0.0097\n",
      "Epoch 620: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9309e-04 - rmse: 0.0102 - val_loss: 1.9829e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 621/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7693e-04 - rmse: 0.0094\n",
      "Epoch 621: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7557e-04 - rmse: 0.0093 - val_loss: 2.0003e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 622/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.1015e-04 - rmse: 0.0110\n",
      "Epoch 622: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7998e-04 - rmse: 0.0095 - val_loss: 1.9471e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 623/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9180e-04 - rmse: 0.0101\n",
      "Epoch 623: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6995e-04 - rmse: 0.0090 - val_loss: 1.8414e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 624/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7960e-04 - rmse: 0.0095\n",
      "Epoch 624: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7609e-04 - rmse: 0.0093 - val_loss: 1.7887e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 625/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6070e-04 - rmse: 0.0085\n",
      "Epoch 625: val_loss improved from 0.00017 to 0.00017, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.6797e-04 - rmse: 0.0089 - val_loss: 1.6999e-04 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 626/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8338e-04 - rmse: 0.0097\n",
      "Epoch 626: val_loss improved from 0.00017 to 0.00017, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.6359e-04 - rmse: 0.0086 - val_loss: 1.6913e-04 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 627/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8466e-04 - rmse: 0.0098\n",
      "Epoch 627: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7024e-04 - rmse: 0.0090 - val_loss: 1.7742e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 628/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6507e-04 - rmse: 0.0087\n",
      "Epoch 628: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6264e-04 - rmse: 0.0086 - val_loss: 2.0785e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 629/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8319e-04 - rmse: 0.0097\n",
      "Epoch 629: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9093e-04 - rmse: 0.0101 - val_loss: 2.2227e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 630/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0525e-04 - rmse: 0.0108\n",
      "Epoch 630: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 2.2583e-04 - rmse: 0.0117 - val_loss: 1.7555e-04 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 631/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4526e-04 - rmse: 0.0075\n",
      "Epoch 631: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7669e-04 - rmse: 0.0094 - val_loss: 1.7745e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 632/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6724e-04 - rmse: 0.0089\n",
      "Epoch 632: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8797e-04 - rmse: 0.0100 - val_loss: 2.0320e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 633/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2126e-04 - rmse: 0.0115\n",
      "Epoch 633: val_loss did not improve from 0.00017\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.8736e-04 - rmse: 0.0099 - val_loss: 1.9386e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 634/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9752e-04 - rmse: 0.0104\n",
      "Epoch 634: val_loss improved from 0.00017 to 0.00016, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 44ms/step - loss: 1.9351e-04 - rmse: 0.0102 - val_loss: 1.6298e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 635/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6238e-04 - rmse: 0.0086\n",
      "Epoch 635: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8000e-04 - rmse: 0.0096 - val_loss: 1.9999e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 636/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7776e-04 - rmse: 0.0094\n",
      "Epoch 636: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8123e-04 - rmse: 0.0096 - val_loss: 1.7407e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 637/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0138e-04 - rmse: 0.0106\n",
      "Epoch 637: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.7344e-04 - rmse: 0.0092 - val_loss: 1.6358e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 638/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6772e-04 - rmse: 0.0089\n",
      "Epoch 638: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6372e-04 - rmse: 0.0087 - val_loss: 1.7743e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 639/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8694e-04 - rmse: 0.0099\n",
      "Epoch 639: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7855e-04 - rmse: 0.0095 - val_loss: 1.6891e-04 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 640/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5705e-04 - rmse: 0.0083\n",
      "Epoch 640: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6093e-04 - rmse: 0.0085 - val_loss: 1.6659e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 641/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6100e-04 - rmse: 0.0085\n",
      "Epoch 641: val_loss improved from 0.00016 to 0.00016, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.6215e-04 - rmse: 0.0086 - val_loss: 1.5922e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 642/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3177e-04 - rmse: 0.0066\n",
      "Epoch 642: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4896e-04 - rmse: 0.0078 - val_loss: 1.6032e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 643/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5803e-04 - rmse: 0.0083\n",
      "Epoch 643: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6818e-04 - rmse: 0.0089 - val_loss: 1.7925e-04 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 644/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6449e-04 - rmse: 0.0087\n",
      "Epoch 644: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6331e-04 - rmse: 0.0087 - val_loss: 1.6145e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 645/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3603e-04 - rmse: 0.0069\n",
      "Epoch 645: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5675e-04 - rmse: 0.0083 - val_loss: 1.6034e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 646/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8551e-04 - rmse: 0.0099\n",
      "Epoch 646: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6302e-04 - rmse: 0.0086 - val_loss: 1.6408e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 647/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6343e-04 - rmse: 0.0087\n",
      "Epoch 647: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7009e-04 - rmse: 0.0090 - val_loss: 1.8676e-04 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 648/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5794e-04 - rmse: 0.0083\n",
      "Epoch 648: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6612e-04 - rmse: 0.0088 - val_loss: 1.6283e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 649/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7526e-04 - rmse: 0.0093\n",
      "Epoch 649: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.6962e-04 - rmse: 0.0090 - val_loss: 1.6483e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 650/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5701e-04 - rmse: 0.0083\n",
      "Epoch 650: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7954e-04 - rmse: 0.0096 - val_loss: 1.9167e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 651/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8558e-04 - rmse: 0.0099\n",
      "Epoch 651: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.7579e-04 - rmse: 0.0094 - val_loss: 1.6498e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 652/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6540e-04 - rmse: 0.0088\n",
      "Epoch 652: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.8056e-04 - rmse: 0.0096 - val_loss: 1.6216e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 653/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7921e-04 - rmse: 0.0095\n",
      "Epoch 653: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5869e-04 - rmse: 0.0084 - val_loss: 1.6289e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 654/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7047e-04 - rmse: 0.0091\n",
      "Epoch 654: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6561e-04 - rmse: 0.0088 - val_loss: 1.6715e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 655/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5634e-04 - rmse: 0.0083\n",
      "Epoch 655: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.5256e-04 - rmse: 0.0080 - val_loss: 1.6540e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 656/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7834e-04 - rmse: 0.0095\n",
      "Epoch 656: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7374e-04 - rmse: 0.0093 - val_loss: 1.6591e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 657/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6394e-04 - rmse: 0.0087\n",
      "Epoch 657: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7154e-04 - rmse: 0.0091 - val_loss: 1.9529e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 658/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8174e-04 - rmse: 0.0097\n",
      "Epoch 658: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7282e-04 - rmse: 0.0092 - val_loss: 1.7949e-04 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 659/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8009e-04 - rmse: 0.0096\n",
      "Epoch 659: val_loss improved from 0.00016 to 0.00016, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 1.8513e-04 - rmse: 0.0099 - val_loss: 1.5501e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 660/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5228e-04 - rmse: 0.0080\n",
      "Epoch 660: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6542e-04 - rmse: 0.0088 - val_loss: 1.9092e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 661/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7409e-04 - rmse: 0.0093\n",
      "Epoch 661: val_loss did not improve from 0.00016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7897e-04 - rmse: 0.0095 - val_loss: 1.8388e-04 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 662/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5875e-04 - rmse: 0.0084\n",
      "Epoch 662: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5766e-04 - rmse: 0.0084 - val_loss: 1.7073e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 663/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5345e-04 - rmse: 0.0081\n",
      "Epoch 663: val_loss did not improve from 0.00016\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6898e-04 - rmse: 0.0090 - val_loss: 1.5523e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 664/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4075e-04 - rmse: 0.0073\n",
      "Epoch 664: val_loss improved from 0.00016 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 1.4832e-04 - rmse: 0.0078 - val_loss: 1.5336e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 665/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6590e-04 - rmse: 0.0088\n",
      "Epoch 665: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5158e-04 - rmse: 0.0080 - val_loss: 1.5633e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 666/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6538e-04 - rmse: 0.0088\n",
      "Epoch 666: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5636e-04 - rmse: 0.0083 - val_loss: 1.6326e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 667/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6929e-04 - rmse: 0.0090\n",
      "Epoch 667: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5341e-04 - rmse: 0.0081 - val_loss: 1.6336e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 668/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6558e-04 - rmse: 0.0088\n",
      "Epoch 668: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5511e-04 - rmse: 0.0082 - val_loss: 1.6292e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 669/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6676e-04 - rmse: 0.0089\n",
      "Epoch 669: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5895e-04 - rmse: 0.0084 - val_loss: 1.5607e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 670/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5275e-04 - rmse: 0.0081\n",
      "Epoch 670: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4744e-04 - rmse: 0.0077 - val_loss: 1.5644e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 671/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7074e-04 - rmse: 0.0091\n",
      "Epoch 671: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6411e-04 - rmse: 0.0087 - val_loss: 1.7056e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 672/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7634e-04 - rmse: 0.0094\n",
      "Epoch 672: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.6145e-04 - rmse: 0.0086 - val_loss: 1.5359e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 673/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5946e-04 - rmse: 0.0085\n",
      "Epoch 673: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.5501e-04 - rmse: 0.0082 - val_loss: 1.5269e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 674/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3912e-04 - rmse: 0.0072\n",
      "Epoch 674: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4655e-04 - rmse: 0.0077 - val_loss: 1.5442e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 675/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5175e-04 - rmse: 0.0080\n",
      "Epoch 675: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6175e-04 - rmse: 0.0086 - val_loss: 1.6248e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 676/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5898e-04 - rmse: 0.0085\n",
      "Epoch 676: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7122e-04 - rmse: 0.0092 - val_loss: 1.8199e-04 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 677/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7049e-04 - rmse: 0.0091\n",
      "Epoch 677: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6046e-04 - rmse: 0.0085 - val_loss: 1.7006e-04 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 678/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.8839e-04 - rmse: 0.0101\n",
      "Epoch 678: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.7574e-04 - rmse: 0.0094 - val_loss: 1.4789e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 679/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4870e-04 - rmse: 0.0078\n",
      "Epoch 679: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.4480e-04 - rmse: 0.0076 - val_loss: 1.6626e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 680/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7291e-04 - rmse: 0.0093\n",
      "Epoch 680: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6251e-04 - rmse: 0.0087 - val_loss: 1.6073e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 681/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6088e-04 - rmse: 0.0086\n",
      "Epoch 681: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5764e-04 - rmse: 0.0084 - val_loss: 1.5038e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 682/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6319e-04 - rmse: 0.0087\n",
      "Epoch 682: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.5666e-04 - rmse: 0.0083 - val_loss: 1.5975e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 683/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5052e-04 - rmse: 0.0080\n",
      "Epoch 683: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5318e-04 - rmse: 0.0081 - val_loss: 1.5255e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 684/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4369e-04 - rmse: 0.0075\n",
      "Epoch 684: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 1.5378e-04 - rmse: 0.0082 - val_loss: 1.4674e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 685/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3989e-04 - rmse: 0.0073\n",
      "Epoch 685: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4346e-04 - rmse: 0.0075 - val_loss: 1.5014e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 686/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7180e-04 - rmse: 0.0092\n",
      "Epoch 686: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4810e-04 - rmse: 0.0078 - val_loss: 1.4946e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 687/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5944e-04 - rmse: 0.0085\n",
      "Epoch 687: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5274e-04 - rmse: 0.0081 - val_loss: 1.5024e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 688/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5530e-04 - rmse: 0.0083\n",
      "Epoch 688: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.5005e-04 - rmse: 0.0079 - val_loss: 1.4653e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 689/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4970e-04 - rmse: 0.0079\n",
      "Epoch 689: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4846e-04 - rmse: 0.0078 - val_loss: 1.4808e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 690/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4926e-04 - rmse: 0.0079\n",
      "Epoch 690: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4940e-04 - rmse: 0.0079 - val_loss: 1.5297e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 691/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4635e-04 - rmse: 0.0077\n",
      "Epoch 691: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4604e-04 - rmse: 0.0077 - val_loss: 1.4998e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 692/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4637e-04 - rmse: 0.0077\n",
      "Epoch 692: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5450e-04 - rmse: 0.0082 - val_loss: 1.5761e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 693/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6269e-04 - rmse: 0.0087\n",
      "Epoch 693: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5333e-04 - rmse: 0.0082 - val_loss: 1.4683e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 694/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5750e-04 - rmse: 0.0084\n",
      "Epoch 694: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.5505e-04 - rmse: 0.0083 - val_loss: 1.4902e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 695/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4343e-04 - rmse: 0.0075\n",
      "Epoch 695: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4505e-04 - rmse: 0.0076 - val_loss: 1.6347e-04 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 696/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7188e-04 - rmse: 0.0092\n",
      "Epoch 696: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.6736e-04 - rmse: 0.0090 - val_loss: 1.5240e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 697/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3422e-04 - rmse: 0.0069\n",
      "Epoch 697: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5647e-04 - rmse: 0.0083 - val_loss: 1.7075e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 698/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5589e-04 - rmse: 0.0083\n",
      "Epoch 698: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6540e-04 - rmse: 0.0089 - val_loss: 1.6551e-04 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 699/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6821e-04 - rmse: 0.0090\n",
      "Epoch 699: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.9672e-04 - rmse: 0.0105 - val_loss: 1.6190e-04 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 700/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6205e-04 - rmse: 0.0087\n",
      "Epoch 700: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.8090e-04 - rmse: 0.0097 - val_loss: 2.1166e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 701/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.0177e-04 - rmse: 0.0107\n",
      "Epoch 701: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.7900e-04 - rmse: 0.0096 - val_loss: 1.7490e-04 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 702/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7343e-04 - rmse: 0.0093\n",
      "Epoch 702: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6421e-04 - rmse: 0.0088 - val_loss: 1.5850e-04 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 703/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5418e-04 - rmse: 0.0082\n",
      "Epoch 703: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5988e-04 - rmse: 0.0086 - val_loss: 1.5146e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 704/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3733e-04 - rmse: 0.0071\n",
      "Epoch 704: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5272e-04 - rmse: 0.0081 - val_loss: 1.5312e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 705/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5229e-04 - rmse: 0.0081\n",
      "Epoch 705: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5957e-04 - rmse: 0.0085 - val_loss: 1.4865e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 706/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3493e-04 - rmse: 0.0070\n",
      "Epoch 706: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4719e-04 - rmse: 0.0078 - val_loss: 1.4910e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 707/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5995e-04 - rmse: 0.0086\n",
      "Epoch 707: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4891e-04 - rmse: 0.0079 - val_loss: 1.5235e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 708/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4580e-04 - rmse: 0.0077\n",
      "Epoch 708: val_loss did not improve from 0.00015\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4862e-04 - rmse: 0.0079 - val_loss: 1.4697e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 709/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4432e-04 - rmse: 0.0076\n",
      "Epoch 709: val_loss improved from 0.00015 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.4599e-04 - rmse: 0.0077 - val_loss: 1.4145e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 710/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4632e-04 - rmse: 0.0077\n",
      "Epoch 710: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4782e-04 - rmse: 0.0078 - val_loss: 1.4392e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 711/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4616e-04 - rmse: 0.0077\n",
      "Epoch 711: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.4988e-04 - rmse: 0.0080 - val_loss: 1.4068e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 712/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4602e-04 - rmse: 0.0077\n",
      "Epoch 712: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4366e-04 - rmse: 0.0076 - val_loss: 1.4184e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 713/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4122e-04 - rmse: 0.0074\n",
      "Epoch 713: val_loss did not improve from 0.00014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4302e-04 - rmse: 0.0075 - val_loss: 1.5108e-04 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 714/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7653e-04 - rmse: 0.0095\n",
      "Epoch 714: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6097e-04 - rmse: 0.0086 - val_loss: 1.4235e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 715/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3001e-04 - rmse: 0.0066\n",
      "Epoch 715: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4414e-04 - rmse: 0.0076 - val_loss: 1.5348e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 716/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4538e-04 - rmse: 0.0077\n",
      "Epoch 716: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4703e-04 - rmse: 0.0078 - val_loss: 1.5439e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 717/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4743e-04 - rmse: 0.0078\n",
      "Epoch 717: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5908e-04 - rmse: 0.0085 - val_loss: 1.5586e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 718/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5549e-04 - rmse: 0.0083\n",
      "Epoch 718: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.6087e-04 - rmse: 0.0086 - val_loss: 1.7124e-04 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 719/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6509e-04 - rmse: 0.0089\n",
      "Epoch 719: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5731e-04 - rmse: 0.0084 - val_loss: 1.5355e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 720/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5792e-04 - rmse: 0.0085\n",
      "Epoch 720: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5384e-04 - rmse: 0.0082 - val_loss: 1.4692e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 721/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5082e-04 - rmse: 0.0080\n",
      "Epoch 721: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5076e-04 - rmse: 0.0080 - val_loss: 1.5666e-04 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 722/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5457e-04 - rmse: 0.0083\n",
      "Epoch 722: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5374e-04 - rmse: 0.0082 - val_loss: 1.5362e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 723/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5256e-04 - rmse: 0.0082\n",
      "Epoch 723: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 1.5039e-04 - rmse: 0.0080 - val_loss: 1.3897e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 724/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4122e-04 - rmse: 0.0074\n",
      "Epoch 724: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4332e-04 - rmse: 0.0076 - val_loss: 1.4989e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 725/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4477e-04 - rmse: 0.0077\n",
      "Epoch 725: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4718e-04 - rmse: 0.0078 - val_loss: 1.4807e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 726/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6019e-04 - rmse: 0.0086\n",
      "Epoch 726: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5631e-04 - rmse: 0.0084 - val_loss: 1.4218e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 727/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2559e-04 - rmse: 0.0063\n",
      "Epoch 727: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3748e-04 - rmse: 0.0072 - val_loss: 1.4035e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 728/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4789e-04 - rmse: 0.0079\n",
      "Epoch 728: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4515e-04 - rmse: 0.0077 - val_loss: 1.4464e-04 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 729/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4445e-04 - rmse: 0.0077\n",
      "Epoch 729: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4292e-04 - rmse: 0.0076 - val_loss: 1.5017e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 730/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5813e-04 - rmse: 0.0085\n",
      "Epoch 730: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4903e-04 - rmse: 0.0080 - val_loss: 1.3912e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 731/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3507e-04 - rmse: 0.0070\n",
      "Epoch 731: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 1.3667e-04 - rmse: 0.0071 - val_loss: 1.3709e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 732/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4271e-04 - rmse: 0.0076\n",
      "Epoch 732: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4151e-04 - rmse: 0.0075 - val_loss: 1.3834e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 733/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4133e-04 - rmse: 0.0075\n",
      "Epoch 733: val_loss did not improve from 0.00014\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4189e-04 - rmse: 0.0075 - val_loss: 1.3964e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 734/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2546e-04 - rmse: 0.0063\n",
      "Epoch 734: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.4251e-04 - rmse: 0.0075 - val_loss: 1.3529e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 735/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2726e-04 - rmse: 0.0065\n",
      "Epoch 735: val_loss improved from 0.00014 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 1.3319e-04 - rmse: 0.0069 - val_loss: 1.3460e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 736/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3509e-04 - rmse: 0.0070\n",
      "Epoch 736: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3760e-04 - rmse: 0.0072 - val_loss: 1.3570e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 737/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4194e-04 - rmse: 0.0075\n",
      "Epoch 737: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4097e-04 - rmse: 0.0074 - val_loss: 1.3538e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 738/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1925e-04 - rmse: 0.0058\n",
      "Epoch 738: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.4032e-04 - rmse: 0.0074 - val_loss: 1.5966e-04 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 739/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6198e-04 - rmse: 0.0087\n",
      "Epoch 739: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5552e-04 - rmse: 0.0084 - val_loss: 1.4055e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 740/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3979e-04 - rmse: 0.0074\n",
      "Epoch 740: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4136e-04 - rmse: 0.0075 - val_loss: 1.3570e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 741/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3735e-04 - rmse: 0.0072\n",
      "Epoch 741: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3922e-04 - rmse: 0.0073 - val_loss: 1.4088e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 742/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3904e-04 - rmse: 0.0073\n",
      "Epoch 742: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 1.4396e-04 - rmse: 0.0077 - val_loss: 1.3364e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 743/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2599e-04 - rmse: 0.0064\n",
      "Epoch 743: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3258e-04 - rmse: 0.0069 - val_loss: 1.4637e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 744/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6541e-04 - rmse: 0.0089\n",
      "Epoch 744: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5242e-04 - rmse: 0.0082 - val_loss: 1.3796e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 745/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4529e-04 - rmse: 0.0077\n",
      "Epoch 745: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4391e-04 - rmse: 0.0077 - val_loss: 1.3836e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 746/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4082e-04 - rmse: 0.0075\n",
      "Epoch 746: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4287e-04 - rmse: 0.0076 - val_loss: 1.3862e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 747/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4232e-04 - rmse: 0.0076\n",
      "Epoch 747: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3763e-04 - rmse: 0.0072 - val_loss: 1.3725e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 748/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4446e-04 - rmse: 0.0077\n",
      "Epoch 748: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4160e-04 - rmse: 0.0075 - val_loss: 1.3956e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 749/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4234e-04 - rmse: 0.0076\n",
      "Epoch 749: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3735e-04 - rmse: 0.0072 - val_loss: 1.3510e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 750/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.5982e-04 - rmse: 0.0086\n",
      "Epoch 750: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3995e-04 - rmse: 0.0074 - val_loss: 1.3459e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 751/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3614e-04 - rmse: 0.0071\n",
      "Epoch 751: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4528e-04 - rmse: 0.0078 - val_loss: 1.3993e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 752/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3560e-04 - rmse: 0.0071\n",
      "Epoch 752: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3683e-04 - rmse: 0.0072 - val_loss: 1.3468e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 753/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4135e-04 - rmse: 0.0075\n",
      "Epoch 753: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3821e-04 - rmse: 0.0073 - val_loss: 1.3518e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 754/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3963e-04 - rmse: 0.0074\n",
      "Epoch 754: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3718e-04 - rmse: 0.0072 - val_loss: 1.3799e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 755/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4203e-04 - rmse: 0.0076\n",
      "Epoch 755: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4301e-04 - rmse: 0.0076 - val_loss: 1.3674e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 756/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3672e-04 - rmse: 0.0072\n",
      "Epoch 756: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3361e-04 - rmse: 0.0070 - val_loss: 1.3662e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 757/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3541e-04 - rmse: 0.0071\n",
      "Epoch 757: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3967e-04 - rmse: 0.0074 - val_loss: 1.3793e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 758/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3730e-04 - rmse: 0.0072\n",
      "Epoch 758: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.5160e-04 - rmse: 0.0082 - val_loss: 1.3808e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 759/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3531e-04 - rmse: 0.0071\n",
      "Epoch 759: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3583e-04 - rmse: 0.0071 - val_loss: 1.4691e-04 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 760/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4784e-04 - rmse: 0.0079\n",
      "Epoch 760: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4643e-04 - rmse: 0.0078 - val_loss: 1.3698e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 761/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3190e-04 - rmse: 0.0069\n",
      "Epoch 761: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3948e-04 - rmse: 0.0074 - val_loss: 1.3544e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 762/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3433e-04 - rmse: 0.0070\n",
      "Epoch 762: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4315e-04 - rmse: 0.0076 - val_loss: 1.4138e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 763/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4972e-04 - rmse: 0.0081\n",
      "Epoch 763: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.3804e-04 - rmse: 0.0073 - val_loss: 1.3129e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 764/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4209e-04 - rmse: 0.0076\n",
      "Epoch 764: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3753e-04 - rmse: 0.0073 - val_loss: 1.3444e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 765/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3752e-04 - rmse: 0.0073\n",
      "Epoch 765: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4005e-04 - rmse: 0.0074 - val_loss: 1.3676e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 766/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3453e-04 - rmse: 0.0071\n",
      "Epoch 766: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3366e-04 - rmse: 0.0070 - val_loss: 1.3525e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 767/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4671e-04 - rmse: 0.0079\n",
      "Epoch 767: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4276e-04 - rmse: 0.0076 - val_loss: 1.3266e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 768/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3729e-04 - rmse: 0.0073\n",
      "Epoch 768: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.3987e-04 - rmse: 0.0074 - val_loss: 1.3116e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 769/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2833e-04 - rmse: 0.0066\n",
      "Epoch 769: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3880e-04 - rmse: 0.0074 - val_loss: 1.3468e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 770/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3646e-04 - rmse: 0.0072\n",
      "Epoch 770: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4284e-04 - rmse: 0.0076 - val_loss: 1.4055e-04 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 771/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3824e-04 - rmse: 0.0073\n",
      "Epoch 771: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4374e-04 - rmse: 0.0077 - val_loss: 1.3917e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 772/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3326e-04 - rmse: 0.0070\n",
      "Epoch 772: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3800e-04 - rmse: 0.0073 - val_loss: 1.3968e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 773/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4359e-04 - rmse: 0.0077\n",
      "Epoch 773: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4612e-04 - rmse: 0.0079 - val_loss: 1.4765e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 774/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4242e-04 - rmse: 0.0076\n",
      "Epoch 774: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4286e-04 - rmse: 0.0076 - val_loss: 1.4279e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 775/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3685e-04 - rmse: 0.0072\n",
      "Epoch 775: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4298e-04 - rmse: 0.0077 - val_loss: 1.3512e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 776/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4161e-04 - rmse: 0.0076\n",
      "Epoch 776: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3758e-04 - rmse: 0.0073 - val_loss: 1.5256e-04 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 777/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4768e-04 - rmse: 0.0080\n",
      "Epoch 777: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4352e-04 - rmse: 0.0077 - val_loss: 1.4213e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 778/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4947e-04 - rmse: 0.0081\n",
      "Epoch 778: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4825e-04 - rmse: 0.0080 - val_loss: 1.3493e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 779/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3791e-04 - rmse: 0.0073\n",
      "Epoch 779: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4411e-04 - rmse: 0.0077 - val_loss: 1.3580e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 780/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4153e-04 - rmse: 0.0076\n",
      "Epoch 780: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4108e-04 - rmse: 0.0075 - val_loss: 1.3725e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 781/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3881e-04 - rmse: 0.0074\n",
      "Epoch 781: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4077e-04 - rmse: 0.0075 - val_loss: 1.4134e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 782/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4351e-04 - rmse: 0.0077\n",
      "Epoch 782: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.4024e-04 - rmse: 0.0075 - val_loss: 1.2715e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 783/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2393e-04 - rmse: 0.0063\n",
      "Epoch 783: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2741e-04 - rmse: 0.0066 - val_loss: 1.3199e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 784/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4029e-04 - rmse: 0.0075\n",
      "Epoch 784: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3638e-04 - rmse: 0.0072 - val_loss: 1.3049e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 785/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3398e-04 - rmse: 0.0071\n",
      "Epoch 785: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3094e-04 - rmse: 0.0068 - val_loss: 1.2826e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 786/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3780e-04 - rmse: 0.0073\n",
      "Epoch 786: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3943e-04 - rmse: 0.0074 - val_loss: 1.2959e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 787/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2293e-04 - rmse: 0.0062\n",
      "Epoch 787: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3782e-04 - rmse: 0.0073 - val_loss: 1.2949e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 788/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2165e-04 - rmse: 0.0061\n",
      "Epoch 788: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3195e-04 - rmse: 0.0069 - val_loss: 1.3732e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 789/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4514e-04 - rmse: 0.0078\n",
      "Epoch 789: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3751e-04 - rmse: 0.0073 - val_loss: 1.3213e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 790/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2813e-04 - rmse: 0.0066\n",
      "Epoch 790: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3544e-04 - rmse: 0.0072 - val_loss: 1.2735e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 791/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2563e-04 - rmse: 0.0065\n",
      "Epoch 791: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2803e-04 - rmse: 0.0066 - val_loss: 1.2873e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 792/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3227e-04 - rmse: 0.0070\n",
      "Epoch 792: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 46ms/step - loss: 1.3609e-04 - rmse: 0.0072 - val_loss: 1.2605e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 793/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3135e-04 - rmse: 0.0069\n",
      "Epoch 793: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3558e-04 - rmse: 0.0072 - val_loss: 1.3195e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 794/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2703e-04 - rmse: 0.0066\n",
      "Epoch 794: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3568e-04 - rmse: 0.0072 - val_loss: 1.2643e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 795/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2953e-04 - rmse: 0.0068\n",
      "Epoch 795: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3067e-04 - rmse: 0.0068 - val_loss: 1.3107e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 796/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3978e-04 - rmse: 0.0075\n",
      "Epoch 796: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3441e-04 - rmse: 0.0071 - val_loss: 1.3177e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 797/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3139e-04 - rmse: 0.0069\n",
      "Epoch 797: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3486e-04 - rmse: 0.0072 - val_loss: 1.3219e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 798/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3423e-04 - rmse: 0.0071\n",
      "Epoch 798: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3236e-04 - rmse: 0.0070 - val_loss: 1.2696e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 799/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3519e-04 - rmse: 0.0072\n",
      "Epoch 799: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2608e-04 - rmse: 0.0065 - val_loss: 1.2657e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 800/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3861e-04 - rmse: 0.0074\n",
      "Epoch 800: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3613e-04 - rmse: 0.0072 - val_loss: 1.2810e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 801/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2385e-04 - rmse: 0.0063\n",
      "Epoch 801: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.3111e-04 - rmse: 0.0069 - val_loss: 1.2582e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 802/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3551e-04 - rmse: 0.0072\n",
      "Epoch 802: val_loss improved from 0.00013 to 0.00013, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.3188e-04 - rmse: 0.0070 - val_loss: 1.2569e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 803/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2801e-04 - rmse: 0.0067\n",
      "Epoch 803: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2896e-04 - rmse: 0.0067 - val_loss: 1.2703e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 804/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3526e-04 - rmse: 0.0072\n",
      "Epoch 804: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3380e-04 - rmse: 0.0071 - val_loss: 1.2719e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 805/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2864e-04 - rmse: 0.0067\n",
      "Epoch 805: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3180e-04 - rmse: 0.0070 - val_loss: 1.3298e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 806/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4698e-04 - rmse: 0.0080\n",
      "Epoch 806: val_loss did not improve from 0.00013\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3487e-04 - rmse: 0.0072 - val_loss: 1.3440e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 807/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4229e-04 - rmse: 0.0077\n",
      "Epoch 807: val_loss improved from 0.00013 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.4001e-04 - rmse: 0.0075 - val_loss: 1.2492e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 808/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3577e-04 - rmse: 0.0072\n",
      "Epoch 808: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3868e-04 - rmse: 0.0074 - val_loss: 1.2903e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 809/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4550e-04 - rmse: 0.0079\n",
      "Epoch 809: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3243e-04 - rmse: 0.0070 - val_loss: 1.2963e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 810/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3902e-04 - rmse: 0.0075\n",
      "Epoch 810: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4244e-04 - rmse: 0.0077 - val_loss: 1.3841e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 811/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3218e-04 - rmse: 0.0070\n",
      "Epoch 811: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4037e-04 - rmse: 0.0076 - val_loss: 1.3668e-04 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 812/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4332e-04 - rmse: 0.0077\n",
      "Epoch 812: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3767e-04 - rmse: 0.0074 - val_loss: 1.3387e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 813/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4132e-04 - rmse: 0.0076\n",
      "Epoch 813: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3437e-04 - rmse: 0.0072 - val_loss: 1.2885e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 814/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4404e-04 - rmse: 0.0078\n",
      "Epoch 814: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3689e-04 - rmse: 0.0073 - val_loss: 1.3067e-04 - val_rmse: 0.0069 - lr: 1.0000e-04\n",
      "Epoch 815/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4090e-04 - rmse: 0.0076\n",
      "Epoch 815: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3149e-04 - rmse: 0.0070 - val_loss: 1.2780e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 816/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2927e-04 - rmse: 0.0068\n",
      "Epoch 816: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3074e-04 - rmse: 0.0069 - val_loss: 1.2735e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 817/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2984e-04 - rmse: 0.0068\n",
      "Epoch 817: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.3154e-04 - rmse: 0.0070 - val_loss: 1.2446e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 818/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2402e-04 - rmse: 0.0064\n",
      "Epoch 818: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.3030e-04 - rmse: 0.0069 - val_loss: 1.2427e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 819/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3396e-04 - rmse: 0.0071\n",
      "Epoch 819: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2734e-04 - rmse: 0.0067 - val_loss: 1.2605e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 820/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2814e-04 - rmse: 0.0067\n",
      "Epoch 820: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3359e-04 - rmse: 0.0071 - val_loss: 1.2587e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 821/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3138e-04 - rmse: 0.0070\n",
      "Epoch 821: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 1.3215e-04 - rmse: 0.0070 - val_loss: 1.2324e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 822/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2668e-04 - rmse: 0.0066\n",
      "Epoch 822: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.2475e-04 - rmse: 0.0065 - val_loss: 1.2653e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 823/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3351e-04 - rmse: 0.0071\n",
      "Epoch 823: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3272e-04 - rmse: 0.0071 - val_loss: 1.2497e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 824/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2954e-04 - rmse: 0.0068\n",
      "Epoch 824: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 1.3149e-04 - rmse: 0.0070 - val_loss: 1.2293e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 825/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2749e-04 - rmse: 0.0067\n",
      "Epoch 825: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.2742e-04 - rmse: 0.0067 - val_loss: 1.2270e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 826/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3544e-04 - rmse: 0.0073\n",
      "Epoch 826: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2588e-04 - rmse: 0.0066 - val_loss: 1.2408e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 827/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4432e-04 - rmse: 0.0078\n",
      "Epoch 827: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.2946e-04 - rmse: 0.0068 - val_loss: 1.2252e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 828/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3650e-04 - rmse: 0.0073\n",
      "Epoch 828: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3687e-04 - rmse: 0.0074 - val_loss: 1.3727e-04 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 829/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4417e-04 - rmse: 0.0078\n",
      "Epoch 829: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3954e-04 - rmse: 0.0075 - val_loss: 1.2361e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 830/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2587e-04 - rmse: 0.0066\n",
      "Epoch 830: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2759e-04 - rmse: 0.0067 - val_loss: 1.3323e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 831/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4445e-04 - rmse: 0.0079\n",
      "Epoch 831: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3496e-04 - rmse: 0.0072 - val_loss: 1.2608e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 832/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3353e-04 - rmse: 0.0071\n",
      "Epoch 832: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3238e-04 - rmse: 0.0070 - val_loss: 1.2655e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 833/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2909e-04 - rmse: 0.0068\n",
      "Epoch 833: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2981e-04 - rmse: 0.0069 - val_loss: 1.2479e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 834/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3258e-04 - rmse: 0.0071\n",
      "Epoch 834: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.3646e-04 - rmse: 0.0073 - val_loss: 1.2112e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 835/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1445e-04 - rmse: 0.0056\n",
      "Epoch 835: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2092e-04 - rmse: 0.0062 - val_loss: 1.2296e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 836/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2837e-04 - rmse: 0.0068\n",
      "Epoch 836: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3070e-04 - rmse: 0.0069 - val_loss: 1.2174e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 837/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2912e-04 - rmse: 0.0068\n",
      "Epoch 837: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2984e-04 - rmse: 0.0069 - val_loss: 1.2795e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 838/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3141e-04 - rmse: 0.0070\n",
      "Epoch 838: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3216e-04 - rmse: 0.0070 - val_loss: 1.2565e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 839/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3523e-04 - rmse: 0.0073\n",
      "Epoch 839: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3286e-04 - rmse: 0.0071 - val_loss: 1.2398e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 840/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2744e-04 - rmse: 0.0067\n",
      "Epoch 840: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3374e-04 - rmse: 0.0072 - val_loss: 1.3406e-04 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 841/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3770e-04 - rmse: 0.0074\n",
      "Epoch 841: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3142e-04 - rmse: 0.0070 - val_loss: 1.2890e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 842/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3870e-04 - rmse: 0.0075\n",
      "Epoch 842: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3256e-04 - rmse: 0.0071 - val_loss: 1.2160e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 843/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2820e-04 - rmse: 0.0068\n",
      "Epoch 843: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2715e-04 - rmse: 0.0067 - val_loss: 1.2321e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 844/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2462e-04 - rmse: 0.0065\n",
      "Epoch 844: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.3030e-04 - rmse: 0.0069 - val_loss: 1.2070e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 845/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2745e-04 - rmse: 0.0067\n",
      "Epoch 845: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3146e-04 - rmse: 0.0070 - val_loss: 1.2490e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 846/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2625e-04 - rmse: 0.0066\n",
      "Epoch 846: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2961e-04 - rmse: 0.0069 - val_loss: 1.2901e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 847/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3604e-04 - rmse: 0.0073\n",
      "Epoch 847: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.3598e-04 - rmse: 0.0073 - val_loss: 1.2065e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 848/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1771e-04 - rmse: 0.0060\n",
      "Epoch 848: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.2429e-04 - rmse: 0.0065 - val_loss: 1.1938e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 849/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2564e-04 - rmse: 0.0066\n",
      "Epoch 849: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2729e-04 - rmse: 0.0067 - val_loss: 1.2241e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 850/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2499e-04 - rmse: 0.0065\n",
      "Epoch 850: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3264e-04 - rmse: 0.0071 - val_loss: 1.2337e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 851/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2684e-04 - rmse: 0.0067\n",
      "Epoch 851: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2457e-04 - rmse: 0.0065 - val_loss: 1.2792e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 852/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3834e-04 - rmse: 0.0075\n",
      "Epoch 852: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3399e-04 - rmse: 0.0072 - val_loss: 1.2760e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 853/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4293e-04 - rmse: 0.0078\n",
      "Epoch 853: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3889e-04 - rmse: 0.0075 - val_loss: 1.3944e-04 - val_rmse: 0.0076 - lr: 1.0000e-04\n",
      "Epoch 854/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4031e-04 - rmse: 0.0076\n",
      "Epoch 854: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3278e-04 - rmse: 0.0071 - val_loss: 1.3155e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 855/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3477e-04 - rmse: 0.0073\n",
      "Epoch 855: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3204e-04 - rmse: 0.0071 - val_loss: 1.3039e-04 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 856/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4204e-04 - rmse: 0.0077\n",
      "Epoch 856: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3581e-04 - rmse: 0.0073 - val_loss: 1.2564e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 857/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2790e-04 - rmse: 0.0068\n",
      "Epoch 857: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3456e-04 - rmse: 0.0073 - val_loss: 1.2356e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 858/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1764e-04 - rmse: 0.0060\n",
      "Epoch 858: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2731e-04 - rmse: 0.0067 - val_loss: 1.2215e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 859/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2999e-04 - rmse: 0.0069\n",
      "Epoch 859: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2815e-04 - rmse: 0.0068 - val_loss: 1.2049e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 860/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2474e-04 - rmse: 0.0065\n",
      "Epoch 860: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2710e-04 - rmse: 0.0067 - val_loss: 1.2445e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 861/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4150e-04 - rmse: 0.0077\n",
      "Epoch 861: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2847e-04 - rmse: 0.0068 - val_loss: 1.2024e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 862/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2927e-04 - rmse: 0.0069\n",
      "Epoch 862: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3069e-04 - rmse: 0.0070 - val_loss: 1.2094e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 863/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2727e-04 - rmse: 0.0067\n",
      "Epoch 863: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.3255e-04 - rmse: 0.0071 - val_loss: 1.3184e-04 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 864/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3700e-04 - rmse: 0.0074\n",
      "Epoch 864: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3569e-04 - rmse: 0.0073 - val_loss: 1.4501e-04 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 865/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4294e-04 - rmse: 0.0078\n",
      "Epoch 865: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 1.4157e-04 - rmse: 0.0077 - val_loss: 1.1903e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 866/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3349e-04 - rmse: 0.0072\n",
      "Epoch 866: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4336e-04 - rmse: 0.0079 - val_loss: 1.4962e-04 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 867/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.6026e-04 - rmse: 0.0089\n",
      "Epoch 867: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.4427e-04 - rmse: 0.0079 - val_loss: 1.2074e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 868/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2076e-04 - rmse: 0.0063\n",
      "Epoch 868: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2950e-04 - rmse: 0.0069 - val_loss: 1.2807e-04 - val_rmse: 0.0068 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 869/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3107e-04 - rmse: 0.0070\n",
      "Epoch 869: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.4147e-04 - rmse: 0.0077 - val_loss: 1.1878e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 870/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2272e-04 - rmse: 0.0064\n",
      "Epoch 870: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2958e-04 - rmse: 0.0069 - val_loss: 1.2257e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 871/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2922e-04 - rmse: 0.0069\n",
      "Epoch 871: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2950e-04 - rmse: 0.0069 - val_loss: 1.1948e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 872/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2902e-04 - rmse: 0.0069\n",
      "Epoch 872: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.2917e-04 - rmse: 0.0069 - val_loss: 1.1761e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 873/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2424e-04 - rmse: 0.0065\n",
      "Epoch 873: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2460e-04 - rmse: 0.0066 - val_loss: 1.2215e-04 - val_rmse: 0.0064 - lr: 1.0000e-04\n",
      "Epoch 874/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3655e-04 - rmse: 0.0074\n",
      "Epoch 874: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2928e-04 - rmse: 0.0069 - val_loss: 1.2626e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 875/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2958e-04 - rmse: 0.0069\n",
      "Epoch 875: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2773e-04 - rmse: 0.0068 - val_loss: 1.2645e-04 - val_rmse: 0.0067 - lr: 1.0000e-04\n",
      "Epoch 876/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3417e-04 - rmse: 0.0073\n",
      "Epoch 876: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3129e-04 - rmse: 0.0071 - val_loss: 1.2000e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 877/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2572e-04 - rmse: 0.0067\n",
      "Epoch 877: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2823e-04 - rmse: 0.0068 - val_loss: 1.1892e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 878/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2611e-04 - rmse: 0.0067\n",
      "Epoch 878: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2545e-04 - rmse: 0.0066 - val_loss: 1.2076e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 879/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2952e-04 - rmse: 0.0069\n",
      "Epoch 879: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3245e-04 - rmse: 0.0072 - val_loss: 1.2094e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 880/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2938e-04 - rmse: 0.0069\n",
      "Epoch 880: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3520e-04 - rmse: 0.0073 - val_loss: 1.4275e-04 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 881/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4639e-04 - rmse: 0.0081\n",
      "Epoch 881: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.4412e-04 - rmse: 0.0079 - val_loss: 1.2109e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 882/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2849e-04 - rmse: 0.0069\n",
      "Epoch 882: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.3136e-04 - rmse: 0.0071 - val_loss: 1.2063e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 883/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2804e-04 - rmse: 0.0068\n",
      "Epoch 883: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2797e-04 - rmse: 0.0068 - val_loss: 1.2490e-04 - val_rmse: 0.0066 - lr: 1.0000e-04\n",
      "Epoch 884/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2685e-04 - rmse: 0.0068\n",
      "Epoch 884: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2841e-04 - rmse: 0.0069 - val_loss: 1.1828e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 885/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2394e-04 - rmse: 0.0065\n",
      "Epoch 885: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2621e-04 - rmse: 0.0067 - val_loss: 1.2309e-04 - val_rmse: 0.0065 - lr: 1.0000e-04\n",
      "Epoch 886/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3005e-04 - rmse: 0.0070\n",
      "Epoch 886: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2877e-04 - rmse: 0.0069 - val_loss: 1.2114e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 887/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2765e-04 - rmse: 0.0068\n",
      "Epoch 887: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2357e-04 - rmse: 0.0065 - val_loss: 1.1946e-04 - val_rmse: 0.0062 - lr: 1.0000e-04\n",
      "Epoch 888/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3021e-04 - rmse: 0.0070\n",
      "Epoch 888: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2918e-04 - rmse: 0.0069 - val_loss: 1.1878e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 889/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3194e-04 - rmse: 0.0071\n",
      "Epoch 889: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3005e-04 - rmse: 0.0070 - val_loss: 1.2056e-04 - val_rmse: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 890/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2142e-04 - rmse: 0.0064\n",
      "Epoch 890: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2839e-04 - rmse: 0.0069 - val_loss: 1.1768e-04 - val_rmse: 0.0061 - lr: 1.0000e-04\n",
      "Epoch 891/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2185e-04 - rmse: 0.0064\n",
      "Epoch 891: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.2350e-04 - rmse: 0.0065 - val_loss: 1.1747e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 892/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1855e-04 - rmse: 0.0061\n",
      "Epoch 892: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "\n",
      "Epoch 892: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.2451e-04 - rmse: 0.0066 - val_loss: 1.1730e-04 - val_rmse: 0.0060 - lr: 1.0000e-04\n",
      "Epoch 893/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2322e-04 - rmse: 0.0065\n",
      "Epoch 893: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.2587e-04 - rmse: 0.0067 - val_loss: 1.1533e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 894/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1311e-04 - rmse: 0.0057\n",
      "Epoch 894: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2275e-04 - rmse: 0.0065 - val_loss: 1.1689e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 895/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1529e-04 - rmse: 0.0059\n",
      "Epoch 895: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2263e-04 - rmse: 0.0065 - val_loss: 1.1761e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 896/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1887e-04 - rmse: 0.0062\n",
      "Epoch 896: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2489e-04 - rmse: 0.0066 - val_loss: 1.1599e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 897/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2964e-04 - rmse: 0.0070\n",
      "Epoch 897: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2509e-04 - rmse: 0.0066 - val_loss: 1.1674e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 898/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1328e-04 - rmse: 0.0057\n",
      "Epoch 898: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1967e-04 - rmse: 0.0062 - val_loss: 1.1555e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 899/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3084e-04 - rmse: 0.0071\n",
      "Epoch 899: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2721e-04 - rmse: 0.0068 - val_loss: 1.1548e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 900/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1584e-04 - rmse: 0.0059\n",
      "Epoch 900: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2089e-04 - rmse: 0.0063 - val_loss: 1.1592e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 901/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1499e-04 - rmse: 0.0058\n",
      "Epoch 901: val_loss did not improve from 0.00012\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1944e-04 - rmse: 0.0062 - val_loss: 1.1533e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 902/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4177e-04 - rmse: 0.0078\n",
      "Epoch 902: val_loss improved from 0.00012 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.2570e-04 - rmse: 0.0067 - val_loss: 1.1500e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 903/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2408e-04 - rmse: 0.0066\n",
      "Epoch 903: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2513e-04 - rmse: 0.0067 - val_loss: 1.1541e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 904/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2495e-04 - rmse: 0.0066\n",
      "Epoch 904: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2554e-04 - rmse: 0.0067 - val_loss: 1.1710e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 905/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1976e-04 - rmse: 0.0062\n",
      "Epoch 905: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2760e-04 - rmse: 0.0068 - val_loss: 1.2054e-04 - val_rmse: 0.0063 - lr: 5.0000e-05\n",
      "Epoch 906/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2203e-04 - rmse: 0.0064\n",
      "Epoch 906: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3173e-04 - rmse: 0.0071 - val_loss: 1.2104e-04 - val_rmse: 0.0063 - lr: 5.0000e-05\n",
      "Epoch 907/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2681e-04 - rmse: 0.0068\n",
      "Epoch 907: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3029e-04 - rmse: 0.0070 - val_loss: 1.1949e-04 - val_rmse: 0.0062 - lr: 5.0000e-05\n",
      "Epoch 908/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2718e-04 - rmse: 0.0068\n",
      "Epoch 908: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.3024e-04 - rmse: 0.0070 - val_loss: 1.2108e-04 - val_rmse: 0.0064 - lr: 5.0000e-05\n",
      "Epoch 909/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2546e-04 - rmse: 0.0067\n",
      "Epoch 909: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2685e-04 - rmse: 0.0068 - val_loss: 1.2238e-04 - val_rmse: 0.0065 - lr: 5.0000e-05\n",
      "Epoch 910/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3053e-04 - rmse: 0.0071\n",
      "Epoch 910: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3018e-04 - rmse: 0.0070 - val_loss: 1.1780e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 911/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2128e-04 - rmse: 0.0064\n",
      "Epoch 911: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2043e-04 - rmse: 0.0063 - val_loss: 1.1673e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 912/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2560e-04 - rmse: 0.0067\n",
      "Epoch 912: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.3141e-04 - rmse: 0.0071 - val_loss: 1.2255e-04 - val_rmse: 0.0065 - lr: 5.0000e-05\n",
      "Epoch 913/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2722e-04 - rmse: 0.0068\n",
      "Epoch 913: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2871e-04 - rmse: 0.0069 - val_loss: 1.2090e-04 - val_rmse: 0.0063 - lr: 5.0000e-05\n",
      "Epoch 914/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2543e-04 - rmse: 0.0067\n",
      "Epoch 914: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2662e-04 - rmse: 0.0068 - val_loss: 1.2114e-04 - val_rmse: 0.0064 - lr: 5.0000e-05\n",
      "Epoch 915/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2509e-04 - rmse: 0.0067\n",
      "Epoch 915: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2790e-04 - rmse: 0.0069 - val_loss: 1.2340e-04 - val_rmse: 0.0065 - lr: 5.0000e-05\n",
      "Epoch 916/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2671e-04 - rmse: 0.0068\n",
      "Epoch 916: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2296e-04 - rmse: 0.0065 - val_loss: 1.2242e-04 - val_rmse: 0.0065 - lr: 5.0000e-05\n",
      "Epoch 917/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3824e-04 - rmse: 0.0076\n",
      "Epoch 917: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2730e-04 - rmse: 0.0068 - val_loss: 1.2096e-04 - val_rmse: 0.0064 - lr: 5.0000e-05\n",
      "Epoch 918/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2510e-04 - rmse: 0.0067\n",
      "Epoch 918: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2148e-04 - rmse: 0.0064 - val_loss: 1.1698e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 919/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2463e-04 - rmse: 0.0066\n",
      "Epoch 919: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2583e-04 - rmse: 0.0067 - val_loss: 1.1526e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 920/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2195e-04 - rmse: 0.0064\n",
      "Epoch 920: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 1.2240e-04 - rmse: 0.0065 - val_loss: 1.1490e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 921/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2308e-04 - rmse: 0.0065\n",
      "Epoch 921: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.2319e-04 - rmse: 0.0065 - val_loss: 1.1456e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 922/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1564e-04 - rmse: 0.0059\n",
      "Epoch 922: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2045e-04 - rmse: 0.0063 - val_loss: 1.1464e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 923/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2106e-04 - rmse: 0.0064\n",
      "Epoch 923: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2284e-04 - rmse: 0.0065 - val_loss: 1.1946e-04 - val_rmse: 0.0062 - lr: 5.0000e-05\n",
      "Epoch 924/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3883e-04 - rmse: 0.0076\n",
      "Epoch 924: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2609e-04 - rmse: 0.0068 - val_loss: 1.1618e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 925/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2242e-04 - rmse: 0.0065\n",
      "Epoch 925: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 1.2345e-04 - rmse: 0.0066 - val_loss: 1.1442e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 926/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2801e-04 - rmse: 0.0069\n",
      "Epoch 926: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2161e-04 - rmse: 0.0064 - val_loss: 1.1450e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 927/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2871e-04 - rmse: 0.0069\n",
      "Epoch 927: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2536e-04 - rmse: 0.0067 - val_loss: 1.1541e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 928/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2831e-04 - rmse: 0.0069\n",
      "Epoch 928: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2500e-04 - rmse: 0.0067 - val_loss: 1.1478e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 929/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1498e-04 - rmse: 0.0059\n",
      "Epoch 929: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.2307e-04 - rmse: 0.0065 - val_loss: 1.1416e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 930/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2346e-04 - rmse: 0.0066\n",
      "Epoch 930: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2190e-04 - rmse: 0.0064 - val_loss: 1.1764e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 931/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2821e-04 - rmse: 0.0069\n",
      "Epoch 931: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2921e-04 - rmse: 0.0070 - val_loss: 1.1804e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 932/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2063e-04 - rmse: 0.0063\n",
      "Epoch 932: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2277e-04 - rmse: 0.0065 - val_loss: 1.1606e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 933/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2648e-04 - rmse: 0.0068\n",
      "Epoch 933: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2417e-04 - rmse: 0.0066 - val_loss: 1.1881e-04 - val_rmse: 0.0062 - lr: 5.0000e-05\n",
      "Epoch 934/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2082e-04 - rmse: 0.0064\n",
      "Epoch 934: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2425e-04 - rmse: 0.0066 - val_loss: 1.1775e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 935/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2900e-04 - rmse: 0.0070\n",
      "Epoch 935: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2646e-04 - rmse: 0.0068 - val_loss: 1.1427e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 936/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1528e-04 - rmse: 0.0059\n",
      "Epoch 936: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2466e-04 - rmse: 0.0067 - val_loss: 1.1427e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 937/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2276e-04 - rmse: 0.0065\n",
      "Epoch 937: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2632e-04 - rmse: 0.0068 - val_loss: 1.1469e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 938/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1347e-04 - rmse: 0.0058\n",
      "Epoch 938: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2392e-04 - rmse: 0.0066 - val_loss: 1.1706e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 939/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2227e-04 - rmse: 0.0065\n",
      "Epoch 939: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2632e-04 - rmse: 0.0068 - val_loss: 1.1559e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 940/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2723e-04 - rmse: 0.0069\n",
      "Epoch 940: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2222e-04 - rmse: 0.0065 - val_loss: 1.1605e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 941/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1633e-04 - rmse: 0.0060\n",
      "Epoch 941: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2334e-04 - rmse: 0.0066 - val_loss: 1.1590e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 942/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2957e-04 - rmse: 0.0070\n",
      "Epoch 942: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2229e-04 - rmse: 0.0065 - val_loss: 1.1614e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 943/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2830e-04 - rmse: 0.0069\n",
      "Epoch 943: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2123e-04 - rmse: 0.0064 - val_loss: 1.1578e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 944/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2401e-04 - rmse: 0.0066\n",
      "Epoch 944: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2429e-04 - rmse: 0.0066 - val_loss: 1.1600e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 945/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2717e-04 - rmse: 0.0069\n",
      "Epoch 945: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2222e-04 - rmse: 0.0065 - val_loss: 1.1467e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 946/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1773e-04 - rmse: 0.0061\n",
      "Epoch 946: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2311e-04 - rmse: 0.0066 - val_loss: 1.1433e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 947/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1581e-04 - rmse: 0.0060\n",
      "Epoch 947: val_loss did not improve from 0.00011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1824e-04 - rmse: 0.0062 - val_loss: 1.1535e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 948/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3244e-04 - rmse: 0.0072\n",
      "Epoch 948: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.2564e-04 - rmse: 0.0067 - val_loss: 1.1386e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 949/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1974e-04 - rmse: 0.0063\n",
      "Epoch 949: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2290e-04 - rmse: 0.0065 - val_loss: 1.1504e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 950/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2114e-04 - rmse: 0.0064\n",
      "Epoch 950: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2159e-04 - rmse: 0.0064 - val_loss: 1.1524e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 951/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2266e-04 - rmse: 0.0065\n",
      "Epoch 951: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.2213e-04 - rmse: 0.0065 - val_loss: 1.1341e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 952/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2826e-04 - rmse: 0.0069\n",
      "Epoch 952: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2163e-04 - rmse: 0.0064 - val_loss: 1.1348e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 953/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1803e-04 - rmse: 0.0062\n",
      "Epoch 953: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2100e-04 - rmse: 0.0064 - val_loss: 1.1362e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 954/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2485e-04 - rmse: 0.0067\n",
      "Epoch 954: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2309e-04 - rmse: 0.0066 - val_loss: 1.1567e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 955/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1896e-04 - rmse: 0.0062\n",
      "Epoch 955: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2490e-04 - rmse: 0.0067 - val_loss: 1.1874e-04 - val_rmse: 0.0062 - lr: 5.0000e-05\n",
      "Epoch 956/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2210e-04 - rmse: 0.0065\n",
      "Epoch 956: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2137e-04 - rmse: 0.0064 - val_loss: 1.2342e-04 - val_rmse: 0.0066 - lr: 5.0000e-05\n",
      "Epoch 957/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3263e-04 - rmse: 0.0073\n",
      "Epoch 957: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2611e-04 - rmse: 0.0068 - val_loss: 1.1933e-04 - val_rmse: 0.0063 - lr: 5.0000e-05\n",
      "Epoch 958/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1921e-04 - rmse: 0.0063\n",
      "Epoch 958: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2244e-04 - rmse: 0.0065 - val_loss: 1.1759e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 959/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3276e-04 - rmse: 0.0073\n",
      "Epoch 959: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2302e-04 - rmse: 0.0066 - val_loss: 1.1714e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 960/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2667e-04 - rmse: 0.0068\n",
      "Epoch 960: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2742e-04 - rmse: 0.0069 - val_loss: 1.1729e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 961/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1956e-04 - rmse: 0.0063\n",
      "Epoch 961: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2041e-04 - rmse: 0.0064 - val_loss: 1.1678e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 962/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3923e-04 - rmse: 0.0077\n",
      "Epoch 962: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2654e-04 - rmse: 0.0068 - val_loss: 1.1544e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 963/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1999e-04 - rmse: 0.0063\n",
      "Epoch 963: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2609e-04 - rmse: 0.0068 - val_loss: 1.1603e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 964/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1472e-04 - rmse: 0.0059\n",
      "Epoch 964: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1984e-04 - rmse: 0.0063 - val_loss: 1.1519e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 965/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2424e-04 - rmse: 0.0067\n",
      "Epoch 965: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2195e-04 - rmse: 0.0065 - val_loss: 1.1377e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 966/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2294e-04 - rmse: 0.0066\n",
      "Epoch 966: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2338e-04 - rmse: 0.0066 - val_loss: 1.1578e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 967/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2245e-04 - rmse: 0.0065\n",
      "Epoch 967: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2213e-04 - rmse: 0.0065 - val_loss: 1.1433e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 968/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1904e-04 - rmse: 0.0063\n",
      "Epoch 968: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.2115e-04 - rmse: 0.0064 - val_loss: 1.1327e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 969/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1480e-04 - rmse: 0.0059\n",
      "Epoch 969: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2309e-04 - rmse: 0.0066 - val_loss: 1.1355e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 970/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1013e-04 - rmse: 0.0055\n",
      "Epoch 970: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1865e-04 - rmse: 0.0062 - val_loss: 1.1438e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 971/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1991e-04 - rmse: 0.0063\n",
      "Epoch 971: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2250e-04 - rmse: 0.0065 - val_loss: 1.1355e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 972/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2100e-04 - rmse: 0.0064\n",
      "Epoch 972: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.2341e-04 - rmse: 0.0066 - val_loss: 1.1284e-04 - val_rmse: 0.0057 - lr: 5.0000e-05\n",
      "Epoch 973/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2345e-04 - rmse: 0.0066\n",
      "Epoch 973: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2000e-04 - rmse: 0.0063 - val_loss: 1.1325e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 974/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2630e-04 - rmse: 0.0068\n",
      "Epoch 974: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.2395e-04 - rmse: 0.0066 - val_loss: 1.1283e-04 - val_rmse: 0.0057 - lr: 5.0000e-05\n",
      "Epoch 975/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1897e-04 - rmse: 0.0063\n",
      "Epoch 975: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2064e-04 - rmse: 0.0064 - val_loss: 1.1351e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 976/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2359e-04 - rmse: 0.0066\n",
      "Epoch 976: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2064e-04 - rmse: 0.0064 - val_loss: 1.1383e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 977/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2188e-04 - rmse: 0.0065\n",
      "Epoch 977: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2091e-04 - rmse: 0.0064 - val_loss: 1.1537e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 978/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2887e-04 - rmse: 0.0070\n",
      "Epoch 978: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2404e-04 - rmse: 0.0067 - val_loss: 1.1568e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 979/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2050e-04 - rmse: 0.0064\n",
      "Epoch 979: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1932e-04 - rmse: 0.0063 - val_loss: 1.1479e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 980/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2835e-04 - rmse: 0.0070\n",
      "Epoch 980: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2344e-04 - rmse: 0.0066 - val_loss: 1.1412e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 981/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2738e-04 - rmse: 0.0069\n",
      "Epoch 981: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 1.2356e-04 - rmse: 0.0066 - val_loss: 1.1271e-04 - val_rmse: 0.0057 - lr: 5.0000e-05\n",
      "Epoch 982/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1597e-04 - rmse: 0.0060\n",
      "Epoch 982: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1902e-04 - rmse: 0.0063 - val_loss: 1.1379e-04 - val_rmse: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 983/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2030e-04 - rmse: 0.0064\n",
      "Epoch 983: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2533e-04 - rmse: 0.0068 - val_loss: 1.1440e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 984/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1968e-04 - rmse: 0.0063\n",
      "Epoch 984: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2342e-04 - rmse: 0.0066 - val_loss: 1.1514e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 985/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2467e-04 - rmse: 0.0067\n",
      "Epoch 985: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2215e-04 - rmse: 0.0065 - val_loss: 1.1558e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 986/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2939e-04 - rmse: 0.0071\n",
      "Epoch 986: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2425e-04 - rmse: 0.0067 - val_loss: 1.1627e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 987/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2366e-04 - rmse: 0.0066\n",
      "Epoch 987: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2104e-04 - rmse: 0.0064 - val_loss: 1.1879e-04 - val_rmse: 0.0063 - lr: 5.0000e-05\n",
      "Epoch 988/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2421e-04 - rmse: 0.0067\n",
      "Epoch 988: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2625e-04 - rmse: 0.0068 - val_loss: 1.1718e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 989/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1893e-04 - rmse: 0.0063\n",
      "Epoch 989: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2361e-04 - rmse: 0.0066 - val_loss: 1.1480e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 990/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2643e-04 - rmse: 0.0068\n",
      "Epoch 990: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.2495e-04 - rmse: 0.0067 - val_loss: 1.1244e-04 - val_rmse: 0.0057 - lr: 5.0000e-05\n",
      "Epoch 991/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1885e-04 - rmse: 0.0063\n",
      "Epoch 991: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2591e-04 - rmse: 0.0068 - val_loss: 1.1381e-04 - val_rmse: 0.0059 - lr: 5.0000e-05\n",
      "Epoch 992/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1487e-04 - rmse: 0.0059\n",
      "Epoch 992: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2070e-04 - rmse: 0.0064 - val_loss: 1.1646e-04 - val_rmse: 0.0061 - lr: 5.0000e-05\n",
      "Epoch 993/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2541e-04 - rmse: 0.0068\n",
      "Epoch 993: val_loss did not improve from 0.00011\n",
      "\n",
      "Epoch 993: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2106e-04 - rmse: 0.0064 - val_loss: 1.1550e-04 - val_rmse: 0.0060 - lr: 5.0000e-05\n",
      "Epoch 994/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3319e-04 - rmse: 0.0073\n",
      "Epoch 994: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2304e-04 - rmse: 0.0066 - val_loss: 1.1330e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 995/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2035e-04 - rmse: 0.0064\n",
      "Epoch 995: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.2318e-04 - rmse: 0.0066 - val_loss: 1.1230e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 996/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1659e-04 - rmse: 0.0061\n",
      "Epoch 996: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2001e-04 - rmse: 0.0064 - val_loss: 1.1252e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 997/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1855e-04 - rmse: 0.0062\n",
      "Epoch 997: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1775e-04 - rmse: 0.0062 - val_loss: 1.1257e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 998/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3592e-04 - rmse: 0.0075\n",
      "Epoch 998: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 43ms/step - loss: 1.2024e-04 - rmse: 0.0064 - val_loss: 1.1209e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 999/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2455e-04 - rmse: 0.0067\n",
      "Epoch 999: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2390e-04 - rmse: 0.0067 - val_loss: 1.1257e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1000/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2306e-04 - rmse: 0.0066\n",
      "Epoch 1000: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1932e-04 - rmse: 0.0063 - val_loss: 1.1287e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1001/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1859e-04 - rmse: 0.0063\n",
      "Epoch 1001: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 1.1936e-04 - rmse: 0.0063 - val_loss: 1.1201e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1002/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2301e-04 - rmse: 0.0066\n",
      "Epoch 1002: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.1964e-04 - rmse: 0.0063 - val_loss: 1.1182e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1003/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2359e-04 - rmse: 0.0066\n",
      "Epoch 1003: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1827e-04 - rmse: 0.0062 - val_loss: 1.1247e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1004/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2452e-04 - rmse: 0.0067\n",
      "Epoch 1004: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2207e-04 - rmse: 0.0065 - val_loss: 1.1210e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1005/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2306e-04 - rmse: 0.0066\n",
      "Epoch 1005: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2007e-04 - rmse: 0.0064 - val_loss: 1.1244e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1006/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2314e-04 - rmse: 0.0066\n",
      "Epoch 1006: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2006e-04 - rmse: 0.0064 - val_loss: 1.1205e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1007/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2149e-04 - rmse: 0.0065\n",
      "Epoch 1007: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2032e-04 - rmse: 0.0064 - val_loss: 1.1217e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1008/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1851e-04 - rmse: 0.0063\n",
      "Epoch 1008: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.1935e-04 - rmse: 0.0063 - val_loss: 1.1181e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1009/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1823e-04 - rmse: 0.0062\n",
      "Epoch 1009: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2116e-04 - rmse: 0.0065 - val_loss: 1.1187e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1010/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1317e-04 - rmse: 0.0058\n",
      "Epoch 1010: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1837e-04 - rmse: 0.0062 - val_loss: 1.1514e-04 - val_rmse: 0.0060 - lr: 2.5000e-05\n",
      "Epoch 1011/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2041e-04 - rmse: 0.0064\n",
      "Epoch 1011: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1961e-04 - rmse: 0.0063 - val_loss: 1.1436e-04 - val_rmse: 0.0059 - lr: 2.5000e-05\n",
      "Epoch 1012/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2424e-04 - rmse: 0.0067\n",
      "Epoch 1012: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2446e-04 - rmse: 0.0067 - val_loss: 1.1243e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1013/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2714e-04 - rmse: 0.0069\n",
      "Epoch 1013: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2200e-04 - rmse: 0.0065 - val_loss: 1.1323e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1014/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1303e-04 - rmse: 0.0058\n",
      "Epoch 1014: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1969e-04 - rmse: 0.0063 - val_loss: 1.1210e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1015/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1777e-04 - rmse: 0.0062\n",
      "Epoch 1015: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2044e-04 - rmse: 0.0064 - val_loss: 1.1238e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1016/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1915e-04 - rmse: 0.0063\n",
      "Epoch 1016: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2027e-04 - rmse: 0.0064 - val_loss: 1.1214e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1017/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1770e-04 - rmse: 0.0062\n",
      "Epoch 1017: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1883e-04 - rmse: 0.0063 - val_loss: 1.1195e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1018/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2556e-04 - rmse: 0.0068\n",
      "Epoch 1018: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.1946e-04 - rmse: 0.0063 - val_loss: 1.1174e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1019/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1967e-04 - rmse: 0.0063\n",
      "Epoch 1019: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2407e-04 - rmse: 0.0067 - val_loss: 1.1186e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1020/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1806e-04 - rmse: 0.0062\n",
      "Epoch 1020: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1662e-04 - rmse: 0.0061 - val_loss: 1.1240e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1021/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2400e-04 - rmse: 0.0067\n",
      "Epoch 1021: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2096e-04 - rmse: 0.0065 - val_loss: 1.1177e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1022/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1211e-04 - rmse: 0.0057\n",
      "Epoch 1022: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2106e-04 - rmse: 0.0065 - val_loss: 1.1196e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1023/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1218e-04 - rmse: 0.0057\n",
      "Epoch 1023: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1749e-04 - rmse: 0.0062 - val_loss: 1.1268e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1024/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1903e-04 - rmse: 0.0063\n",
      "Epoch 1024: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1913e-04 - rmse: 0.0063 - val_loss: 1.1192e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1025/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1994e-04 - rmse: 0.0064\n",
      "Epoch 1025: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1926e-04 - rmse: 0.0063 - val_loss: 1.1192e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1026/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2299e-04 - rmse: 0.0066\n",
      "Epoch 1026: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 1.1915e-04 - rmse: 0.0063 - val_loss: 1.1168e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1027/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1576e-04 - rmse: 0.0060\n",
      "Epoch 1027: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2063e-04 - rmse: 0.0064 - val_loss: 1.1194e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1028/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2955e-04 - rmse: 0.0071\n",
      "Epoch 1028: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.1935e-04 - rmse: 0.0063 - val_loss: 1.1147e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1029/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1414e-04 - rmse: 0.0059\n",
      "Epoch 1029: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2172e-04 - rmse: 0.0065 - val_loss: 1.1160e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1030/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0570e-04 - rmse: 0.0051\n",
      "Epoch 1030: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1452e-04 - rmse: 0.0059 - val_loss: 1.1206e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1031/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2177e-04 - rmse: 0.0065\n",
      "Epoch 1031: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.2221e-04 - rmse: 0.0066 - val_loss: 1.1136e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1032/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1749e-04 - rmse: 0.0062\n",
      "Epoch 1032: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 1.1903e-04 - rmse: 0.0063 - val_loss: 1.1128e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1033/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2348e-04 - rmse: 0.0066\n",
      "Epoch 1033: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2173e-04 - rmse: 0.0065 - val_loss: 1.1327e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1034/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1276e-04 - rmse: 0.0058\n",
      "Epoch 1034: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1622e-04 - rmse: 0.0061 - val_loss: 1.1195e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1035/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2386e-04 - rmse: 0.0067\n",
      "Epoch 1035: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2329e-04 - rmse: 0.0066 - val_loss: 1.1214e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1036/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2241e-04 - rmse: 0.0066\n",
      "Epoch 1036: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1854e-04 - rmse: 0.0063 - val_loss: 1.1228e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1037/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1936e-04 - rmse: 0.0063\n",
      "Epoch 1037: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1948e-04 - rmse: 0.0063 - val_loss: 1.1272e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1038/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2396e-04 - rmse: 0.0067\n",
      "Epoch 1038: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1640e-04 - rmse: 0.0061 - val_loss: 1.1144e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1039/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2833e-04 - rmse: 0.0070\n",
      "Epoch 1039: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2085e-04 - rmse: 0.0065 - val_loss: 1.1134e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1040/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2710e-04 - rmse: 0.0069\n",
      "Epoch 1040: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2338e-04 - rmse: 0.0066 - val_loss: 1.1149e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1041/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1606e-04 - rmse: 0.0061\n",
      "Epoch 1041: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2099e-04 - rmse: 0.0065 - val_loss: 1.1241e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1042/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1501e-04 - rmse: 0.0060\n",
      "Epoch 1042: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1928e-04 - rmse: 0.0063 - val_loss: 1.1187e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1043/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1812e-04 - rmse: 0.0062\n",
      "Epoch 1043: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2096e-04 - rmse: 0.0065 - val_loss: 1.1146e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1044/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1327e-04 - rmse: 0.0058\n",
      "Epoch 1044: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.1698e-04 - rmse: 0.0061 - val_loss: 1.1107e-04 - val_rmse: 0.0056 - lr: 2.5000e-05\n",
      "Epoch 1045/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1909e-04 - rmse: 0.0063\n",
      "Epoch 1045: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2126e-04 - rmse: 0.0065 - val_loss: 1.1129e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1046/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0868e-04 - rmse: 0.0054\n",
      "Epoch 1046: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1741e-04 - rmse: 0.0062 - val_loss: 1.1247e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1047/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2047e-04 - rmse: 0.0064\n",
      "Epoch 1047: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1723e-04 - rmse: 0.0062 - val_loss: 1.1211e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1048/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3329e-04 - rmse: 0.0074\n",
      "Epoch 1048: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 49ms/step - loss: 1.2254e-04 - rmse: 0.0066 - val_loss: 1.1094e-04 - val_rmse: 0.0056 - lr: 2.5000e-05\n",
      "Epoch 1049/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1697e-04 - rmse: 0.0061\n",
      "Epoch 1049: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1878e-04 - rmse: 0.0063 - val_loss: 1.1102e-04 - val_rmse: 0.0056 - lr: 2.5000e-05\n",
      "Epoch 1050/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2110e-04 - rmse: 0.0065\n",
      "Epoch 1050: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1870e-04 - rmse: 0.0063 - val_loss: 1.1148e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1051/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2299e-04 - rmse: 0.0066\n",
      "Epoch 1051: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1999e-04 - rmse: 0.0064 - val_loss: 1.1146e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1052/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1695e-04 - rmse: 0.0062\n",
      "Epoch 1052: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1892e-04 - rmse: 0.0063 - val_loss: 1.1110e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1053/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1710e-04 - rmse: 0.0062\n",
      "Epoch 1053: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1871e-04 - rmse: 0.0063 - val_loss: 1.1162e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1054/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2415e-04 - rmse: 0.0067\n",
      "Epoch 1054: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1980e-04 - rmse: 0.0064 - val_loss: 1.1314e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1055/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2511e-04 - rmse: 0.0068\n",
      "Epoch 1055: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2145e-04 - rmse: 0.0065 - val_loss: 1.1345e-04 - val_rmse: 0.0059 - lr: 2.5000e-05\n",
      "Epoch 1056/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1810e-04 - rmse: 0.0062\n",
      "Epoch 1056: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1936e-04 - rmse: 0.0063 - val_loss: 1.1194e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1057/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1709e-04 - rmse: 0.0062\n",
      "Epoch 1057: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1871e-04 - rmse: 0.0063 - val_loss: 1.1108e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1058/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2698e-04 - rmse: 0.0069\n",
      "Epoch 1058: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2036e-04 - rmse: 0.0064 - val_loss: 1.1172e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1059/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2243e-04 - rmse: 0.0066\n",
      "Epoch 1059: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.2052e-04 - rmse: 0.0064 - val_loss: 1.1287e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1060/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1672e-04 - rmse: 0.0061\n",
      "Epoch 1060: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1967e-04 - rmse: 0.0064 - val_loss: 1.1192e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1061/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1692e-04 - rmse: 0.0062\n",
      "Epoch 1061: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1951e-04 - rmse: 0.0064 - val_loss: 1.1126e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1062/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2328e-04 - rmse: 0.0066\n",
      "Epoch 1062: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1503e-04 - rmse: 0.0060 - val_loss: 1.1156e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1063/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3309e-04 - rmse: 0.0074\n",
      "Epoch 1063: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2623e-04 - rmse: 0.0069 - val_loss: 1.1327e-04 - val_rmse: 0.0059 - lr: 2.5000e-05\n",
      "Epoch 1064/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1784e-04 - rmse: 0.0062\n",
      "Epoch 1064: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1768e-04 - rmse: 0.0062 - val_loss: 1.1258e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1065/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2247e-04 - rmse: 0.0066\n",
      "Epoch 1065: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1856e-04 - rmse: 0.0063 - val_loss: 1.1098e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1066/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3018e-04 - rmse: 0.0072\n",
      "Epoch 1066: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2131e-04 - rmse: 0.0065 - val_loss: 1.1196e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1067/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2181e-04 - rmse: 0.0065\n",
      "Epoch 1067: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2199e-04 - rmse: 0.0066 - val_loss: 1.1292e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1068/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1554e-04 - rmse: 0.0060\n",
      "Epoch 1068: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1872e-04 - rmse: 0.0063 - val_loss: 1.1157e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1069/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1775e-04 - rmse: 0.0062\n",
      "Epoch 1069: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2011e-04 - rmse: 0.0064 - val_loss: 1.1152e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1070/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1426e-04 - rmse: 0.0059\n",
      "Epoch 1070: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1964e-04 - rmse: 0.0064 - val_loss: 1.1187e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1071/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2262e-04 - rmse: 0.0066\n",
      "Epoch 1071: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1904e-04 - rmse: 0.0063 - val_loss: 1.1126e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1072/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1970e-04 - rmse: 0.0064\n",
      "Epoch 1072: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.1950e-04 - rmse: 0.0064 - val_loss: 1.1129e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1073/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2290e-04 - rmse: 0.0066\n",
      "Epoch 1073: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1855e-04 - rmse: 0.0063 - val_loss: 1.1104e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1074/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2380e-04 - rmse: 0.0067\n",
      "Epoch 1074: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1613e-04 - rmse: 0.0061 - val_loss: 1.1244e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1075/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2772e-04 - rmse: 0.0070\n",
      "Epoch 1075: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2325e-04 - rmse: 0.0067 - val_loss: 1.1311e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1076/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2699e-04 - rmse: 0.0069\n",
      "Epoch 1076: val_loss did not improve from 0.00011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2108e-04 - rmse: 0.0065 - val_loss: 1.1274e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1077/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2062e-04 - rmse: 0.0065\n",
      "Epoch 1077: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2038e-04 - rmse: 0.0064 - val_loss: 1.1147e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1078/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2167e-04 - rmse: 0.0065\n",
      "Epoch 1078: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1992e-04 - rmse: 0.0064 - val_loss: 1.1328e-04 - val_rmse: 0.0059 - lr: 2.5000e-05\n",
      "Epoch 1079/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2312e-04 - rmse: 0.0066\n",
      "Epoch 1079: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2475e-04 - rmse: 0.0068 - val_loss: 1.1376e-04 - val_rmse: 0.0059 - lr: 2.5000e-05\n",
      "Epoch 1080/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1817e-04 - rmse: 0.0063\n",
      "Epoch 1080: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1875e-04 - rmse: 0.0063 - val_loss: 1.1269e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1081/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1550e-04 - rmse: 0.0060\n",
      "Epoch 1081: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1839e-04 - rmse: 0.0063 - val_loss: 1.1168e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1082/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3015e-04 - rmse: 0.0072\n",
      "Epoch 1082: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2073e-04 - rmse: 0.0065 - val_loss: 1.1110e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1083/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2190e-04 - rmse: 0.0066\n",
      "Epoch 1083: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2012e-04 - rmse: 0.0064 - val_loss: 1.1401e-04 - val_rmse: 0.0059 - lr: 2.5000e-05\n",
      "Epoch 1084/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2423e-04 - rmse: 0.0067\n",
      "Epoch 1084: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2129e-04 - rmse: 0.0065 - val_loss: 1.1269e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1085/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1131e-04 - rmse: 0.0057\n",
      "Epoch 1085: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1991e-04 - rmse: 0.0064 - val_loss: 1.1132e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1086/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1949e-04 - rmse: 0.0064\n",
      "Epoch 1086: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 1.2232e-04 - rmse: 0.0066 - val_loss: 1.1087e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1087/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1655e-04 - rmse: 0.0061\n",
      "Epoch 1087: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2048e-04 - rmse: 0.0064 - val_loss: 1.1157e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1088/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2573e-04 - rmse: 0.0068\n",
      "Epoch 1088: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1909e-04 - rmse: 0.0063 - val_loss: 1.1203e-04 - val_rmse: 0.0058 - lr: 2.5000e-05\n",
      "Epoch 1089/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2120e-04 - rmse: 0.0065\n",
      "Epoch 1089: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.2041e-04 - rmse: 0.0064 - val_loss: 1.1348e-04 - val_rmse: 0.0059 - lr: 2.5000e-05\n",
      "Epoch 1090/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1319e-04 - rmse: 0.0059\n",
      "Epoch 1090: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1619e-04 - rmse: 0.0061 - val_loss: 1.1108e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1091/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1932e-04 - rmse: 0.0064\n",
      "Epoch 1091: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 1.2126e-04 - rmse: 0.0065 - val_loss: 1.1076e-04 - val_rmse: 0.0056 - lr: 2.5000e-05\n",
      "Epoch 1092/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1761e-04 - rmse: 0.0062\n",
      "Epoch 1092: val_loss did not improve from 0.00011\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1967e-04 - rmse: 0.0064 - val_loss: 1.1111e-04 - val_rmse: 0.0057 - lr: 2.5000e-05\n",
      "Epoch 1093/10000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2195e-04 - rmse: 0.0066Restoring model weights from the end of the best epoch: 893.\n",
      "\n",
      "Epoch 1093: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\20221230\\Case13_WithoutParameters20221230unsteadyPrediction_MLP_Case13_WithoutParameters_val_0.2_test0.1_5layers_256units_checkpoint.h5\n",
      "\n",
      "Epoch 1093: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.1615e-04 - rmse: 0.0061 - val_loss: 1.1070e-04 - val_rmse: 0.0056 - lr: 2.5000e-05\n",
      "Epoch 1093: early stopping\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size,\n",
    "                    validation_data=[x_val, y_val],\n",
    "                    steps_per_epoch = STEP_SIZE_TRAIN, validation_steps=VALIDATION_STEPS,\n",
    "                    epochs=10000, shuffle=True, callbacks=[es, ckpt, rp])\n",
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79204ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:01:18.001701\n"
     ]
    }
   ],
   "source": [
    "time = end - start\n",
    "print(\"Training time:\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fbce220",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = \"D:\\\\VAWT_data\\\\flap_unsteady\\\\result\\\\\"+\"20221230_MLP_Case13_WithoutParameters\\\\test\"+str(test_rate)+\"_val\"+str(val_rate)+\"_\"+str(n_layers)+\"layers_\"+ str(n_units) +\"units_CmPrediction\"\n",
    "if not os.path.exists(storage_dir):\n",
    "    os.makedirs(storage_dir)\n",
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77cc60d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = history.history\n",
    "plt.plot(hist['loss'], lw=2, label='Training loss')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation loss')\n",
    "plt.title('Training loss (mean squared error)\\nMLP, optimal settings, $C_m$ prediction', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "#plt.ylim([5e-5, 1e-1])\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"TrainingLoss_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e42b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2)\n",
    "plt.plot(hist['val_rmse'], lw=2)\n",
    "plt.title('Root Mean Squared Error, optimal settings\\n$C_m$ prediction', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "983b76cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2, label='Training RMSE')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation RMSE')\n",
    "plt.title('Root Mean Squared Error\\nMLP, optimal settings\\n$C_m$ prediction', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"RMSE_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c76ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 4ms/step - loss: 1.2243e-04 - rmse: 0.0064\n"
     ]
    }
   ],
   "source": [
    "train_results = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b1d836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1533e-04 - rmse: 0.0059\n"
     ]
    }
   ],
   "source": [
    "val_results = model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abc70d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 1.1186e-04 - rmse: 0.0056\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "745feda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "decoded_train_ = model.predict(x_train)\n",
    "decoded_val_ = model.predict(x_val)\n",
    "decoded_test_ = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51faee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_index(cm, y):\n",
    "    return np.unique(np.where(np.isin(cm, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e8e16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_absolute(y_pred, y_true):\n",
    "    return np.abs(y_pred - y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aee7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize \n",
    "def denormalize(y):\n",
    "    return y*(np.max(cm)-np.min(cm))+np.min(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "946bb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_error(y_pred, y_real):\n",
    "    return np.sqrt(np.sum((y_pred - y_real)**2) / np.sum(y_real**2))\n",
    "\n",
    "def mape(y_pred, y_real):\n",
    "    return 100/len(y_real)*np.sum(np.abs((y_real-y_pred)/y_real))\n",
    "\n",
    "def smape(y_pred, y_real):\n",
    "    return 100*np.sum(np.abs(y_pred-y_real))/np.sum(y_real+y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb958632",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = define_index(y, y_train)\n",
    "index_val = define_index(y, y_val)\n",
    "index_test = define_index(y, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31fb219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01376960168429242\n",
      "0.5421342611099593\n"
     ]
    }
   ],
   "source": [
    "l2_error_train = l2_error(decoded_train_, y_train)\n",
    "mape_train = smape(decoded_train_, y_train)\n",
    "print(l2_error_train)\n",
    "print(mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c63ffb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013185247127814553\n",
      "0.5138928634671633\n"
     ]
    }
   ],
   "source": [
    "l2_error_val = l2_error(decoded_val_, y_val)\n",
    "mape_val= smape(decoded_val_, y_val)\n",
    "print(l2_error_val)\n",
    "print(mape_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3770434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012088680638002041\n",
      "0.4761567468207769\n"
     ]
    }
   ],
   "source": [
    "l2_error_test = l2_error(decoded_test_, y_test)\n",
    "mape_test= smape(decoded_test_, y_test)\n",
    "print(l2_error_test)\n",
    "print(mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "945ad132",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = denormalize(y_train)\n",
    "y_val = denormalize(y_val)\n",
    "y_test = denormalize(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93ca4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_train = denormalize(decoded_train_)\n",
    "decoded_val = denormalize(decoded_val_)\n",
    "decoded_test = denormalize(decoded_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55d01da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221230\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "os.chdir(model_directory)\n",
    "model_name = \"20221230unsteadyValidation_MLP_Case13_WithoutParameters_val_\"+str(val_rate)+\"_test\"+str(test_rate)+ \"_\" + str(n_layers) +\"layers_\"+str(n_units)+\"units_CmPrediction.h5\"\n",
    "model.save(model_name, overwrite=True, include_optimizer=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c38a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train_abs = error_absolute(decoded_train, y_train)\n",
    "error_val_abs = error_absolute(decoded_val, y_val)\n",
    "error_test_abs = error_absolute(decoded_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e21d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9785cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_train_list = []\n",
    "for i in range(0, len(x_train)):\n",
    "    l2_error_train_data = l2_error(decoded_train[i], y_train[i])\n",
    "    l2_error_train_list.append(l2_error_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "053a1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_error_val_list = []\n",
    "for i in range(0, len(x_val)):\n",
    "    l2_error_val_data = l2_error(decoded_val[i], y_val[i])\n",
    "    l2_error_val_list.append(l2_error_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72b6a696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_test_list = []\n",
    "for i in range(0, len(x_test)):\n",
    "    l2_error_test_data = l2_error(decoded_test[i], y_test[i])\n",
    "    l2_error_test_list.append(l2_error_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e5afbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_train_list = []\n",
    "for i in range(0, len(x_train)):\n",
    "    mape_train_data = smape(decoded_train[i], y_train[i])\n",
    "    mape_train_list.append(mape_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fa71fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_val_list = []\n",
    "for i in range(0, len(x_val)):\n",
    "    mape_val_data = smape(decoded_val[i], y_val[i])\n",
    "    mape_val_list.append(mape_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2f0fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_test_list = []\n",
    "for i in range(0, len(x_test)):\n",
    "    mape_test_data = smape(decoded_test[i], y_test[i])\n",
    "    mape_test_list.append(mape_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f75a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plot(y_pred, y_real, dataset=\"train\"):\n",
    "    dictionary_name = {\"train\":\"training\", \"val\":\"validation\", \"test\":\"test\"}\n",
    "    dictionary_data = {\"train\":l2_error_train_list, \"val\":l2_error_val_list, \"test\":l2_error_test_list}\n",
    "    dictionary_error = {\"train\":l2_error_train, \"val\":l2_error_val, \"test\":l2_error_test}\n",
    "    plot_title = '$L_2$ error norm distribution - MLP, unsteady, '+ dictionary_name.get(dataset) +'.\\nValidation rate {0}, test rate {1}, {2} layers, {3} units ($C_m$)'.format(\n",
    "        val_rate, test_rate, n_layers, n_units)\n",
    "    plt.plot(np.linspace(1,y_real.shape[0],y_real.shape[0]),\n",
    "             dictionary_error.get(dataset)*np.ones(y_real.shape[0],), 'k', lw=2.5)\n",
    "    plt.scatter(np.linspace(1, y_real.shape[0], y_real.shape[0]), dictionary_data.get(dataset), c='b')\n",
    "    plt.xlabel('Index', fontsize=15)\n",
    "    plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "    plt.yscale('log')\n",
    "    plt.title(plot_title, fontsize=15)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd795141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_plot(decoded_train, y_train, dataset=\"train\")\n",
    "saveName = \"trainingErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d10cf447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_plot(decoded_val, y_val, dataset=\"val\")\n",
    "saveName = \"validationErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "342bbd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_plot(decoded_test, y_test, dataset=\"test\")\n",
    "saveName = \"testErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8978795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "714cd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tTrain = t[index_train]+10\n",
    "tVal = t[index_val]+10\n",
    "tTest = t[index_test]+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fac613c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(701,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58d8635f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9997666], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4a78a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013505124096231176"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_error(predicted, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1d9be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "iTrain=[]\n",
    "iVal=[]\n",
    "iTest=[]\n",
    "for i, index in enumerate(index_train):\n",
    "    iTrain.append(y[index])\n",
    "for k , index in enumerate(index_val):\n",
    "    iVal.append(y[index])\n",
    "for j, index in enumerate(index_test):\n",
    "    iTest.append(y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ab1b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iTrain = np.array(iTrain)\n",
    "iVal = np.array(iVal)\n",
    "iTest = np.array(iTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87c41222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cm_trainTestSplit_Plot(i, Cm, cm, tTrain, tVal, tTest, iTrain, iVal, iTest):\n",
    "    \n",
    "    title_0_Cm = 'Gurney flap not attached (NACA0018)\\n$C_m$ prediction, $L_2$ error=%.4f' % l2_error_Cm    \n",
    "    title_n_Cm = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_m$ prediction, $L_2$ error=%.4f'%(l2_error_Cm)\n",
    "    \n",
    "    if i==0:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    else:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    \n",
    "    # CD graph plot\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm), 'k--', label='Predicted value')\n",
    "    plt.scatter(tTrain, denormalize(iTrain), color='b', label='Training set')\n",
    "    plt.scatter(tVal, denormalize(iVal), color='g', label='Validation set')\n",
    "    plt.scatter(tTest,denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylim([-0.05, 0.22])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "893cf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0] *= 5\n",
    "x[:,0] += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf177275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 error of Cm: 0.0135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEwCAYAAACQSIdYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABn5UlEQVR4nO2dd3wVVfbAvycNCCVAQJSSEBRFaigWBAuCWLB3fCjIIs2C+nMtZFexwLrqqugKCC6iEsHeURGERcQCKII0WwrFFXhISIHU+/vjzkteHi/9JXlJzvfzmc+8uW3unZk3Z+69554jxhgURVEUpaqE1HYFFEVRlPqBChRFURQlIKhAURRFUQKCChRFURQlIKhAURRFUQKCChRFURQlIKhAURRFUQKCChRFURQlIKhACSJE5FIRWSoibhHJEZFdIrJYRAbVdt0CiYjc77StQEQWONu62q6XNyJytYiMKW94AM9bbddCRHqKiBGRs2qxDt1FZLmIZInIbhF5SERCq5pPRK4UkTXOf+ewiGwXkb+JSEQV69tLRJY45bpF5B0ROaqKZV4qIhtFJFtEkkTkTj9pKnWdahsVKEGCiDwFvAXsAsYBw4B7gebAahE5tharFzBEZADwIPBvYBDwcO3WqESuBsZUIFwpAxFpBSwDDHAJ8BDwf9jnoar5ooEV2P/O+cB8IAF4sgr17eCUaQAXMAk4A7ijCmUOAt4GvgUucur5TxG53StNpa5TMBBW2xVQQEQuAW4HbjTGLPCJfkVELgIOVfEcoUCoMSanKuUEgG7O/jljzEEAEanF6ig1yESgCXC5c+8/E5EWwDQReczzPFQmnzHmeZ88K5w0N4vIraZyNqZuAw46580GEJGx2I+8ynI/sNoYM845XuoIkPtFZJbz/6zsdap1tIcSHNwOrPUjTAAwxnxgjNkNICIrReRN73gROcsZyujpFbZARNY53evNwGHgFK/wc5xud6aIrBaRHj5lDhaR/zpdbreIzBOR5l7xI5whqziffHFO+MW+7RCRBcArzmFaacMvIjJQRN53uvuZIrJBRFy+5Xm1cZsz1LFaRLr7K7O8ZTv1vAI406mjEZFpJYWXt75OujNEZIWIZIhImnM/+/pJV6X746SZLCI7nDI+AI4p7bpUtA6V4HzgU58X4mLsy/PMasjnBqoy5DUCeMdLmLQCBgNrq1BmPLb34c1SoBUw0DmubHtrHRUotYyIhGEfpKXVUHxn4DHgH8AFQJITHgM8DkwHRgJHAa+L01VwuuXLgf8BV2IF3gXAi15lfwLsBkb7nHMMsBdY4qc+DwOPOL/Pxrb7uxLqHgt8iR3CuAg7HPiiiIz0k+5Jp+zrgCjgUxFpXEK55Sn7YexQx/dOHQcCL5QSXq76OsJzOZCLvW7XAF8AHXzqV+X74/R6nwM+BC4HNmGHV8pLWXUQEQkra/MpsxuwzTvAGJMKZFHUc/VHufOJSKiIRIrIYGwPY3Zleici0hQ4EVgrIs1F5HTsM78TeM1JU5lr0BjwHSXIdvYnVrS9QYcxRrda3IB22LHSCT7hgh2S9GzihK8E3vRJe5ZTRk+vsAVOWLxP2gVAHtDVK+xSJ2035/gLYIVPvrP9nOMRrJASrzonA0+U0t4xTjnNfOq0rpQ8nmvxPPC5nzae5hUW67RvYjmvf0llvwms9JPeb3g5y/wKWOe5XiXkDcj9wY7Rf+yTZp6T5qwy6l+eOnjuY6mbT7m5wO1+zrcTmFFKfcqdD9sT95z/JSCkkv/LgU4ZJwD7nd+HgVP9PMsVuQbrgbd8wu5x0k6tynUKhk17KLWPZwLB9yvq/7APlme7uRJl7zLGbPATnmyM+dnreIuz7ygikdg/0+s+X1mrnXr098o3H/sCP8s5HuIce/dkKoWItBKRZ0QkhaJrMB443ifpHmPMGs+BMSYF+6c9OQBlB6y+zhfvKcBLxnk7lEKV7o/Y+bK+wHs+5b5dgSaVWAdn/wFwUjk2X/y1XUoIr0y+04DTsf+fS7DKH5UhHsgAfsP2AidiP54+EpGjnTSVuQZzgEtE5CbnmTnXqStAvle6yl6nWkUn5Wuffdgub0ef8FewvRGo/JjtHyWEH/A59nTBG2PHckOBWc7mSyfPD2PMbyKyErgROxR0I/CtMWZzJevrzQLgVOww0xbs5Ogk7EvCmz1+8u6h9PmC8pYdyPq2wr4Qfi9HWQd8jit6f9pi/9u+18bftapMHcB+tadVoDyAP4GWfsKj/JyvUvmMMZ4h1NUisg94SUT+ZYz5tYJ17Qv8YIzJBT4HPheRz4GfsPMYr1G5azAf6APMBuZih7HuAZ6l6P9a2etU66hAqWWMMXki8hUwHKsB4gn/A+cBk+JaUIc5cqKxdUnFV6JKB5x80/A/D7Lb5/gFYJ6I3Icdq/+/I7NUDGf+YwRwizFmjle4vx61vzUBRwF+hVoFyw5kff8ECqjgxLgfDlD2/dmLHbLyvTZVWj/hw2jK1xP1fni3ceScRyegKT5zBj5UNp9HuMQBFRUo8cA3PmGHnb3nxV/ha2CMyQduEZG/Yz8ikyhq29fOvrLtrXVUoAQHTwPvisj1xphXyki7E6sL7805gaqIMSZTRL4GTjDGPFSOLG9jJ38XY5U8FgegGo2wX+GeyUocDaaLOVJIHiUip3mGvUQkBuhHyX/08padQ9HXOGWEl1mmc12/AW4QkX+XY9jLL+W9PyKyAds7muMVfHllzlkCnuGeivAx8FcRaW6MSXfCrsGqxP+3GvJ5FgQnVaSSzpBhT2wbvXFheyWrnePKXAMAjDF/Yj8yEJHJwBpjjEdYVLa9tY4KlCDAGPOeiDwNLBCRIdgHdR92sZZHWGQ4+3eAv4hdCPkRdt7i3ABX6W5guYgUYCeh07FaPyOABGPMT151Pywiidg5nkXGmANVPbkxJk1E1mJ18w9iv+zvxQ4vtPBJvg+7Vufv2D/cQ9ihnQVVLHsbdqz7UqwQ322s6rbf8HKWeS9WZfRjEZkLZGLnQ9YZYz6swCUqz/2ZAbwtIrOxz8yZwHkVOEepGGPcWLXcijAHq3n1toj8E+iC7Wk9aYrWJN2AHRY61pkPK2++T7DXdjN2LmIQtrf8mvdwl6NptwIYYoxZWUI9u2FVdO8WETewFasunABMMsbkVfYaiMipTlkbsM/GSOz/d3BFrlPQUttaAboVbcBlwGfYr6Bc7PDFW8D5PunuA3ZgXyQLKfoS9tXyOkJzyl84Vr3YABd6hZ2CVZM8iH3xbcGq50b5KXOYk39YOdo4hnJoeQHHYceuM4FU7Et0GrDPNx/2y/snbA/hS+/rUEIdylN2G+yL2KPhM62M8DLLdNKdCazCjp0fwL7c4qvj/gC3YIVeFnZ4bDjl1/Iqsw6VfMa7O9fpEHY+6WHsglvf56NzBfM9DPyI/fA6gB3uuhUI9ynnAqf87qXU0YXtib7sXN807HDUFQH4j/fHzolmOGV/BPSq6HUK1s2j7qkolUZEHsN2yeOMMQU1eN4FWOExoKbOqdRtRORB4AxjzJBS0jwODDfG9Km5mtUPdMhLqTQicgL2S2oS8GBNChNFqSSnUbZ9r77YxatKBVGBolSF57FDL+8Dz9RyXRSlTIwx5VFg6YO1MKBUEB3yUhRFUQKCrpRXFEVRAoIKFEVRFCUgqEBRFEVRAoIKFEVRFCUgqEBRFEVRAoIKFEVRFCUgqECph4hIuIjcISLfinUze0hE1jthVXGJWmuISE/xcRksjgvgCpRxtYiM8RNeoXKqCxF5VkRKcjnQIBGR7iKyXKyr490i8pBjvLHKeUXkOBF5XkR+EJF8xxWDv3KuFJE1Yl0tHxaR7SLyN+//koiMkSK30N7bxCpfhDqELmysZ4j1e70MOBbrY8FjEv984FFgF/B67dQu4DyMNeJXXq7G2uJaUMVyqoteWFe9CsWe5S1Yy8nHAv/Cfgj/LQB5e2Bte31N6b7no7E21x7H2gk7GWun7WisvTRvzsba3/LwW2n1rG+oQKlHiHWc8jbQHuuq1Nt3wici8goVtxAbqLqFYo3b+frTrjSm4k6TqrWcANAT61itVijpHlX13lUh/0SsoL/cWCu7n4lIC2CaiDxmSre8W568Hxhj3nPq+Cb2Y+MIjDHP+wStcMq6WURuNcVXh681xmTQQNEhr/rFaKw73ok+wgQAY8w6Y0yFfEP44hkeEpFLRWSbMwSwWkS6l5JuM9Y50SlO3GAR+a8zFOEWkXli/Yd4558sIjtEJFNEPsCPYyp/Q1UicoaIrBCRDGe4b6WI9HUMSV4BnOk1HDGtlHKuFpFNIpLt1GO6WFe7vu07R0Q2OvVcLSI9Knld22O/hAPWQynrOpd0j8q4d6Vel9LKrUQTzgc+9REci7GC4syq5q2i7Tk3pfdqGiQqUOoXdwJbPV9d1Ugs1sDew8B1WNekn4r1XOhNZ+Ax4B/YoYUkERkELAf+h/XVfbsTV+gQS0QuwTrt+hBrmn4T1kdGqYidX1mONf0/GmsB+Qugg1PXFVijfwOd7YUSyhmOdfH6HXa45FngLo70Tx6DHQaZjvVrcRTW17tQcXo5+4AIlPJcZ4fO+NyjksIrcF1Kyi8iElbW5lVGN3w8FBpjUrHm+It5NPRDVfL6RURCRSRSRAZj/ZXMNkfarvpVRPLEzrNMqMx56jS1bT9ft8Bs2Je8wTpYqs7zLHDOc5rPufOwPSPfdPE++b8AVviEnY2XPxfgW+BjnzTz8PHlgY/fDuArrH8UKaHubwIrS2iTdzlf+6nj3VjHTR298uQBXb3SXOrUsVslrutdTvmRAbpP5bnOJd2jksLLvC5l5B/jhJe6eaXPBW7307adwIwy2l+hvCU9Gz5pDnvV8yUgxCvuXOzczHBs7+hlJ90dgbifdWXTHkr9wfOF+2MNnGuPcVzuAhjrWW89drLSm13GmA2eAxGJxPYMXvf5Il2NfQH0d8bb+wK+vay3S6uQiDTFDqu8ZJx/eGVwzt8PeMMn6jVsj36gV1iyMeZnr+Mtzr5jJU7dC/jNGJPlp06dxGorbRWRzSLyWGm9oPJcZ6/kxe5RSeEVvC4lletxmVvW5o2/eyklhPtSlbz+OA04HesJ8hK8embGmE+NMY8YY5YaYz42xtyAVX75m4g0mPesTsrXH6KcfU2one4pIcx3nsO3Lq2wvtdnOZsvnYC22OfS9xz+zulbtmC921WFNkA4R9bdc9zaK+yATxrPpLM/X/RlUZqGVx5wjzFmnVhV1c+wQ4FvlZC+PNfZQ0nPi294Ra5LSeXux3o/LC9/Ai39hEdx5LUPZF6/GGO+c36uFpF9wEsi8i9TslLHm1jNws40EG0vFSj1B88Lt31ZCUXEo7XSFTuePBU7/n859oU+wviZ1PfiqBLCNvuE+X4JHnDCpmFd0vqyG9iLfYH6nsPfOb35E+vL/YjJ+wqyD/sV73u+ds5+fxXLPwLn6/9E7Bf8ERhjfscRlMaYHBHZSHGh4MsByr7OhcWXUIZveEWvi79yR3PkHI4/PL2vbfjMd4hIJ6ApPvMjfqhK3vLgES5xQFlagg3GR0iD6Yo1AL7C+qi+0V+kM5HoIR6rKz8UO6n+LLDJGHMqdkjj8jLOdZSInOZVdgx2OOTb0jIZYzKx4/AnGKtx5rvtNsbkAxuwQwrelFonp+xvgBtKGQ7KoYzeg3P+9cBVPlFXYwXWV6XlryRdnXqVOSEvItHYuZpPS0pTnutc0QoG6LpUdMjrY+BcHw3Aa7DP7n/LOFdV8paHQc6+NK3JK7CCOCUA56sTaA+lnmCMyRCRe4DZIvIedj3DXuyCrquAFsAgZzz3OGCoMcaIiAG+NsZ87BQVQtlf4fuAV0Tk79g/6EPYHtKCclT1bmC5iBRghwTSsdpSI7AKBT8BM4C3RWQ28A5WzfO8cpR9L3Yx28ciMhfIxI7trzPGfIj9Mr1ERC7FTs7uLuHl+gBWa+1FrKppL6yW2DxjzM5y1KMQR/NsBTDEGLOyhGSe+a+OTt28+cE4qt4i0gh7zZ42xmwt49Tluc4VpUrXxRjjpmLroOZgtaneFpF/Al2wva4njZc6sIjcgNUCPNaZzytXXmeu6QInfQeghYhc6Rwv8cxnicgn2OdqM1YBYRB2HuU1z3CXiLyF/aDaiB1uvMbZbjMNyTV2bWsF6BbYDftl/wWQ4WxbsH+uk534E4FvvNLfhvUH7zn+FC8NLj/lL8BqUl0O/ARkA1/iaA75piuhjFOAT7A9qkynjk8CUV5pbsG+9LOwwzbDKUPLywk7E1jl5DuAfZnHO3FtsAJqv1PWtFLKuQbbY8hx6jEdCCvj3J2dci/0CrvACeteyjV9iJK1ni520oRiBcOTFXgWSr3OJd2jMu5dqdelrPyVeJ67A59jP1x+xwqwUJ80Y5xr1bkieb3ul7+ts1e6h7HKLhnOM/UdcCsQ7pVmBrDdee4OYXtz19fGO6A2N3UB3MAQkZHAmcaYic7xi8B7xph3nePdwPGmhNW+YhcI9jTGDKiZGtdtRORB4AxjzJAqlvMCVqiMNfqnVYIUnUNpePTBzlF46Os5FpGjgcyShIlSKU7D9goqjbNI8S/AAOB7EdkgIrcFonKKEki0h6JUCO2hKIpSEipQFEVRlICgQ16KoihKQGjQasNt2rQxnTt3rlTezMxMmjZtGtgKBTna5oaBtrlhUJU2r1+/fp8xpq1veIMWKJ07d2bduso56lu5ciVnnXVWYCsU5GibGwba5oZBVdosIn4Xa+qQl6IoihIQVKAoiqIoAUEFiqIoihIQGvQcij9yc3PZuXMnhw8fLjVdVFQUW7eWZU6pflEX2ty4cWM6duxIeHh4bVdFURocKlB82LlzJ82bN6dz586U5sk1PT2d5s2blxhfHwn2NhtjcLvd7Ny5k7i4uNqujqI0OHTIy4fDhw8THR1dqjBRghMRITo6uszepaIo1YMKFD+oMKm76L1TlNpDBYqiKIoSEFSgBCF//PEH1113HV26dKF///4MHDiQd955p0brkJycTM+ePf2Gv/rqq5Uq8+mnnyYrK6vwuFmzZpWun6IowYcKlCDDGMOll17KGWecwW+//cb69etZvHgxO3ce6RAvLy+vxutXmkApqz6+AkVRlPqFankFGZ9//jkRERFMnDixMCw2NpZbb70VgAULFvDRRx9x+PBhMjMzefPNNxk7diy//fYbkZGRzJ07l969ezNt2jSaNWvGXXfdBUDPnj358MMPATj//PMZPHgwa9asoUOHDrz33ns0adKE9evXM3bsWCIjIxk8ePCRlQPuvfdetm7dSnx8PKNHj6ZVq1bF6nP//ffzxBNPFJ7rlltuYcCAARw8eJDdu3czZMgQ2rRpw4oVKwBISEjgww8/pEmTJrz33nu0a9eu2q6toijVS1AJFBE5D5iJ9Uz3gjHmUZ94F3CPc5gBTDLG/FCevJXh9ttvZ8OGDX7j8vPzCQ0NrXCZ8fHxPP300yXGb968mX79+pVaxldffcXGjRtp3bo1t956K3379uXdd9/l888/54Ybbiixzh5+/vlnFi1axLx587j66qt56623GDVqFDfeeCPPPvssZ555Jn/961/95n300UeLCYwFCxYUq8/KlSv95rvtttt48sknWbFiBW3atAGscbpTTz2V6dOnc/fddzNv3jz+9re/lVp3RVGCl6AZ8hKRUOA54HysL+iRItLdJ1kS1n1tb6yf57kVyFsnufnmm+nTpw8nnXRSYdg555xD69atAVi9ejXXX389AGeffTZut5u0tLRSy4yLiyM+Ph6A/v37k5ycTFpaGgcOHODMM88EKCyzPHjXpyJERERw4YUXFquHoih1l2DqoZwM/GKM+Q1ARBYDlwBbPAmMMWu80n8NdCxv3spQWk+iuhb59ejRg7feeqvw+LnnnmPfvn0MGFDkINHb5LQ/B2kiQlhYGAUFBYVh3mszGjVqVPg7NDSUQ4cOYYyptMqtd31KO68v4eHhhecMDQ2tlTkhRVECRzAJlA7ADq/jncAppaT/C/BxRfOKyHhgPEC7du2OGKKJiooiPT29zMrm5+eXK11FOemkk8jMzOSpp55i3LhxAOzZswdjDOnp6Rw+fJicnJzCc5966qnMnz+fe+65hy+++ILWrVsjIrRr145PPvmE9PR0NmzYQFJSEhkZ1lV8QUFBYf7s7Gyys7MJDQ2lefPmLF26lIEDB/Liiy8WS+dpc0hICAcOHCgM961PdHQ0mzdvZt++fRw+fJhly5YxYMAA0tPTadq0Kb///nsxgebJd+jQIXJzcwNyTQ8fPlzi0FtFycjICFhZdQVtc8OgOtocTALF3+exX//EIjIEK1A8M8flzmuMmYszVDZgwADj6w9g69at5ep5VKcZkg8++IA77riDZ555hrZt29K0aVMee+wxmjdvTuPGjYmIiCg894wZM7jxxhsZNGgQkZGRvPLKKzRv3pxRo0bxxhtvcPrpp3PSSSdx/PHHF6rphoSEFOZv1KgRubm5NG/enJdeeqlwUv7cc88tls7T5oEDB9KoUSMGDx7MmDFjaNWqVbH6nHjiiVxzzTUMGjSIrl270q9fPxo3bkzz5s2ZOHEiV111Fcccc0zhpLwnX5MmTQgPDw/INW3cuDF9+/atcjmgfjIaCtrmAGGMCYoNGAh86nV8H3Cfn3S9gV+B4yua13fr37+/8WXLli1HhPnj4MGD5UpXn6grbS7vPSwPK1asCFhZdQVtc8OgKm0G1hk/79SgmZQH1gJdRSRORCKAa4H3vROISAzwNnC9MeaniuRVFEVRqpegGfIyxuSJyC3Ap1jV3/nGmM0iMtGJnwPcD0QDs5zJ3DxjzICS8tZKQxRFURooQSNQAIwxS4AlPmFzvH6PA8aVN6+iKIpScwTTkJeiKIpSh1GBoiiKogQEFSiKoihKQFCBEoSEhoYSHx9Pz549ueqqq6pkoXfMmDG8+eabAIwbN44tW0o2HrBy5UrWrFlTYnxJdO7cmX379lW6joEuR1GU2kEFShDSpEkTNmzYwI8//khERARz5swpFp+fn1+pcl944QW6dy/ZxFllBYqiKAqoQAl6Tj/9dH755RdWrlzJkCFDuO666+jVqxf5+fn89a9/5aSTTqJ37948//zzgF2oesstt9C9e3dGjBjBnj17Css666yzWLduHQCffPIJ/fr1o0+fPgwdOpTk5GTmzJnDU089RXx8PF988QV79+7liiuu4KSTTuKkk07i66+/BsDtdjN8+HD69u3LhAkT/NoTmz17NnfffXfh8YIFCwpN8F966aX079+fHj16MHfu3CPy+jr3euKJJ5g2bRoAv/76K+eddx79+/fn9NNPZ9u2bVW8woqiBIqgUhsORvyZJrj66qu5/vrrycrK4oILLjgifsyYMYwZM4Z9+/Zx5ZVXFouriO2cvLw8Pv74Y8477zwAvv32W3788Ufi4uKYO3cuUVFRrF27luzsbAYNGsTw4cP5/vvv2b59O5s2beKPP/6ge/fujB07tli5e/fu5aabbmLVqlXExcWxf/9+WrduzcSJE4v5ULnuuuu44447GDx4MKmpqZxzzjls376dBx98kMGDB3P//ffz0Ucf+RUKV155JQMHDuSxxx4D4LXXXiMhIYGUFLj11vnERRlaH/6VwaNHcuGxx3O4U0ty8nPYsGszjXNblHhNxo8fz5w5c+jatSvffPMNkydP5vPPPy/3NVUUpfpQgRKEHDp0qNC8/Omnn85f/vIX1qxZw8knn0xcXBwAS5cuZePGjYXzI2lpafz888+sWrWKkSNHEhoaSvv27Tn77LOPKP/rr7/mjDPOKCyrJNPzy5YtKzbnkp6eTnp6OqtWreLtt98GYMSIEbRq1eqIvG3btqVLly58/fXXdO3ale3bt9OsQ0f2hq3j/ffmsvKTlQDs+OMPUlKTOCWqFxH50GdvLvuzd5CbnXtEmRkZGaxZs4arrrqqMCw7O7usy6koSg2hAqUMSupRpKenExkZWWqPo02bNpWy5umZQ/HF12z9s88+y7nnnlsszZIlS8o0Q2/Kaaq+oKCAr776iiZNmgDFDWKWJ/8111zD66+/Trdu3Rg87EzyGu1j/Zr1fLvqWxbOn09248ZMmDCBjTk5NAMKsFY+mxJKaH4uKd+7ie0bXWgCv6CggJYtW5bpQExRlNpB51DqKOeeey6zZ88mN9d+yf/0009kZmZyxhlnsHjxYvLz8/n9998Lrfp6M3DgQP773/+SlJQEwP79+wFr+dfbfPzw4cP597//XXi8ceNGAM444wwSExMB+Pjjj/nzzz/91vHyyy/n3Xff5YX5LzP0krMAyNiTQXjz5uQ1bszvycn8+OOPhAPZQB7W70C76Gj27N9PU/f3/PTN7kLvkC1atCAuLo433ngDsILxhx9+qNT1UxQl8KhAqaOMGzeO7t27069fP3r27MmECRPIy8vjsssuo2vXrvTq1YtJkyYVemD0pm3btsydO5fLL7+cPn36cM011wBw0UUX8c477xROyj/zzDOsW7eO3r170717d+bPnw/AAw88wKpVq+jXrx9Lly4lJibGbx1btWrFsZ2OY+/OJG5o14MWv8PA+IGY/HxuGDmSl+fMYWDPnnQBejh59gF/hIXx93HjOPXGG5l822V0OqZjYZmJiYn85z//oU+fPvTo0YP33nsvkJdVUZQqIP40dBoKAwYMMB6tJw9bt27lxBNPLDNvdfpDCVYq2ub0FDeRe1MIpYBkrLAIB9pFQttDEOrz6BkgFdgLRAFdnfDsUEhuE8cJnaLLdd7y3sPyoH4yGgba5oohIuuNMQN8w7WHolQbEXt3EUoB+7HCJBToCbTKhpQoKygMkCtF8ycxQHMgDfjdU04+pLMLt7sWGqEoSrlRgaJUC9u3QwQ5FADJTlg3rFCJyIf9kbApqjmpbQewu1McyS0FgxUqxzr7XUCOk7fP3hwOJqtEUZRgRgWKEnDcbkgPSSEnFP7A9j6OApo48TkhwJ9xtA09gdhYiD0qmqj2nUmKCiNfrOphrJP2Z6xwCS+AWJOMdlMUJXhRgaIEnJQ9bmi6lx3N7LBVFHYoCyBfYFdBHG2bRRMbW5QnOjKaLl3jSQlviwHaAI2BQ9jhL4AQDLkpO2quIYqiVAgVKEpAcbuhZegOev0BeWm2d3KM2LmS7FA7d5LbvLgw8SaqQ1HEsc5+NzY/QFhBnnZSFCVIUYGiBJT0FDex6Xnk5UMGtpfRBEhqCZvawZ+NIzjhhJLzR0dDnkSAk68TkAl4ZEhOKCT/oRJFUYIRFShBhtvtJj4+nvj4eI4++mg6dOhQeJyTk1Nq3nXr1nHbbbeVeY7TTjstUNUthtsNxxTsItRAihMWi1UP7pAOLz7zIp1bdyiznPDOHShwfh+FFUq7sL2dXc3BRKXgzlKhoijBhppeCTKio6MLTYtMmzatmLFGsAYjw8L837YBAwYwYMARquFHUF0m6nfsgD7kkA1kARFYFWCwml0vPruA5x59ruyCoqPZfzCDqP17CTO2p/InsK0JZEUCFJD85w6iI8u3LkVRlJpBeyhVJDEROneGkBC7dyySBJQxY8Zw5513MmTIEO655x6+/fZbTjvtNPr27ctpp53G9u3bAbtQ6cILLwSsMBo7dixnnXUWXbp04Zlnniksr1mzZoXpzzrrLK688kq6deuGy+UqNEW/ZMkSa4Nr8GBuu+22wnK92bx5MyeffDLx8fH07t2b3/auJCcUnl2yhNGjR+O67jomzJhBfn4+d//7ObIPHyY+Ph6Xy1Vmm9vExfJD4zjWt4c/j7ZhWV52II3kaS9FUYIM7aFUgcREGD8ePA4VU1LsMUA53pkV4qeffmLZsmWEhoZy8OBBVq1aRVhYGMuWLWPq1Km89dZbR+TZtm0bK1asID09nRNOOIFJkyYRHh5eLM3333/P5s2bad++PYMGDeLLL79kwIABTJgwodC8/ciRI/3Wac6cOUyZMgWXy8XvB37nV/evrPw5ifc/+4yX/vMfBoSFMfnRR3nlk0+4+bYnmP3mGxUy7Ni2WTR783dBaI5dYp+LXZhip1hI+XOX9lIUJYhQgVIFEhKKhImHrCwbHmiBctVVVxEaGgpYU/WjR4/m559/RkQKDUT6MmLECBo1akSjRo046qij+OOPP+jYsWOxNCeffHJhWHx8PMnJyTRr1owuXboUmrcfOXKkX58nAwcOZPr06ezcuZNug7vTIe4Y3v96LVu3bWPM6BsIM5CVk014+9ZcFFvxF39sLOzd3AFaJVnd433AQaxOMVAgpc8pKYpSs6hAqQKpqRULrwrepuv//ve/M2TIEN555x2Sk5NLtMfTqFGjwt+hoaHk5eWVK0157btdd911nHLKKXz00Ufceu1E5v7tb4RnGi4aMYKEv93Cn5FOwvwIoivZkWjbLJq9BTsgIs8O0OZgZ+dDbLluN5UuW1GUwKJzKFWgBCO7JYYHirS0NDp0sNpSCxYsCHj53bp147fffiM5ORmw3hb98dtvv9GlSxduvMTFZaefwcbtP9PnpJNY/vnnRCbtp3UWpP2ZRvYeq7MVHh5eYm+qJGJjgbROYELA4wcs09lLgaoQK0oQoQKlCkyfDpGRxcMiI214dXL33Xdz3333MWjQIPLz8wNefpMmTZg1axbnnXcegwcPpl27dkRFRR2R7rXXXqNnz54MPmcQ25OSGTJiBF26dGHaxImcf/MtXHL+SG655nZCc62DrPHjx9O7d+9yTcp7E5IdDQdiISzMzp94hhlD8lSFWFGCCDVfX0Xz9YmJds4kNdX2TKZPD/z8SW2QkZFBs2bNMMZw880307VrV8aNG3eE+Xq3G1on2Wv4HdbuVj8nzgA/hA3A8WZcadxuSEoC2m2k0Z85ZOfYVfSRoXZdysGmEcQf07swvZqvrxra5oaBmq8PQlwuSE6GggK7rw/CBGDevHnEx8fTo0cP0tLSmDBhgt90O3ZAjoSxHytAWnrF5RBBp05Vr4tnjqR1dg5dnHn4/wGN8iE2DVpk6uS8ogQDOimv+OWOO+7gjjvuKBbm7R7YQ164m13N8sg5YI/bO+H5CLulA3EBmjBv2xaOdkMj7EObiRVgoQY6HEQn5xUlCNAeilI1onawvwlkil0V3wiPEUihRefAveFjYyHCscfSygnb5+wjCuwaIEVRahftoSiVxu0GQvIgG4yB9Faw3uP0hAK6BLjHUBAWQWheDsdg3QT/D2iLHVorKCg9r6Io1U9Q9VBE5DwR2S4iv4jIvX7iu4nIVyKSLSJ3+cQli8gmEdkgIut88yqBZ8c+R7vKMxLWqMSkASG0UwfyCSECO1eTDeQQwi6sCrWatVeU2iVoBIqIhALPAecD3YGRItLdJ9l+4DbgiRKKGWKMifenfaAEnrzIXfZHDla9y+tpCguphs5vdDQpxJJNBI55L34lmv3YrtAO9b2lKLVK0AgU4GTgF2PMb8aYHGAxcIl3AmPMHmPMWqxVp3rJWWedxaefflos7Omnn2by5Mml5vGoP19wwQUcOHDgiDTTpk3jiSdKksOWd999ly1bthQe33///SxbtsxvWrcba2PLswymuIkwOrWovHrXjBkzSozbTzSb6M02+gOhZJJRGOfHEICiKDVIMAmUDoD3N+ZOJ6y8GGCpiKwXkfEBrVkNMnLkSBYvXlwsbPHixSUaaPRlyZIltGzZslLn9hUoDz30EMOGDfObdscOID+iaNV6Y6/IgrAqGW0sTaBERHh+CVaKHQKKJIkOeylK7RFMk/LiJ6wiqy4HGWN2i8hRwGciss0Ys+qIk1hhMx6gXbt2rFy5slh8VFSUX/VYX/Lz80lPT+f1ra/z4OoH2Zm+k47NO/LA4Ae4+sSrK1Dt4px77rkkJCSwb98+GjVqREpKCrt27aJPnz6MGzeO7777jkOHDnHJJZeQkJBQWJfMzEzS09Pp2bMn//3vf4mOjubxxx9n0aJFdOzYkejoaPr27Ut6ejoLFizgxRdfJDc3ly5dujB37lw2bdrEe++9x8qVK3nooYd45ZVXeOyxxzjvvPO49NJLWblyJQkJCeTn59OvXz8emfE4jSLb0e+UvowYMYKvv/mavLw85s6fx/HH9ih2Dbdu3cqkSZPIzc2loKCAV155heOOO47FixczZ84ccnNzGTBgAE8++SQPPfQQhw4donfv3nTr1o3//Oc/xa5Pp06Q7ZixT09vRlraYVq02EGLFtZiZF4eHD58+Ij7WlkyMjICVlZdQdvcMKiWNhtjgmIDBgKfeh3fB9xXQtppwF2llFVqvGfr37+/8WXLli1HhPnj4MGDZuHGhSZyeqRhGoVb5PRIs3DjwnKVURIXXHCBeffdd40xxvzjH/8wd911lzHGGLfbbYwxJi8vz5x55pnmhx9+MMYYc+aZZ5q1a9caY4yJjY01e/fuNevWrTM9e/Y0mZmZJi0tzRx77LHm8ccfN8YYs2/fvsJzJSQkmGeeecYYY8zo0aPNG2+8URjnOT506JDp2LGjWb9+vTHGmKtHXm3umPZ/Zu3OteaYY44xd919l1m7a625+5H7zCWXu4xX8cYYY2655RazcKG9JtnZ2SYrK8ts2bLFXHjhhSYnJ8cYY8ykSZPMSy+9ZIwxpmnTpqVen/XrjVm71pi1a/PM2rVrzdq1G51ju5X3HpaHFStWBKysuoK2uWFQlTYD64yfd2owDXmtBbqKSJyIRADXAu+XJ6OINBWR5p7fwHDgx2qrqUPC8gSycovbr8/KzSJheUKVyvUe9vIe7nr99dfp168fffv2ZfPmzcWGp3z54osvuOyyy4iMjKRFixZcfPHFhXE//vgjp59+Or169SIxMZHNmzeXWp/t27cTFxdH165dATj70mF8/836wpmsIRcNAeDEXt3ZveN/RywwHDhwIDNmzOCf//wnKSkpNGnShOXLl7N+/XpOOukk4uPjWb58Ob/99lu5rk9srOdXKEgYkA1Hr4N2G6GJm8zMUjIrilJtBM2QlzEmT0RuAT4FQoH5xpjNIjLRiZ8jIkcD64AWQIGI3I7VCGsDvCMiYNv0qjHmk+quc2qafzv1JYWXl0svvZQ777yzcHirX79+JCUl8cQTT7B27VpatWrFmDFjOHz4cKnlONfjCMaMGcO7775Lnz59WLBgQZndXuNj7y3fOJLkkN1FtLATGyHhBRQUHDkz7m3m/txzz+WFF17AGMPo0aP5xz/+Ueq5/REd7dj2auKG0HzIcOrSNAdaprB/ry5KUZTaIJh6KBhjlhhjjjfGHGuMme6EzTHGzHF+/88Y09EY08IY09L5fdBYzbA+ztbDk7e6iYnyb6e+pPDy0qxZM8466yzGjh1b2Ds5ePAgTZs2JSoqij/++IOPP/641DLOOOMM3nnnHQ4dOkR6ejoffPBBYVx6ejrHHHMMubm5JHr5LG7evLnf+aNu3bqRnJzMr7/+itsNy1/7hAt79EOcnkBLj1zLD6eRn7UoHjP3t912GxdffDEbN25k6NChvPnmm+zZsweA/fv3k+Isdy+PmfuwMKDFLmhu7OybJ7kUUBD+Z6l5FUWpHoJKoNQ1pg+dTmR4cfv1keGRTB9adXk2cuRIfvjhB6699loA+vTpQ9++fenRowdjx45l0KBBpebv168f11xzDfHx8VxxxRWcfvrphXEPP/wwp5xyCueccw7dunUrDL/22mt5/PHH6du3L7/++mtheOPGjXnxxRcZPXo0g07tTos8GH3ZFRjsA9QpHVpnCWS2xcfDMFBk5j4+Pp5t27Zxww030L17dx555BGGDx9O7969Oeecc/j999+B8pm579QJq7YsOPZeKFLhCMnHS04qilJDqPn6qpqv35RIwvIEUtNSiYmKYfrQ6bh61ROTwz6kp6cTsT2JRuTwK/AncDx2/DFbwtgaGl9lU/UVYd2OjRCaQ2M3HM6G44AmofDtwf1cdsVw9u0rs4gyUbPmDQNtc8UoyXx90Myh1FVcvVz1VoD4IwJrKj4d2zlo4Qk3eQExVV8RwrI60CI0mTbZhp+wxiKPy4fWhwoY7k4EGs59UZRgQIe8lHLjWYmej11K2NgnvqbNx3dqE02HtFBaYIWbZ818CDCDqmnaKYpScVSgKOUmx/Fj5Xlxt621mliio23PCCASK+QcxTNiSNF5FEWpYVSgKOXGGGsq/iC2R+DdISkIiyghV/WSgz2vR7jtcfapLYQpL6hEUZSaRAWKUiF20QE3tkcQ6oTlE0Jop4qYXQscf4R1IF+scBNsL6UAmDrM4D5pSq3USVEaKipQlHLhMbq4nybkASEIBsgmghRia83/btNO0aREQU6oVRDIAvZHwqLeQKSbxE3aS1GUmkIFSpDhdruJj48nPj6eo48+mg4dOhQe53gmMUph5cqVrFmzpsr1OHDgALNmzSo83rED652xyTYA0luEsr5JHJvoXeiPpDaIjob9jSLY1A7SmtvlKJmep1qoshkcRVHKjwqUICM6OpoNGzawYcMGJk6cyB133FF4HBFR9jxFdQmUvHC3XUiY45g1aZoHLVOgiduuWq9FwrKc4TZPPbKL4lLS1Nm8otQUKlCqSmIidO4MISF2Xw2qRevXr+fMM8+kf//+nHvuuYUryp955hm6d+9O7969ufbaa0lOTmbOnDk89dRTxMfH88UXXxQr57///W9hb8djyh7g8ccf56STTqJ379488MADANx77738+uuvxMfH89e//tWaOcFYneEQ7ISFFECLXTW+/sSXTm2i7Sp5jx5zMXNi/u2ZKYoSeHRhY1VITITx4yHLsTickmKPAUoxG1IRjDHceuutvPfee7Rt25bXXnuNhIQE5s+fz6OPPkpSUhKNGjXiwIEDtGzZkokTJ9KsWTPuuuuuI8p64okneO655xg0aBAZGRk0btyYpUuX8vPPP/Ptt99ijOHiiy9m1apVPProo/z4449s2LABtxvS/lxHqNg3dXMD4Vl2roLQnNqaPikkOhqSdjsHEVjh4sbO1BtD4qbEBrX4VFFqC+2hVIWEhCJh4iEry4YHiOzsbH788UfOOecc4uPjeeSRR9i5cydAob2rhQsXElaOcadBgwZx55138swzz3DgwAHCwsJYunQpS5cupW/fvvTr149t27bx888/F8uXucNNbJp15AVwtIHYNGidBWFSO+rCR5Dv1KOJc/ydsxeY8r7OoyhKTaA9lKqQWoKZ+pLCK4Exhh49evDVV18dEffRRx+xatUq3n//fR5++OEy/Zrce++9jBgxgiVLlnDqqaeybNkyjDHcd999TJgwoVja5OTkwt/t8nYRCmQ7SgGRQKiBDukQ1b521IV9CcvqQF7zJFphTbD0/RLe/hGmDoVFvXQeRVFqAu2hVIWYEszUlxReCRo1asTevXsLBUpubi6bN2+moKCAHTt2MGTIEB577DEOHDhARkZGiSboAX799Vd69erFPffcw4ABA9i2bRvnnnsu8+fPJyPDrn/ftWsXe/bsKVaOx37X/vR0wrGe3AEi8qmS7/hA0qlNNK2zoPNBO+rVDOicBvM+gJEba7t2itIwUIFSFaZPh8ji5uuJjLThASIkJIQ333yTe+65hz59+hAfH8+aNWvIz89n1KhR9OrVi759+3LHHXfQsmVLLrroIt555x2/k/JPP/00PXv2pE+fPjRp0oTzzz+f4cOHc9111zFw4EB69erFlVdeSXp6OtHR0QwaNIiePXvyfzP/TS6QX1CA9wBXDkEy3IWdR+mQbntOjYGvgd1A01yY8Tm6HkVRagAd8qoKnon3hAQ7zBUTY4VJgCbkp02bVvh71apVR8SvXr36iLDjjz+ejRv9f5I/++yzfsOnTJnClClHrip/9dVX7YR8kpu9JAOGKCcunxB20YEuZTWiBomwUzw0xfrb+j9gERCTBgM+nqIT84pSzahAqSouV8AESDCyYwfkEU0Yu4Fs2mBXx++iAwfDgmO4y0NBeBihuXlEYIflVjjhqVHgPuSuxZopSsNAh7yUUskLd0O7jeRJNiLCRq/V8bW9/sSX0I6dyHeWncRhDUWmhdqJeUy1LBFSFMULFSh+aMheLL1xZ7ntanjJAQMRjSMKV8dDrZnvKpnoaJJbQB4FnIFdjnJBP8euFwHV5lYUxQ8qUHxo3LgxbrdbhQqwK32XXQ3vmBBrEdWicHV8MGKMYT+GFfILSafbsLZrIekpGLmuKSmqPawo1YrOofjQsWNHdu7cyd69e0tNd/jwYRo39vVZWL/YfcBZfn4IyIE8k8eBvAMAhKSHs3VrrVWtRLJzsvnh5Yd5+ys4GtiEoz68JBdB3QIrSnWiAsWH8PBw4uLiyky3cuVK+vbtWwM1qj163Hs6pokbZgAh8MT8J7jrp7sgM5qFffYxfHht19A/Hd8wtMiDBOBvwP+Ao00O00kgMdFVn3UoFKVW0SEvxS+JiWAKgHTskJfPfEkwv5RjcvcDcIZz7FGujiEVP9rRiqIECBUoil8SEoBIN/zgBBznFRm5vxZqVH6yoq2lAmcunqecfSoxhY7CFEUJPCpQFL+ktEgEBH5yAvoUxUWHB860THXQbOZ0MokkCmt37Ecgk0imYi0YqPqwolQPKlAUv8iwBBBjLS2GAq2cCCPMvDhwpmWqBZeLm5hLMrGcABwERvFPFjkT8qo+rCjVgwoU5QgSE8G08LKY3B4vP1WmTpgwWRrtIo5kCrgWgM7cShKdGUmiqg8rSjWhAkU5goQEIC0GDgNZFJ8/SYutnUpVkJkzYSSJfM67CPAh0JkU5jEel+iYl6JUBypQlCNISQGWT4cNjqH6jk6ECSF6Q5APdzm4XDCDBFpzmJ4Urs2kKVk8YnTMS1GqAxUoyhGEhMDITdDjkwIANr0LI7+OhgOxzBwX/MNdHmKww3ZXADuAjMLwFJ2YV5RqQAWKcgTXFCQyj/EkkU9LoGc6zPvkEK0PBff6E19SsdpofbF2vd4pjBG+maISRVECTVAJFBE5T0S2i8gvInKvn/huIvKViGSLyF0VyauUnxkkcIAssoBeTlhTsuhAcNrwKokno6dTgNDfOfZ4gwnBcKdbh70UJdAEjUARkVDgOeB8oDswUkS6+yTbD9wGPFGJvEo5SEy0Q0UvOccjvOIiCmci6ganzHQBhg5AS2ADUODEeYbDFEUJHEEjUICTgV+MMb8ZY3KAxcAl3gmMMXuMMWuxDvkqlFcpH1OmgJvWLHGOR3vFFYQFj8vf8uByQSpWK+1c7EOz3IlLDW+t8yiKEmCCyThkB+zcqYedwCmBzisi44HxAO3atWPlypUVrihARkZGpfMGM4/ft58fmcqeJ//FccC2O+9kG2AQsmI71bk273j6SX7LS6bX5h957cUXefKkkwi/5hr2RsK+zLdZubJ1qfnr630uDW1zw6Ba2myMCYoNuAp4wev4euDZEtJOA+6qTF7vrX///qayrFixotJ5g5kkYs0BMCFgHgBjnG0P0XWyzQsXGvNs76bmkJ2XN8c77ckIx4w8L7rM/HWxzVVF29wwqEqbgXXGzzs1mIa8dgLeTmU7ArtrIK/iYOdPUngDO9fQwysumuA2CFkSLhdcmJJJY6An0MwJb5oLM75y67CXogSQYBIoa4GuIhInIhHAtcD7NZBXcfCo0n7oHHfzivNY8K2LxKTZ/V+A77DGIj3hU15QiaIogSJoBIoxJg+4BfgU2Aq8bozZLCITRWQigIgcLSI7gTuBv4nIThFpUVLe2mlJ3eXOg1MIAbZgTXf1dMILsBZ86ypZx1hnLlc4x485+9QocJ+kDlIUJVAE06Q8xpglUKhg5Amb4/X7fxQZAikzr1IxYnKts5AdWDVb8Y50uaCOTlo2e3wmmTeOomOuNZy8HMgMh6lDsT5fFEUJCEHTQ1Fql8RE+8W+A2sT0nv+JDWqlioVKFwubroIUqKsOuD/gNBcmLHcmphRFCUwVEqgiEh3ETlfRPz2FpS6x5QpMHVwU95yngjPgsbMcHjywugS89UVFh0bzdShMFDsEN57QOc0mPc+6nFLUQJEZXsoDwLNgfEi8lJZiZXgx90+kUV9c3jRmYkfAyRHwU0jQjnlnpm1WbWAEL12JjOWw9+NPZ7thDfNg4wpaoZFUQJBZedQPjPGvA68HsjKKLXI0AQIy2VjNnAUHDPZCc9syat1wKFWWcwc5yJm1ChCgKZQzPBKpFvNsChKIKhsD+U0EXlbROaJyJ0BrZFSO0SlWtskv1G0WAMgsm6uP/HF2wzLlUAmdqUjWKvEOuqlKFWnsgLlR2PM5cAkiswjKXWUxESsh8Yt2Ldsm6K4kPS6u/7El6lMJ5NIBgB7gF+ATCKZynT1M68oAaCyAuVCEbkV6GKM+SGQFVJqnoQEYPsFdgUPFNmsz4mk4LO6u/7El6XRLm5iLvG0B+AqIriJuSzCpX7mFSUAlClQROTvIvJ/PsHXAD8Dl4vIvGqpmVJjpLRIhL4vWWM1gl3pYwS+H03swbo/f+Jh5kxYhIvT2QW04gdCWET9aZ+i1Dbl6aFcT5FSDADGmD+wrx0xxtxUHRVTag4ZlgBhWZCO1d0TQAycsITp9aeD4uNtsjd2xU2RkWqdR1GUqlEegXLIGJPlJ/xlYFSA66PUMImJYFqk2kkFA8R5RUal1imXvxXjfGdfpPU+Ra2wKEqVKJdAEZFjfAONdWSVF/gqKTVJQgKQ1RocA4qc5BWZVn8m5D1EF67R9EjKjwvj3GqFRVGqRHkEyr+A90Qk1jtQRI6iyKOqUkdJaZEIjQ7CPifAY2YlL4LoDfVovMthZuEazY5Yi2WHisXrsJeiVJ4yBYox5g2sv/b1IvKhiDwiIjOAL/Hx7a7UPULOsQsa+Qr7NHjWoGQ3Z+a4+jfeVXwI72asp/nkwhAd9lKUylMutWFjzEvY0fXXgXDsbOZIY4x+z9VxCpqn2n5mBrZ34jExHLm/3s6fFA173YCdOHqlME6HvRSl8pR7HYoxJt0Y87Ix5h5jzEPGmHXVWTGl+ilc0PirE+Dt87Iezp94KBr2Og4rQRfUWl0UpT6h5usbMFOmAMunw8ZQG+DxqJUTWS/nTzwU9bxCgKOxlr1MiekVRSkfKlAaMG43sMkF2x1bK8cCB2Lhg7n1cv7EPwOwyorrC0N0Yl5RKocKlIZMr0S4vTOE/QFtw+HdhfB0Mmxy1dv5kyO5wdk/VhiiE/OKUjlUoDRQEjclwkXjwaRAFjAg1x73ahif50UT85cTQhjNeJN8QkiiM8PdDeMaKEqgUYHSQJnyfgJEZBVNyHfAHg9N8HrZ1l88E/MjWcQNGDIwpGHoTArzGK/jXopSCVSgNFDcuY5TqW1OQKSzj0r10oKqv3iG9GaQwA3kA7DGiWtKlnpxVJRKoAKloeJRC96H1ZxtXRTeUOZPoqMhhhROAUKBf3vFRbrVnr2iVBQVKA2V7RfYBY0HsT5xwWrO/nRB7dWphpk5E1KbhxKJNRDwX684dyPRUS9FqSAqUBogiYnACUvgd6xQae9ECHD8klqrV03jcsHUc/I5HAJ9sFa9fnPiWuYYvpmiEkVRKoIKlAZIQgLWh/xGJ6C7V2RUai3UqPZY2i+WnFC41Dme6+zDDfzdrfrDilIRVKA0QFJSsHMoedjJAy+BEh1ef02u+GPmxdNpngt/wXbQXvGKa4Ma9lKUiqACpQEigjW5kirQGYhwInIimXlx/TW54g9XL6uB0ALrw3E3cKAW66ModRkVKA2MxEQwBth0Guw10KK59R/vmFzxvGAbEn9G2r/BU86xZ3J+X+MQ9u+vlSopSp1EBUoDI6FwecUCu/v+QXiwoNDkSkPk1vMKyA6F04AmwFIgOxSmXFDAjh1lZFYUpRAVKA2MlBQYSSKn8U8AvuFJRmK1mRrCCnl/fHl6LDdeAr9HQTvgP0B4Psz4LJQWedpFUZTyogKlgXEdicxjPKlkEwaczE7mMZ6RJDaIFfL+mD50Oou6RTJ1KHQTyMbaHu6cnk8sKWqGRVHKSVAJFBE5T0S2i8gvInKvn3gRkWec+I0i0s8rLllENonIBhFR519+SEyE6SQQSRb/Axyj9TQlixkkNJgV8r64erngg7nM+CyUMY5blOeduBAK1AyLopSTsNqugAcRCcX6rj8H2AmsFZH3jTFbvJKdD3R1tlOA2c7ewxBjzL4aqnKdIyEBfiOVbViN4f5ecTE0rPUnvkTvdhGTPoqO2K+spV5xaoZFUcpHMPVQTgZ+Mcb8ZozJARYDl/ikuQR42Vi+BlqKyDE1XdG6SkoKpBLDL87xbV5xWdENa/2JLx4zLGFAHPaLJsOJS41yzP0rilIqYkxwuD4VkSuB84wx45zj64FTjDG3eKX5EHjUGLPaOV4O3GOMWSciScCfWItUzxtj5h5xEptnPDAeoF27dv0XL15cqfpmZGTQrFmzSuWtLdavh9bs56P3ZvLBV2v44OGHiQgPp4AQQuJioXXrUvPXxTZXhKRf1xObBgs+/oRXli3jgeuvp/+IEezL2MnBZmH0adentqtYI9T3++wPbXPFGDJkyHpjzIAjIowxQbEBVwEveB1fDzzrk+YjYLDX8XKgv/O7vbM/CvgBOKOsc/bv399UlhUrVlQ6b22wcKExdgWKMeE0MccSZvIRk0SsGcnCcpVR19pcUaIfiTUjL8dsb4GJAzMoFDP/hScM0zBMo7arV2PU9/vsj5puc2Zmpvnmm2/M888/b5577jnz/fffm4KCghqtQ1XaDKwzft6pQTOHgh1l6OR13BG7cLlcaYwxnv0eEXkHO4S2qtpqW8coWn/yE7kc4leGEcpnhfGv1kqtgouZF09nVN4oFvUGPoGkb2B4XqaNNFapoaEqLihlk5gIN9xQQEHB99Dun9D+A5DD1rxRKJy4H074Fbbmw89Yu6zetGoRwkOHCrgp1xqvKBAQA6nh0STkziRqkotZs2q+XRUhmOZQ1gJdRSRORCKAa4H3fdK8D9zgaHudCqQZY34XkaYi0hxARJoCw4Efa7LywU5K4bzyi87+ssK4hrr+xJdiVgKaAAZWL11dGDTlBZ1HqSsMG2ZNDBVuF0xGHghBpgkyTbjuCmFvpGDEbnnfr+e6K6QwXu4PQR7wOnbyJLcUCkQ4GCHkh9i8h0X4V3vhgxuFdgWhwAD44w3Yehh+An6EsHXw60/wXT50A/4OvI21bv1zCFwdA8ccLODWXDgW+BeQaewLunOum4WM4tk5wr/7NGPy7OB9DoOmh2KMyRORW4BPsTJ9vjFms4hMdOLnAEuAC4BfsJ7Qb3SytwPeERGwbXrVGPNJDTchqAkJgYICsKOEAEUvz4a6/sQf0U2icR9y2/7tCtj4zUZr5EvAHZ+A93VrKKSlpfHdd9+RkpJCdnY2bdu2pXv37hx//PGEhJTxTZqYiPu2KbTa7yY1CqYOhUU9gfxGEJZdmGzkJpixHGLS7Jd5iCnap0bBh13hwp9tvLsxIBB96Mi0nv0LUTD1clh0QjPYcQocu9xa/wRGboT570Jjry5CWAG8+J79vag3EFp8bvnaH2DKB/BBnhUCO3NhF0Vb/u/2G+RU7Fj9gFB45wKnrC2Q8D40z4PDWDcJbSn6pLurAA6n2qGXECAV+CvwIHANcJ6TNtTAzRszSb99FPO//ZKxLwZhd8XfOFhD2RrSHAoYQ6+FhsYhhjAMt8fa4wpMDdS1NleGhRsXGh6wcyY0w0iIGP7uHD8gtV29amXoUPucPPHECnNth4nm1Ugxl4IJt4oufrbWhraxhoFiOB9zxqmYj5uEmv1gfmmBeXYAJkcomrzDHo+8nMJ5KabZ44zw4ul8t4JS4nLA/AlmN5jfwGwGsx7MwTDnXJMwXIXhMgwXYe5vjHnMK/+rYC489VRzPZjhYRiOxdAVQwKGKzBEY0J82h4G5mww14Pp4ufadAOTFOW0MaZ4nIAZ7HX+S8D0AnMymDPBDAdzI5ixYCKdPI3BTAWT7+TJCMd88Y9JVbrf9X0ORalOeiXCRePhXwXQEmiZYo+BhvjVXRKuXi5GvTIFmrrhWDA/GNgC9ALSYurdPMrk2YnM/jkBWqRCrxjYMx0y97J39xyuMxCNVS8/IxReGAYf9MDqU++KgG2NIS0FvgXy7YTlKvJtwQftME8C0Aj75d0YiDLQ7G3sNe0KdIDLP4VHcou+3g9hfz9po1mEHQI67BV3CNgMHAPMAKb5aVtanu31LOoBrCkKf8jZ34kdCvkK+GLTJpoCYXnAH0Au8KhtF+EQgzXM3QWr9dMG+D+nnM+BvU77mjhbC2xvCoBrYP0cODbdpomgsLMEwLtAnkCYH4XbR4G7nGswA2u49GOgeS50fHQ2iSMGBZVBVxUoDYDERGBoAuRkWbsig5yIiCxCzmmYwzil8slMK2xPy7L6ghuAEyJh+XQmzKsfAmXYnYksbzoaQvMhyglslgKRY7jnoQIwcDv2JZYE3JkPfy7DzkzmAnk5cP5uKxR+BvwM618BHI198S/zjdzmbFj1TrAv9wisAGqCfXmeBoRjx7SbUPyl3cjJNxz7AveOb+T87pQG9HQKyALS4bJv4UAenIDV6DkEkJmJ5/1PFtaLaQxwHNAZVsyEzoUJinO2/2CSPde1KTxxjh1ma+Q7E481RDqvL4z7rvgwHNihsZeAB7ALkb/Eru5eihVYXV6foAJFqVmmTAFuSbUvR7Czfg4FLRr2Cnl/RO924f4AGJpA+5hcdh/cCx/MhU0uMqnb2l7zhyVy9pcTWXo4g9QouO9sWNwG+8ZPAQryyAdaYV9gjm4CB4DIfCegOfbN0dgpNNpOMkdiBUKEk+xcrBrmHmyHxPPCbwx0uxn7cXMAbn8f9uTYidFfgP3O+TyXWLACo6VTr5ZYGTgZKzgaYTsS2djei6cHcy+QDEVuOB2WNIJeeTDAqV8bwH3VVZz01hvMOx2WDaLIR5DD1KEw7wNomuv/uhqK9zoyQ8KYOrgRGKsluKiXDZ/5MbQ5VJRuXxOYcnZTFhXcwJrzX2bm55mF8Z4yBdsz+gQrYL8EJgHTWoAJzaTHyEQ2LwqOB1IFSgPA7QayWsNKt306vWwLxEY17BXy/pg5E0aNcsEmF31HTGD3R3Nh7+DC+ISEuilQbu49mce2zmZvnh32eT0Nkt/xStAI6AmLL3mAa+5/sDC4B/A19os7bpRXes8QTWsYG1XyF/xRzuYhX7Cf3gAd4Y+C4i/rDGCHs6Vi1wq4gTSsoDmA7TUdxgqRbGzvprHX1ghr6FOOg23HhkB2L+j2A7SE7Ajouqn4y335wFOYn/8Gy3p5VVJMoZTwCASP4kBGODTNswoA+QLLO8MJ+22cR823ZYwLM83/NfHQFquyb9X2i0+yv7opkYNjxzJ+XQ4hWBtTm4DXsPdv8EFIehqmDp7A5MnBoVKsAqUh0CsRwg/Yf2RzipTF8yKYPrRheWgsDy4XjHJenD16DOKjj+ZiR8zfBLxVsOsOPUYm8lHKHGbnWQ0isC/dSUC7JnDfWApf8lnHRJEZXvxrPDPcfqUXkhMJqQMLtaf8fcHnYIervL/cDTC7P5BbpOXl+7JuLHCCga5V0PLyaJRtO6EZQw/PYdmTpX8BhK1cyat3mHKvx2rhnRfbG/PQGb8jgBXG1csFa130GJlIn+xRzPgcuqXB37DzKLcAG9Ng3tJMbhrahsmzZzJrUu1+6ahAqecUzp/syrf/Zu9loTnNg2r8NZiIjrY9u6OOisEO1hTXQq9Lw16TJ8OWoxNYc9AUCpPhwH3AmYA5BPd5egwG9ufEclO7Scw4+DwxBwvsy/mMCBYd2xzMfkiLsS6kN7ng/MkwYC6LeuVDgTBjWSgxGXk2z9lwWkooE7/PJ9RAPiEkNp1Aq9tnYUq5diE++87Yl6eHtqWk9c5T9OVft9m8yEWPkRB3+yiSnrY9wUjsVNZVwPpcePlTNzdE3EjiYGr1P60CpZ5TOH/isRnQ0ysyUp1HlYQd9vIcnYFdHrXK+W2va10QKImJMHs2cFUKnuZcjbW86uk5pHgmjw3wR3do3pofWsyi8w47htKZ0l7Os/AdqvHO400YMLpyzWjwbF7kIuzvE4lJsyZL/4NVPNyEnbCfYWDex7lMzJ+C68PaezCDaaW8Ug243dgvSo8Rm65FcTp/UjIul10Mapnq7B8pjHe7a7pGleOT6xPZRiyd3rCTzx2wc9QeYVI4lGUgNHUoCwdvpn9/2Ly51qqslMBLV88h1RH+x2LnUsCqFn+DHW58eLWbHiNrbyW9CpT6Tq9ECM+wn4eRFPVJcyJ1/qQMJkzw/DoDaIZVZM0vjA92R45/vWgYc8JGsYxUdgALgTUCOU2sHankKLhpeFP2xC7ETDPkzV9WJ3pdDRVXLxefXzaJzDD7OXARdmbPAFcCmUBsGvQ5PLrWzLOoQKnHJG5yFjM2ccM+4Hjs05cZDR/M1fmTMvDWmomjH5DDq4SRRGdGkmiHE4OUyR9N5u7lywnJtSq9/bCe62IMZEZA6O2xxMUsZMTIjDInrJXgYeyLs5h5/CvkOa/uf2DtT+3EKlsIsPDdfLrPmcDkyTVfPxUo9Zgp7ydARBZsx+pYHot94nKbEb1bXyLlZSSJrGctx2JdA3cmhXnhoxjeYVhtV61Euk+bTZtDMAHrJKgRRcNcMWkCTycz9CiX9kjqIFM3u3is+8tkhlstuvnYnsps4HXsS33yxkwOzE6s8V60CpR6jDvXWbT4lRPQ0dlHpapByHISFgYzSKAVhxiNHfT6ATtePSNlOZM/qoXPwDL46zU9mLzOrlNciNVR815ukkoM3bvDsiOWryt1hambXUwcHl24FGgqdjT7Buzi0BBgRvPRjH2qZiWKCpT6TFoMIzdC01RoCiQtsJZWSYvRL9Ny0qkTxGAF88lOmMe1TEwazFnr1zFordHjnsnc/cEWQoDLsSOcs7CWR3COn2s/XSfd6wHn/WNmoYZea2wvJRuIx45wx6bns/3n61n9aM199KhAqceMfPMCHnzXTtb1xuqvz3vfhivlo3Vr+LOZ1YY7Fzs1vwz7Yi4QuHZTftBMzk+eDH1+mkObQ/Ay8D122myMV5p94Y14fJd+TdQHPJP0HvNfjwIjsP/3S5ywzgcNfe+fXWNCRQVKPSUxEWbsXMJ7ztN2oRPeNA/+uXtJrdWrLhI9ZzqZ4fb3JdivwFew1mHnfQD/nRAcEmX2bJixwiCAx0LbK17xBcD2cf+p+Yop1cbYF2exqvskCpwZsjewdi3XUGTCrKnHMvGm6n9OVaDUU6ZMgRhSWOEcX+YV17FADUJWCJeLW/oNJU+KVqI87uyb5sLUzIRa76X06GH3R6fBRmAmtkflGaYzwPu9hzJ4lvZO6htnbZ7FjWGvkNw8lMbYHnQbrHHM3500sWnwyX3Vr5aoAqWe4m6fSGoLoTV2/qSbV5zE6oLGijLs1mWEGLsCvDPWeq7HFmIMKbWqQjx/WCIfbelMPiHci/UamI31IeLhzyZhXPqDzsLXV4YvcBHX+SVSouBEYDXW4vIkiqwWz/vYzYwe1fvlowKlvjI0gfuGGpZhfSYXrowOE5iuCxorissFqcQCdpK7gCIDgKktBHf72umirJ6ciGv5WDqTwkcYZmJfJAlYK8FgTam3nregVuqn1AwuFww9ysXUwU3JDLe+Xm4H3gPGOWkaF8A9W25g9eTqe1ZVoNRDEhOBqBQWh8L/gJ6NvVZGX2zqhhGqIOTJ6OlkhgnnYX2FPAekh8LUYQaG1vywV2IinDB7Co3I4Q9gLLY32g6YhFCAkEwsrw1ZoPe8AbBsGfzQ/HluGhGCAe7H+o2ZD3zmpAmlgL7zqk/zSwVKPWTKC4mAwHp7/MBfIHQaxN0BS/vF1mbV6jSnzHRx08WGlChr9mILcEZ3WNQbiEr1MtVSM4z5VyJtcJMPjMIOwWViV8Y3xxBKAceFJjN2mQqThsLmRS5+aPwyYC0tve2EX4F1XAbQNM/Q8dHZ7D8UeOOwKlDqIe74BOscaDd2tVOhaXJh5sU63FVZXC5YFBNL3B0w7W4btiHJiUyLITOz5ux7DXtyMlc53q4ysC+LCKyJFW+59tJLNVMfJXjYvMjFgdBowLonHg6kAxdjfdSAnaRP2x14xz4qUOoZdrgr1X6qZlPcVR5G7XdVkegN061zqUisb5kM4FesAc5eiTXSS0nclMjyg3OYYX1bsQX4CTvUtRD7DbGPaCZN0pGuhkqrl2aS7fgxfsoJ+wP7WgD73MSmFQR8kl4FSj1jyhSsu9+tTkCcV2SaDndVlZnjXNa/fGa01XYA63urqRsuGk9ml+q3n3Rw7FhyHzI0SoO+wFnA0VizMCditXo+GDozKFzCKrWEy0Xi0PnsJZoTsb2T/Vj3yB5CDFz385SAPq8qUOoZ7vaJ0OigNT8K9g0DkBdhv66VKuFyQbMkF+Q2g2OgWTNgL2yeBknPZTGyw5RqVSFeecYwJq7L4WMDXYANwGnAlxSZajsYFq3zJgpjl7k4q/s+XCOacidWoEwGzgfWOWlict1M+HfgJIoKlHpEobvfsFyrO9oC61EJILu5/bpWqsycOUBUKiM3wsuHbNgiHNM2y90Md1dPF2XyZBi8ejmvYlfsH8Zqdn1O0chmpkQQtUAtfyqWzZut5ldslJ1bW4A1yeNMAZIaBZmnJpSYv6KoQKlHJCRg509ysOP6J1K0ACVyv46nBwiXC5oVxDBjOVyWbzVoZmIN8jXNhZlMCbgvCo8r342myDbX+cAL2FtsgOTGzfh+4nydOFGKsXmRi7+3nMQdYfaFPwKr/VUgjrfOqMBZzlCBUo9IScHOn6wC8ijqnQDR4bo6PpDMuWo6Mc5S+YewWjQXOXFtcHMgwB7zxowBOIwL+6c9Gmuny/O9kC8wblC6mlZR/HLe9Fk8OCyas8PgQ2BPC0iJsirvIemBezeoQKlHSO9EaHygaELeY28lL0LVhQOMq5eLnRFWNbM7cBzwNfAt9iX/MqMDpkHTowdclZfIPRzDNuwK/U+AaCfeAPOih6p/E6VEXC7Yc+JMll3RiD3ACefC/kggJ5IJxwXu3aACpYKsOH0oU0SYP306eSHC8yc3qhErnuXBjJgIofl29q05OFqDkBeu6sLVQOpfZhY6OFrg7Ec5+zDyuXPL6CovTJk8GfpsSeQRbuJZDnAB1n94L6wgyRN4rtVQWjyt0kQpnWVPuphw+jxoEWpn5fMjmNRxLrMmBe7doAKlAqw8Yxhnrf6cP4HFK1eSbmD82hwOjL2+1oVK4qZEiMiw5kUNcIxXZKPMEnIpVWHwLBcHw2w/YRDWHMvPWFMXAI3J59DYsVU6x+zViUxrdgM3cIgs7HcC2F5QCrGEG8O7/ZbptIlSLubcfD0z7n0YkuBoaR1QYQIqUCrE4NXLEaxOd25eHm9h/9gT1hvrv70WmfJ+gq3Md05AMfPCtVChBkLUgplkEglYQ3whwK1ArhPfOCeHbcMqN0Pf457JcPn1vJtRwJfYtZSPecXHkKqufJUKM27cONasWUP79u0DXnZQCRQROU9EtovILyJyr594EZFnnPiNItKvvHkDQagzvpHvHD/hFV7ov72WKDz/PiAUOybiEN0k2l8WJRC4XMzsPheD1YFIALKAfzvRAhy/fHaFh76G3ZnIliZz4JDh707YIsB7+vT3sBh15atUmLZt2zJw4EBEAv+lGTQCRURCsQZcz8fOc44Uke4+yc4HujrbeGB2BfJWmXzn+l8NHN2qFduBFE94WkytOVlKTISR37Rm25PQIgmuCoGRnol5AzPP13UJ1cnUzS7cTexf6UGsd8x7gbVOfAiwc/I4/5n9kJgIR22dQtLThgses1rg12N7xh4yiaTDAlW0UIKLoBEoWOdyvxhjfjPG5ACLKXKN7OES4GVj+RpoKSLHlDNvlVk9eGihs5qx550HwFXAnP5AeAajn6gdifLNlETmfZrO1wfhIDA217qmHbkR+HaSTsjXANvun1D4bLyA7SSeBfzpxHc4eJibr+pQQu7iLHliGPOWu8lOg0+xfuGfE9jbxHFD0DyU14bO1fUmStARTAKlA7DD63gnxVZSlJqmPHmrzFmrlvFcK+sK9pz+/emI/Qq9dTDQ1E3+hTcyOcDrD8rDne4EmpqcQg99p2AX2M34IJrob9WgU00w+N5ZzIqPwGCNNN6BHfqKxwoBAR57bzc3nz2s1HI6TB3G9KTlNMqFv2An4f8LNDeQGQGhDwjHHfuSmlZRghIxxpSdqgYQkauAc40x45zj64GTjTG3eqX5CPiHMWa1c7wca0WgS1l5vcoYjx0uo127dv0XL15coXqmpsLevdAxbg/vLnuHxbMXc8E1F3D2JWfbBAVh9O/Qp4Ktrzz790PrJOv4ZPi999IkIoL3HnqoKD6uP61bB+ZcGRkZNGvWLDCF1REq0ubUvftpsi+Jtln2+O5581i7fTvdY2P59y23ICLkhMKWJnHEn3DkTfnfz6m0ztxLRD7cMXs2G379lbuvvprzTz65MM36qLbEtY4J2D31h97nhkFV2jxkyJD1xpgBR0QYY4JiAwYCn3od3wfc55PmeWCk1/F2rIJsmXn9bf379zeVAYx54tUnDNMwtMfQDsMD2OMHMJMmVarYShEdbcweos33VlnYDANbQTBJxAb0XCtWrAhoeXWBira5+7ULzcjLMQVg8sF0c+7LaRSF8QCGv4eaof8qelAmt1xoMsLsffubkyfOyVN4P8Oja+TZ0vvcMKhKm4F1xs87NZiGvNYCXUUkTkQigGuB933SvA/c4Gh7nQqkGWN+L2fegDFpktdBX6yjgU+KgmbPrq4zH8lwdyItOcA/nOObnX02ETwZrZO2Nc3mRS7+e8JQUqLsePImoCewBrgR+KkFdvwrNJ+jvpxNckuhQISZaaNomgdPA49gVYTXUqTxnRkOr3ZVk/RKcBM0AsUYkwfcgp2H3Aq8bozZLCITRWSik2wJ8BvwCzAPa425xLzVVddZs0BMmD3oB4Rj//15QJZV0Q20ccCSmMkUwsnnU+zCeI8mQjbhnDJTx9lrg10zlvH4Oe3JDLfOrn4ApgEvA/GHgVVwzQ9WcaJzmv0TZhqrongH9j5+izWtYoDkKLh78FCmbtb7qQQ3YbVdAW+MMUuwQsM7bI7Xb0PRR3iZeauTzq06QV4EhOVYofINsDIE0qyK7uzZVPvXZGIiXIebZKw/8UkUfdE2J1OVgGqR597YhauvnWCPSYPRUbD8RPjiW+Bza+11A9AE62XgO6zzx2OBzyjyi5YSBV2mRFDwoK5eVIKfoOmh1DVaN2lNo0/mw4FY67hZgC/DYdPVhWmqe12Kx5HTl87xpBJTKrXBBXctIy5mIaG3xxJ3O3xxHvBXoA80w7rt3QBsxo7Rrsd2vT3CJDMcpg4J4ZUr5x9ZuKIEISpQqsB/bnfB08nwD0MXcwaYbO4mgiQ6M5Lq9y/udlvf4U9ih0mKWVuJ1tXxtY3LBQvvcZ6RbyfZ8asmwGXwXZRVJzZY4wbzsB3dPHHWmkTBTUOjaTn8ZV1HpNQZVKBUAc+Q0kgS+YZviQBWAJ1JYV7Y9VzcanK19VI85Y5lKt9hTQeEO3GHCYeZujo+GHC5rIpW95RZ8OtQPOaJpw61PRBvMsPhhovCCe21kC4HDSNG7Qu48T5FqU5UoFSR6GiYQQJtOMwU7Fj4/4CmeYYZB+cw9qnqkSgTJwK9EvnweGvpaVxjsV+2xDKWF3UVdZCxeTMsPG8ZER8uhMxoFvWCmy6C5BZSrEeyKOlFhh7loqBAb6FS91CBUkVmzrRWX8GqheYDN2A/RGMOGnJOTwh4LyUxETLiEuGi8fBbFoTBHfcYQqdGEtdrOkuj9U0UjLhckL3OhXlsH2aa4dW3DJ3TCggxhs4HDK9+vA+z0aXWg5U6iwqUKuJywZ/NrA3YE4GhWC2dl4DUKCAqNeBzKVOmAEMTIDXLqiofh1UKiMiCoQk62qUoSq2gAiUARM+ZTmaYVdj16OP8DTtOTloMmQH2b+VunwhRKdbUMYC3AYSoVB0qURSlVlCBEghcLl4bNZHkFkJHoEcI7AIWNY6A5Xa1eqAWOiZuSoRLHC+AvwGtsJbMHKLDY/xlUxRFqXZUoASIsS/OIi72FUJvj2Wzx7H4G81gk+0uBMocy8R3p9jFlFuxUmsgRXcxJ5KZF6u5FUVRagcVKAGkWZKz5uBlA5wOufvhumPggRC4vTPD7qz67HxGvtv++Bg7b+IxbGyg6edzdc2Coii1hgqUADJnjtdBt6utl6Vt/wMx0DKF5U3G2yGrSlI4bJbubEcBjYrin79FhYmiKLWHCpQA4nJB48bOwXlPWEvEG4DdTlhEFn95c0qly5+9OhGM2N4J2KXVHrKidTJeUZRaRQVKgHnhBedHVKp1TFwAvFMUny3uSvVSJs921p3kGDt/EgGc5ETmhdP0C9UVVhSldlGBEmAKeylpMXZIqjWwF955HPKnQdLTsPz+ivdS0t6fQtJzWdz0KGCgd1fs3csPhfde1OEuRVFqHRUo1cALL2DVhQ2c3d2G3ZhpV893ToNnP3JXyBTx6smJzF3upnOa9ZPRC1i9DUZuBEIKiNju0uEuRVFqHRUo1YDLBUOPckFWNP/ZZE2TH8A6TwJomgu7Jk8suQAfus2eQtNc+AjrrGks0DwfZiwH0mKYr9bNFUUJAlSgVBPLlgGfzCQmDRZgrZb/BzjoxB9zMIMeI8vupcwflkg0bvYClwJtAI8ll5g0iPhiuvZOFEUJClSgVCPNklykRlnN3k+ALGCqE5caBVu6ji5VqEyeDGevmoJgXfvmAQ9ghRNAang08+9QaaIoSnCgAqUamTMHpg6MJjMczgCmAM8B/whx7HyF5rPlhFEMe/JIuyyJiTA7fRgxuW6+Bb4CYoFbnHgDTM2dqb0TRVGCBhUo1YjLBXtOnGn9XkTBI1ht34QCcH1apPV11JezrVqwF6PW9IBjl7O5ufUwDNarn4d9RNNSnS8pihJEqECpZpY96eL1vEnE3S40nwYnn2J7F1dnwu9Yra95H8CBjyYwbJjtmVw3uAdJiVvIfxASMiETuAs4xykzMxz+L3Qms2bVVqsURVGORAVKDfDS1bPg7VfAwCvbYBp2PuVEIBWr9fXKR5l8tly4bpSQ+OUWYtLgKeCDAnABdzcp7mv83Je0d6IoSnChAqUGcLlg0mCrRhyTZifW/w9rjqs/sA1r9kucLRWIw/ZKLscOdWVGQOg0iLvdDqPp3ImiKMGGCpQaYtYsGJo303pxBJ4AnsEOZ3UHujr7Y4DOWKFyHvAmVqsrJg0w0L3xUJY9qdJEUZTgQwVKDbLsSRevjhhKZrg9vhVIAv4O7Mf2VPZjhctCiizUg1UzbpnXnc33qsNxRVGCExUoNczUxGXMHDqJ5Cg7JxIt8CDgxh5nAz9h5008ZIbDI6d2589HNtdCjRVFUcqHCpRaYOrHsxgxYRKhD8ANl8FhP3fBUDQJP+3cobzwiQoTRVGCm7DarkBDZfM/ZzH5I5jNHMAw82Noc8jG7WsCU86HRTGxTDp+OrN0vYmiKHUAFSi1yKwRs5g14sjFJG2BV51NURSlrqBDXoqiKEpAUIGiKIqiBISgECgi0lpEPhORn519qxLSnSci20XkFxG51yt8mojsEpENznZBzdVeURRFgSARKMC9wHJjTFdguXNcDBEJxRrrPR+7BnCkiHT3SvKUMSbe2ZbURKUVRVGUIoJFoFwCvOT8fgnrS8qXk4FfjDG/GWNygMVOPkVRFCUICBaB0s4Y8zuAsz/KT5oOwA6v451OmIdbRGSjiMwvachMURRFqT5qTG1YRJYBR/uJSihvEX7CjLOfDTzsHD8M/Avret1fPcYD453DDBHZXs7z+9IG2FfJvHUVbXPDQNvcMKhKm2P9BdaYQDHGDCspTkT+EJFjjDG/i8gxwB4/yXYCnbyOOwK7nbL/8CprHvBhKfWYC8ytYPX91XmdMWZAVcupS2ibGwba5oZBdbQ5WIa83gdGO79HA+/5SbMW6CoicSISAVzr5MMRQh4uA36sxroqiqIofgiWlfKPAq+LyF+wltuvAhCR9sALxpgLjDF5InIL8CnWfch8Y4zHwNVjIhKPHfJKBibUcP0VRVEaPEEhUIwxbmCon/DdwAVex0uAI1SCjTHXV2sF/VPlYbM6iLa5YaBtbhgEvM1ijCk7laIoiqKUQbDMoSiKoih1HBUoPjjrWPaIyI9eYVUyDRPsVLbNItJJRFaIyFYR2SwiU2q25pWnKvfZSRsqIt+LSIkahcFGFZ/tliLypohsc+73wJqreeWpYpvvcJ7rH0VkkYg0rrmaV54S2nyV05YCESlRs6uq7zAVKEeyAOvO3ZtAmIYJZhZQiTYDecD/GWNOBE4Fbm4AbfYwBdhaPVWrNhZQ+TbPBD4xxnQD+lB32r6Ayv2fOwC3AQOMMT2xikDXVm9VA8YCjmzzj8DlwKqSMgXiHaYCxQdjzCqsa3dv6rVpmMq22RjzuzHmO+d3OvYl08E3XTBShfuMiHQERgAvVFf9qoPKtllEWgBnAP9xyskxxhyotooGkKrcZ6zSUhMRCQMicda9BTv+2myM2WqMKWsRd5XfYSpQykcgTMPUNcrT5kJEpDPQF/im+qtWbZS3zU8Dd2O9NNd1ytPmLsBe4EVnmO8FEWlak5UMMGW22RizC3gCu4zhdyDNGLO0RmtZ81T5HaYCJXCUZhqmXiMizYC3gNuNMQdruz7ViYhcCOwxxqyv7brUIGFAP2C2MaYvkEnpw4F1Hmde5RIgDmgPNBWRUbVbq2qnyu8wFSjl4w/PavzKmIapo5SnzYhIOFaYJBpj3q7B+lUH5WnzIOBiEUnGDgmcLSILa66KAae8z/ZOY4yn9/kmVsDUVcrT5mFAkjFmrzEmF3gbOK0G61gbVPkdpgKlfFTJNEwdpcw2i4hgx9W3GmOerMG6VRdlttkYc58xpqMxpjP2Hn9ujKnLX67lafP/gB0icoITNBTYUjPVqxbK839OBU4VkUjnOR9K3VFEqCxVf4cZY3Tz2oBF2DHTXKzE/gsQjdUG+dnZt3bStgeWeOW9APgJ+BVIqO22VHebgcHYLvFGYIOzXVDb7anu++xVxlnAh7XdlppoMxAPrHPu9btAq9puTw20+UFgG1ZD6hWgUW23pwptvsz5nQ38AXxaQpur9A7TlfKKoihKQNAhL0VRFCUgqEBRFEVRAoIKFEVRFCUgqEBRFEVRAoIKFEVRFCUgBIWDLUVpSIhIPrAJ+/9LAq43dcQ2lqKUhvZQFKXmOWSMiTfWiu1+4ObarpCiBAIVKIpSu3yFY4BPRI4VkU9EZL2IfCEi3UQkSkSSRSTESRMpIjsckzeKElSoQFGUWsLxPzGUIvMWc4FbjTH9gbuAWcaYNOAH4EwnzUXYVc65NV1fRSkLnUNRlJqniYhsADoD64HPHIvNpwFvWNNRADRy9q8B1wArsPaVZtVkZRWlvKjpFUWpYUQkwxjTTESigA+BN7Be9rYbY47xk74ZsBnrb2YDEGeMya+5GitK+dAhL0WpJZzhrNuww1uHgCQRuQqsJWcR6eOkywC+xbrh/VCFiRKsqEBRlFrEGPM9do7kWsAF/EVEfsD2SLzdr74GjHL2iMgAEalTLoiV+o8OeSmKoigBQXsoiqIoSkBQgaIoiqIEBBUoiqIoSkBQgaIoiqIEBBUoiqIoSkBQgaIoiqIEBBUoiqIoSkBQgaIoiqIEhP8Hy9CrlhDdsX4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = [0.03]\n",
    "beta = [90]\n",
    "for i in range(0,1):\n",
    "    #Index from each dataset\n",
    "    iTrain_ = []\n",
    "    iVal_ = []\n",
    "    iTest_ = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    t_train = []\n",
    "    t_val = []\n",
    "    t_test = []\n",
    "    \n",
    "    predictedValue = predicted[t_len*i:t_len*(i+1),:]\n",
    "    y_corres = y[t_len*i:t_len*(i+1),:]\n",
    "    \n",
    "    l2_error_Cm = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    print('L2 error of Cm: {0:0.4f}'.format(l2_error_Cm))\n",
    "    \n",
    "    cm_ = predictedValue#denormalize(predictedValue)\n",
    "    Cm = y_corres#denormalize(y_corres)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        iTrain_.append(predicted[index])\n",
    "    for jj, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        iVal_.append(predicted[index])    \n",
    "    for kk, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & (index_test>=i*t_len))]):\n",
    "        iTest_.append(predicted[index])\n",
    "        \n",
    "#     iTrain = denormalize(np.array(iTrain))\n",
    "#     iTest = denormalize(np.array(iTest))\n",
    "#     iVal = denormalize(np.array(iVal))\n",
    "    iTrain_ = np.array(iTrain_)\n",
    "    iVal_ = np.array(iVal_)\n",
    "    iTest_ = np.array(iTest_)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        t_train.append(t[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        t_val.append(t[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & ((index_test>=i*t_len)))]):\n",
    "        t_test.append(t[index])\n",
    "        \n",
    "    tTrain = np.array(t_train)\n",
    "    tVal = np.array(t_val)\n",
    "    tTest = np.array(t_test)\n",
    "        \n",
    "    Cm_trainTestSplit_Plot(i, Cm, cm_, tTrain, tVal, tTest, iTrain_, iVal_, iTest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7eb11de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cm_trainTestSplit_Plot2(i, Cm, cm, tTrain, tVal, tTest, iTrain, iVal, iTest):\n",
    "    \n",
    "    title_0_Cm = 'Gurney flap not attached (NACA0018)\\n$C_m$ prediction, $L_2$ error=%.4f' % l2_error_Cm    \n",
    "    title_n_Cm = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_m$ prediction, $L_2$ error=%.4f'%(l2_error_Cm)\n",
    "    \n",
    "    if i==0:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    else:\n",
    "        title_Cm = title_n_Cm\n",
    "        savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    \n",
    "    # CD graph plot\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm), 'b--', label='Predicted value')\n",
    "#     plt.scatter(tTrain, denormalize(iTrain), color='b', label='Training set')\n",
    "#     plt.scatter(tVal, denormalize(iVal), color='g', label='Validation set')\n",
    "#     plt.scatter(tTest,denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b2cc294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 error of Cm: 0.0135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEwCAYAAACkMUZEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABN8ElEQVR4nO3dd3gU1frA8e+bTSdAgNBbAiIYKaGDdBQQsQLqxUJTsbdrRa9eu/eqV/zZQPQqtiuiKCgCokhAIagUQTpIDSAQegLp5/fHbDAsKZvN7k42+36eZ59kZ2fOvDO7O+/OmTPniDEGpZRSqjQhdgeglFIqMGjCUEop5RZNGEoppdyiCUMppZRbNGEopZRyiyYMpZRSbtGEoZRSyi2aMJRSSrlFE4afiMjlIjJPRA6KSLaI7BaRqSLSw+7YvElEHnduW76ITHE+ltkdV2EicpWIjHZ3uhfX67N9ISKtRcSISF8bY0gUkfkickJE9ojIUyLiKO9yIjJcRJY4vzuZIrJRRP4hIuHljLeNiMx2lntQRL4UkTrlLPNyEVktIlkisk1E/l7EPB7tp4pAE4YfiMgEYDqwG7gRuAB4GKgK/CQizW0Mz2tEpBPwJPA60AN42t6IinUVMLoM01UpRKQG8D1ggMuAp4D7sD4P5V2uFrAA67szGHgXeBR4uRzxNnSWaYBrgVuB3sC95SizB/AF8AtwiTPOf4vIPYXm8Wg/VRShdgdQ2YnIZcA9wBhjzBSXlz8UkUuAk+VchwNwGGOyy1OOF7Ry/n3DGHMMQERsDEf50S1AFDDU+d5/JyLVgCdE5IWCz4Mnyxlj3nJZZoFznttF5E7jWf9GdwHHnOvNAhCRsVg/4jz1OPCTMeZG5/N5zgTxuIi86fx+erqfKgQ9w/C9e4Bfi0gWABhjvjbG7AEQkWQR+bzw6yLS11nV0LrQtCkissx5+rsWyAS6Fpo+wHlanCEiP4nIuS5l9hSRhc5T4oMi8raIVC30+hBnlVKCy3IJzumXum6HiEwBPnQ+PVpS9YiIdBeRr5yn4xki8puIXOtaXqFt3OCsivhJRBKLKtPdsp1xDgP6OGM0IvJEcdPdjdc5X28RWSAi6SJy1Pl+ti9ivnK9P855bhORXc4yvgbql7RfyhqDBwYD37oc8KZiHRz7+GC5g0B5qqSGAF8WShY1gJ7Ar+UoMwnr7KGweUANoLvzuafbWyFowvAhEQnF+qDM80Hx8cALwPPARcA25/QmwIvAs8AIoA4wTZw/9Z2nzfOBP4HhWAntIuC9QmXPBfYAo1zWORo4AMwuIp6ngWec//fH2u4VxcTeFFiMVcVwCVZ13XsiMqKI+V52ln0NUB34VkQiiynXnbKfxqqKWOmMsTvwTgnT3YrXmRznAzlY++1q4EegoUt85X5/nGetbwCzgKHA71jVH+4qLQYRkdDSHi5ltgI2FJ5gjNkJnOCvM8+iuL2ciDhEJFpEemKdIUz05OxCRKoA5wC/ikhVEemF9ZlPBT51zuPJPogEXM/ys5x/zynr9lZIxhh9+OgB1MWqq7zZZbpgVQcWPMQ5PRn43GXevs4yWheaNsU5Lcll3ilALtCi0LTLnfO2cj7/EVjgslz/ItbxDFYSkkIxbwdeKmF7RzvLiXGJaVkJyxTsi7eAH4rYxvMKTWvq3L5b3Nz/xZX9OZBcxPxFTnezzBRgWcH+KmZZr7w/WHXkc1zmeds5T99S4ncnhoL3scSHS7k5wD1FrC8VeK6EeNxeDutMumD97wMhHn4vuzvLaAkccv6fCXQr4rNcln2wHJjuMu0h57yPlGc/VZSHnmH4VkEFvuuvoPuwPjgFj9s9KHu3Mea3IqZvN8ZsLvR8nfNvIxGJxvqyTHP5lfSTM46OhZZ7F+sA3df5vJ/zeeEzEY+ISA0ReVVEdvDXPhgHnO0y635jzJKCJ8aYHVhfyi5eKNtr8Tp/sXYF3jfOb38JyvX+iHW9qj0w06XcL8qwScXG4Pz7NdDZjYerorZdipnuyXLnAb2wvj+XYTWu8EQSkA5sxTqLuwXrx9E3IlLPOY8n+2AScJmI3OT8zAxyxgqQV2g+T/eT7fSit2+lYZ2SNnKZ/iHW2QR4Xme6r5jpR1yeF5wiR2LVpTqAN50PV40L/jHGbBWRZGAMVlXNGOAXY8xaD+MtbArQDasaaB3WxcdbsQ4Che0vYtn9lFxf727Z3oy3BtYXfq8bZR1xeV7W96c21vfWdd8Uta88iQGsX91Hy1AewGEgtojp1YtYn0fLGWMKqjh/EpE04H0R+Y8x5o8yxtoeWGWMyQF+AH4QkR+ATVjXET7Fs33wLtAOmAhMxqpmegh4jb++r57upwpBE4YPGWNyRSQFGIjVgqJg+j6cHyA5vRVRJmdeyKtZXPEehHTEudwTFH0dYo/L83eAt0VkPFZd+X1nLlI2zusPQ4A7jDGTCk0v6my3qDbxdYAik1YZy/ZmvIeBfMp44bkIRyj9/TmAVaXkum/Kdf+Ai1G4dyZZ+MO7gTOvOTQGquBSZ+/C0+UKkkcCUNaEkQT87DIt0/m34MBe5n1gjMkD7hCRx7B+JG7jr21b6vzr6fZWCJowfO8VYIaIXG+M+bCUeVOx2oIXNsBbgRhjMkRkKdDSGPOUG4t8gXVxdSpWA4mpXggjAutXdMHFQJwtgC7lzCRYR0TOK6iWEpEmQAeK/yK7W3Y2f/2appTppZbp3K8/AyNF5HU3qqWK5O77IyK/YZ3dTCo0eagn6yxGQXVMWcwBHhCRqsaY485pV2M1GV/og+UKbnjdVpYgnVV6rbG2sbBrsc4qfnI+92QfAGCMOYz1IwIRuQ1YYowpSAaebm+FoAnDx4wxM0XkFWCKiPTD+iCmYd2MVJAM0p1/vwRuEOtGv2+wrhsM8nJIDwLzRSQf6yLvcaxWM0OAR40xmwrFnikiH2NdY/nEGHOkvCs3xhwVkV+x2qYfw/pl/jDW6X81l9nTsO5VeQzrC/UUVtXLlHKWvQGrrvlyrCS9x1hNm4uc7maZD2M1qZwjIpOBDKzrEcuMMbPKsIvceX+eA74QkYlYn5k+wIVlWEeJjDEHsZqtlsUkrJZLX4jIv4FmWGdKL5u/7skZiVVt09x5Pcrd5eZi7du1WNcCemCd7X5auDrK2VJtAdDPGJNcTJytsJqwPigiB4H1WM1pHwVuNcbkeroPRKSbs6zfsD4bI7C+vz3Lsp8qNLuvugfLA7gC+A7rV0wOVvXCdGCwy3zjgV1YB4qP+OuXrGsrqTNaHhU1Hav5rQEuLjStK1YzwmNYB7Z1WM1XqxdR5gXO5S9wYxtH40YrKeAsrLrjDGAn1kHyCSDNdTmsX86bsH7hLy68H4qJwZ2y47AOtAUtZJ4oZXqpZTrn6wMswqq7PoJ18EryxfsD3IGV1E5gVV8NxP1WUqXG4OFnPNG5n05iXc95GuuGUtfPR3wZl3saWIP1w+oIVnXUnUCYSzkXOctPLCHGa7HOJD9w7t+jWNVFw7zwHe+IdU0y3Vn2N0Cbsu6nivwoaDKpVJFE5AWsU+YEY0y+H9c7BSs5dPLXOlVgE5Engd7GmH4lzPMiMNAY085/kVUeWiWliiQiLbF+Cd0KPOnPZKGUh86j9P6l2mPdnKk8oAlDFectrKqRr4BXbY5FqVIZY9xpINIO6w555QGtklJKKeUWvdNbKaWUWzRhKKWUcosmDKWUUm7RhKGUUsotmjCUUkq5RROGUkopt2jCCEAiEiYi94rIL2INBXpSRJY7p5Vn2ErbiEhrcRnWVZzDtJahjKtEZHQR08tUjq+IyGsiUly39EFJRBJFZL5Yw9HuEZGnnB0ElntZETlLRN4SkVUikufsrr+ocoaLyBKxhsPNFJGNIvKPwt8lERktfw3dW/hxS7l3QgDRG/cCjFhjD38PNMfqZ7+g2/TBwL+A3cA0e6LzuqexOopz11VY/UFNKWc5vtIGazhVxWmf5XVYve82B/6D9UP2H15Y9lys/qWWUvL437Ww+v16Eauvqi5YfYXVw+qzq7D+WH1AFdhaUpyVjSaMACLW4BlfAA2whpMs3H/+XBH5kLL3Muqt2BxYHai5jmnsMVP2gXF8Wo4XtMYaPMsWxb1H5X3vyrH8LViJfKixemr9TkSqAU+IyAum5N5b3Vn2a2PMTGeMn2P9mDiDMeYtl0kLnGXdLiJ3mtPvbv7VGJNOkNIqqcAyCmvI1FtckgUAxphlxpgyjQ/gqqD6RkQuF5ENzlP0n0QksYT51mINQNPV+VpPEVnorCo4KCJvizWGROHlbxORXSKSISJfU8TgQ0VVJYlIbxFZICLpzuq4ZBFp7+yscBjQp1B1wRMllHOViPwuIlnOOJ4VazhU1+0bICKrnXH+JCLnerhfG2D9kvXaGUZp+7m496iU967E/VJSuR5swmDgW5fEMBUrEfQp77Ll7P/sICWflQQlTRiB5e/A+oJfTT7UFKsTt6eBa7CGj/xWrNHnCosHXgCexzr13yYiPYD5wJ9Y4yXf43zt1KBHInIZ1sBMs7C6L/8da5yEEol1fWM+Vvfwo7B60f0RaOiMdQFWx3LdnY93iilnINYwnCuwqjNeA+7nzDGim2BVUzyLNbZBHazxtoWya+P865WE4c5+dorH5T0qbnoZ9ktxy4sUGou8uEehMlrhMsqcMWYnVpftp41KV4TyLFskEXGISLSI9MQas2KiObPvpD9EJFes6xw3e7KegGZ3/+r6cO+BdRA3WIPo+HI9U5zrOc9l3blYZzau8yW5LP8jsMBlWn8KjekB/ALMcZnnbVzGc8Bl7AYgBWuMDCkm9s+B5GK2qXA5S4uI8UGswXkaFVomF2hRaJ7LnTG28mC/3u8sP9pL75M7+7m496i46aXul1KWH+2cXuKj0Pw5wD1FbFsq8Fwp21+mZYv7bLjMk1kozveBkEKvDcK6NjIQ6+zmA+d893rj/QyUh55hBI6CX6hr/LCu/cY5LCqAsUZHW451MbCw3caY3wqeiEg01i/7aS6/KH/C+oJ3dNZ3twdcz5K+KCkgEamCVe3xvnF+gz3hXH8H4DOXlz7FOuPuXmjadmPM5kLP1zn/NvJg1W2ArcaYE0XE1Fis1j7rRWStiLxQ0lmMO/u50OynvUfFTS/jfimu3IJhTUt7FFbUeynFTHdVnmWLch7QC2s0v8sodGZljPnWGPOMMWaeMWaOMWYkVuOSf0g5xowPNHrRO3BUd/71R7PM/cVMc73O4BpLDazxr990Plw1Bmpjfe5c11HUOl3LFqwRysojDgjjzNgLntcsNO2IyzwFF3WLGg+8NCW1kMoFHjLGLBOrKed3WFV104uZ3539XKC4z4vr9LLsl+LKPYQ1gp27DgOxRUyvzpn73pvLFskYs8L5708ikga8LyL/McU3mvgcq2VePEHSWkoTRuAoOKA2KG1GESlo9dECqz73Eaz696FYB+whpoiL5oXUKWbaWpdprr/kjjinPYE1bKirPcABrAOk6zqKWmdhh7HG0z7j4ngZpWH9CnddX13n30PlLP8Mzl/v52D9Aj+DMWYvzkRojMkWkdWcftB3dYTS9/Op4ospw3V6WfdLUeWO4sxrKEUpOHvagMv1BhFpDFTB5fpEEcqzrDsKkkcCUForu6AZIyJoTqUqgRSscYLHFPWi80JdgSSstuLnY120fg343RjTDavKYWgp66ojIucVKrsJVnXFLyUtZIzJwKoHb2msFluujz3GmDzgN6xT/sJKjMlZ9s/AyBKqa7Ip5de/c/3LgStdXroKKyGllLS8h1o44yr1greI1MK6VvJtcfO4s5/LGqCX9ktZq6TmAINcWtBdjfXZXVjKusqzrDt6OP+W1OpwGFai3eGF9QUEPcMIEMaYdBF5CJgoIjOx2vMfwLph6UqgGtDDWZ96FnC+McaIiAGWGmPmOIsKofRf0WnAhyLyGNYX8CmsM5wpboT6IDBfRPKxTtmPY7U2GoJ1wX4T8BzwhYhMBL7EagZ5oRtlP4x1s9YcEZkMZGDVrS8zxszC+mV5mYhcjnXxc08xB89/YrX6eg+rKWYbrFZWbxtjUt2I4xRny60FQD9jTHIxsxVcf2rkjK2wVcbZFFpEIrD22SvGmPWlrNqd/VxW5dovxpiDlO0+oElYrZG+EJF/A82wzppeNoWay4rISKxWdM2d19PcWtZ5reci5/wNgWoiMtz5fHbB9SQRmYv1uVqLdYG/B9Z1jE8LqqNEZDrWD6bVWNWBVzsfd5lgGr7Y7qvu+ijbA+uX+Y9AuvOxDuvL08X5+jnAz4XmvwtrTO6C599SqAVUEeVPwWqJNBTYBGQBi3G2vHGdr5gyugJzsc6IMpwxvgxULzTPHVgH9RNY1SoDKaWVlHNaH2CRc7kjWAfrJOdrcVgJ6JCzrCdKKOdqrF/82c44ngVCS1l3vLPciwtNu8g5LbGEffoUxbcautQ5jwPrwP9yGT4LJe7n4t6jUt67EvdLact78HlOBH7A+mGyFytBOVzmGe3cV/FlWbbQ+1XUI77QfE9jNSZJd36mVgB3AmGF5nkO2Oj83J3EOhu73o5jgJ0PHaK1khGREUAfY8wtzufvATONMTOcz/cAZ5ti7lYV6wa41saYTv6JOLCJyJNAb2NMv3KW8w5W0hhr9EupKii9hlH5tMO6RlCgfcFzEakHZBSXLJRHzsP6Ve8x5014NwCdgJUi8puI3OWN4JTyJj3DUKfRMwylVHE0YSillHKLVkkppZRyS6VtVhsXF2fi4+M9Xj4jI4MqVap4L6AAEGzbHGzbC7rNwaI827x8+fI0Y0ztol6rtAkjPj6eZcs8H2QtOTmZvn37ei+gABBs2xxs2wu6zcGiPNssIsXeiKhVUkoppdyiCUMppZRbNGEopZRyS6W9hlGUnJwcUlNTyczMLHXe6tWrs359ad35VC6BsM2RkZE0atSIsLAwu0NRKugEVcJITU2latWqxMfHU9oom8ePH6dq1aolzlPZVPRtNsZw8OBBUlNTSUhIsDscpYJOUFVJZWZmUqtWrVKThaqYRIRatWq5dYaolPK+oEoYgCaLAKfvn1L2CaoqKeUfxhiOHz9BWloe+fmG6OgQ6tatgsMRdL9PlKpU9BvsZ/v27eOaa66hWbNmdOzYke7du/Pll1/6NYbt27fTunXrM6bv2LGD//3vfx6V+corr3DixAkyMzPZuHEj9erV5tChGI4cqc6ePVVZuTKXffsyyhu6UspGmjD8yBjD5ZdfTu/evdm6dSvLly9n6tSppKaeOZhZbm6u3+PbuXNnsQmjtHheeeUV9u5NY+3aTE6ezEREOOecPFq3zqN+/QxEDLt2RfHnn0d9EbpSyg+0SsqPfvjhB8LDw7nllltOTWvatCl33nknAFOmTOGbb74hMzOTjIwMPv/8c8aOHcvWrVuJjo5m8uTJtG3blieeeIKYmBjuv/9+AFq3bs2sWbMAGDx4MD179mTJkiU0bNiQmTNnEhUVxfLlyxk7dizR0dH07NnzzOCAf/7zn2zatImkpCRGjRpFjRo1Tovn8ccf56WXXjq1rjvuuINOnTpx7Ngx9uzZw6BBFxEbG8eCBd8iIjz33BPMmjWLqKgopk37giNHwkhN3UlYWDy1atXy5a5WSvlA0CaMe+65h99++63Y1/Py8nA4HGUqMykpiVdeeaXY19euXUuHDh1KLCMlJYXVq1dTs2ZN7rzzTtq3b8+MGTP44YcfGDlyZIkxA2zevJlPPvmEt99+m6uuuorp06dz3XXXMWbMGF577TX69OnDAw88UOSyTz75JG+++eaphDBlypTT4klOTi5yudtvv4N//es/TJr0A+3bV6dq1QgyMjLo1q0bzz77LA8++CAffPAu48ePZ9OmNLZt201eXgR16sSUuC1KqYpFq6RsdPvtt9OuXTs6d+58atqAAQOoWbMmAD/99BPXX389AP379+fgwYMcPVpylU5CQgJJSUkAdOzYke3bt3P06FGOHDlCnz59AE6V6Y7C8RRn8+Z0QKhd+yS1akUAEB4ezsUXX3xaHA6Hg5YtWyJyFrt2OcjNzXM7DqWU/YL2DKOkMwHwzU1s5557LtOnTz/1/I033iAtLY1Onf4a3K5wl8RFDW4lIoSGhpKfn39qWuH7EiIiIk7973A4OHnypDV4u4fNUQvHU9R6s7JySE+PRMTQsOFf84aFhZ1ap8PhOHUNJCQkhAYNhN27o9i69Shnn13do7iUUv6nZxh+1L9/fzIzM5k4ceKpaSdOnCh2/t69e/Pxxx8DVnfFcXFxVKtWjfj4eFasWAHAihUr2LZtW4nrjY2NpXr16vz0008Ap8p0FRMTw/Hjx4stp2nTpqxbt46srCyOHj3K/PnzOXLkECEhG4iNrVrisoXVrx9FWFgGx47FcOJEjlvLKKXspwnDj0SEGTNmsHDhQhISEujSpQujRo3i3//+d5HzP/HEEyxbtoy2bdvy8MMP8/777wMwbNgwDh06RFJSEhMnTuTss88udd3vvfcet99+O927dycqKqrIeVq3bk1oaCjt2rVjwoQJZ7zeuHFjrrrqKtq2bcu1115L69ZtOHHiBPXqxXHLLTczePBg+vXr59a+SEhwAML27XrXtlKBotKO6d2pUyfjOoDS+vXrOeecc9xavqL3q+QLZd3mVavSyc01JCVFl7mBAMCaNcfJzMyibdtqhIeHu71cWd7HkujAOsFBt7lsRGS5MaZTUa/pGYbyyOHDmeTkxBATYzxKFgAtWoQjsoPdu3d7OTqllC9owlAe2b07G8inadNoj8uIiIigbt26HDx4goyMk94LTinlE5owVJllZeWSmVmFyMgTREaWr6FdbGw94FxSU7O8E5xSymc0Yagy27XrBOCgQYPyt8qOiQklNPQEx49Hn9ZkVylV8WjCUGVijOHkyZ1ERqZSs2akV8qsXdsA4ezdW3wTY6WU/TRhqDJJT08nKyuT+vWLbprrifr1o4Ec0tK8VqRSygc0YfiZw+EgKSmJ1q1bc+WVV5Z4415pRo8ezeeffw7AjTfeyLp164qdNzk5mSVLlpR5HfHx8aQVOpKnpuYgUp/Y2NhylVNYSIgQHZ1JTk4kWVl6I59SFZUmDD+Liorit99+Y82aNYSHhzNp0qTTXs/L86x/pXfeeYfExMRiX/c0YRSWl5dPRkZVwsJiPW5KW5wmTcKA3zl8+KBXy1VKeY8mDBv16tWLLVu2kJycTL9+/bjmmmto06YNeXl5PPDAA3Tu3Jm2bdvy1ltvAdb1gzvuuIPExESGDBnC/v37T5XVt29fCm5UnDt3Lh06dKBdu3acf/75bN++nUmTJjFhwgSSkpL48ccfOXDgAMOGDaNz58507tyZxYsXA3Dw4EEGDhxI+/btufnmm0/rz8oaACmMb755hwcffPDU9ClTppzqov3yyy+nY8eOnHvuuUyePPmMbXYdvOmll15ydtceyaFDVkwdO3akV69ebNiwwXs7WylVbkHb+SBAUTdCXnUV3HYbnDgBl1xy5uujR1uPtDQYPvz014rp/btIubm5zJkzhwsvvBCAX375hTVr1pCQkMDkyZOpXr06v/76K1lZWfTo0YOBAweycuVKNm7cyO+//86+fftITExk7Nixp5V74MABbrrpJhYtWkRCQgKHDh2iZs2a3HLLLaeNoXHNNddw77330rNnT3bu3MmgQYP45ZdfePLJJ+nZsyePP/4433zzzWkH/YMHDZDHmDEj6NnzPF544QUAPv30Ux599FEA3n33XWrWrMnJkyfp3Lkzw4YNc3vsi6effo777nuTHj2as337Wm677TZ++OEH93eqUsqngjph2OHkyZOnuh/v1asXN9xwA0uWLKFLly4kJCQAMG/ePFavXn3q+sTRo0fZvHkzixYtYsSIETgcDho0aED//v3PKH/p0qX07t37VFnFdU3+/fffn3bN49ixYxw/fpxFixbxxRdfADBkyBBq1KgBQG5uHllZ0URGZlKvXh2aNWvG0qVLadGiBRs3bqRHjx4AvPrqq6eGnN21axebN292K2Gkp6ezbNnPPPzwKEJC8oiMdJCVpfdmKFWRBHXCKOmMIDq65Nfj4sp2RlGg4BqGK9duzV977TUGDRp02jyzZ88utZtyd7syz8/PJyUl5bSOCAt6my1q+cOHrdfi4qzu06+++mqmTZtGq1atuOKKKxARkpOT+f7770lJSSE6Opq+ffue1vU6FN1FekE8sbGxfPbZEnJyHHTsGO5xl+xKKd/QaxgV0KBBg5g4cSI5OVaLoU2bNpGRkUHv3r2ZOnUqeXl57N27lwULFpyxbPfu3Vm4cOGpLs8PHToEQNWqp3c/PnDgQF5//fVTzwuSWOEu1efMmcPhw4cBOH78EKGh26lb17r3YujQocyYMYNPPvmEq6++GrDOhGrUqEF0dDQbNmxg6dKlZ8RXt25d9u/fz8GDB8nKyjo1ul+1atVISEjgp5+mAxGkpZ1g1apVHu9DpZT3acKogG688UYSExPp0KEDrVu35uabbyY3N5crrriCFi1a0KZNG2699dZTI+gVVrt2bSZPnszQoUNp167dqYP5JZdcwpdffnnqoverr756quv0xMTEU621/vnPf7Jo0SI6dOjAvHnzaNKkCfn5+Rw5coLY2NhTv/pr1KhBYmIiO3bsoEuXLgBceOGF5Obm0rZtWx577DG6det2RnxhYWE8/vjjdO3alYsvvphWrVqdeu3jjz9m5syPuOaadnTr1pGZM2d6fd8qpTyn3ZsXQ7s3/8u+fens2hVD/frpNGzo+3G4V68+jDHHaNu2SZHVUtq9ued0m4ODdm+ubHPwYC5gqF3b855py6J+/Vxycg6U66ZGpZT3acJQJbL6jgrF4cgiPNw/H5caNWoQEhLBn38e9cv6lFLu8WvCEJELRWSjiGwRkYeLeL2ViKSISJaI3O/y2nYR+V1EfhORZa7LuquyVsH5SkbGSYyJJibGfz3JhoaGItKCI0eqn/F+6funlH38ljBExAG8AQwGEoERIuLal8Uh4C7gpWKK6WeMSSqufq00kZGRHDx4UA86ZXDkSC4QQmysd7sCKU1MTB7GRJOR8de9GMYYDh48SGSkd3rJVUqVjT/vw+gCbDHGbAUQkanAZcCpu8eMMfuB/SIyxBcBNGrUiNTUVA4cOFDqvJmZmUF3YCpqmw8dOsrx47lERdUkLc1/90WcPJlLWlooK1ZkUrv2XzFFRkbSqFEjv8WhlPqLPxNGQ2BXoeepQNcyLG+AeSJigLeMMWd0VCQi44BxYLX3T/bkzjqn9PR0YmJ83yKoIilqm++66y6ysrJO9WflL8bAxRfHExNzhBkzTr+WsWPHDq+sIz09vVyfkUCk2xwcfLXN/kwYRf08LUvdUA9jzB4RqQN8JyIbjDGLTivMSiKTwWpWW56mdNoUD7ZtO86aNT25666GtuyLc89NZvXqLnTo4KBatQivl6/vcXDQbfYef170TgUaF3reCNjj7sLGmD3Ov/uBL7GquJQPvfbaRox5k3POudCW9d99dy7QiqVLF9qyfqXU6fyZMH4FWohIgoiEA38DvnJnQRGpIiJVC/4HBgJrfBapAmDOnBzgENdf37rUeX3hb387j1q1TvDee+/Zsn6l1On8ViVljMkVkTuAbwEH8K4xZq2I3OJ8fZKI1AOWAdWAfBG5B6tFVRzwpfOu31Dgf8aYuf6KPRgZA1u2xFOv3jqio3vaEkN0dDTt2j3MzJlV3e5UUSnlO37trdYYMxuY7TJtUqH//8SqqnJ1DGjn2+hUYfPnp5Kb24g+fYof9tUfatbsw8mTnUlO3kS/fmfbGotSwU7v9FZF+uSTTQCMHRtvaxxjxtQH4N13U22NQymlCUMV49ixidSv34YLLmhmaxwXXtiQkJB9JCcH1z0xSlVEmjDUGfLz80lOTmbAgA6EhNh73SAkRIiP38Tu3a3Iy9M79JWykyYMdYavvtpCWtoUzj77CrtDAaBfv3yM2cHixZvsDkWpoKYJQ53ho4/2AkPo08ejLru87pFHGgMdWLVqnt2hKBXUNGGoM6SkhBMauouePStGn00JCQm0bt2a//3vU7tDUSqoacJQp8nNzWfv3pbEx2+3O5RTRIT69Z9i6dIvOHYsq/QFlFI+oQlDnWbGjM0YU5N+/SrWTXLt29cD6jB16h92h6JU0NKEoU6zePFKYBHXX29vc1pX117bFIBZs47YG4hSQcyvd3qrim/btqk0a/Y7vXpVrF/ybdrUJzR0E8uXV7E7FKWClp5hqFPy8/NZuDCFfv362R3KGUSERo3+YO/es8jJsTsapYKTJgx1yurVuzhyZAdwg92hFGnQoCMY8yJr11assx+lgoUmDHXKwoWZQCTnn1+xrl8UePzxPjgcz/Dpp+/YHYpSQUkThjpl8+ZaQB4XXVTb7lCK1KBBA7p06c8336TZHYpSQUkThjplz56mhIdvonr1ivuxyMp6mN9/f43jx3PtDkWpoFNxjwzKr/Lz4ciRs6lff4fdoZTIuh4fybRpu+wORamgowlDAbBr1z6MeYZBg47YHUqJrryyIQCzZx+xNxClgpAmDAXA8uWLgZcYPbqp3aGUqFOnswgJWcfSpfrRVcrf9FunAPjyy62EhTWkQ4cOdodSIofDQcuWB9mzJ55Dh47aHY5SQUUThgJg+vTLiIycSkREhN2hlOr++x1AP3799Re7Q1EqqGjCUOzbd4KTJ5sTH7/P7lDccvnlrYCVrFq10u5QlAoqmjAUH3+8CQihY8fA6HOjZs2a1K59HZ99VtPuUJQKKtr5oGLOnMMADBwYa28gZRAVNYoVKzqRnw8h+rNHKb/Qr5pi1aooIiK2Ur9+pN2huK1jx3Ty82NZskQvfCvlL5owglxeXh4nTtzOoEHT7Q6lTMaObQHA66+vsjkSpYKHJowgt2bNGjIyVnDllfXtDqVMLrookdDQ/SxapF2EKOUvmjCC3NSpG4GxdOzYy+5QyiQkRGjQYAdpaVXtDkWpoKEJI8h99VU4MIn4+CZ2h1Jmt922lJycLuzbFxjNgZUKdJowgtzWrTWIjU0lKkrsDqXMOnc+F4BVq/Q6hlL+oAkjiKWlHSQzszWtWh23OxSPtGvXDpjISy9F2x2KUkFBE0YQS0n5E6hFu3Z5dofikVq1ahEV1ZqUlAZ2h6JUUNCEEcSWLs0AoEePcJsj8VyXLpmkp8ezYYNex1DK1zRhBLHIyG+B2gwe3NDuUDx21VX1gRA++mib3aEoVelpwghiKSkpJCbWIS4u1u5QPHbllU2BbBYuzLc7FKUqPU0YQSo3N5/vv7+Bxo1vtDuUcqldO4aqVWdz7Ngmu0NRqtLThBGk5s3bTk7OMOrX72h3KOU2YMCHZGY+b3cYSlV6fk0YInKhiGwUkS0i8nARr7cSkRQRyRKR+8uyrCqbGTN2AXD55YF7/aJA27Zt2bTpDw4ezLA7FKUqNb8lDBFxAG8Ag4FEYISIJLrMdgi4C3jJg2VVGaSk5AAZDB6cYHco5da2bV8gjQce2GJ3KEpVav48w+gCbDHGbDXGZANTgcsKz2CM2W+M+RVwHcmn1GVV2WzdWovY2G2Ehwd+reSll/bE4TjGd9+dtDsUpSo1fw6g1BDYVeh5KtDVm8uKyDhgHEDdunVJTk72KFCA9PT0ci1fkR0/ns6JE9E0b36S5OS0U9MDeZvj4rLYs6cjCxYkI272chLI2+sp3ebg4Ktt9mfCKOprbLy5rDFmMjAZoFOnTqZv375uB+cqOTmZ8ixfkX333XfA+bz00renbWMgb3O3brOYOTOO6tU706FDFbeWCeTt9ZRuc3Dw1Tb7sz4iFWhc6HkjYI8fllUuUlJSEBG6dnX3BK/iGzjQGi3ws8/0jm+lfMWfCeNXoIWIJIhIOPA34Cs/LKtcvPdeG6pUWUj16tXtDsVrLr30HOCfHD/+g92hKFVp+a1KyhiTKyJ3AN8CDuBdY8xaEbnF+fokEakHLAOqAfkicg+QaIw5VtSy/oq9stmz5yxiYwOvO/OSNGrUkB495rNo0TEgsG9GVKqi8uc1DIwxs4HZLtMmFfr/T6zqJreWVWWXmWnIzm5JfPxiu0PxugsuuIQnn5zDrl3HadxYR+JTytsCv02lKpPk5INAOOeeW/maoEZHnwck8/HHenlLKV/QhBFkvv7aOpgOGlTL5ki875JLGgHZJCdn2h2KUpWSJowgk5a2CpH/cfHFbewOxetatmyKw7GSlSt1BD6lfEETRpBJS5tC+/b/oUqVyndQDQkJoVWrA+zf35QjR7LsDkepSkcTRhDJysrj55830a1bN7tD8ZlLL60FhPO//2m/Ukp5myaMIDJjxlYyMnYREXGl3aH4zDXXNAC6ASl2h6JUpaMJI4h88411wXvIkMDvobY4557bhKpV17Fu3W92h6JUpaMJI4j8+mseIsfp27eJ3aH4jIjQvPlwPv+8C3l5dkejVOWiCSOI7NhRgxo1duBwVK67vF21ajWUfftGMmfOTrtDUapS0YQRJNLSDnPy5Nm0bHnC7lB87rbb2gPw0UebbY5EqcpFE0aQWLLkF+BeRoyo3GcXAD16NEDkACtWRNgdilKViiaMIPHbbz8j8g6jR7eyOxSfCwkR4uI2s3Nn5b1Wo5QdNGEEiV9/zaNOnZ5UrRocnfIlJh4iKyuCgwddR/tVSnlKE0aQ+OmnK0hPn2h3GH4zZsxxoB67d6+3OxSlKg2PEoaIJIrIYBEpsityVbHk58PRo82pVy/V7lD8plcvazTBOXPm2ByJUpWHp2cYTwJVgXEi8r4X41E+8PPPhzCmKh06BM+NCc2aNSM+/jWefbaT3aEoVWl4OoDSd8aYacA0bwajfGP69B1ATS66qI7dofhVs2Zt2b69Jzt3HqVJk8ozHK1SdvH0DOM8EflCRN4Wkb97NSLldT/+mAGcYOjQRLtD8asBA6KAEKZO3W53KEpVCp4mjDXGmKHArcB8L8ajfCAs7BXOPvtuqlWrfF2al+TqqxOAfObPz7A7FKUqBU8TxsUicifQzBizypsBKe/Ky8tj1apvGTAg+G5iS0iIIyxsI6tXV7E7FKUqhVIThog8JiL3uUy+GtgMDBWRt30SmfKK5OSNpKePo2XL/naHYosWLVZx/PhScnNz7Q5FqYDnzhnG9cBpDfiNMfuARoAYY27yRWDKO6ZNSwX+Q+PGXewOxRbPPx9NRsYtzJ492+5QlAp47iSMk8aYonqs+wC4zsvxKC9bujQbyGXQoIZ2h2KLiy66iMjIaGbPXmp3KEoFPLcShojUd51ojLGORKpC27UrjujoHURFVf5OB4sSGhpKePhipk693O5QlAp47iSM/wAzRaRp4YkiUgfI90lUymuOHWtO3brBc4d3UZo3P87Ro+eSnp5tdyhKBbRSE4Yx5jPgDWC5iMwSkWdE5DlgMfCSrwNUntuzJ5u8vBo0a3bM7lBs1b9/CFCFTz/dancoSgU0t5rVGmPeBxKw7uwOAzKBEcaYj30YmyqnXbtWAjGMGhXcNYfXXtsYgK++OmJvIEoFOLe7BjHGHMe60K0CREpKCpDF+ed3tTsUWyUlNcbh2MCKFVF2h6JUQNPuzSuxyZObEBv7BA0aNLA7FFuJCO3afYHI63aHolRA04RRSRkDGzf2Ija2r92hVAjXXx/Nrl3vsGLFCrtDUSpgacKopH75ZQ/5+bXp1MnYHUqFMHr0aCIi+vLsswvtDkWpgOVp9+aqgvvss21AAwYPDq4uzYsTGxtLWNh/mTfvgN2hKBWw9AyjkrK6NM9l+PAWdodSYbRuvY/09Hb8+edRu0NRKiBpwqik9uzZTfXqq6hWLczuUCqMiy+OAiJ5772NdoeiVEDShFFJ5eY+wvDhE0ufMYiMHXsWkMusWUV1jaaUKo0mjEooOzubffv20bhxY7tDqVDq14+hSpX1rF2rw7Uq5Qm/JgwRuVBENorIFhF5uIjXRURedb6+WkQ6FHptu4j8LiK/icgyf8YdaP773z8xZg1RUW3tDqXCueqqL8nK6k92tvYrpVRZ+S1hiIgDq0+qwUAiMEJEXAeZHgy0cD7G4TIOB9DPGJNkjOnk63gD2Zw5aUBLBgw41+5QKpwhQ84lM/MIK1eutDsUpQKOP88wugBbjDFbnV2jTwUuc5nnMuADY1kKxBbVtboq2W+/CQ7HFpKStIWUqx49egCP8dxzeh1DqbLyZ8JoCOwq9DzVOc3deQwwT0SWi8g4n0UZ4IyBPXsaUr/+bkSCcwyMktSrV4/o6Av4/vvmdoeiVMDx5417RR29XG9DLmmeHsaYPc5xOL4TkQ3GmEWnLWwlknEAdevWJTk52eNg09PTy7W8XbZsOUle3mCaNNlf5vgDdZvLKinpAEuW9Gb+/O/sDsXvguU9Lky32Xv8mTBSgcLNdhoBe9ydxxhT8He/iHyJVcV1WsIwxkwGJgN06tTJ9O3b1+Ngk5OTKc/ydtmw4VvgfW66qR19+yaVadlA3eayevnlo3TrBt9/7+Dpp/vaHY5fBct7XJg/t/nPP/9k8eLFpx4bN27krLPOYvjw4YwbN46aNWv6JQ5fbbM/q6R+BVqISIKIhAN/A75ymecrYKSztVQ34KgxZq+IVBGRqgAiUgUYCKzxY+wBY8uW74iIuJkRI86xO5QKq0uX6oSH72HdOm12rNyXlZXFrFmzGDlyJE2aNCEmJoaYmBhiY2OJi4ujTp061K9fn+HDhzNx4kQiIiIYMWIE4eHhjB8/nmbNmvHMM8+Qnp5u96Z4zG9nGMaYXBG5A/gWcADvGmPWisgtztcnAbOBi4AtwAlgjHPxusCXzjr5UOB/xpi5/oo9kCQnr6Njx05ERETYHUqFJQJJSZv59dfdpKenExMTY3dIqoI6ceIEq1at4qOPPuLjjz/m6NGj1KhRgwsvvJD69a32OLm5uacezZs3p0+fPrRv357w8PBT5fz+++889thjPPbYY7z22ms89NBDjBkzhho1ati1aR7xa+eDxpjZWEmh8LRJhf43wO1FLLcVaOfzAANcfr5h+fIPaddund2hVHjPPJPNwIGj+PHH2gwePNjucGxjjGHFihWsWbOGQ4cOkZ2dTUJCAh06dKB58+ZB03Di8OHDzJkzh8WLF7N9eyo7dx4iNfUwR45kAlUIDf2DESOG0qvXDURH9yA7O5QTJyAjA06cgPvvh5gY+OILeO01a/rJk5CTA0ePwpIlbZgxYwajRu3m44+rc9992dx//ybatNnDAw8kcO217QJiX2tvtZXI8uUHgThatsyxO5QKr0ePHoSFhTF37qKgTBjZ2dl8+OGHvPDCC2zatKnIeRo1akTfvn3p3bs3zZo1o0GDBjRo0IBq1arZdnDLzoZ9+6yD8cmT1sH6xAlo0wbq1IEdO2DWrNNfW7++BXXrwjnnwIIF8Mwz1uvHjuWyf/8xjh3LISfnQvLzfyMy8i4yM2eesd6ffjpM1641eOklGFdEG81x46yEsX07pKRAlSoQGQkREVZc+fnWfJdc0pAaNSA1NY3Fi2uxenVnrr/+IK+91pfJk1+lXbuK/btYE0YlMn/+USCOzp0r/i8Vu0VHR1OlykzeeacB//d/dkfjX2vWrOGKK65gy5YtdOzYkXfffZeePXsSFxdHaGgof/zxBykpKSxYsIBvv/2Wjz766LTlIyIiiI6OJjw8goiIaoSHVycmJpKzzqpLQsLZhIZ2oXbtpsTFNSYysiYnTwrt2kG7dnD4MLz8svULPD39rwP72LEwZAhs3AjDh1vTCpLCyZPw9ttw/fXwyy/Qq9eZ2zR9OgwdCuvXwx13/DU9PBwiI+tw4AA0bXqCNWu2snlzdY4f/5OjR/dizAliYyMZO/Y6Ro6cSGhoF+bMsQ7+0dHWo0oVSEy0qo5GjoSLLrKmFbweFQUhzqvBf/+79SjO8OHWA+KAODZuTOeddxbxwQcb6NatG1OnTuWyy1xvT6s4NGFUIvPnHwOyGDKkid2hBISEhBxWrmzNpk0HOfvsWnaH43P5+flMmzaN//53J9HR/bj33s8566y2HDokrF8Pl15qzTd5chLHjiWRlXUrbdsaDh3KolOnPfTr9zO7du3liSdu5vjxUHJzwyhoN5OQ8BmrV/+DGTOSyc199ox1d+kyi2uv3Ur16ufy7LP9iY6GmBg5ddA9fNiaLzoaWrSwDsJRUX8dkM9xtuE4+2yYPNmaVvig3bJlHjt2pCKynQkTdrNnzxZ27tzIzp3b2L59OxdddIyMjAzAGrK3Y8eOnH/++YwcOZLExNM7nOhUQj8SdepYD29p2TKGF18cxj339KZz57kMG/YlCxbUpFdRWbEC0IRRiaxeHU5Y2DpatUqyO5SAcMEF2axc6eCNNzbxf//X3e5wfCYvL5+pU3/m3Xf/wQ8//EBk5AGOHIljwoS/5rnqqr8SxoIFkJUFVatClSpCrVqRtG7djBEjmgGwc6dV3VJwMI+Ohk6drqRHjyvJzs7j44/3cehQKnv3bmXHjnX88cca1q9fyt13p55aX0YGVK1aj7p146lXrx4LF8axZk0t4uLiuPTSOGrUqEFYWBgOh4OQkBCOHXOwcKGD9PR0QkP3s3v3fvbv38/OnTvZsGEDmzZtOq1/MIfDQZMmTUhISKB169a0adOGOnXqcNZZZ9GvXz+/NW91V716tWna9G/s23cFl1wykA0bvqBevXp2h3UGTRiVhDGG3NyX6NKlNSLt7Q4nIAwYUIOXXkrjm29MpauW2rp1KzNmzGfGjGosXdqRnJyzqFVrI3fffTejR9ciNPT0KpeoqL+WXb++5LJff73418LDHYwZUxerYWPHU9ONMRw4cIA//viDbdu2nfb4448/+Pnnn0lLSyMnx/3rb9HR0TRs2JBWrVoxePBgWrRoQbNmzWjWrBmNGzcmNNQ6vAXCvScOB0ybFkabNiEcO/YiN9xwK7NmfVHhLoRrwqgktm/fzqFDUxgxooRvszpNWJiDJk3Ws3VrIjk5+YSFBX5v/4sXL2b8+Df58cfOwFigGtWqbWTs2E38+9+bWLnyF5KS/H8QEhHq1KlDnTp16N696LM5YwzHjx8nLS2Nw4cPk5ubS15eHvn5+eTl5ZGXl0dMTAx16tShdu3aVKlSxc9b4VuNG8NLLzm46aYezJ49gw8++IBRo0bZHdZpNGFUEtOmrQK60aNHb7tDCSjXXZfBs8/+neXL76Zbtw6lL1CB/fzzz1xwwQXExFxESMidXHzxScaPN3Tr1hJoaXd4pRIRqlWrRrVq1ewOxTY33ghffmmYN+8f3HtvR4YMGUJcXJzdYZ0S+D+pFAAffhgDfE9ionZpXhZ33dUBkQ/57rtv7A6lXPbt28eIEddQt25d1q17i927HcycGUO3bhWrSkOV7v33hW+/3cvx4zt48MEH7Q7nNJowKont2+OIjd1GeLi+pWVRp04d2rUbzHvvhWBcu8IMEPn5+QwePJidO59j8OCF1K4dRwW8XqrcFBcH/fu34t577+e991JZtGhR6Qv5iR5dKoHcXMjIOJtGjfbZHUpAatjwNrZte5SFCw/ZHYpHvvrqK1aubEFe3tU0bdrU7nCUlzRq9CQwj+uu+7DCjBCpCaMS+PXXdCCaxMQMu0MJSHfd1RDIY8KE3XaH4pFnn/0vISGT6NzZcP/9dkejvOXmm8Np1CidXbse5amnXrM7HEATRqXw+efbABg0qPLffOYLF1zQlsjIJcydW52cnFy7wymTJUuWsGzZtYSEVOWDD4RQbcZSaUREwKefxiDSiOefb8wff/xhd0iaMCoDkc9xOPpx5ZVJdocSkEJCQrjuugiys5swadLPdofjNmMMd9zxGjCchx7Ko1UruyNS3nbeeXD33SfJz7+Kyy+fjrH5QpsmjEpgyZLv6No1m6pVK1e7dH968sl2QBZTp261OxS3LVq0iJUrpzJ+/HQef1y7s6+s/vWvqnTvvpY1az5n2rRptsaiCSPA7d+fztKll3DuucPtDiWgNWgQwZVX3smGDfeU6W5jOz377HRq1KjBP/5xCYWGXlCVTEQE/PhjKzp2zOe2227j55/tOwvWhBHgPvxwHcaMp1GjvnaHEvCuueYiDh06xIIFyXaHUqr33z/Ad9+9Ss+erxIdHW13OMrHHA4H7777KVlZ7zBkyLdkZmbaEocmjAA3e7bVFPT661vYHEngGzhwEA7HNzz4YJ7doZTo2DG4444IYDUvvlgxezVV3temTXMSE3tx8ODD/P3v79oSgyaMALdhQ3XCwnaSkKDDjJZXdHQUdevW5fff25CdXXFbSz3wQC7p6TH06vU+LVvqfRfBQgS++SaOiIhMJk3qwpo1/m81pQkjgBkD+/efRe3a9je3qyyGDcsjP78hkyb9ZncoRfrhB5g8ORSYwOOPB99IgcGudm147bVcjOnExRev9fv6NWEEsN27s8jNDefsswPzDuWK6NFH2wCZTJx4xO5QirRhwyFgOZddtpzzzz/f7nCUDW66qSY9e65kx46uTJv2rV/XrQkjgG3ZkgLU4PbbtYmMt9StG0VCwmo2bOjIgQPpdodzhu3b/4VIVyZMeLbCjZWg/GfGjHM566zLefTRO/x6AVwTRgCbP38+ISHCBRfohU9veuCBcOB5Zsz42u5QTklJgTfeOMHEiW9x9dVXkpCQYHdIyka1aoXz5ptPsWXLdkaNet9v69WEEcDeeqs7jRpNIDY21u5QKpVbbmlHQsLnfPrpf+0OBbCGS73pJnjkkRzS03N56KGH7A5JVQADBgygdeuPmTbtZj788IRf1qkJI0AdOHCMAwfOp0GDJLtDqXREhGuuGcv8+fX46ac9dofDk0/C2rWQlTWaYcMGk5SUZHdIqoKYPLkZsJzbbjPs80Nn1ZowAtT7768GIrjwwhp2h1IpXXLJ9cB7PPzwdlvj+PVX+Pe/DdHRU3E45vHiiy/aGo+qWLp370RS0qtkZDi45hrj8zFdNGEEqPnzTwJw3XVn2RxJ5dS1a1Pq1PmZlJSzOXHCnnsycnNh5MhcHI59wH0sXLhQr12oMzz66CUY8xA//CC85uNe0DVhBKh162rhcGynefMou0OptMaNc5CfH8cTT6ywZf0LF87nwIGxwBi++OJdOnXqZEscqmK77LLLqFPnc1q0+Iz+/X27Lk0YAerkyU00bJhidxiV2j/+0RmHYzf//a/Dr+vNyMjg8suHccEFFxAbu4RffnmeQYMG+TUGFTjCwsIYN24smzdfRWrqXIyBvDzfNLnWhBGA9u/fz4EDIxg3LnC64g5EERGh9OmzmUOHQlm1apNf1pmbm8vQoVczc+b9DBnyPatXr9aL3KpUjzzyCM2bN+fZZ19iwACYPds3g7prwghAX389H4ALL7zQ5kgqvylTziE8vCtvvvkfv6zvpZdeYt68NkB3Ro8+X3uiVW6JiopizJgx/PTTfG69dS8XX7zXJ+vRhBGAnnmmCaGhK2jfvr3doVR6jRvXZezYMbz33pesX/+nT9e1adMmHn/8I0JCnmDoUBiuQ5yoMrj22msB2LDhXXzVCYAmjACTl5fPzp3xNGiQRUiIvn3+cPPND5CTs4HRozf4dD2PPPIo+fmTqFYtjFdf9emqVCUUHx9P7969+fDDD302lKsecVwYY5g7dy7Z2dl2h1Kkb7/dQH5+Q/r00bfOX5KSmtGkyXp++aUjW7ce9sk6Vq1axfTpWzGmOy++GELDhj5Zjarkrr/+ejZu3MjGjRt9Ur4edVz88ssvDB48mK+/rjj9CBX21VfWAWvYsNo2RxJcXn65DlCV667zfhNbYwyPPfYY1av/wZIl6dxwg9dXoYLE8OHDiYiIYN68eT4pXxOGi65du9KlSxfmzv3B7lCKtHRpFHCQgQMb2B1KUBk2rAVNmy4lJaUbK1Z494LiK6/8H19/fYDx48fTtWt1n9U/q8ovNjaW6667jnAfDfKuCcOFMZCe/iFbttxLamqq3eGcISfnYxo3nkhUVITdoQSdKVMaAmHcddd8r5WZkZHBY4+lAil06fKg18pVweudd97hlltu8UnZfk0YInKhiGwUkS0i8nARr4uIvOp8fbWIdHB3We/FCF261ALG8tZbFevGuLS0NDZseIUxY3LsDiUo9e3bmDFjniclZRTLly/3SpnPPfcJGRlP0LnzEfr21VMLVbH5LWGIiAN4AxgMJAIjRCTRZbbBQAvnYxwwsQzLes1//lMTkUymTIn11So88vbbi8nPb8eQIZfYHUrQmjDhHurUqcPIkc+Qm1u+PqaOHUvnxRdb4XAIn30Wq1VRqsLz5xlGF2CLMWarMSYbmApc5jLPZcAHxrIUiBWR+m4u6zU1awpnnbWY1NTezJ69zFerKbPJk6siMp8OHTqUPrPyierVq3PrrR+zbt1nXH99+a5zXXPNQnJyevLAA/to2tRLASrlQ/5MGA2BXYWepzqnuTOPO8t61f33RwARPPJIxeh+Iycnhx074mnSZDuhoXrpyU6PPNKPWrXWMnVqLz7/3LMuQ/bv38/338+kYcMlPPdcMy9HqJRvhPpxXUWdcLveXVLcPO4si4iMw6rKom7duiQnJ5cxxL80aJBO+/bPs2HD68ybF+uzVgfuWr78AMZcSYsW35CcfNQn60hPTy/XPgs05dneF188wQ031GXEiFCMmUft2mX7fDz99NPk5S3iued6snCh/+75Cbb3GHSbvcoY45cH0B34ttDz8cB4l3neAkYUer4RqO/Osq6Pjh07mvJYsGCBmTt3rgHMzJkzy1WWN9xyy68GjJk2ba3P1rFgwQKflV0RlXd7J0xYbiDb1Kix2qSnZ7u93B13LDNwg3niiSfLtX5PBNt7bIxuc1kBy0wxx1V/1m38CrQQkQQRCQf+BnzlMs9XwEhna6luwFFjzF43l/W6/v37ExMzgjvvtH/MiXnz8hHZzcUXa/VFRXHPPR0YN+5HDh+eyo03jnKrd4CvvtrC66+3omrVm3joIZ819lPKJ/yWMIwxucAdwLfAemCaMWatiNwiIgWNhmcDW4EtwNvAbSUt6+uYw8LC6Nt3FDt3DuCNN+y7+J2bm8vRo1cyaNCrREVF2haHOtNbb/Xn3/+uztSpn9C3701s23aw2HlnzdrA0KFCSMgJfvihMZGR9lZzKlVW/ryGgTFmNlZSKDxtUqH/DXC7u8v6w7vv9qFOnaO8/HIutxcZme8tWrSIgwd3cuONXewJQJXowQcfJDKyKXfffT4tWhxl9OgVTJjQk6pVrTPT9PR0br01mY8+6oNIHlOmHKZTJx1qVQUebW5Titq1I2nVKoWtWzuyY4c9N8z96197CQ19ikGDdPyLiuquu67mvfeOEBYWyn//O4Bq1Q5RvfpSWrceS926dfnooy+pUWMnKSknGTlSk4UKTJow3PDAA1GAg3HjVtqy/p9/bktExHBiYqrYsn7lntGjzyI9vTHPPbeG5s0PAbFERFRj7Nix/PjjWPbvT6Rr1/p2h6mUx/xaJRWoxozpzVNPfU5KyjxyczsQGuq/3bZjRybHjiXSocMc4By/rVd5xuEQxo9vzfjxBVNesTEapbxLzzDcICJMmBDK8ePvsGDBAr+u+7nn1mOd3dT063qVUsqVJgw3DR48mJiYBjz99A58NJhVkb78MgSHYwtjx+oFb6WUvTRhuCkyMpJzz32aH3+8kblzj/tlnQcPHiYtbRtdumwiLExrD5VS9tKEUQb/+U8ScIg77tjsl/V9/fVMjLmCV1+t45f1KaVUSTRhlEGPHh1o3TqFrVvbsmlTps/Xl5y8hurVq9OxY0efr0sppUqjCaOMHnmkBiA88MA2n67nwIF83n//OerVewbRgRKUUhWAJowyuvrqbkRFzWXp0uK7gPCG559fD4Rz/fXNfboepZRylyaMMgoJCeGRR9axf38vPv30U5+t53//yyYkZBv33dffZ+tQSqmy0IThgfHj/05iYiLPP/+uT5rYrl+/j337WtOx43YiIyO8vwKllPKAJgwPOBwOBgx4mlWrvmbSpI1eL3/ChD+AMO6+26eDCiqlVJlowvDQ+PHnA0f5179Oer3sQ4feIzLyHq6+Wq9fKKUqDk0YHqpbtzpJSUvZubMtv/3mvRv5tm3bxowZ73H77aGEhjq8Vq5SSpWXJoxyeO65JkAe99zjvRv5rrtuJXAt9957r9fKVEopb9CEUQ6DB7cjLi6ZRYsS2LZtb7nL27nzKEuWDKRZs5to2FCvXyilKhZNGOX0zjt1iYjoxQMP3Fnusp56ahsQw3332T+GuFJKudKEUU6XXdaWBx4YyvTp01m2bJXH5Zw8mceHH9YmPHw5Y8a08WKESinlHZowvOCee+4lNPQrhg/f73EZN9+8mOzshtxzz3HCw8O9GJ1SSnmHJgwvqFmzBp061WbHjv5MnryizMsfPXqUmTMnUbv2XJ5/vo8PIlRKqfLThOEl06e3xeE4wF13VSU9PadMy3766accO/YJM2ZUIyREOxpUSlVMmjC8pEGDaB59dAdZWS245JJf3F5u8eLd3H33MTp06E737t19GKFSSpWPJgwvevLJrsTHJ5Oc3JDk5JRS58/Jyeeii/aRmTmOCRP+p92YK6UqNE0YXrZwYXuaNLmGESOGsmXLlhLnHTJkNceOdWDMmNX07h3vnwCVUspDmjC8rEmT6syZ8w7Z2Xl06vQVycl/nDGPMYYhQ5L57rskGjX6msmTu9kQqVJKlY0mDB9ITExk8uRkjh27gQsuCOehh34kIyMLgB07djB27P3Mnt2Rxo0Xs27dAEJDQ22OWCmlSqcJw0eGDUtkypRdhITACy/0IiYmm5CQg8THx/P++xO4+urJ/PFHd6pWjbQ7VKWUcov+tPWhkSNbc+WVeTzzzHLmzMkiOzuD0aNfYdiwS0lISLA7PKWUKhNNGD4WFeXg2Wc78uyzBVMG2BmOUkp5TKuklFJKuUUThlJKKbdowlBKKeUWTRhKKaXcoglDKaWUWzRhKKWUcosmDKWUUm7RhKGUUsotYoyxOwafEJEDwI5yFBEHpHkpnEARbNscbNsLus3Bojzb3NQYU7uoFyptwigvEVlmjOlkdxz+FGzbHGzbC7rNwcJX26xVUkoppdyiCUMppZRbNGEUb7LdAdgg2LY52LYXdJuDhU+2Wa9hKKWUcoueYSillHJL0CUMEXlXRPaLyJpC02qKyHcistn5t0Yxy14oIhtFZIuIPOy/qD3n6faKSGMRWSAi60VkrYjc7d/IPVee99g5r0NEVorILP9EXH7l/FzHisjnIrLB+X5391/knivnNt/r/FyvEZFPRCQghr4sZpuvdG5LvogU2zLKG8evoEsYwBTgQpdpDwPzjTEtgPnO56cREQfwBjAYSARGiEiib0P1iil4sL1ALnCfMeYcoBtwe4BsL3i+zQXuBtb7JjSfmYLn2/x/wFxjTCugHYGz7VPw7LvcELgL6GSMaQ04gL/5NlSvmcKZ27wGGAosKm4hbx2/gi5hGGMWAYdcJl8GvO/8/33g8iIW7QJsMcZsNcZkA1Ody1Vonm6vMWavMWaF8//jWAeRhr6L1HvK8R4jIo2AIcA7vorPFzzdZhGpBvQG/ussJ9sYc8RngXpRed5nrNFGo0QkFIgG9vgiRm8rapuNMeuNMRtLWdQrx6+gSxjFqGuM2QvWgRKoU8Q8DYFdhZ6nEiAH0CK4s72niEg80B742feh+Yy72/wK8CCQ76e4fMmdbW4GHADec1bDvSMiVfwZpJeVus3GmN3AS8BOYC9w1Bgzz69R+p9Xjl+aMNwnRUyr9E3MRCQGmA7cY4w5Znc8viQiFwP7jTHL7Y7Fj0KBDsBEY0x7IIOSq+sCnvO6xmVAAtAAqCIi19kblc955filCcOyT0TqAzj/7i9inlSgcaHnjQiQ09giuLO9iEgYVrL42BjzhR/j8wV3trkHcKmIbMc6Ze8vIh/5L0Svc/dznWqMKTh7/BwrgQQqd7b5AmCbMeaAMSYH+AI4z48x2sErxy9NGJavgFHO/0cBM4uY51eghYgkiEg41kWyr/wUn7eVur0iIlj12uuNMS/7MTZfKXWbjTHjjTGNjDHxWO/vD8aYQP7l6c42/wnsEpGWzknnA+v8E55PuPNd3gl0E5Fo5+f8fALnQr+nvHP8MsYE1QP4BKveMgcr694A1MJqUbHZ+bemc94GwOxCy14EbAL+AB61e1t8ub1AT6xT1tXAb87HRXZvj6/f40Jl9AVm2b0t/thmIAlY5nyvZwA17N4eP2zzk8AGrBZGHwIRdm9PObb5Cuf/WcA+4Ntitrncxy+901sppZRbtEpKKaWUWzRhKKWUcosmDKWUUm7RhKGUUsotmjCUUkq5JdTuAJSqTEQkD/gd67u1DbjeBEjfTEqVRs8wlPKuk8aYJGP1gnoIuN3ugJTyFk0YSvlOCs4O3kSkuYjMFZHlIvKjiLQSkeoisl1EQpzzRIvILmeXLEpVOJowlPIB5/gD5/NX9wuTgTuNMR2B+4E3jTFHgVVAH+c8l2DdpZvj73iVcodew1DKu6JE5DcgHlgOfOfs8fc84DOr6yIAIpx/PwWuBhZg9e/zpj+DVaostGsQpbxIRNKNMTEiUh2YBXyGNUraRmNM/SLmjwHWYo038huQYIzJ81/ESrlPq6SU8gFnddNdWNVPJ4FtInIlWD0Bi0g753zpwC9Yw6TO0mShKjJNGEr5iDFmJdY1ir8B1wI3iMgqrDOKwsNjfgpc5/yLiHQSkYAaIlYFB62SUkop5RY9w1BKKeUWTRhKKaXcoglDKaWUWzRhKKWUcosmDKWUUm7RhKGUUsotmjCUUkq5RROGUkopt/w/Udchj+a7d/MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    #Index from each dataset\n",
    "    iTrain_ = []\n",
    "    iVal_ = []\n",
    "    iTest_ = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    t_train = []\n",
    "    t_val = []\n",
    "    t_test = []\n",
    "    \n",
    "    predictedValue = predicted[t_len*i:t_len*(i+1),:]\n",
    "    y_corres = y[t_len*i:t_len*(i+1),:]\n",
    "    \n",
    "    l2_error_Cm = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    print('L2 error of Cm: {0:0.4f}'.format(l2_error_Cm))\n",
    "    \n",
    "    cm_ = predictedValue#denormalize(predictedValue)\n",
    "    Cm = y_corres#denormalize(y_corres)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        iTrain_.append(predicted[index])\n",
    "    for jj, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        iVal_.append(predicted[index])    \n",
    "    for kk, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & (index_test>=i*t_len))]):\n",
    "        iTest_.append(predicted[index])\n",
    "        \n",
    "#     iTrain = denormalize(np.array(iTrain))\n",
    "#     iTest = denormalize(np.array(iTest))\n",
    "#     iVal = denormalize(np.array(iVal))\n",
    "    iTrain_ = np.array(iTrain_)\n",
    "    iVal_ = np.array(iVal_)\n",
    "    iTest_ = np.array(iTest_)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        t_train.append(t[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        t_val.append(t[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & ((index_test>=i*t_len)))]):\n",
    "        t_test.append(t[index])\n",
    "        \n",
    "    tTrain = np.array(t_train)\n",
    "    tVal = np.array(t_val)\n",
    "    tTest = np.array(t_test)\n",
    "        \n",
    "    Cm_trainTestSplit_Plot2(i, Cm, cm_, tTrain, tVal, tTest, iTrain_, iVal_, iTest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b420f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 error of Cm: 0.0135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    #Index from each dataset\n",
    "    iTrain_ = []\n",
    "    iVal_ = []\n",
    "    iTest_ = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    t_train = []\n",
    "    t_val = []\n",
    "    t_test = []\n",
    "    title_n_Cm = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_m$ prediction, $L_2$ error=%.4f'%(l2_error_Cm)\n",
    "    \n",
    "    title_Cm = title_n_Cm\n",
    "    savename1 = \"CmComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "\n",
    "    predictedValue = predicted[t_len*i:t_len*(i+1),:]\n",
    "    y_corres = y[t_len*i:t_len*(i+1),:]\n",
    "    \n",
    "    l2_error_Cm = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    print('L2 error of Cm: {0:0.4f}'.format(l2_error_Cm))\n",
    "    \n",
    "    cm_ = predictedValue#denormalize(predictedValue)\n",
    "    Cm = y_corres#denormalize(y_corres)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        iTrain_.append(predicted[index])\n",
    "    for jj, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        iVal_.append(predicted[index])    \n",
    "    for kk, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & (index_test>=i*t_len))]):\n",
    "        iTest_.append(predicted[index])\n",
    "        \n",
    "#     iTrain = denormalize(np.array(iTrain))\n",
    "#     iTest = denormalize(np.array(iTest))\n",
    "#     iVal = denormalize(np.array(iVal))\n",
    "    iTrain_ = np.array(iTrain_)\n",
    "    iVal_ = np.array(iVal_)\n",
    "    iTest_ = np.array(iTest_)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*t_len) & ((index_train>=i*t_len)))]):\n",
    "        t_train.append(t[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*t_len) & ((index_val>=i*t_len)))]):\n",
    "        t_val.append(t[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*t_len) & ((index_test>=i*t_len)))]):\n",
    "        t_test.append(t[index])\n",
    "        \n",
    "    tTrain = np.array(t_train)\n",
    "    tVal = np.array(t_val)\n",
    "    tTest = np.array(t_test)\n",
    "        \n",
    "#     Cm_trainTestSplit_Plot2(i, Cm, cm_, tTrain, tVal, tTest, iTrain_, iVal_, iTest_)\n",
    "\n",
    "    # CD graph plot\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm_), 'k--', label='Predicted value')\n",
    "    #plt.scatter(tTrain, iTrain, color='b', label='Training set')\n",
    "    #plt.scatter(tVal, iVal, color='g', label='Validation set')\n",
    "    plt.scatter(tTest, denormalize(iTest_), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper right')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    #plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm_), 'k--', label='Predicted value')\n",
    "    plt.scatter(tTrain, denormalize(iTrain_), color='b', label='Training set')\n",
    "    #plt.scatter(tVal, iVal, color='g', label='Validation set')\n",
    "#     plt.scatter(tTest, denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper right')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    #plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()\n",
    "    plt.plot(t[:1000], denormalize(Cm), 'k-', label='Ground truth')\n",
    "    plt.plot(t[:1000], denormalize(cm_), 'k--', label='Predicted value')\n",
    "    #plt.scatter(tTrain, iTrain, color='b', label='Training set')\n",
    "    plt.scatter(tVal, denormalize(iVal_), color='g', label='Validation set')\n",
    "#     plt.scatter(tTest, denormalize(iTest), color='r', label='Test set')\n",
    "    plt.xlabel('Rev.')\n",
    "    plt.ylabel('$C_m$')\n",
    "    plt.title(title_Cm, fontsize=15)        \n",
    "    plt.legend(loc='upper right')\n",
    "    #plt.ylim([0, 0.0042])\n",
    "    plt.grid()\n",
    "    #plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f20d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

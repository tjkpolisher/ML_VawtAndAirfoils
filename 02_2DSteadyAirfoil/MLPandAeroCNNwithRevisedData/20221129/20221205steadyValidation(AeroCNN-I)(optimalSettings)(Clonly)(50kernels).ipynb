{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc91fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the steady-state simulation - Case 2: AeroCNN-I\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1130c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining parameters and hyperparameters of the model\n",
    "\n",
    "n_kernels=50 # Number of kernels in convolutional network\n",
    "n_units=128 # Number of units in the hidden layer of the MLP network\n",
    "input_size = 100 + 3 # Size of input for the network (100 coefficients and 3 other parameters, AoA, h, beta)\n",
    "lr = 1e-04 # Learning rate of the network\n",
    "test_rate=0.1 # Defines the ratio of training dataset and test dataset\n",
    "val_rate=0.2\n",
    "n_data = 16 # Number of txt files from which the aerodynamic coefficients are extracted\n",
    "batch_size = 20 # Mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d84b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing working directory\n",
    "\n",
    "main_directory = 'D:\\\\VAWT_data\\\\flap_steady\\\\flap_steady'\n",
    "os.chdir(main_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic parameters\n",
    "\n",
    "c = 1 # Chord length\n",
    "h = np.array([0.01, 0.02, 0.03]) * c # Height of the Gurney flaps\n",
    "t = 0.02 * h # Thickness of the Gurney flaps\n",
    "alpha = np.linspace(0, 16, 9).reshape((9,1)) # Angles of attack\n",
    "beta = np.linspace(30, 90, 5).reshape((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18aaa915",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h.reshape((-1,1))\n",
    "t = t.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9745480",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alpha = alpha.shape[0] # Number of the angles of attack\n",
    "n_beta = beta.shape[0] # Number of the Gurney flap inclination\n",
    "n_h = h.shape[0] # Number of the height of the Gurney flaps\n",
    "n_cases = n_data * n_alpha # Total number of cases(Number of geometries * Number of angles of attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d7f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Input dataset\n",
    "# Defining the angles of attack\n",
    "\n",
    "aa = np.zeros((n_cases,1))\n",
    "for i in range(0, n_data):\n",
    "    aa[n_alpha*i:n_alpha*(i+1),:] = alpha[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5014fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = aa / np.max(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa96208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937cc8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 1)\n"
     ]
    }
   ],
   "source": [
    "# Defining beta, the Gurney flap inclination\n",
    "# In case of mere NACA0018, the bb in those indexes are considered as zero.\n",
    "beta_0 = np.zeros((n_alpha,1)) # Values for sheer NACA0018\n",
    "b_ = np.ones((n_alpha,1)) # Template for the inclination for a single h and single beta\n",
    "bb_imp = np.zeros((n_alpha*n_beta,1))\n",
    "\n",
    "for j in range(n_beta):\n",
    "    b_imp = b_ * beta[j]\n",
    "    bb_imp[n_alpha*j:n_alpha*(j+1),:] = b_imp[:,:]\n",
    "    \n",
    "bb_imp = bb_imp.reshape((-1,1))\n",
    "\n",
    "bb = np.vstack((beta_0, bb_imp, bb_imp, bb_imp))\n",
    "bb = bb / np.max(beta)\n",
    "    \n",
    "print(bb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6302058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Gurney flap height\n",
    "# In case of mere NACA0018, the hh in those indexes are considered as zero.\n",
    "\n",
    "hh = np.concatenate((np.zeros(n_alpha), h[0]*np.ones(n_beta*n_alpha), h[1]*np.ones(n_beta*n_alpha), h[2]*np.ones(n_beta*n_alpha)))\n",
    "hh = hh.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "174336d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = hh / np.max(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5737c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the coordinates of NACA0018 (airfoil15)\n",
    "origin_coord = \"D:\\\\AirfoilClCdCoordinates_out\\\\AirfoilClCdCoordinates_out\\\\airfoil15\"\n",
    "\n",
    "csv_file_name = origin_coord + '\\\\airfoilOut15.txt'\n",
    "data = pd.read_csv(csv_file_name, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41daf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_coord = data.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd4c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_coord = baseline_coord.reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba48669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 100)\n"
     ]
    }
   ],
   "source": [
    "airfoil_coord = np.repeat(standard_coord, n_cases, axis=0)\n",
    "print(airfoil_coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492ab857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rows mean the number of points at the Gurney flap\n",
    "# and the columns mean the number of the cases\n",
    "flap_left = np.zeros((15,5))\n",
    "flap_right = np.zeros((15,5))\n",
    "\n",
    "for i in range(n_h):\n",
    "    # Defining coordinates of the flaps with respect to beta=90 degree.\n",
    "    yLeft = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "    yRight = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "    xLeft = 0.5*np.ones((5,1)) - 0.02*h[i]\n",
    "    xRight = 0.5*np.ones((5,1))\n",
    "    \n",
    "    for j in range(n_beta):\n",
    "        betaValue = beta[j]\n",
    "        \n",
    "        # Rotating transformation\n",
    "        rotateTransf = np.array([[np.cos(90-betaValue), -np.sin(90-betaValue)],\n",
    "                                 [np.sin(90-betaValue), np.cos(90-betaValue)]])\n",
    "        rotateTransf = rotateTransf.reshape((2,2))\n",
    "        \n",
    "        LeftImp = np.hstack((xLeft-0.5, yLeft))\n",
    "        RightImp = np.hstack((xRight-0.5, yRight))\n",
    "        \n",
    "        rotatedFlapLeft = rotateTransf @ LeftImp.T # shape: 2*5 (x-coordinates on first row, y-coordinates on second row)\n",
    "        rotatedFlapRight = rotateTransf @ RightImp.T\n",
    "        \n",
    "        # All we need is the y-coordinates of the flaps\n",
    "        flap_left[5*i+j,:] = rotatedFlapLeft[1,:]\n",
    "        flap_right[5*i+j,:] = rotatedFlapRight[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "790fb777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10)\n"
     ]
    }
   ],
   "source": [
    "# Combining y-coordinates from the left and the right side of the flaps\n",
    "flap_coords = np.hstack((flap_left, np.flip(flap_right, axis=1)))\n",
    "print(flap_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66f6855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 10)\n"
     ]
    }
   ],
   "source": [
    "# Placing the flap_coords into total coordinate variable\n",
    "# Total coordinate = Airfoil coordinates + flap coordinates\n",
    "flap_coords2 = np.zeros((n_cases, 10))\n",
    "for i in range(n_alpha, n_cases):\n",
    "    flap_coords2[i,:] = flap_coords[i%15,:]\n",
    "    \n",
    "print(flap_coords2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8323888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 110)\n"
     ]
    }
   ],
   "source": [
    "total_coords = np.hstack((airfoil_coord, flap_coords2))\n",
    "print(total_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d72e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the coordinates, in order to apply convolutional operation\n",
    "x = total_coords.reshape((16*9, 2, 55, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c207ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input of parameters\n",
    "# These variables are put into the network after convolution and flattening\n",
    "x_para = np.hstack((aa, hh, bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b41130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating output dataset - Cl and Cd\n",
    "files_orig = os.listdir(main_directory)\n",
    "files_target = [file for file in files_orig if file.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4c62765",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame()\n",
    "for file in files_target:\n",
    "    data = pd.read_table(file, header=None)\n",
    "    target_df = pd.concat([target_df, data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac849035",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_orig = target_df.iloc[:,4].values # Cd values\n",
    "#target_c4 = target_df.iloc[:,4].values# Cl values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d97485dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = (cl_orig-np.min(cl_orig))/(np.max(cl_orig)-np.min(cl_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93bba2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cl.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7444ef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec562b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_para_all, x_para_test, x_all, x_test, y_all, y_test = train_test_split(x_para, x, y, test_size=test_rate, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de373720",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_para_train, x_para_val, x_train, x_val, y_train, y_val = train_test_split(x_para_all, x_all, y_all, test_size=val_rate/(1-test_rate), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82d60c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37290049",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tf.keras.Input(shape=(2,55,1))\n",
    "input_2 = tf.keras.Input(shape=(3))\n",
    "\n",
    "x_conv1 = tf.keras.layers.Conv2D(filters=n_kernels, kernel_size=(2,2), strides=1, padding='same',\n",
    "                                 activation='relu', name='convLayer')(input_1)\n",
    "x_pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))(x_conv1)\n",
    "\n",
    "x_flat = tf.keras.layers.Flatten()(x_pool)\n",
    "x_concat = tf.keras.layers.Concatenate()([x_flat, input_2])\n",
    "\n",
    "x_fc1 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc1')(x_concat)\n",
    "x_fc2 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc2')(x_fc1)\n",
    "x_fc3 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc3')(x_fc2)\n",
    "x_fc4 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc4')(x_fc3)\n",
    "x_fc5 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc5')(x_fc4)\n",
    "\n",
    "output_data = tf.keras.layers.Dense(units=1, activation='linear', name='outputLayer')(x_fc5)\n",
    "# AeroCNN-I\n",
    "model = tf.keras.Model([input_1, input_2], output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3c482f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2, 55, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " convLayer (Conv2D)             (None, 2, 55, 50)    250         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 27, 50)    0           ['convLayer[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1350)         0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1353)         0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " fc1 (Dense)                    (None, 128)          173312      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 128)          16512       ['fc1[0][0]']                    \n",
      "                                                                                                  \n",
      " fc3 (Dense)                    (None, 128)          16512       ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      " fc4 (Dense)                    (None, 128)          16512       ['fc3[0][0]']                    \n",
      "                                                                                                  \n",
      " fc5 (Dense)                    (None, 128)          16512       ['fc4[0][0]']                    \n",
      "                                                                                                  \n",
      " outputLayer (Dense)            (None, 1)            129         ['fc5[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 239,739\n",
      "Trainable params: 239,739\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30f3abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss = tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcb484c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221205\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf477620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = model_directory + \"20221205steadyValidation_AeroCNN1_val_\"+str(val_rate) + \"_test\"+str(test_rate)+ \"_\" + str(n_units) +\"units_OptimalSettingsCl_checkpoint.h5\"\n",
    "\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(ckpt_name, monitor=\"val_loss\", mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1000, min_delta=6e-7,\n",
    "                                      restore_best_weights=True, verbose=1)\n",
    "rp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=200, factor=0.5,\n",
    "                                          min_delta = 1e-09, min_lr=1e-06, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4cc904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = len(x_train)//batch_size\n",
    "VALIDATION_STEPS = len(x_val)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17d6b26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/5 [=====>........................] - ETA: 20s - loss: 0.5077 - rmse: 0.7125\n",
      "Epoch 1: val_loss improved from inf to 0.43775, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 6s 81ms/step - loss: 0.4299 - rmse: 0.6557 - val_loss: 0.4377 - val_rmse: 0.6616 - lr: 1.0000e-04\n",
      "Epoch 2/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4750 - rmse: 0.6892\n",
      "Epoch 2: val_loss improved from 0.43775 to 0.41582, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4076 - rmse: 0.6384 - val_loss: 0.4158 - val_rmse: 0.6448 - lr: 1.0000e-04\n",
      "Epoch 3/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.3861 - rmse: 0.6214\n",
      "Epoch 3: val_loss improved from 0.41582 to 0.39180, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.3856 - rmse: 0.6210 - val_loss: 0.3918 - val_rmse: 0.6259 - lr: 1.0000e-04\n",
      "Epoch 4/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.2578 - rmse: 0.5077\n",
      "Epoch 4: val_loss improved from 0.39180 to 0.36389, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3609 - rmse: 0.6007 - val_loss: 0.3639 - val_rmse: 0.6032 - lr: 1.0000e-04\n",
      "Epoch 5/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.3651 - rmse: 0.6042\n",
      "Epoch 5: val_loss improved from 0.36389 to 0.32891, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3319 - rmse: 0.5761 - val_loss: 0.3289 - val_rmse: 0.5735 - lr: 1.0000e-04\n",
      "Epoch 6/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1810 - rmse: 0.4254\n",
      "Epoch 6: val_loss improved from 0.32891 to 0.28653, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.2947 - rmse: 0.5429 - val_loss: 0.2865 - val_rmse: 0.5353 - lr: 1.0000e-04\n",
      "Epoch 7/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.3373 - rmse: 0.5807\n",
      "Epoch 7: val_loss improved from 0.28653 to 0.23522, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.2525 - rmse: 0.5025 - val_loss: 0.2352 - val_rmse: 0.4850 - lr: 1.0000e-04\n",
      "Epoch 8/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1720 - rmse: 0.4148\n",
      "Epoch 8: val_loss improved from 0.23522 to 0.17685, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.2009 - rmse: 0.4482 - val_loss: 0.1769 - val_rmse: 0.4205 - lr: 1.0000e-04\n",
      "Epoch 9/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1225 - rmse: 0.3500\n",
      "Epoch 9: val_loss improved from 0.17685 to 0.11762, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.1428 - rmse: 0.3779 - val_loss: 0.1176 - val_rmse: 0.3430 - lr: 1.0000e-04\n",
      "Epoch 10/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1097 - rmse: 0.3312\n",
      "Epoch 10: val_loss improved from 0.11762 to 0.06800, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0973 - rmse: 0.3120 - val_loss: 0.0680 - val_rmse: 0.2608 - lr: 1.0000e-04\n",
      "Epoch 11/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0528 - rmse: 0.2298\n",
      "Epoch 11: val_loss improved from 0.06800 to 0.04657, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0642 - rmse: 0.2535 - val_loss: 0.0466 - val_rmse: 0.2158 - lr: 1.0000e-04\n",
      "Epoch 12/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0485 - rmse: 0.2202\n",
      "Epoch 12: val_loss did not improve from 0.04657\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0589 - rmse: 0.2427 - val_loss: 0.0475 - val_rmse: 0.2180 - lr: 1.0000e-04\n",
      "Epoch 13/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0403 - rmse: 0.2009\n",
      "Epoch 13: val_loss improved from 0.04657 to 0.04564, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0618 - rmse: 0.2487 - val_loss: 0.0456 - val_rmse: 0.2136 - lr: 1.0000e-04\n",
      "Epoch 14/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1006 - rmse: 0.3171\n",
      "Epoch 14: val_loss improved from 0.04564 to 0.04281, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0557 - rmse: 0.2361 - val_loss: 0.0428 - val_rmse: 0.2069 - lr: 1.0000e-04\n",
      "Epoch 15/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0405 - rmse: 0.2012\n",
      "Epoch 15: val_loss did not improve from 0.04281\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0511 - rmse: 0.2260 - val_loss: 0.0451 - val_rmse: 0.2123 - lr: 1.0000e-04\n",
      "Epoch 16/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0419 - rmse: 0.2047\n",
      "Epoch 16: val_loss did not improve from 0.04281\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0501 - rmse: 0.2237 - val_loss: 0.0444 - val_rmse: 0.2108 - lr: 1.0000e-04\n",
      "Epoch 17/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0456 - rmse: 0.2135\n",
      "Epoch 17: val_loss improved from 0.04281 to 0.04090, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0477 - rmse: 0.2184 - val_loss: 0.0409 - val_rmse: 0.2022 - lr: 1.0000e-04\n",
      "Epoch 18/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0552 - rmse: 0.2349\n",
      "Epoch 18: val_loss improved from 0.04090 to 0.03829, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0452 - rmse: 0.2126 - val_loss: 0.0383 - val_rmse: 0.1957 - lr: 1.0000e-04\n",
      "Epoch 19/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0449 - rmse: 0.2119\n",
      "Epoch 19: val_loss improved from 0.03829 to 0.03644, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0435 - rmse: 0.2087 - val_loss: 0.0364 - val_rmse: 0.1909 - lr: 1.0000e-04\n",
      "Epoch 20/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0355 - rmse: 0.1885\n",
      "Epoch 20: val_loss improved from 0.03644 to 0.03563, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0414 - rmse: 0.2035 - val_loss: 0.0356 - val_rmse: 0.1888 - lr: 1.0000e-04\n",
      "Epoch 21/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0379 - rmse: 0.1947\n",
      "Epoch 21: val_loss improved from 0.03563 to 0.03495, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0391 - rmse: 0.1976 - val_loss: 0.0349 - val_rmse: 0.1869 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0364 - rmse: 0.1909\n",
      "Epoch 22: val_loss improved from 0.03495 to 0.03334, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0371 - rmse: 0.1927 - val_loss: 0.0333 - val_rmse: 0.1826 - lr: 1.0000e-04\n",
      "Epoch 23/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0436 - rmse: 0.2087\n",
      "Epoch 23: val_loss improved from 0.03334 to 0.03153, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0348 - rmse: 0.1865 - val_loss: 0.0315 - val_rmse: 0.1776 - lr: 1.0000e-04\n",
      "Epoch 24/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0380 - rmse: 0.1950\n",
      "Epoch 24: val_loss improved from 0.03153 to 0.02815, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0320 - rmse: 0.1789 - val_loss: 0.0281 - val_rmse: 0.1678 - lr: 1.0000e-04\n",
      "Epoch 25/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0323 - rmse: 0.1798\n",
      "Epoch 25: val_loss improved from 0.02815 to 0.02623, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0290 - rmse: 0.1702 - val_loss: 0.0262 - val_rmse: 0.1619 - lr: 1.0000e-04\n",
      "Epoch 26/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0272 - rmse: 0.1650\n",
      "Epoch 26: val_loss improved from 0.02623 to 0.02372, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0259 - rmse: 0.1610 - val_loss: 0.0237 - val_rmse: 0.1540 - lr: 1.0000e-04\n",
      "Epoch 27/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0172 - rmse: 0.1312\n",
      "Epoch 27: val_loss improved from 0.02372 to 0.02143, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0232 - rmse: 0.1522 - val_loss: 0.0214 - val_rmse: 0.1464 - lr: 1.0000e-04\n",
      "Epoch 28/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0216 - rmse: 0.1471\n",
      "Epoch 28: val_loss improved from 0.02143 to 0.01929, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0207 - rmse: 0.1439 - val_loss: 0.0193 - val_rmse: 0.1389 - lr: 1.0000e-04\n",
      "Epoch 29/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0188 - rmse: 0.1372\n",
      "Epoch 29: val_loss improved from 0.01929 to 0.01724, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0182 - rmse: 0.1348 - val_loss: 0.0172 - val_rmse: 0.1313 - lr: 1.0000e-04\n",
      "Epoch 30/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0160 - rmse: 0.1263\n",
      "Epoch 30: val_loss improved from 0.01724 to 0.01558, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0159 - rmse: 0.1262 - val_loss: 0.0156 - val_rmse: 0.1248 - lr: 1.0000e-04\n",
      "Epoch 31/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0193 - rmse: 0.1388\n",
      "Epoch 31: val_loss improved from 0.01558 to 0.01397, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0146 - rmse: 0.1207 - val_loss: 0.0140 - val_rmse: 0.1182 - lr: 1.0000e-04\n",
      "Epoch 32/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0097 - rmse: 0.0987\n",
      "Epoch 32: val_loss improved from 0.01397 to 0.01305, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0134 - rmse: 0.1157 - val_loss: 0.0130 - val_rmse: 0.1142 - lr: 1.0000e-04\n",
      "Epoch 33/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0067 - rmse: 0.0820\n",
      "Epoch 33: val_loss improved from 0.01305 to 0.01242, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0122 - rmse: 0.1103 - val_loss: 0.0124 - val_rmse: 0.1115 - lr: 1.0000e-04\n",
      "Epoch 34/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0124 - rmse: 0.1115\n",
      "Epoch 34: val_loss improved from 0.01242 to 0.01121, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0115 - rmse: 0.1071 - val_loss: 0.0112 - val_rmse: 0.1059 - lr: 1.0000e-04\n",
      "Epoch 35/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0144 - rmse: 0.1202\n",
      "Epoch 35: val_loss improved from 0.01121 to 0.01045, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0110 - rmse: 0.1048 - val_loss: 0.0105 - val_rmse: 0.1022 - lr: 1.0000e-04\n",
      "Epoch 36/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0127 - rmse: 0.1128\n",
      "Epoch 36: val_loss improved from 0.01045 to 0.01022, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0104 - rmse: 0.1020 - val_loss: 0.0102 - val_rmse: 0.1011 - lr: 1.0000e-04\n",
      "Epoch 37/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0120 - rmse: 0.1097\n",
      "Epoch 37: val_loss improved from 0.01022 to 0.00976, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0100 - rmse: 0.1002 - val_loss: 0.0098 - val_rmse: 0.0988 - lr: 1.0000e-04\n",
      "Epoch 38/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0071 - rmse: 0.0840\n",
      "Epoch 38: val_loss improved from 0.00976 to 0.00926, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0098 - rmse: 0.0990 - val_loss: 0.0093 - val_rmse: 0.0962 - lr: 1.0000e-04\n",
      "Epoch 39/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0619\n",
      "Epoch 39: val_loss did not improve from 0.00926\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0095 - rmse: 0.0977 - val_loss: 0.0095 - val_rmse: 0.0975 - lr: 1.0000e-04\n",
      "Epoch 40/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0087 - rmse: 0.0935\n",
      "Epoch 40: val_loss improved from 0.00926 to 0.00913, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.0101 - rmse: 0.1005 - val_loss: 0.0091 - val_rmse: 0.0956 - lr: 1.0000e-04\n",
      "Epoch 41/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0115 - rmse: 0.1072\n",
      "Epoch 41: val_loss did not improve from 0.00913\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0094 - rmse: 0.0971 - val_loss: 0.0091 - val_rmse: 0.0956 - lr: 1.0000e-04\n",
      "Epoch 42/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0181 - rmse: 0.1345\n",
      "Epoch 42: val_loss improved from 0.00913 to 0.00906, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0090 - rmse: 0.0947 - val_loss: 0.0091 - val_rmse: 0.0952 - lr: 1.0000e-04\n",
      "Epoch 43/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0063 - rmse: 0.0796\n",
      "Epoch 43: val_loss improved from 0.00906 to 0.00859, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0093 - rmse: 0.0966 - val_loss: 0.0086 - val_rmse: 0.0927 - lr: 1.0000e-04\n",
      "Epoch 44/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0501\n",
      "Epoch 44: val_loss did not improve from 0.00859\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0087 - rmse: 0.0935 - val_loss: 0.0086 - val_rmse: 0.0928 - lr: 1.0000e-04\n",
      "Epoch 45/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0532\n",
      "Epoch 45: val_loss improved from 0.00859 to 0.00848, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0087 - rmse: 0.0931 - val_loss: 0.0085 - val_rmse: 0.0921 - lr: 1.0000e-04\n",
      "Epoch 46/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0100 - rmse: 0.0998\n",
      "Epoch 46: val_loss improved from 0.00848 to 0.00837, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0086 - rmse: 0.0929 - val_loss: 0.0084 - val_rmse: 0.0915 - lr: 1.0000e-04\n",
      "Epoch 47/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0162 - rmse: 0.1272\n",
      "Epoch 47: val_loss did not improve from 0.00837\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0086 - rmse: 0.0925 - val_loss: 0.0084 - val_rmse: 0.0919 - lr: 1.0000e-04\n",
      "Epoch 48/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0165 - rmse: 0.1283\n",
      "Epoch 48: val_loss improved from 0.00837 to 0.00824, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0083 - rmse: 0.0912 - val_loss: 0.0082 - val_rmse: 0.0908 - lr: 1.0000e-04\n",
      "Epoch 49/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0080 - rmse: 0.0895\n",
      "Epoch 49: val_loss did not improve from 0.00824\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0085 - rmse: 0.0923 - val_loss: 0.0084 - val_rmse: 0.0918 - lr: 1.0000e-04\n",
      "Epoch 50/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0598\n",
      "Epoch 50: val_loss improved from 0.00824 to 0.00803, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0084 - rmse: 0.0914 - val_loss: 0.0080 - val_rmse: 0.0896 - lr: 1.0000e-04\n",
      "Epoch 51/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0126 - rmse: 0.1123\n",
      "Epoch 51: val_loss did not improve from 0.00803\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0083 - rmse: 0.0911 - val_loss: 0.0082 - val_rmse: 0.0903 - lr: 1.0000e-04\n",
      "Epoch 52/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0148 - rmse: 0.1215\n",
      "Epoch 52: val_loss improved from 0.00803 to 0.00798, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0082 - rmse: 0.0905 - val_loss: 0.0080 - val_rmse: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 53/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0144 - rmse: 0.1199\n",
      "Epoch 53: val_loss improved from 0.00798 to 0.00789, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0083 - rmse: 0.0910 - val_loss: 0.0079 - val_rmse: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 54/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0087 - rmse: 0.0932\n",
      "Epoch 54: val_loss did not improve from 0.00789\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0084 - rmse: 0.0919 - val_loss: 0.0082 - val_rmse: 0.0906 - lr: 1.0000e-04\n",
      "Epoch 55/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0072 - rmse: 0.0846\n",
      "Epoch 55: val_loss improved from 0.00789 to 0.00782, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0081 - rmse: 0.0902 - val_loss: 0.0078 - val_rmse: 0.0884 - lr: 1.0000e-04\n",
      "Epoch 56/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0055 - rmse: 0.0745\n",
      "Epoch 56: val_loss did not improve from 0.00782\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0081 - rmse: 0.0898 - val_loss: 0.0080 - val_rmse: 0.0895 - lr: 1.0000e-04\n",
      "Epoch 57/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0041 - rmse: 0.0638\n",
      "Epoch 57: val_loss did not improve from 0.00782\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0081 - rmse: 0.0899 - val_loss: 0.0080 - val_rmse: 0.0892 - lr: 1.0000e-04\n",
      "Epoch 58/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0090 - rmse: 0.0947\n",
      "Epoch 58: val_loss improved from 0.00782 to 0.00768, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0080 - rmse: 0.0896 - val_loss: 0.0077 - val_rmse: 0.0877 - lr: 1.0000e-04\n",
      "Epoch 59/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0113 - rmse: 0.1063\n",
      "Epoch 59: val_loss did not improve from 0.00768\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0081 - rmse: 0.0902 - val_loss: 0.0082 - val_rmse: 0.0905 - lr: 1.0000e-04\n",
      "Epoch 60/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0685\n",
      "Epoch 60: val_loss did not improve from 0.00768\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0081 - rmse: 0.0899 - val_loss: 0.0077 - val_rmse: 0.0877 - lr: 1.0000e-04\n",
      "Epoch 61/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0086 - rmse: 0.0927\n",
      "Epoch 61: val_loss improved from 0.00768 to 0.00758, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0082 - rmse: 0.0905 - val_loss: 0.0076 - val_rmse: 0.0871 - lr: 1.0000e-04\n",
      "Epoch 62/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0095 - rmse: 0.0972\n",
      "Epoch 62: val_loss did not improve from 0.00758\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0079 - rmse: 0.0891 - val_loss: 0.0080 - val_rmse: 0.0895 - lr: 1.0000e-04\n",
      "Epoch 63/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0609\n",
      "Epoch 63: val_loss improved from 0.00758 to 0.00751, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0079 - rmse: 0.0890 - val_loss: 0.0075 - val_rmse: 0.0867 - lr: 1.0000e-04\n",
      "Epoch 64/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0468\n",
      "Epoch 64: val_loss improved from 0.00751 to 0.00745, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0083 - rmse: 0.0910 - val_loss: 0.0074 - val_rmse: 0.0863 - lr: 1.0000e-04\n",
      "Epoch 65/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0106 - rmse: 0.1031\n",
      "Epoch 65: val_loss did not improve from 0.00745\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0079 - rmse: 0.0890 - val_loss: 0.0081 - val_rmse: 0.0900 - lr: 1.0000e-04\n",
      "Epoch 66/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0674\n",
      "Epoch 66: val_loss did not improve from 0.00745\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0078 - rmse: 0.0885 - val_loss: 0.0075 - val_rmse: 0.0863 - lr: 1.0000e-04\n",
      "Epoch 67/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0101 - rmse: 0.1007\n",
      "Epoch 67: val_loss improved from 0.00745 to 0.00740, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.0079 - rmse: 0.0889 - val_loss: 0.0074 - val_rmse: 0.0860 - lr: 1.0000e-04\n",
      "Epoch 68/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0081 - rmse: 0.0900\n",
      "Epoch 68: val_loss did not improve from 0.00740\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0077 - rmse: 0.0877 - val_loss: 0.0076 - val_rmse: 0.0871 - lr: 1.0000e-04\n",
      "Epoch 69/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0711\n",
      "Epoch 69: val_loss did not improve from 0.00740\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0078 - rmse: 0.0882 - val_loss: 0.0075 - val_rmse: 0.0865 - lr: 1.0000e-04\n",
      "Epoch 70/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0579\n",
      "Epoch 70: val_loss improved from 0.00740 to 0.00738, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0078 - rmse: 0.0883 - val_loss: 0.0074 - val_rmse: 0.0859 - lr: 1.0000e-04\n",
      "Epoch 71/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0097 - rmse: 0.0986\n",
      "Epoch 71: val_loss did not improve from 0.00738\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0077 - rmse: 0.0879 - val_loss: 0.0076 - val_rmse: 0.0869 - lr: 1.0000e-04\n",
      "Epoch 72/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0163 - rmse: 0.1278\n",
      "Epoch 72: val_loss improved from 0.00738 to 0.00736, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0077 - rmse: 0.0880 - val_loss: 0.0074 - val_rmse: 0.0858 - lr: 1.0000e-04\n",
      "Epoch 73/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0457\n",
      "Epoch 73: val_loss improved from 0.00736 to 0.00731, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0080 - rmse: 0.0893 - val_loss: 0.0073 - val_rmse: 0.0855 - lr: 1.0000e-04\n",
      "Epoch 74/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0112 - rmse: 0.1059\n",
      "Epoch 74: val_loss did not improve from 0.00731\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0080 - rmse: 0.0896 - val_loss: 0.0076 - val_rmse: 0.0873 - lr: 1.0000e-04\n",
      "Epoch 75/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0141 - rmse: 0.1185\n",
      "Epoch 75: val_loss improved from 0.00731 to 0.00725, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0075 - rmse: 0.0865 - val_loss: 0.0073 - val_rmse: 0.0851 - lr: 1.0000e-04\n",
      "Epoch 76/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0172 - rmse: 0.1310\n",
      "Epoch 76: val_loss did not improve from 0.00725\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0079 - rmse: 0.0888 - val_loss: 0.0075 - val_rmse: 0.0863 - lr: 1.0000e-04\n",
      "Epoch 77/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0182 - rmse: 0.1349\n",
      "Epoch 77: val_loss did not improve from 0.00725\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0076 - rmse: 0.0872 - val_loss: 0.0073 - val_rmse: 0.0853 - lr: 1.0000e-04\n",
      "Epoch 78/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0065 - rmse: 0.0804\n",
      "Epoch 78: val_loss improved from 0.00725 to 0.00718, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0076 - rmse: 0.0871 - val_loss: 0.0072 - val_rmse: 0.0847 - lr: 1.0000e-04\n",
      "Epoch 79/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0593\n",
      "Epoch 79: val_loss did not improve from 0.00718\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0076 - rmse: 0.0872 - val_loss: 0.0073 - val_rmse: 0.0853 - lr: 1.0000e-04\n",
      "Epoch 80/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0435\n",
      "Epoch 80: val_loss did not improve from 0.00718\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0076 - rmse: 0.0871 - val_loss: 0.0073 - val_rmse: 0.0856 - lr: 1.0000e-04\n",
      "Epoch 81/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0105 - rmse: 0.1022\n",
      "Epoch 81: val_loss did not improve from 0.00718\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0075 - rmse: 0.0868 - val_loss: 0.0073 - val_rmse: 0.0856 - lr: 1.0000e-04\n",
      "Epoch 82/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0428\n",
      "Epoch 82: val_loss improved from 0.00718 to 0.00707, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0080 - rmse: 0.0893 - val_loss: 0.0071 - val_rmse: 0.0841 - lr: 1.0000e-04\n",
      "Epoch 83/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0073 - rmse: 0.0853\n",
      "Epoch 83: val_loss did not improve from 0.00707\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0079 - rmse: 0.0888 - val_loss: 0.0075 - val_rmse: 0.0865 - lr: 1.0000e-04\n",
      "Epoch 84/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0100 - rmse: 0.0998\n",
      "Epoch 84: val_loss improved from 0.00707 to 0.00700, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0077 - rmse: 0.0876 - val_loss: 0.0070 - val_rmse: 0.0837 - lr: 1.0000e-04\n",
      "Epoch 85/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0167 - rmse: 0.1293\n",
      "Epoch 85: val_loss did not improve from 0.00700\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0077 - rmse: 0.0877 - val_loss: 0.0077 - val_rmse: 0.0876 - lr: 1.0000e-04\n",
      "Epoch 86/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0085 - rmse: 0.0924\n",
      "Epoch 86: val_loss did not improve from 0.00700\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0075 - rmse: 0.0868 - val_loss: 0.0070 - val_rmse: 0.0837 - lr: 1.0000e-04\n",
      "Epoch 87/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0086 - rmse: 0.0929\n",
      "Epoch 87: val_loss did not improve from 0.00700\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0076 - rmse: 0.0872 - val_loss: 0.0071 - val_rmse: 0.0840 - lr: 1.0000e-04\n",
      "Epoch 88/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0336\n",
      "Epoch 88: val_loss did not improve from 0.00700\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0078 - rmse: 0.0884 - val_loss: 0.0075 - val_rmse: 0.0866 - lr: 1.0000e-04\n",
      "Epoch 89/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0173 - rmse: 0.1314\n",
      "Epoch 89: val_loss improved from 0.00700 to 0.00698, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0073 - rmse: 0.0855 - val_loss: 0.0070 - val_rmse: 0.0836 - lr: 1.0000e-04\n",
      "Epoch 90/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0134 - rmse: 0.1156\n",
      "Epoch 90: val_loss did not improve from 0.00698\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0078 - rmse: 0.0885 - val_loss: 0.0074 - val_rmse: 0.0859 - lr: 1.0000e-04\n",
      "Epoch 91/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0415\n",
      "Epoch 91: val_loss improved from 0.00698 to 0.00690, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0075 - rmse: 0.0868 - val_loss: 0.0069 - val_rmse: 0.0831 - lr: 1.0000e-04\n",
      "Epoch 92/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0077 - rmse: 0.0878\n",
      "Epoch 92: val_loss did not improve from 0.00690\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0074 - rmse: 0.0862 - val_loss: 0.0071 - val_rmse: 0.0843 - lr: 1.0000e-04\n",
      "Epoch 93/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0071 - rmse: 0.0840\n",
      "Epoch 93: val_loss improved from 0.00690 to 0.00680, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0074 - rmse: 0.0863 - val_loss: 0.0068 - val_rmse: 0.0825 - lr: 1.0000e-04\n",
      "Epoch 94/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0078 - rmse: 0.0886\n",
      "Epoch 94: val_loss did not improve from 0.00680\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0075 - rmse: 0.0864 - val_loss: 0.0070 - val_rmse: 0.0836 - lr: 1.0000e-04\n",
      "Epoch 95/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0069 - rmse: 0.0832\n",
      "Epoch 95: val_loss improved from 0.00680 to 0.00677, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0077 - rmse: 0.0877 - val_loss: 0.0068 - val_rmse: 0.0823 - lr: 1.0000e-04\n",
      "Epoch 96/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0527\n",
      "Epoch 96: val_loss did not improve from 0.00677\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0072 - rmse: 0.0848 - val_loss: 0.0074 - val_rmse: 0.0860 - lr: 1.0000e-04\n",
      "Epoch 97/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0060 - rmse: 0.0773\n",
      "Epoch 97: val_loss improved from 0.00677 to 0.00672, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0074 - rmse: 0.0862 - val_loss: 0.0067 - val_rmse: 0.0820 - lr: 1.0000e-04\n",
      "Epoch 98/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0397\n",
      "Epoch 98: val_loss did not improve from 0.00672\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0077 - rmse: 0.0875 - val_loss: 0.0069 - val_rmse: 0.0831 - lr: 1.0000e-04\n",
      "Epoch 99/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0492\n",
      "Epoch 99: val_loss did not improve from 0.00672\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0075 - rmse: 0.0865 - val_loss: 0.0069 - val_rmse: 0.0829 - lr: 1.0000e-04\n",
      "Epoch 100/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0087 - rmse: 0.0930\n",
      "Epoch 100: val_loss improved from 0.00672 to 0.00670, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0076 - rmse: 0.0871 - val_loss: 0.0067 - val_rmse: 0.0819 - lr: 1.0000e-04\n",
      "Epoch 101/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0052 - rmse: 0.0719\n",
      "Epoch 101: val_loss did not improve from 0.00670\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0073 - rmse: 0.0857 - val_loss: 0.0074 - val_rmse: 0.0861 - lr: 1.0000e-04\n",
      "Epoch 102/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0105 - rmse: 0.1027\n",
      "Epoch 102: val_loss improved from 0.00670 to 0.00665, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0073 - rmse: 0.0854 - val_loss: 0.0066 - val_rmse: 0.0815 - lr: 1.0000e-04\n",
      "Epoch 103/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0730\n",
      "Epoch 103: val_loss did not improve from 0.00665\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0073 - rmse: 0.0855 - val_loss: 0.0071 - val_rmse: 0.0840 - lr: 1.0000e-04\n",
      "Epoch 104/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0143 - rmse: 0.1196\n",
      "Epoch 104: val_loss improved from 0.00665 to 0.00657, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0072 - rmse: 0.0849 - val_loss: 0.0066 - val_rmse: 0.0810 - lr: 1.0000e-04\n",
      "Epoch 105/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0067 - rmse: 0.0817\n",
      "Epoch 105: val_loss did not improve from 0.00657\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0072 - rmse: 0.0850 - val_loss: 0.0066 - val_rmse: 0.0812 - lr: 1.0000e-04\n",
      "Epoch 106/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0094 - rmse: 0.0969\n",
      "Epoch 106: val_loss did not improve from 0.00657\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0072 - rmse: 0.0847 - val_loss: 0.0069 - val_rmse: 0.0829 - lr: 1.0000e-04\n",
      "Epoch 107/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0097 - rmse: 0.0985\n",
      "Epoch 107: val_loss did not improve from 0.00657\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0073 - rmse: 0.0851 - val_loss: 0.0066 - val_rmse: 0.0810 - lr: 1.0000e-04\n",
      "Epoch 108/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0078 - rmse: 0.0882\n",
      "Epoch 108: val_loss did not improve from 0.00657\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0070 - rmse: 0.0839 - val_loss: 0.0068 - val_rmse: 0.0825 - lr: 1.0000e-04\n",
      "Epoch 109/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0730\n",
      "Epoch 109: val_loss did not improve from 0.00657\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0071 - rmse: 0.0842 - val_loss: 0.0067 - val_rmse: 0.0818 - lr: 1.0000e-04\n",
      "Epoch 110/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0128 - rmse: 0.1131\n",
      "Epoch 110: val_loss improved from 0.00657 to 0.00655, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0071 - rmse: 0.0841 - val_loss: 0.0066 - val_rmse: 0.0810 - lr: 1.0000e-04\n",
      "Epoch 111/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0072 - rmse: 0.0849\n",
      "Epoch 111: val_loss did not improve from 0.00655\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0072 - rmse: 0.0848 - val_loss: 0.0068 - val_rmse: 0.0828 - lr: 1.0000e-04\n",
      "Epoch 112/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0121 - rmse: 0.1099\n",
      "Epoch 112: val_loss improved from 0.00655 to 0.00643, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0072 - rmse: 0.0850 - val_loss: 0.0064 - val_rmse: 0.0802 - lr: 1.0000e-04\n",
      "Epoch 113/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0621\n",
      "Epoch 113: val_loss did not improve from 0.00643\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0071 - rmse: 0.0843 - val_loss: 0.0065 - val_rmse: 0.0806 - lr: 1.0000e-04\n",
      "Epoch 114/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0525\n",
      "Epoch 114: val_loss did not improve from 0.00643\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - rmse: 0.0833 - val_loss: 0.0068 - val_rmse: 0.0824 - lr: 1.0000e-04\n",
      "Epoch 115/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0149 - rmse: 0.1219\n",
      "Epoch 115: val_loss improved from 0.00643 to 0.00637, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0071 - rmse: 0.0845 - val_loss: 0.0064 - val_rmse: 0.0798 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0164 - rmse: 0.1282\n",
      "Epoch 116: val_loss did not improve from 0.00637\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0073 - rmse: 0.0856 - val_loss: 0.0067 - val_rmse: 0.0818 - lr: 1.0000e-04\n",
      "Epoch 117/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0579\n",
      "Epoch 117: val_loss improved from 0.00637 to 0.00637, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0074 - rmse: 0.0860 - val_loss: 0.0064 - val_rmse: 0.0798 - lr: 1.0000e-04\n",
      "Epoch 118/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0674\n",
      "Epoch 118: val_loss did not improve from 0.00637\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0070 - rmse: 0.0835 - val_loss: 0.0068 - val_rmse: 0.0827 - lr: 1.0000e-04\n",
      "Epoch 119/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0167 - rmse: 0.1293\n",
      "Epoch 119: val_loss improved from 0.00637 to 0.00635, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0069 - rmse: 0.0832 - val_loss: 0.0064 - val_rmse: 0.0797 - lr: 1.0000e-04\n",
      "Epoch 120/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0558\n",
      "Epoch 120: val_loss did not improve from 0.00635\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0073 - rmse: 0.0856 - val_loss: 0.0066 - val_rmse: 0.0811 - lr: 1.0000e-04\n",
      "Epoch 121/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0087 - rmse: 0.0934\n",
      "Epoch 121: val_loss did not improve from 0.00635\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0070 - rmse: 0.0838 - val_loss: 0.0065 - val_rmse: 0.0808 - lr: 1.0000e-04\n",
      "Epoch 122/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0144 - rmse: 0.1201\n",
      "Epoch 122: val_loss improved from 0.00635 to 0.00623, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0069 - rmse: 0.0829 - val_loss: 0.0062 - val_rmse: 0.0789 - lr: 1.0000e-04\n",
      "Epoch 123/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0092 - rmse: 0.0958\n",
      "Epoch 123: val_loss did not improve from 0.00623\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - rmse: 0.0828 - val_loss: 0.0066 - val_rmse: 0.0814 - lr: 1.0000e-04\n",
      "Epoch 124/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0085 - rmse: 0.0924\n",
      "Epoch 124: val_loss improved from 0.00623 to 0.00622, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0068 - rmse: 0.0827 - val_loss: 0.0062 - val_rmse: 0.0789 - lr: 1.0000e-04\n",
      "Epoch 125/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0354\n",
      "Epoch 125: val_loss did not improve from 0.00622\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0071 - rmse: 0.0842 - val_loss: 0.0067 - val_rmse: 0.0818 - lr: 1.0000e-04\n",
      "Epoch 126/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0073 - rmse: 0.0857\n",
      "Epoch 126: val_loss did not improve from 0.00622\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0070 - rmse: 0.0837 - val_loss: 0.0064 - val_rmse: 0.0797 - lr: 1.0000e-04\n",
      "Epoch 127/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0059 - rmse: 0.0770\n",
      "Epoch 127: val_loss improved from 0.00622 to 0.00615, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0071 - rmse: 0.0841 - val_loss: 0.0062 - val_rmse: 0.0785 - lr: 1.0000e-04\n",
      "Epoch 128/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0080 - rmse: 0.0893\n",
      "Epoch 128: val_loss did not improve from 0.00615\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0075 - rmse: 0.0866 - val_loss: 0.0069 - val_rmse: 0.0832 - lr: 1.0000e-04\n",
      "Epoch 129/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0609\n",
      "Epoch 129: val_loss improved from 0.00615 to 0.00614, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0072 - rmse: 0.0850 - val_loss: 0.0061 - val_rmse: 0.0783 - lr: 1.0000e-04\n",
      "Epoch 130/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0128 - rmse: 0.1132\n",
      "Epoch 130: val_loss did not improve from 0.00614\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0073 - rmse: 0.0853 - val_loss: 0.0070 - val_rmse: 0.0836 - lr: 1.0000e-04\n",
      "Epoch 131/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0633\n",
      "Epoch 131: val_loss did not improve from 0.00614\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0074 - rmse: 0.0857 - val_loss: 0.0062 - val_rmse: 0.0785 - lr: 1.0000e-04\n",
      "Epoch 132/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0120 - rmse: 0.1096\n",
      "Epoch 132: val_loss did not improve from 0.00614\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - rmse: 0.0833 - val_loss: 0.0066 - val_rmse: 0.0814 - lr: 1.0000e-04\n",
      "Epoch 133/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0493\n",
      "Epoch 133: val_loss did not improve from 0.00614\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - rmse: 0.0830 - val_loss: 0.0062 - val_rmse: 0.0789 - lr: 1.0000e-04\n",
      "Epoch 134/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0413\n",
      "Epoch 134: val_loss did not improve from 0.00614\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0067 - rmse: 0.0820 - val_loss: 0.0063 - val_rmse: 0.0796 - lr: 1.0000e-04\n",
      "Epoch 135/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0667\n",
      "Epoch 135: val_loss did not improve from 0.00614\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0067 - rmse: 0.0818 - val_loss: 0.0063 - val_rmse: 0.0795 - lr: 1.0000e-04\n",
      "Epoch 136/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0104 - rmse: 0.1021\n",
      "Epoch 136: val_loss improved from 0.00614 to 0.00605, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0067 - rmse: 0.0821 - val_loss: 0.0061 - val_rmse: 0.0778 - lr: 1.0000e-04\n",
      "Epoch 137/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0490\n",
      "Epoch 137: val_loss did not improve from 0.00605\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0068 - rmse: 0.0823 - val_loss: 0.0067 - val_rmse: 0.0817 - lr: 1.0000e-04\n",
      "Epoch 138/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0342\n",
      "Epoch 138: val_loss improved from 0.00605 to 0.00604, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0067 - rmse: 0.0821 - val_loss: 0.0060 - val_rmse: 0.0777 - lr: 1.0000e-04\n",
      "Epoch 139/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0082 - rmse: 0.0907\n",
      "Epoch 139: val_loss did not improve from 0.00604\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - rmse: 0.0830 - val_loss: 0.0064 - val_rmse: 0.0798 - lr: 1.0000e-04\n",
      "Epoch 140/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0075 - rmse: 0.0866\n",
      "Epoch 140: val_loss did not improve from 0.00604\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0070 - rmse: 0.0836 - val_loss: 0.0061 - val_rmse: 0.0781 - lr: 1.0000e-04\n",
      "Epoch 141/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0392\n",
      "Epoch 141: val_loss improved from 0.00604 to 0.00604, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0072 - rmse: 0.0846 - val_loss: 0.0060 - val_rmse: 0.0777 - lr: 1.0000e-04\n",
      "Epoch 142/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0147 - rmse: 0.1213\n",
      "Epoch 142: val_loss did not improve from 0.00604\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0072 - rmse: 0.0851 - val_loss: 0.0064 - val_rmse: 0.0800 - lr: 1.0000e-04\n",
      "Epoch 143/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0074 - rmse: 0.0860\n",
      "Epoch 143: val_loss improved from 0.00604 to 0.00594, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0065 - rmse: 0.0806 - val_loss: 0.0059 - val_rmse: 0.0771 - lr: 1.0000e-04\n",
      "Epoch 144/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0061 - rmse: 0.0780\n",
      "Epoch 144: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0066 - rmse: 0.0813 - val_loss: 0.0066 - val_rmse: 0.0810 - lr: 1.0000e-04\n",
      "Epoch 145/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0108 - rmse: 0.1039\n",
      "Epoch 145: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0067 - rmse: 0.0820 - val_loss: 0.0060 - val_rmse: 0.0772 - lr: 1.0000e-04\n",
      "Epoch 146/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0507\n",
      "Epoch 146: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0065 - rmse: 0.0807 - val_loss: 0.0063 - val_rmse: 0.0796 - lr: 1.0000e-04\n",
      "Epoch 147/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0683\n",
      "Epoch 147: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0066 - rmse: 0.0811 - val_loss: 0.0060 - val_rmse: 0.0773 - lr: 1.0000e-04\n",
      "Epoch 148/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0050 - rmse: 0.0708\n",
      "Epoch 148: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0065 - rmse: 0.0808 - val_loss: 0.0062 - val_rmse: 0.0784 - lr: 1.0000e-04\n",
      "Epoch 149/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0388\n",
      "Epoch 149: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0065 - rmse: 0.0804 - val_loss: 0.0063 - val_rmse: 0.0793 - lr: 1.0000e-04\n",
      "Epoch 150/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0103 - rmse: 0.1013\n",
      "Epoch 150: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0065 - rmse: 0.0808 - val_loss: 0.0060 - val_rmse: 0.0775 - lr: 1.0000e-04\n",
      "Epoch 151/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0408\n",
      "Epoch 151: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0066 - rmse: 0.0810 - val_loss: 0.0060 - val_rmse: 0.0772 - lr: 1.0000e-04\n",
      "Epoch 152/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0141 - rmse: 0.1189\n",
      "Epoch 152: val_loss did not improve from 0.00594\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0066 - rmse: 0.0810 - val_loss: 0.0061 - val_rmse: 0.0782 - lr: 1.0000e-04\n",
      "Epoch 153/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0130 - rmse: 0.1140\n",
      "Epoch 153: val_loss improved from 0.00594 to 0.00589, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0064 - rmse: 0.0797 - val_loss: 0.0059 - val_rmse: 0.0767 - lr: 1.0000e-04\n",
      "Epoch 154/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0381\n",
      "Epoch 154: val_loss did not improve from 0.00589\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0064 - rmse: 0.0799 - val_loss: 0.0060 - val_rmse: 0.0777 - lr: 1.0000e-04\n",
      "Epoch 155/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0634\n",
      "Epoch 155: val_loss did not improve from 0.00589\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0064 - rmse: 0.0799 - val_loss: 0.0059 - val_rmse: 0.0769 - lr: 1.0000e-04\n",
      "Epoch 156/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0660\n",
      "Epoch 156: val_loss did not improve from 0.00589\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0063 - rmse: 0.0796 - val_loss: 0.0059 - val_rmse: 0.0770 - lr: 1.0000e-04\n",
      "Epoch 157/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0073 - rmse: 0.0856\n",
      "Epoch 157: val_loss improved from 0.00589 to 0.00587, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0066 - rmse: 0.0814 - val_loss: 0.0059 - val_rmse: 0.0766 - lr: 1.0000e-04\n",
      "Epoch 158/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0436e-04 - rmse: 0.0284\n",
      "Epoch 158: val_loss did not improve from 0.00587\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0065 - rmse: 0.0809 - val_loss: 0.0059 - val_rmse: 0.0769 - lr: 1.0000e-04\n",
      "Epoch 159/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0130 - rmse: 0.1139\n",
      "Epoch 159: val_loss did not improve from 0.00587\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0063 - rmse: 0.0796 - val_loss: 0.0059 - val_rmse: 0.0771 - lr: 1.0000e-04\n",
      "Epoch 160/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0096 - rmse: 0.0978\n",
      "Epoch 160: val_loss improved from 0.00587 to 0.00571, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0062 - rmse: 0.0787 - val_loss: 0.0057 - val_rmse: 0.0755 - lr: 1.0000e-04\n",
      "Epoch 161/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0490\n",
      "Epoch 161: val_loss did not improve from 0.00571\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0063 - rmse: 0.0793 - val_loss: 0.0061 - val_rmse: 0.0779 - lr: 1.0000e-04\n",
      "Epoch 162/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0061 - rmse: 0.0779\n",
      "Epoch 162: val_loss improved from 0.00571 to 0.00571, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0064 - rmse: 0.0797 - val_loss: 0.0057 - val_rmse: 0.0755 - lr: 1.0000e-04\n",
      "Epoch 163/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0090 - rmse: 0.0950\n",
      "Epoch 163: val_loss did not improve from 0.00571\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0063 - rmse: 0.0794 - val_loss: 0.0059 - val_rmse: 0.0766 - lr: 1.0000e-04\n",
      "Epoch 164/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0669\n",
      "Epoch 164: val_loss did not improve from 0.00571\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - rmse: 0.0787 - val_loss: 0.0058 - val_rmse: 0.0761 - lr: 1.0000e-04\n",
      "Epoch 165/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0171 - rmse: 0.1308\n",
      "Epoch 165: val_loss improved from 0.00571 to 0.00565, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0063 - rmse: 0.0792 - val_loss: 0.0057 - val_rmse: 0.0752 - lr: 1.0000e-04\n",
      "Epoch 166/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0319\n",
      "Epoch 166: val_loss did not improve from 0.00565\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - rmse: 0.0788 - val_loss: 0.0058 - val_rmse: 0.0758 - lr: 1.0000e-04\n",
      "Epoch 167/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0103 - rmse: 0.1013\n",
      "Epoch 167: val_loss did not improve from 0.00565\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0061 - rmse: 0.0782 - val_loss: 0.0058 - val_rmse: 0.0764 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0061 - rmse: 0.0779\n",
      "Epoch 168: val_loss improved from 0.00565 to 0.00562, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0061 - rmse: 0.0781 - val_loss: 0.0056 - val_rmse: 0.0750 - lr: 1.0000e-04\n",
      "Epoch 169/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0068 - rmse: 0.0824\n",
      "Epoch 169: val_loss did not improve from 0.00562\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0061 - rmse: 0.0782 - val_loss: 0.0058 - val_rmse: 0.0758 - lr: 1.0000e-04\n",
      "Epoch 170/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0141 - rmse: 0.1187\n",
      "Epoch 170: val_loss did not improve from 0.00562\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - rmse: 0.0789 - val_loss: 0.0056 - val_rmse: 0.0751 - lr: 1.0000e-04\n",
      "Epoch 171/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0062 - rmse: 0.0788\n",
      "Epoch 171: val_loss improved from 0.00562 to 0.00559, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0063 - rmse: 0.0791 - val_loss: 0.0056 - val_rmse: 0.0748 - lr: 1.0000e-04\n",
      "Epoch 172/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0136 - rmse: 0.1165\n",
      "Epoch 172: val_loss did not improve from 0.00559\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - rmse: 0.0789 - val_loss: 0.0057 - val_rmse: 0.0754 - lr: 1.0000e-04\n",
      "Epoch 173/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0085 - rmse: 0.0923\n",
      "Epoch 173: val_loss improved from 0.00559 to 0.00558, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0061 - rmse: 0.0778 - val_loss: 0.0056 - val_rmse: 0.0747 - lr: 1.0000e-04\n",
      "Epoch 174/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0342\n",
      "Epoch 174: val_loss did not improve from 0.00558\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0060 - rmse: 0.0777 - val_loss: 0.0059 - val_rmse: 0.0768 - lr: 1.0000e-04\n",
      "Epoch 175/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0089 - rmse: 0.0944\n",
      "Epoch 175: val_loss did not improve from 0.00558\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0061 - rmse: 0.0783 - val_loss: 0.0056 - val_rmse: 0.0751 - lr: 1.0000e-04\n",
      "Epoch 176/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0457\n",
      "Epoch 176: val_loss did not improve from 0.00558\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0063 - rmse: 0.0792 - val_loss: 0.0058 - val_rmse: 0.0764 - lr: 1.0000e-04\n",
      "Epoch 177/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0048 - rmse: 0.0692\n",
      "Epoch 177: val_loss improved from 0.00558 to 0.00552, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0060 - rmse: 0.0777 - val_loss: 0.0055 - val_rmse: 0.0743 - lr: 1.0000e-04\n",
      "Epoch 178/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0118 - rmse: 0.1088\n",
      "Epoch 178: val_loss did not improve from 0.00552\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0060 - rmse: 0.0776 - val_loss: 0.0057 - val_rmse: 0.0752 - lr: 1.0000e-04\n",
      "Epoch 179/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0120 - rmse: 0.1097\n",
      "Epoch 179: val_loss improved from 0.00552 to 0.00548, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0061 - rmse: 0.0782 - val_loss: 0.0055 - val_rmse: 0.0741 - lr: 1.0000e-04\n",
      "Epoch 180/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0043 - rmse: 0.0659\n",
      "Epoch 180: val_loss did not improve from 0.00548\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - rmse: 0.0789 - val_loss: 0.0056 - val_rmse: 0.0748 - lr: 1.0000e-04\n",
      "Epoch 181/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0472\n",
      "Epoch 181: val_loss did not improve from 0.00548\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - rmse: 0.0789 - val_loss: 0.0055 - val_rmse: 0.0743 - lr: 1.0000e-04\n",
      "Epoch 182/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0069 - rmse: 0.0832\n",
      "Epoch 182: val_loss did not improve from 0.00548\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0061 - rmse: 0.0781 - val_loss: 0.0055 - val_rmse: 0.0743 - lr: 1.0000e-04\n",
      "Epoch 183/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0319\n",
      "Epoch 183: val_loss did not improve from 0.00548\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0059 - rmse: 0.0770 - val_loss: 0.0060 - val_rmse: 0.0772 - lr: 1.0000e-04\n",
      "Epoch 184/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0073 - rmse: 0.0852\n",
      "Epoch 184: val_loss improved from 0.00548 to 0.00536, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0059 - rmse: 0.0770 - val_loss: 0.0054 - val_rmse: 0.0732 - lr: 1.0000e-04\n",
      "Epoch 185/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0625\n",
      "Epoch 185: val_loss did not improve from 0.00536\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0058 - rmse: 0.0762 - val_loss: 0.0057 - val_rmse: 0.0758 - lr: 1.0000e-04\n",
      "Epoch 186/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0060 - rmse: 0.0777\n",
      "Epoch 186: val_loss improved from 0.00536 to 0.00534, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0059 - rmse: 0.0770 - val_loss: 0.0053 - val_rmse: 0.0731 - lr: 1.0000e-04\n",
      "Epoch 187/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0066 - rmse: 0.0812\n",
      "Epoch 187: val_loss did not improve from 0.00534\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0059 - rmse: 0.0768 - val_loss: 0.0055 - val_rmse: 0.0740 - lr: 1.0000e-04\n",
      "Epoch 188/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0548\n",
      "Epoch 188: val_loss did not improve from 0.00534\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0059 - rmse: 0.0765 - val_loss: 0.0055 - val_rmse: 0.0742 - lr: 1.0000e-04\n",
      "Epoch 189/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0080 - rmse: 0.0893\n",
      "Epoch 189: val_loss did not improve from 0.00534\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - rmse: 0.0788 - val_loss: 0.0053 - val_rmse: 0.0731 - lr: 1.0000e-04\n",
      "Epoch 190/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0060 - rmse: 0.0777\n",
      "Epoch 190: val_loss did not improve from 0.00534\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0061 - rmse: 0.0782 - val_loss: 0.0057 - val_rmse: 0.0755 - lr: 1.0000e-04\n",
      "Epoch 191/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0055 - rmse: 0.0742\n",
      "Epoch 191: val_loss did not improve from 0.00534\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0059 - rmse: 0.0766 - val_loss: 0.0054 - val_rmse: 0.0733 - lr: 1.0000e-04\n",
      "Epoch 192/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0392\n",
      "Epoch 192: val_loss did not improve from 0.00534\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0058 - rmse: 0.0765 - val_loss: 0.0057 - val_rmse: 0.0754 - lr: 1.0000e-04\n",
      "Epoch 193/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0110 - rmse: 0.1051\n",
      "Epoch 193: val_loss improved from 0.00534 to 0.00527, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0059 - rmse: 0.0765 - val_loss: 0.0053 - val_rmse: 0.0726 - lr: 1.0000e-04\n",
      "Epoch 194/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0091 - rmse: 0.0957\n",
      "Epoch 194: val_loss did not improve from 0.00527\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0058 - rmse: 0.0763 - val_loss: 0.0055 - val_rmse: 0.0740 - lr: 1.0000e-04\n",
      "Epoch 195/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0688\n",
      "Epoch 195: val_loss improved from 0.00527 to 0.00520, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0059 - rmse: 0.0769 - val_loss: 0.0052 - val_rmse: 0.0721 - lr: 1.0000e-04\n",
      "Epoch 196/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0043 - rmse: 0.0656\n",
      "Epoch 196: val_loss did not improve from 0.00520\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0755 - val_loss: 0.0054 - val_rmse: 0.0733 - lr: 1.0000e-04\n",
      "Epoch 197/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0099 - rmse: 0.0997\n",
      "Epoch 197: val_loss did not improve from 0.00520\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0059 - rmse: 0.0765 - val_loss: 0.0053 - val_rmse: 0.0726 - lr: 1.0000e-04\n",
      "Epoch 198/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0076 - rmse: 0.0872\n",
      "Epoch 198: val_loss did not improve from 0.00520\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0756 - val_loss: 0.0053 - val_rmse: 0.0726 - lr: 1.0000e-04\n",
      "Epoch 199/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0514\n",
      "Epoch 199: val_loss did not improve from 0.00520\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0758 - val_loss: 0.0052 - val_rmse: 0.0722 - lr: 1.0000e-04\n",
      "Epoch 200/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0074 - rmse: 0.0863\n",
      "Epoch 200: val_loss did not improve from 0.00520\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0756 - val_loss: 0.0053 - val_rmse: 0.0730 - lr: 1.0000e-04\n",
      "Epoch 201/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0486\n",
      "Epoch 201: val_loss did not improve from 0.00520\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0755 - val_loss: 0.0053 - val_rmse: 0.0731 - lr: 1.0000e-04\n",
      "Epoch 202/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0064 - rmse: 0.0799\n",
      "Epoch 202: val_loss improved from 0.00520 to 0.00519, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0059 - rmse: 0.0769 - val_loss: 0.0052 - val_rmse: 0.0720 - lr: 1.0000e-04\n",
      "Epoch 203/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0727\n",
      "Epoch 203: val_loss did not improve from 0.00519\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0758 - val_loss: 0.0053 - val_rmse: 0.0730 - lr: 1.0000e-04\n",
      "Epoch 204/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0071 - rmse: 0.0844\n",
      "Epoch 204: val_loss did not improve from 0.00519\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0756 - val_loss: 0.0052 - val_rmse: 0.0723 - lr: 1.0000e-04\n",
      "Epoch 205/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0489\n",
      "Epoch 205: val_loss did not improve from 0.00519\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0757 - val_loss: 0.0053 - val_rmse: 0.0726 - lr: 1.0000e-04\n",
      "Epoch 206/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0605\n",
      "Epoch 206: val_loss did not improve from 0.00519\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0060 - rmse: 0.0773 - val_loss: 0.0052 - val_rmse: 0.0724 - lr: 1.0000e-04\n",
      "Epoch 207/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0088 - rmse: 0.0938\n",
      "Epoch 207: val_loss did not improve from 0.00519\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0057 - rmse: 0.0756 - val_loss: 0.0052 - val_rmse: 0.0722 - lr: 1.0000e-04\n",
      "Epoch 208/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0610\n",
      "Epoch 208: val_loss improved from 0.00519 to 0.00509, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0055 - rmse: 0.0739 - val_loss: 0.0051 - val_rmse: 0.0714 - lr: 1.0000e-04\n",
      "Epoch 209/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0126 - rmse: 0.1123\n",
      "Epoch 209: val_loss did not improve from 0.00509\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0744 - val_loss: 0.0052 - val_rmse: 0.0722 - lr: 1.0000e-04\n",
      "Epoch 210/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0119 - rmse: 0.1091\n",
      "Epoch 210: val_loss improved from 0.00509 to 0.00496, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0055 - rmse: 0.0743 - val_loss: 0.0050 - val_rmse: 0.0704 - lr: 1.0000e-04\n",
      "Epoch 211/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0619\n",
      "Epoch 211: val_loss did not improve from 0.00496\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0741 - val_loss: 0.0052 - val_rmse: 0.0720 - lr: 1.0000e-04\n",
      "Epoch 212/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0636\n",
      "Epoch 212: val_loss did not improve from 0.00496\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0742 - val_loss: 0.0050 - val_rmse: 0.0707 - lr: 1.0000e-04\n",
      "Epoch 213/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0494\n",
      "Epoch 213: val_loss did not improve from 0.00496\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0755 - val_loss: 0.0056 - val_rmse: 0.0746 - lr: 1.0000e-04\n",
      "Epoch 214/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0501\n",
      "Epoch 214: val_loss improved from 0.00496 to 0.00496, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0058 - rmse: 0.0762 - val_loss: 0.0050 - val_rmse: 0.0704 - lr: 1.0000e-04\n",
      "Epoch 215/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0093 - rmse: 0.0964\n",
      "Epoch 215: val_loss did not improve from 0.00496\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0059 - rmse: 0.0770 - val_loss: 0.0056 - val_rmse: 0.0748 - lr: 1.0000e-04\n",
      "Epoch 216/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0730\n",
      "Epoch 216: val_loss improved from 0.00496 to 0.00485, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0057 - rmse: 0.0755 - val_loss: 0.0049 - val_rmse: 0.0696 - lr: 1.0000e-04\n",
      "Epoch 217/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0672\n",
      "Epoch 217: val_loss did not improve from 0.00485\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0742 - val_loss: 0.0052 - val_rmse: 0.0720 - lr: 1.0000e-04\n",
      "Epoch 218/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0334\n",
      "Epoch 218: val_loss did not improve from 0.00485\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0740 - val_loss: 0.0050 - val_rmse: 0.0706 - lr: 1.0000e-04\n",
      "Epoch 219/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0128e-04 - rmse: 0.0300\n",
      "Epoch 219: val_loss did not improve from 0.00485\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0743 - val_loss: 0.0051 - val_rmse: 0.0715 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0481\n",
      "Epoch 220: val_loss did not improve from 0.00485\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0054 - rmse: 0.0732 - val_loss: 0.0053 - val_rmse: 0.0726 - lr: 1.0000e-04\n",
      "Epoch 221/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0511\n",
      "Epoch 221: val_loss did not improve from 0.00485\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0053 - rmse: 0.0730 - val_loss: 0.0049 - val_rmse: 0.0697 - lr: 1.0000e-04\n",
      "Epoch 222/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0595\n",
      "Epoch 222: val_loss did not improve from 0.00485\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0053 - rmse: 0.0730 - val_loss: 0.0051 - val_rmse: 0.0711 - lr: 1.0000e-04\n",
      "Epoch 223/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0503\n",
      "Epoch 223: val_loss did not improve from 0.00485\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0743 - val_loss: 0.0051 - val_rmse: 0.0714 - lr: 1.0000e-04\n",
      "Epoch 224/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0507\n",
      "Epoch 224: val_loss improved from 0.00485 to 0.00477, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0054 - rmse: 0.0736 - val_loss: 0.0048 - val_rmse: 0.0690 - lr: 1.0000e-04\n",
      "Epoch 225/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0074 - rmse: 0.0860\n",
      "Epoch 225: val_loss did not improve from 0.00477\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0755 - val_loss: 0.0052 - val_rmse: 0.0724 - lr: 1.0000e-04\n",
      "Epoch 226/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0547\n",
      "Epoch 226: val_loss did not improve from 0.00477\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0758 - val_loss: 0.0048 - val_rmse: 0.0694 - lr: 1.0000e-04\n",
      "Epoch 227/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0563\n",
      "Epoch 227: val_loss did not improve from 0.00477\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0070 - rmse: 0.0838 - val_loss: 0.0060 - val_rmse: 0.0772 - lr: 1.0000e-04\n",
      "Epoch 228/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0075 - rmse: 0.0865\n",
      "Epoch 228: val_loss did not improve from 0.00477\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0066 - rmse: 0.0815 - val_loss: 0.0048 - val_rmse: 0.0694 - lr: 1.0000e-04\n",
      "Epoch 229/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0625\n",
      "Epoch 229: val_loss did not improve from 0.00477\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0060 - rmse: 0.0775 - val_loss: 0.0051 - val_rmse: 0.0714 - lr: 1.0000e-04\n",
      "Epoch 230/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0072 - rmse: 0.0849\n",
      "Epoch 230: val_loss did not improve from 0.00477\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0055 - rmse: 0.0739 - val_loss: 0.0051 - val_rmse: 0.0717 - lr: 1.0000e-04\n",
      "Epoch 231/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0091 - rmse: 0.0955\n",
      "Epoch 231: val_loss improved from 0.00477 to 0.00470, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0054 - rmse: 0.0738 - val_loss: 0.0047 - val_rmse: 0.0685 - lr: 1.0000e-04\n",
      "Epoch 232/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9762e-04 - rmse: 0.0316\n",
      "Epoch 232: val_loss did not improve from 0.00470\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0052 - rmse: 0.0724 - val_loss: 0.0050 - val_rmse: 0.0710 - lr: 1.0000e-04\n",
      "Epoch 233/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0057 - rmse: 0.0753\n",
      "Epoch 233: val_loss did not improve from 0.00470\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0052 - rmse: 0.0719 - val_loss: 0.0048 - val_rmse: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 234/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0331\n",
      "Epoch 234: val_loss did not improve from 0.00470\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0051 - rmse: 0.0711 - val_loss: 0.0050 - val_rmse: 0.0710 - lr: 1.0000e-04\n",
      "Epoch 235/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0440\n",
      "Epoch 235: val_loss did not improve from 0.00470\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0051 - rmse: 0.0711 - val_loss: 0.0048 - val_rmse: 0.0692 - lr: 1.0000e-04\n",
      "Epoch 236/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0112 - rmse: 0.1060\n",
      "Epoch 236: val_loss did not improve from 0.00470\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0051 - rmse: 0.0717 - val_loss: 0.0048 - val_rmse: 0.0695 - lr: 1.0000e-04\n",
      "Epoch 237/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0662\n",
      "Epoch 237: val_loss did not improve from 0.00470\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0051 - rmse: 0.0717 - val_loss: 0.0048 - val_rmse: 0.0692 - lr: 1.0000e-04\n",
      "Epoch 238/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0625\n",
      "Epoch 238: val_loss improved from 0.00470 to 0.00461, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0051 - rmse: 0.0715 - val_loss: 0.0046 - val_rmse: 0.0679 - lr: 1.0000e-04\n",
      "Epoch 239/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0060 - rmse: 0.0772\n",
      "Epoch 239: val_loss did not improve from 0.00461\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - rmse: 0.0703 - val_loss: 0.0048 - val_rmse: 0.0695 - lr: 1.0000e-04\n",
      "Epoch 240/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0448\n",
      "Epoch 240: val_loss did not improve from 0.00461\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0050 - rmse: 0.0707 - val_loss: 0.0048 - val_rmse: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 241/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0505\n",
      "Epoch 241: val_loss improved from 0.00461 to 0.00458, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0051 - rmse: 0.0711 - val_loss: 0.0046 - val_rmse: 0.0677 - lr: 1.0000e-04\n",
      "Epoch 242/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0042 - rmse: 0.0645\n",
      "Epoch 242: val_loss did not improve from 0.00458\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0050 - rmse: 0.0708 - val_loss: 0.0046 - val_rmse: 0.0678 - lr: 1.0000e-04\n",
      "Epoch 243/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0498\n",
      "Epoch 243: val_loss did not improve from 0.00458\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0051 - rmse: 0.0712 - val_loss: 0.0047 - val_rmse: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 244/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0615\n",
      "Epoch 244: val_loss did not improve from 0.00458\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - rmse: 0.0703 - val_loss: 0.0048 - val_rmse: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 245/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0042 - rmse: 0.0649\n",
      "Epoch 245: val_loss improved from 0.00458 to 0.00449, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0050 - rmse: 0.0708 - val_loss: 0.0045 - val_rmse: 0.0670 - lr: 1.0000e-04\n",
      "Epoch 246/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0042 - rmse: 0.0644\n",
      "Epoch 246: val_loss did not improve from 0.00449\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - rmse: 0.0703 - val_loss: 0.0048 - val_rmse: 0.0696 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0730\n",
      "Epoch 247: val_loss improved from 0.00449 to 0.00448, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0050 - rmse: 0.0705 - val_loss: 0.0045 - val_rmse: 0.0669 - lr: 1.0000e-04\n",
      "Epoch 248/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0113 - rmse: 0.1061\n",
      "Epoch 248: val_loss did not improve from 0.00448\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - rmse: 0.0702 - val_loss: 0.0047 - val_rmse: 0.0685 - lr: 1.0000e-04\n",
      "Epoch 249/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0455\n",
      "Epoch 249: val_loss did not improve from 0.00448\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0050 - rmse: 0.0704 - val_loss: 0.0047 - val_rmse: 0.0684 - lr: 1.0000e-04\n",
      "Epoch 250/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0066 - rmse: 0.0809\n",
      "Epoch 250: val_loss improved from 0.00448 to 0.00444, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0048 - rmse: 0.0694 - val_loss: 0.0044 - val_rmse: 0.0667 - lr: 1.0000e-04\n",
      "Epoch 251/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0510\n",
      "Epoch 251: val_loss did not improve from 0.00444\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - rmse: 0.0698 - val_loss: 0.0045 - val_rmse: 0.0668 - lr: 1.0000e-04\n",
      "Epoch 252/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0077 - rmse: 0.0879\n",
      "Epoch 252: val_loss improved from 0.00444 to 0.00444, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0048 - rmse: 0.0695 - val_loss: 0.0044 - val_rmse: 0.0667 - lr: 1.0000e-04\n",
      "Epoch 253/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0082 - rmse: 0.0907\n",
      "Epoch 253: val_loss did not improve from 0.00444\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0050 - rmse: 0.0704 - val_loss: 0.0045 - val_rmse: 0.0670 - lr: 1.0000e-04\n",
      "Epoch 254/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0519\n",
      "Epoch 254: val_loss did not improve from 0.00444\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0050 - rmse: 0.0710 - val_loss: 0.0048 - val_rmse: 0.0692 - lr: 1.0000e-04\n",
      "Epoch 255/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0048 - rmse: 0.0692\n",
      "Epoch 255: val_loss did not improve from 0.00444\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - rmse: 0.0701 - val_loss: 0.0045 - val_rmse: 0.0669 - lr: 1.0000e-04\n",
      "Epoch 256/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0731\n",
      "Epoch 256: val_loss improved from 0.00444 to 0.00435, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0047 - rmse: 0.0689 - val_loss: 0.0044 - val_rmse: 0.0660 - lr: 1.0000e-04\n",
      "Epoch 257/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0099 - rmse: 0.0993\n",
      "Epoch 257: val_loss did not improve from 0.00435\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - rmse: 0.0700 - val_loss: 0.0048 - val_rmse: 0.0693 - lr: 1.0000e-04\n",
      "Epoch 258/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0074 - rmse: 0.0861\n",
      "Epoch 258: val_loss did not improve from 0.00435\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - rmse: 0.0691 - val_loss: 0.0044 - val_rmse: 0.0661 - lr: 1.0000e-04\n",
      "Epoch 259/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0083 - rmse: 0.0912\n",
      "Epoch 259: val_loss did not improve from 0.00435\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0047 - rmse: 0.0685 - val_loss: 0.0044 - val_rmse: 0.0664 - lr: 1.0000e-04\n",
      "Epoch 260/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0509\n",
      "Epoch 260: val_loss improved from 0.00435 to 0.00423, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0048 - rmse: 0.0695 - val_loss: 0.0042 - val_rmse: 0.0651 - lr: 1.0000e-04\n",
      "Epoch 261/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0531\n",
      "Epoch 261: val_loss did not improve from 0.00423\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - rmse: 0.0693 - val_loss: 0.0047 - val_rmse: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 262/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0072 - rmse: 0.0851\n",
      "Epoch 262: val_loss did not improve from 0.00423\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0047 - rmse: 0.0684 - val_loss: 0.0042 - val_rmse: 0.0651 - lr: 1.0000e-04\n",
      "Epoch 263/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0068 - rmse: 0.0824\n",
      "Epoch 263: val_loss did not improve from 0.00423\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0047 - rmse: 0.0689 - val_loss: 0.0044 - val_rmse: 0.0665 - lr: 1.0000e-04\n",
      "Epoch 264/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0526\n",
      "Epoch 264: val_loss improved from 0.00423 to 0.00417, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0046 - rmse: 0.0678 - val_loss: 0.0042 - val_rmse: 0.0646 - lr: 1.0000e-04\n",
      "Epoch 265/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0428\n",
      "Epoch 265: val_loss did not improve from 0.00417\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0046 - rmse: 0.0675 - val_loss: 0.0045 - val_rmse: 0.0670 - lr: 1.0000e-04\n",
      "Epoch 266/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0484\n",
      "Epoch 266: val_loss did not improve from 0.00417\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0046 - rmse: 0.0678 - val_loss: 0.0043 - val_rmse: 0.0656 - lr: 1.0000e-04\n",
      "Epoch 267/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0104 - rmse: 0.1022\n",
      "Epoch 267: val_loss did not improve from 0.00417\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0046 - rmse: 0.0680 - val_loss: 0.0042 - val_rmse: 0.0651 - lr: 1.0000e-04\n",
      "Epoch 268/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0074 - rmse: 0.0858\n",
      "Epoch 268: val_loss did not improve from 0.00417\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - rmse: 0.0694 - val_loss: 0.0043 - val_rmse: 0.0659 - lr: 1.0000e-04\n",
      "Epoch 269/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0093 - rmse: 0.0967\n",
      "Epoch 269: val_loss improved from 0.00417 to 0.00411, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0045 - rmse: 0.0673 - val_loss: 0.0041 - val_rmse: 0.0641 - lr: 1.0000e-04\n",
      "Epoch 270/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0604\n",
      "Epoch 270: val_loss improved from 0.00411 to 0.00410, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0048 - rmse: 0.0692 - val_loss: 0.0041 - val_rmse: 0.0640 - lr: 1.0000e-04\n",
      "Epoch 271/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0672\n",
      "Epoch 271: val_loss did not improve from 0.00410\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0045 - rmse: 0.0670 - val_loss: 0.0043 - val_rmse: 0.0657 - lr: 1.0000e-04\n",
      "Epoch 272/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0494\n",
      "Epoch 272: val_loss did not improve from 0.00410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0046 - rmse: 0.0677 - val_loss: 0.0045 - val_rmse: 0.0667 - lr: 1.0000e-04\n",
      "Epoch 273/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0583\n",
      "Epoch 273: val_loss improved from 0.00410 to 0.00405, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0046 - rmse: 0.0675 - val_loss: 0.0041 - val_rmse: 0.0637 - lr: 1.0000e-04\n",
      "Epoch 274/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0725\n",
      "Epoch 274: val_loss did not improve from 0.00405\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0044 - rmse: 0.0664 - val_loss: 0.0042 - val_rmse: 0.0650 - lr: 1.0000e-04\n",
      "Epoch 275/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7553e-04 - rmse: 0.0218\n",
      "Epoch 275: val_loss did not improve from 0.00405\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0045 - rmse: 0.0674 - val_loss: 0.0041 - val_rmse: 0.0643 - lr: 1.0000e-04\n",
      "Epoch 276/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0558\n",
      "Epoch 276: val_loss did not improve from 0.00405\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0045 - rmse: 0.0672 - val_loss: 0.0044 - val_rmse: 0.0661 - lr: 1.0000e-04\n",
      "Epoch 277/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0593\n",
      "Epoch 277: val_loss improved from 0.00405 to 0.00392, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0045 - rmse: 0.0667 - val_loss: 0.0039 - val_rmse: 0.0626 - lr: 1.0000e-04\n",
      "Epoch 278/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0058 - rmse: 0.0763\n",
      "Epoch 278: val_loss did not improve from 0.00392\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0045 - rmse: 0.0672 - val_loss: 0.0040 - val_rmse: 0.0636 - lr: 1.0000e-04\n",
      "Epoch 279/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0320\n",
      "Epoch 279: val_loss did not improve from 0.00392\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0044 - rmse: 0.0663 - val_loss: 0.0039 - val_rmse: 0.0628 - lr: 1.0000e-04\n",
      "Epoch 280/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0072 - rmse: 0.0849\n",
      "Epoch 280: val_loss did not improve from 0.00392\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0043 - rmse: 0.0658 - val_loss: 0.0040 - val_rmse: 0.0631 - lr: 1.0000e-04\n",
      "Epoch 281/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0573\n",
      "Epoch 281: val_loss did not improve from 0.00392\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0044 - rmse: 0.0662 - val_loss: 0.0045 - val_rmse: 0.0672 - lr: 1.0000e-04\n",
      "Epoch 282/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0076 - rmse: 0.0870\n",
      "Epoch 282: val_loss did not improve from 0.00392\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0044 - rmse: 0.0665 - val_loss: 0.0040 - val_rmse: 0.0631 - lr: 1.0000e-04\n",
      "Epoch 283/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0464\n",
      "Epoch 283: val_loss did not improve from 0.00392\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0043 - rmse: 0.0656 - val_loss: 0.0049 - val_rmse: 0.0703 - lr: 1.0000e-04\n",
      "Epoch 284/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0555\n",
      "Epoch 284: val_loss improved from 0.00392 to 0.00373, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0046 - rmse: 0.0682 - val_loss: 0.0037 - val_rmse: 0.0610 - lr: 1.0000e-04\n",
      "Epoch 285/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0663\n",
      "Epoch 285: val_loss did not improve from 0.00373\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - rmse: 0.0691 - val_loss: 0.0039 - val_rmse: 0.0626 - lr: 1.0000e-04\n",
      "Epoch 286/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0075 - rmse: 0.0868\n",
      "Epoch 286: val_loss did not improve from 0.00373\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - rmse: 0.0650 - val_loss: 0.0038 - val_rmse: 0.0620 - lr: 1.0000e-04\n",
      "Epoch 287/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0375\n",
      "Epoch 287: val_loss did not improve from 0.00373\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0043 - rmse: 0.0659 - val_loss: 0.0038 - val_rmse: 0.0618 - lr: 1.0000e-04\n",
      "Epoch 288/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0082 - rmse: 0.0906\n",
      "Epoch 288: val_loss did not improve from 0.00373\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0043 - rmse: 0.0658 - val_loss: 0.0040 - val_rmse: 0.0629 - lr: 1.0000e-04\n",
      "Epoch 289/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0687\n",
      "Epoch 289: val_loss did not improve from 0.00373\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - rmse: 0.0650 - val_loss: 0.0037 - val_rmse: 0.0611 - lr: 1.0000e-04\n",
      "Epoch 290/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0473\n",
      "Epoch 290: val_loss did not improve from 0.00373\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - rmse: 0.0649 - val_loss: 0.0042 - val_rmse: 0.0647 - lr: 1.0000e-04\n",
      "Epoch 291/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0095 - rmse: 0.0973\n",
      "Epoch 291: val_loss improved from 0.00373 to 0.00366, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0042 - rmse: 0.0651 - val_loss: 0.0037 - val_rmse: 0.0605 - lr: 1.0000e-04\n",
      "Epoch 292/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0074 - rmse: 0.0859\n",
      "Epoch 292: val_loss did not improve from 0.00366\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0043 - rmse: 0.0654 - val_loss: 0.0043 - val_rmse: 0.0659 - lr: 1.0000e-04\n",
      "Epoch 293/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0061 - rmse: 0.0779\n",
      "Epoch 293: val_loss did not improve from 0.00366\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - rmse: 0.0648 - val_loss: 0.0037 - val_rmse: 0.0608 - lr: 1.0000e-04\n",
      "Epoch 294/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0067 - rmse: 0.0817\n",
      "Epoch 294: val_loss improved from 0.00366 to 0.00362, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0040 - rmse: 0.0631 - val_loss: 0.0036 - val_rmse: 0.0602 - lr: 1.0000e-04\n",
      "Epoch 295/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0112 - rmse: 0.1057\n",
      "Epoch 295: val_loss improved from 0.00362 to 0.00361, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0041 - rmse: 0.0641 - val_loss: 0.0036 - val_rmse: 0.0601 - lr: 1.0000e-04\n",
      "Epoch 296/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0665\n",
      "Epoch 296: val_loss did not improve from 0.00361\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0040 - rmse: 0.0635 - val_loss: 0.0038 - val_rmse: 0.0613 - lr: 1.0000e-04\n",
      "Epoch 297/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0042 - rmse: 0.0651\n",
      "Epoch 297: val_loss improved from 0.00361 to 0.00352, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0040 - rmse: 0.0635 - val_loss: 0.0035 - val_rmse: 0.0593 - lr: 1.0000e-04\n",
      "Epoch 298/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0392\n",
      "Epoch 298: val_loss did not improve from 0.00352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0043 - rmse: 0.0658 - val_loss: 0.0038 - val_rmse: 0.0615 - lr: 1.0000e-04\n",
      "Epoch 299/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0064 - rmse: 0.0801\n",
      "Epoch 299: val_loss did not improve from 0.00352\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - rmse: 0.0649 - val_loss: 0.0038 - val_rmse: 0.0617 - lr: 1.0000e-04\n",
      "Epoch 300/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5843e-04 - rmse: 0.0275\n",
      "Epoch 300: val_loss did not improve from 0.00352\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0044 - rmse: 0.0663 - val_loss: 0.0037 - val_rmse: 0.0605 - lr: 1.0000e-04\n",
      "Epoch 301/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0323\n",
      "Epoch 301: val_loss did not improve from 0.00352\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0041 - rmse: 0.0643 - val_loss: 0.0046 - val_rmse: 0.0680 - lr: 1.0000e-04\n",
      "Epoch 302/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0629\n",
      "Epoch 302: val_loss improved from 0.00352 to 0.00349, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0045 - rmse: 0.0669 - val_loss: 0.0035 - val_rmse: 0.0591 - lr: 1.0000e-04\n",
      "Epoch 303/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0450\n",
      "Epoch 303: val_loss did not improve from 0.00349\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - rmse: 0.0650 - val_loss: 0.0040 - val_rmse: 0.0635 - lr: 1.0000e-04\n",
      "Epoch 304/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0621\n",
      "Epoch 304: val_loss improved from 0.00349 to 0.00346, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.0041 - rmse: 0.0642 - val_loss: 0.0035 - val_rmse: 0.0588 - lr: 1.0000e-04\n",
      "Epoch 305/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0592\n",
      "Epoch 305: val_loss did not improve from 0.00346\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0039 - rmse: 0.0627 - val_loss: 0.0035 - val_rmse: 0.0592 - lr: 1.0000e-04\n",
      "Epoch 306/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0478\n",
      "Epoch 306: val_loss improved from 0.00346 to 0.00334, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0037 - rmse: 0.0608 - val_loss: 0.0033 - val_rmse: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 307/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0094 - rmse: 0.0969\n",
      "Epoch 307: val_loss did not improve from 0.00334\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0041 - rmse: 0.0637 - val_loss: 0.0035 - val_rmse: 0.0595 - lr: 1.0000e-04\n",
      "Epoch 308/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0450\n",
      "Epoch 308: val_loss did not improve from 0.00334\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - rmse: 0.0652 - val_loss: 0.0038 - val_rmse: 0.0613 - lr: 1.0000e-04\n",
      "Epoch 309/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0564\n",
      "Epoch 309: val_loss improved from 0.00334 to 0.00333, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0041 - rmse: 0.0643 - val_loss: 0.0033 - val_rmse: 0.0577 - lr: 1.0000e-04\n",
      "Epoch 310/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0507\n",
      "Epoch 310: val_loss did not improve from 0.00333\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0039 - rmse: 0.0628 - val_loss: 0.0038 - val_rmse: 0.0613 - lr: 1.0000e-04\n",
      "Epoch 311/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0602\n",
      "Epoch 311: val_loss did not improve from 0.00333\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0039 - rmse: 0.0624 - val_loss: 0.0033 - val_rmse: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 312/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0077 - rmse: 0.0880\n",
      "Epoch 312: val_loss improved from 0.00333 to 0.00329, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0041 - rmse: 0.0644 - val_loss: 0.0033 - val_rmse: 0.0573 - lr: 1.0000e-04\n",
      "Epoch 313/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0061 - rmse: 0.0784\n",
      "Epoch 313: val_loss did not improve from 0.00329\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0039 - rmse: 0.0628 - val_loss: 0.0039 - val_rmse: 0.0621 - lr: 1.0000e-04\n",
      "Epoch 314/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0593\n",
      "Epoch 314: val_loss improved from 0.00329 to 0.00327, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0039 - rmse: 0.0621 - val_loss: 0.0033 - val_rmse: 0.0572 - lr: 1.0000e-04\n",
      "Epoch 315/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0629\n",
      "Epoch 315: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0036 - rmse: 0.0603 - val_loss: 0.0036 - val_rmse: 0.0599 - lr: 1.0000e-04\n",
      "Epoch 316/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0478\n",
      "Epoch 316: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0036 - rmse: 0.0598 - val_loss: 0.0033 - val_rmse: 0.0575 - lr: 1.0000e-04\n",
      "Epoch 317/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0635\n",
      "Epoch 317: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0036 - rmse: 0.0600 - val_loss: 0.0036 - val_rmse: 0.0597 - lr: 1.0000e-04\n",
      "Epoch 318/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0073 - rmse: 0.0855\n",
      "Epoch 318: val_loss improved from 0.00327 to 0.00321, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0037 - rmse: 0.0609 - val_loss: 0.0032 - val_rmse: 0.0566 - lr: 1.0000e-04\n",
      "Epoch 319/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0573\n",
      "Epoch 319: val_loss did not improve from 0.00321\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0036 - rmse: 0.0599 - val_loss: 0.0033 - val_rmse: 0.0576 - lr: 1.0000e-04\n",
      "Epoch 320/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1952e-04 - rmse: 0.0205\n",
      "Epoch 320: val_loss improved from 0.00321 to 0.00313, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0035 - rmse: 0.0591 - val_loss: 0.0031 - val_rmse: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 321/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0509\n",
      "Epoch 321: val_loss did not improve from 0.00313\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0037 - rmse: 0.0610 - val_loss: 0.0036 - val_rmse: 0.0602 - lr: 1.0000e-04\n",
      "Epoch 322/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0068 - rmse: 0.0823\n",
      "Epoch 322: val_loss improved from 0.00313 to 0.00310, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0035 - rmse: 0.0595 - val_loss: 0.0031 - val_rmse: 0.0557 - lr: 1.0000e-04\n",
      "Epoch 323/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0064 - rmse: 0.0802\n",
      "Epoch 323: val_loss did not improve from 0.00310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0035 - rmse: 0.0590 - val_loss: 0.0031 - val_rmse: 0.0561 - lr: 1.0000e-04\n",
      "Epoch 324/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0365\n",
      "Epoch 324: val_loss improved from 0.00310 to 0.00310, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0035 - rmse: 0.0588 - val_loss: 0.0031 - val_rmse: 0.0557 - lr: 1.0000e-04\n",
      "Epoch 325/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0457\n",
      "Epoch 325: val_loss improved from 0.00310 to 0.00306, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.0035 - rmse: 0.0591 - val_loss: 0.0031 - val_rmse: 0.0554 - lr: 1.0000e-04\n",
      "Epoch 326/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0553\n",
      "Epoch 326: val_loss did not improve from 0.00306\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0037 - rmse: 0.0605 - val_loss: 0.0038 - val_rmse: 0.0614 - lr: 1.0000e-04\n",
      "Epoch 327/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0634\n",
      "Epoch 327: val_loss did not improve from 0.00306\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0038 - rmse: 0.0618 - val_loss: 0.0033 - val_rmse: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 328/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0552\n",
      "Epoch 328: val_loss did not improve from 0.00306\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0037 - rmse: 0.0607 - val_loss: 0.0040 - val_rmse: 0.0633 - lr: 1.0000e-04\n",
      "Epoch 329/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0052 - rmse: 0.0719\n",
      "Epoch 329: val_loss improved from 0.00306 to 0.00299, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.0037 - rmse: 0.0609 - val_loss: 0.0030 - val_rmse: 0.0547 - lr: 1.0000e-04\n",
      "Epoch 330/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0444\n",
      "Epoch 330: val_loss did not improve from 0.00299\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0035 - rmse: 0.0592 - val_loss: 0.0031 - val_rmse: 0.0559 - lr: 1.0000e-04\n",
      "Epoch 331/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0746e-04 - rmse: 0.0266\n",
      "Epoch 331: val_loss did not improve from 0.00299\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0034 - rmse: 0.0584 - val_loss: 0.0032 - val_rmse: 0.0564 - lr: 1.0000e-04\n",
      "Epoch 332/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0366\n",
      "Epoch 332: val_loss did not improve from 0.00299\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0038 - rmse: 0.0614 - val_loss: 0.0030 - val_rmse: 0.0549 - lr: 1.0000e-04\n",
      "Epoch 333/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0083 - rmse: 0.0912\n",
      "Epoch 333: val_loss did not improve from 0.00299\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0034 - rmse: 0.0583 - val_loss: 0.0030 - val_rmse: 0.0548 - lr: 1.0000e-04\n",
      "Epoch 334/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0496\n",
      "Epoch 334: val_loss did not improve from 0.00299\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0574 - val_loss: 0.0030 - val_rmse: 0.0549 - lr: 1.0000e-04\n",
      "Epoch 335/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0319\n",
      "Epoch 335: val_loss did not improve from 0.00299\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0032 - rmse: 0.0566 - val_loss: 0.0030 - val_rmse: 0.0549 - lr: 1.0000e-04\n",
      "Epoch 336/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0081 - rmse: 0.0900\n",
      "Epoch 336: val_loss improved from 0.00299 to 0.00291, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0032 - rmse: 0.0565 - val_loss: 0.0029 - val_rmse: 0.0539 - lr: 1.0000e-04\n",
      "Epoch 337/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0448\n",
      "Epoch 337: val_loss improved from 0.00291 to 0.00284, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0028 - val_rmse: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 338/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0424\n",
      "Epoch 338: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0031 - rmse: 0.0560 - val_loss: 0.0030 - val_rmse: 0.0550 - lr: 1.0000e-04\n",
      "Epoch 339/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0582\n",
      "Epoch 339: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0576 - val_loss: 0.0029 - val_rmse: 0.0537 - lr: 1.0000e-04\n",
      "Epoch 340/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0469\n",
      "Epoch 340: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0033 - rmse: 0.0576 - val_loss: 0.0029 - val_rmse: 0.0539 - lr: 1.0000e-04\n",
      "Epoch 341/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0403\n",
      "Epoch 341: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0034 - rmse: 0.0586 - val_loss: 0.0038 - val_rmse: 0.0613 - lr: 1.0000e-04\n",
      "Epoch 342/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0524\n",
      "Epoch 342: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0034 - rmse: 0.0579 - val_loss: 0.0029 - val_rmse: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 343/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0598\n",
      "Epoch 343: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0571 - val_loss: 0.0037 - val_rmse: 0.0609 - lr: 1.0000e-04\n",
      "Epoch 344/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0488\n",
      "Epoch 344: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0038 - rmse: 0.0615 - val_loss: 0.0039 - val_rmse: 0.0624 - lr: 1.0000e-04\n",
      "Epoch 345/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0514\n",
      "Epoch 345: val_loss did not improve from 0.00284\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0039 - rmse: 0.0626 - val_loss: 0.0042 - val_rmse: 0.0645 - lr: 1.0000e-04\n",
      "Epoch 346/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0474\n",
      "Epoch 346: val_loss improved from 0.00284 to 0.00279, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0039 - rmse: 0.0627 - val_loss: 0.0028 - val_rmse: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 347/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0393\n",
      "Epoch 347: val_loss did not improve from 0.00279\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0574 - val_loss: 0.0028 - val_rmse: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 348/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0544\n",
      "Epoch 348: val_loss did not improve from 0.00279\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0031 - rmse: 0.0558 - val_loss: 0.0030 - val_rmse: 0.0551 - lr: 1.0000e-04\n",
      "Epoch 349/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0451\n",
      "Epoch 349: val_loss improved from 0.00279 to 0.00274, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0031 - rmse: 0.0556 - val_loss: 0.0027 - val_rmse: 0.0523 - lr: 1.0000e-04\n",
      "Epoch 350/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4138e-04 - rmse: 0.0290\n",
      "Epoch 350: val_loss did not improve from 0.00274\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0030 - rmse: 0.0547 - val_loss: 0.0028 - val_rmse: 0.0527 - lr: 1.0000e-04\n",
      "Epoch 351/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0341\n",
      "Epoch 351: val_loss improved from 0.00274 to 0.00273, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0030 - rmse: 0.0545 - val_loss: 0.0027 - val_rmse: 0.0523 - lr: 1.0000e-04\n",
      "Epoch 352/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0470\n",
      "Epoch 352: val_loss did not improve from 0.00273\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0030 - rmse: 0.0544 - val_loss: 0.0029 - val_rmse: 0.0543 - lr: 1.0000e-04\n",
      "Epoch 353/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0453\n",
      "Epoch 353: val_loss did not improve from 0.00273\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0535 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 354/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0477\n",
      "Epoch 354: val_loss improved from 0.00273 to 0.00264, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0029 - rmse: 0.0542 - val_loss: 0.0026 - val_rmse: 0.0514 - lr: 1.0000e-04\n",
      "Epoch 355/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0391\n",
      "Epoch 355: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0031 - rmse: 0.0559 - val_loss: 0.0028 - val_rmse: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 356/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4742e-04 - rmse: 0.0234\n",
      "Epoch 356: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0534 - val_loss: 0.0027 - val_rmse: 0.0523 - lr: 1.0000e-04\n",
      "Epoch 357/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0683\n",
      "Epoch 357: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0030 - rmse: 0.0545 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 358/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0420\n",
      "Epoch 358: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0535 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 359/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0502\n",
      "Epoch 359: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0577 - val_loss: 0.0029 - val_rmse: 0.0540 - lr: 1.0000e-04\n",
      "Epoch 360/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0492\n",
      "Epoch 360: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0542 - val_loss: 0.0027 - val_rmse: 0.0516 - lr: 1.0000e-04\n",
      "Epoch 361/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0361\n",
      "Epoch 361: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0032 - rmse: 0.0564 - val_loss: 0.0028 - val_rmse: 0.0529 - lr: 1.0000e-04\n",
      "Epoch 362/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0572\n",
      "Epoch 362: val_loss did not improve from 0.00264\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0575 - val_loss: 0.0030 - val_rmse: 0.0544 - lr: 1.0000e-04\n",
      "Epoch 363/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0459\n",
      "Epoch 363: val_loss improved from 0.00264 to 0.00244, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0029 - rmse: 0.0538 - val_loss: 0.0024 - val_rmse: 0.0494 - lr: 1.0000e-04\n",
      "Epoch 364/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0567\n",
      "Epoch 364: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - rmse: 0.0527 - val_loss: 0.0030 - val_rmse: 0.0546 - lr: 1.0000e-04\n",
      "Epoch 365/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0577\n",
      "Epoch 365: val_loss improved from 0.00244 to 0.00242, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0030 - rmse: 0.0544 - val_loss: 0.0024 - val_rmse: 0.0492 - lr: 1.0000e-04\n",
      "Epoch 366/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0041 - rmse: 0.0639\n",
      "Epoch 366: val_loss improved from 0.00242 to 0.00229, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0028 - rmse: 0.0529 - val_loss: 0.0023 - val_rmse: 0.0478 - lr: 1.0000e-04\n",
      "Epoch 367/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0063 - rmse: 0.0793\n",
      "Epoch 367: val_loss did not improve from 0.00229\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - rmse: 0.0520 - val_loss: 0.0023 - val_rmse: 0.0479 - lr: 1.0000e-04\n",
      "Epoch 368/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0491\n",
      "Epoch 368: val_loss did not improve from 0.00229\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - rmse: 0.0521 - val_loss: 0.0023 - val_rmse: 0.0485 - lr: 1.0000e-04\n",
      "Epoch 369/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0049 - rmse: 0.0703\n",
      "Epoch 369: val_loss improved from 0.00229 to 0.00225, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0028 - rmse: 0.0528 - val_loss: 0.0023 - val_rmse: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 370/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0667\n",
      "Epoch 370: val_loss did not improve from 0.00225\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0026 - rmse: 0.0514 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 371/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0471\n",
      "Epoch 371: val_loss did not improve from 0.00225\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0031 - rmse: 0.0555 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 372/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0050 - rmse: 0.0710\n",
      "Epoch 372: val_loss did not improve from 0.00225\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - rmse: 0.0528 - val_loss: 0.0024 - val_rmse: 0.0485 - lr: 1.0000e-04\n",
      "Epoch 373/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9148e-04 - rmse: 0.0299\n",
      "Epoch 373: val_loss did not improve from 0.00225\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - rmse: 0.0529 - val_loss: 0.0024 - val_rmse: 0.0488 - lr: 1.0000e-04\n",
      "Epoch 374/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3808e-04 - rmse: 0.0289\n",
      "Epoch 374: val_loss did not improve from 0.00225\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - rmse: 0.0515 - val_loss: 0.0024 - val_rmse: 0.0486 - lr: 1.0000e-04\n",
      "Epoch 375/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0058 - rmse: 0.0762\n",
      "Epoch 375: val_loss did not improve from 0.00225\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0537 - val_loss: 0.0026 - val_rmse: 0.0512 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0712\n",
      "Epoch 376: val_loss improved from 0.00225 to 0.00221, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0025 - rmse: 0.0503 - val_loss: 0.0022 - val_rmse: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 377/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7502e-04 - rmse: 0.0218\n",
      "Epoch 377: val_loss did not improve from 0.00221\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0025 - rmse: 0.0504 - val_loss: 0.0026 - val_rmse: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 378/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0363\n",
      "Epoch 378: val_loss improved from 0.00221 to 0.00218, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0029 - rmse: 0.0538 - val_loss: 0.0022 - val_rmse: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 379/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7936e-04 - rmse: 0.0279\n",
      "Epoch 379: val_loss improved from 0.00218 to 0.00210, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0024 - rmse: 0.0494 - val_loss: 0.0021 - val_rmse: 0.0459 - lr: 1.0000e-04\n",
      "Epoch 380/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5982e-04 - rmse: 0.0190\n",
      "Epoch 380: val_loss improved from 0.00210 to 0.00209, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0024 - rmse: 0.0488 - val_loss: 0.0021 - val_rmse: 0.0458 - lr: 1.0000e-04\n",
      "Epoch 381/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0433\n",
      "Epoch 381: val_loss did not improve from 0.00209\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0025 - rmse: 0.0504 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 382/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9285e-04 - rmse: 0.0263\n",
      "Epoch 382: val_loss did not improve from 0.00209\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0026 - rmse: 0.0512 - val_loss: 0.0022 - val_rmse: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 383/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0594\n",
      "Epoch 383: val_loss did not improve from 0.00209\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0026 - rmse: 0.0512 - val_loss: 0.0023 - val_rmse: 0.0479 - lr: 1.0000e-04\n",
      "Epoch 384/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0330\n",
      "Epoch 384: val_loss did not improve from 0.00209\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0540 - val_loss: 0.0024 - val_rmse: 0.0492 - lr: 1.0000e-04\n",
      "Epoch 385/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0429\n",
      "Epoch 385: val_loss did not improve from 0.00209\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - rmse: 0.0525 - val_loss: 0.0022 - val_rmse: 0.0473 - lr: 1.0000e-04\n",
      "Epoch 386/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0060 - rmse: 0.0775\n",
      "Epoch 386: val_loss improved from 0.00209 to 0.00199, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0023 - rmse: 0.0481 - val_loss: 0.0020 - val_rmse: 0.0446 - lr: 1.0000e-04\n",
      "Epoch 387/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0415\n",
      "Epoch 387: val_loss did not improve from 0.00199\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0024 - rmse: 0.0489 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 388/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0055 - rmse: 0.0743\n",
      "Epoch 388: val_loss did not improve from 0.00199\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0025 - rmse: 0.0502 - val_loss: 0.0020 - val_rmse: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 389/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0062 - rmse: 0.0789\n",
      "Epoch 389: val_loss improved from 0.00199 to 0.00189, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0025 - rmse: 0.0495 - val_loss: 0.0019 - val_rmse: 0.0435 - lr: 1.0000e-04\n",
      "Epoch 390/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0050 - rmse: 0.0704\n",
      "Epoch 390: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0023 - rmse: 0.0478 - val_loss: 0.0020 - val_rmse: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 391/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0317\n",
      "Epoch 391: val_loss improved from 0.00189 to 0.00185, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0022 - rmse: 0.0470 - val_loss: 0.0018 - val_rmse: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 392/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0489\n",
      "Epoch 392: val_loss did not improve from 0.00185\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - rmse: 0.0469 - val_loss: 0.0023 - val_rmse: 0.0479 - lr: 1.0000e-04\n",
      "Epoch 393/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4422e-04 - rmse: 0.0291\n",
      "Epoch 393: val_loss improved from 0.00185 to 0.00180, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0024 - rmse: 0.0488 - val_loss: 0.0018 - val_rmse: 0.0424 - lr: 1.0000e-04\n",
      "Epoch 394/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0585\n",
      "Epoch 394: val_loss improved from 0.00180 to 0.00179, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0023 - rmse: 0.0484 - val_loss: 0.0018 - val_rmse: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 395/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0381\n",
      "Epoch 395: val_loss did not improve from 0.00179\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0024 - rmse: 0.0486 - val_loss: 0.0022 - val_rmse: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 396/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0032e-04 - rmse: 0.0245\n",
      "Epoch 396: val_loss did not improve from 0.00179\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - rmse: 0.0474 - val_loss: 0.0019 - val_rmse: 0.0434 - lr: 1.0000e-04\n",
      "Epoch 397/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0572\n",
      "Epoch 397: val_loss did not improve from 0.00179\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0023 - rmse: 0.0482 - val_loss: 0.0018 - val_rmse: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 398/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8689e-04 - rmse: 0.0221\n",
      "Epoch 398: val_loss did not improve from 0.00179\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - rmse: 0.0457 - val_loss: 0.0022 - val_rmse: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 399/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0608\n",
      "Epoch 399: val_loss improved from 0.00179 to 0.00171, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0021 - rmse: 0.0461 - val_loss: 0.0017 - val_rmse: 0.0414 - lr: 1.0000e-04\n",
      "Epoch 400/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9319e-04 - rmse: 0.0171\n",
      "Epoch 400: val_loss did not improve from 0.00171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0020 - rmse: 0.0452 - val_loss: 0.0017 - val_rmse: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 401/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0616\n",
      "Epoch 401: val_loss improved from 0.00171 to 0.00169, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0021 - rmse: 0.0453 - val_loss: 0.0017 - val_rmse: 0.0411 - lr: 1.0000e-04\n",
      "Epoch 402/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0404\n",
      "Epoch 402: val_loss did not improve from 0.00169\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0023 - rmse: 0.0476 - val_loss: 0.0019 - val_rmse: 0.0435 - lr: 1.0000e-04\n",
      "Epoch 403/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0050 - rmse: 0.0705\n",
      "Epoch 403: val_loss did not improve from 0.00169\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - rmse: 0.0461 - val_loss: 0.0018 - val_rmse: 0.0419 - lr: 1.0000e-04\n",
      "Epoch 404/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8589e-04 - rmse: 0.0280\n",
      "Epoch 404: val_loss did not improve from 0.00169\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - rmse: 0.0467 - val_loss: 0.0019 - val_rmse: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 405/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0323\n",
      "Epoch 405: val_loss improved from 0.00169 to 0.00163, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0020 - rmse: 0.0451 - val_loss: 0.0016 - val_rmse: 0.0404 - lr: 1.0000e-04\n",
      "Epoch 406/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0054 - rmse: 0.0734\n",
      "Epoch 406: val_loss improved from 0.00163 to 0.00161, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0021 - rmse: 0.0454 - val_loss: 0.0016 - val_rmse: 0.0401 - lr: 1.0000e-04\n",
      "Epoch 407/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0684\n",
      "Epoch 407: val_loss did not improve from 0.00161\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0020 - rmse: 0.0445 - val_loss: 0.0016 - val_rmse: 0.0404 - lr: 1.0000e-04\n",
      "Epoch 408/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0455\n",
      "Epoch 408: val_loss did not improve from 0.00161\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0019 - rmse: 0.0435 - val_loss: 0.0018 - val_rmse: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 409/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0472\n",
      "Epoch 409: val_loss did not improve from 0.00161\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0020 - rmse: 0.0444 - val_loss: 0.0018 - val_rmse: 0.0428 - lr: 1.0000e-04\n",
      "Epoch 410/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0661\n",
      "Epoch 410: val_loss did not improve from 0.00161\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0019 - rmse: 0.0436 - val_loss: 0.0017 - val_rmse: 0.0412 - lr: 1.0000e-04\n",
      "Epoch 411/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0337\n",
      "Epoch 411: val_loss did not improve from 0.00161\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0019 - rmse: 0.0441 - val_loss: 0.0019 - val_rmse: 0.0441 - lr: 1.0000e-04\n",
      "Epoch 412/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0608\n",
      "Epoch 412: val_loss improved from 0.00161 to 0.00157, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0020 - rmse: 0.0445 - val_loss: 0.0016 - val_rmse: 0.0396 - lr: 1.0000e-04\n",
      "Epoch 413/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0317\n",
      "Epoch 413: val_loss improved from 0.00157 to 0.00156, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0019 - rmse: 0.0439 - val_loss: 0.0016 - val_rmse: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 414/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0348\n",
      "Epoch 414: val_loss did not improve from 0.00156\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - rmse: 0.0472 - val_loss: 0.0022 - val_rmse: 0.0472 - lr: 1.0000e-04\n",
      "Epoch 415/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0343\n",
      "Epoch 415: val_loss did not improve from 0.00156\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0025 - rmse: 0.0503 - val_loss: 0.0025 - val_rmse: 0.0496 - lr: 1.0000e-04\n",
      "Epoch 416/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0440\n",
      "Epoch 416: val_loss did not improve from 0.00156\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0026 - rmse: 0.0514 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 417/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0356\n",
      "Epoch 417: val_loss improved from 0.00156 to 0.00139, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0022 - rmse: 0.0465 - val_loss: 0.0014 - val_rmse: 0.0373 - lr: 1.0000e-04\n",
      "Epoch 418/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7695e-04 - rmse: 0.0194\n",
      "Epoch 418: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0018 - rmse: 0.0427 - val_loss: 0.0015 - val_rmse: 0.0391 - lr: 1.0000e-04\n",
      "Epoch 419/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0599\n",
      "Epoch 419: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0024 - rmse: 0.0485 - val_loss: 0.0018 - val_rmse: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 420/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8409e-04 - rmse: 0.0297\n",
      "Epoch 420: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - rmse: 0.0465 - val_loss: 0.0017 - val_rmse: 0.0410 - lr: 1.0000e-04\n",
      "Epoch 421/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0154e-04 - rmse: 0.0300\n",
      "Epoch 421: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - rmse: 0.0463 - val_loss: 0.0015 - val_rmse: 0.0382 - lr: 1.0000e-04\n",
      "Epoch 422/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6950e-04 - rmse: 0.0259\n",
      "Epoch 422: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - rmse: 0.0455 - val_loss: 0.0017 - val_rmse: 0.0415 - lr: 1.0000e-04\n",
      "Epoch 423/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0334\n",
      "Epoch 423: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0024 - rmse: 0.0491 - val_loss: 0.0035 - val_rmse: 0.0590 - lr: 1.0000e-04\n",
      "Epoch 424/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0495\n",
      "Epoch 424: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0025 - rmse: 0.0500 - val_loss: 0.0016 - val_rmse: 0.0398 - lr: 1.0000e-04\n",
      "Epoch 425/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8784e-04 - rmse: 0.0197\n",
      "Epoch 425: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - rmse: 0.0462 - val_loss: 0.0015 - val_rmse: 0.0386 - lr: 1.0000e-04\n",
      "Epoch 426/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0461\n",
      "Epoch 426: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0023 - rmse: 0.0480 - val_loss: 0.0025 - val_rmse: 0.0495 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0421\n",
      "Epoch 427: val_loss did not improve from 0.00139\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - rmse: 0.0469 - val_loss: 0.0014 - val_rmse: 0.0376 - lr: 1.0000e-04\n",
      "Epoch 428/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0346\n",
      "Epoch 428: val_loss improved from 0.00139 to 0.00128, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0013 - val_rmse: 0.0357 - lr: 1.0000e-04\n",
      "Epoch 429/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0041 - rmse: 0.0642\n",
      "Epoch 429: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - rmse: 0.0525 - val_loss: 0.0029 - val_rmse: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 430/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0527\n",
      "Epoch 430: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - rmse: 0.0515 - val_loss: 0.0020 - val_rmse: 0.0450 - lr: 1.0000e-04\n",
      "Epoch 431/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0498\n",
      "Epoch 431: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0024 - rmse: 0.0489 - val_loss: 0.0016 - val_rmse: 0.0399 - lr: 1.0000e-04\n",
      "Epoch 432/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2024e-04 - rmse: 0.0303\n",
      "Epoch 432: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0017 - rmse: 0.0407 - val_loss: 0.0013 - val_rmse: 0.0360 - lr: 1.0000e-04\n",
      "Epoch 433/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0373\n",
      "Epoch 433: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0024 - rmse: 0.0489 - val_loss: 0.0018 - val_rmse: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 434/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7862e-04 - rmse: 0.0296\n",
      "Epoch 434: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0018 - rmse: 0.0422 - val_loss: 0.0015 - val_rmse: 0.0385 - lr: 1.0000e-04\n",
      "Epoch 435/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0353\n",
      "Epoch 435: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0020 - rmse: 0.0443 - val_loss: 0.0013 - val_rmse: 0.0363 - lr: 1.0000e-04\n",
      "Epoch 436/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0474\n",
      "Epoch 436: val_loss did not improve from 0.00128\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0014 - rmse: 0.0374 - val_loss: 0.0015 - val_rmse: 0.0391 - lr: 1.0000e-04\n",
      "Epoch 437/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7376e-04 - rmse: 0.0296\n",
      "Epoch 437: val_loss improved from 0.00128 to 0.00121, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0017 - rmse: 0.0411 - val_loss: 0.0012 - val_rmse: 0.0349 - lr: 1.0000e-04\n",
      "Epoch 438/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5170e-04 - rmse: 0.0308\n",
      "Epoch 438: val_loss improved from 0.00121 to 0.00121, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0014 - rmse: 0.0379 - val_loss: 0.0012 - val_rmse: 0.0348 - lr: 1.0000e-04\n",
      "Epoch 439/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0491\n",
      "Epoch 439: val_loss improved from 0.00121 to 0.00116, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0016 - rmse: 0.0398 - val_loss: 0.0012 - val_rmse: 0.0341 - lr: 1.0000e-04\n",
      "Epoch 440/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9225e-04 - rmse: 0.0299\n",
      "Epoch 440: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0015 - rmse: 0.0388 - val_loss: 0.0014 - val_rmse: 0.0375 - lr: 1.0000e-04\n",
      "Epoch 441/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0514\n",
      "Epoch 441: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0018 - rmse: 0.0420 - val_loss: 0.0012 - val_rmse: 0.0349 - lr: 1.0000e-04\n",
      "Epoch 442/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0374\n",
      "Epoch 442: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0015 - rmse: 0.0382 - val_loss: 0.0014 - val_rmse: 0.0373 - lr: 1.0000e-04\n",
      "Epoch 443/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0400\n",
      "Epoch 443: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0016 - rmse: 0.0406 - val_loss: 0.0012 - val_rmse: 0.0343 - lr: 1.0000e-04\n",
      "Epoch 444/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0402\n",
      "Epoch 444: val_loss improved from 0.00116 to 0.00113, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0015 - rmse: 0.0389 - val_loss: 0.0011 - val_rmse: 0.0336 - lr: 1.0000e-04\n",
      "Epoch 445/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7677e-04 - rmse: 0.0166\n",
      "Epoch 445: val_loss improved from 0.00113 to 0.00109, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0014 - rmse: 0.0375 - val_loss: 0.0011 - val_rmse: 0.0330 - lr: 1.0000e-04\n",
      "Epoch 446/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4983e-04 - rmse: 0.0212\n",
      "Epoch 446: val_loss did not improve from 0.00109\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0014 - rmse: 0.0368 - val_loss: 0.0011 - val_rmse: 0.0331 - lr: 1.0000e-04\n",
      "Epoch 447/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0933e-04 - rmse: 0.0226\n",
      "Epoch 447: val_loss improved from 0.00109 to 0.00102, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0013 - rmse: 0.0361 - val_loss: 0.0010 - val_rmse: 0.0320 - lr: 1.0000e-04\n",
      "Epoch 448/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6239e-04 - rmse: 0.0127\n",
      "Epoch 448: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0014 - rmse: 0.0380 - val_loss: 0.0014 - val_rmse: 0.0369 - lr: 1.0000e-04\n",
      "Epoch 449/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0667\n",
      "Epoch 449: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0014 - rmse: 0.0368 - val_loss: 0.0011 - val_rmse: 0.0335 - lr: 1.0000e-04\n",
      "Epoch 450/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0342\n",
      "Epoch 450: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0015 - rmse: 0.0383 - val_loss: 0.0016 - val_rmse: 0.0394 - lr: 1.0000e-04\n",
      "Epoch 451/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0342\n",
      "Epoch 451: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0016 - rmse: 0.0400 - val_loss: 0.0012 - val_rmse: 0.0350 - lr: 1.0000e-04\n",
      "Epoch 452/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0596\n",
      "Epoch 452: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0017 - rmse: 0.0416 - val_loss: 0.0013 - val_rmse: 0.0361 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6158e-04 - rmse: 0.0237\n",
      "Epoch 453: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0013 - rmse: 0.0366 - val_loss: 0.0011 - val_rmse: 0.0329 - lr: 1.0000e-04\n",
      "Epoch 454/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4794e-04 - rmse: 0.0291\n",
      "Epoch 454: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0013 - rmse: 0.0363 - val_loss: 0.0011 - val_rmse: 0.0331 - lr: 1.0000e-04\n",
      "Epoch 455/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0468\n",
      "Epoch 455: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0013 - rmse: 0.0367 - val_loss: 0.0011 - val_rmse: 0.0328 - lr: 1.0000e-04\n",
      "Epoch 456/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0389\n",
      "Epoch 456: val_loss did not improve from 0.00102\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0013 - rmse: 0.0359 - val_loss: 0.0010 - val_rmse: 0.0321 - lr: 1.0000e-04\n",
      "Epoch 457/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6203e-04 - rmse: 0.0190\n",
      "Epoch 457: val_loss improved from 0.00102 to 0.00093, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0013 - rmse: 0.0355 - val_loss: 9.3366e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 458/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8749e-04 - rmse: 0.0281\n",
      "Epoch 458: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0344 - val_loss: 9.3905e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 459/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1277e-04 - rmse: 0.0146\n",
      "Epoch 459: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0348 - val_loss: 0.0011 - val_rmse: 0.0328 - lr: 1.0000e-04\n",
      "Epoch 460/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3744e-04 - rmse: 0.0209\n",
      "Epoch 460: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0015 - rmse: 0.0391 - val_loss: 0.0012 - val_rmse: 0.0340 - lr: 1.0000e-04\n",
      "Epoch 461/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1392e-04 - rmse: 0.0227\n",
      "Epoch 461: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0016 - rmse: 0.0394 - val_loss: 9.7073e-04 - val_rmse: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 462/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0396\n",
      "Epoch 462: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0342 - val_loss: 0.0011 - val_rmse: 0.0325 - lr: 1.0000e-04\n",
      "Epoch 463/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0481\n",
      "Epoch 463: val_loss improved from 0.00093 to 0.00083, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0013 - rmse: 0.0361 - val_loss: 8.3455e-04 - val_rmse: 0.0289 - lr: 1.0000e-04\n",
      "Epoch 464/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9326e-04 - rmse: 0.0222\n",
      "Epoch 464: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0341 - val_loss: 8.5446e-04 - val_rmse: 0.0292 - lr: 1.0000e-04\n",
      "Epoch 465/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0469\n",
      "Epoch 465: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0346 - val_loss: 8.9143e-04 - val_rmse: 0.0299 - lr: 1.0000e-04\n",
      "Epoch 466/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1344e-04 - rmse: 0.0203\n",
      "Epoch 466: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0345 - val_loss: 9.2216e-04 - val_rmse: 0.0304 - lr: 1.0000e-04\n",
      "Epoch 467/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0462\n",
      "Epoch 467: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0331 - val_loss: 8.5096e-04 - val_rmse: 0.0292 - lr: 1.0000e-04\n",
      "Epoch 468/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8884e-04 - rmse: 0.0243\n",
      "Epoch 468: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0013 - rmse: 0.0355 - val_loss: 0.0010 - val_rmse: 0.0319 - lr: 1.0000e-04\n",
      "Epoch 469/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5385e-04 - rmse: 0.0292\n",
      "Epoch 469: val_loss improved from 0.00083 to 0.00076, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0011 - rmse: 0.0327 - val_loss: 7.6075e-04 - val_rmse: 0.0276 - lr: 1.0000e-04\n",
      "Epoch 470/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1399e-04 - rmse: 0.0203\n",
      "Epoch 470: val_loss improved from 0.00076 to 0.00073, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0012 - rmse: 0.0350 - val_loss: 7.2976e-04 - val_rmse: 0.0270 - lr: 1.0000e-04\n",
      "Epoch 471/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0408\n",
      "Epoch 471: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0010 - rmse: 0.0323 - val_loss: 0.0011 - val_rmse: 0.0337 - lr: 1.0000e-04\n",
      "Epoch 472/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8093e-04 - rmse: 0.0195\n",
      "Epoch 472: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0013 - rmse: 0.0354 - val_loss: 9.4823e-04 - val_rmse: 0.0308 - lr: 1.0000e-04\n",
      "Epoch 473/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0347\n",
      "Epoch 473: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0010 - rmse: 0.0318 - val_loss: 9.7754e-04 - val_rmse: 0.0313 - lr: 1.0000e-04\n",
      "Epoch 474/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4183e-04 - rmse: 0.0233\n",
      "Epoch 474: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0334 - val_loss: 7.3845e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n",
      "Epoch 475/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3907e-04 - rmse: 0.0184\n",
      "Epoch 475: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0327 - val_loss: 8.9335e-04 - val_rmse: 0.0299 - lr: 1.0000e-04\n",
      "Epoch 476/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3786e-04 - rmse: 0.0289\n",
      "Epoch 476: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0015 - rmse: 0.0381 - val_loss: 0.0011 - val_rmse: 0.0337 - lr: 1.0000e-04\n",
      "Epoch 477/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6069e-04 - rmse: 0.0310\n",
      "Epoch 477: val_loss improved from 0.00073 to 0.00065, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0012 - rmse: 0.0347 - val_loss: 6.5081e-04 - val_rmse: 0.0255 - lr: 1.0000e-04\n",
      "Epoch 478/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6046e-04 - rmse: 0.0257\n",
      "Epoch 478: val_loss did not improve from 0.00065\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0012 - rmse: 0.0340 - val_loss: 9.3658e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 479/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0475\n",
      "Epoch 479: val_loss did not improve from 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0333 - val_loss: 7.4219e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n",
      "Epoch 480/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6159e-04 - rmse: 0.0294\n",
      "Epoch 480: val_loss did not improve from 0.00065\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0010 - rmse: 0.0317 - val_loss: 7.3814e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n",
      "Epoch 481/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0324\n",
      "Epoch 481: val_loss did not improve from 0.00065\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8312e-04 - rmse: 0.0314 - val_loss: 7.4348e-04 - val_rmse: 0.0273 - lr: 1.0000e-04\n",
      "Epoch 482/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0422\n",
      "Epoch 482: val_loss improved from 0.00065 to 0.00064, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 9.7496e-04 - rmse: 0.0312 - val_loss: 6.4438e-04 - val_rmse: 0.0254 - lr: 1.0000e-04\n",
      "Epoch 483/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0355\n",
      "Epoch 483: val_loss did not improve from 0.00064\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8322e-04 - rmse: 0.0314 - val_loss: 7.3866e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n",
      "Epoch 484/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1895e-04 - rmse: 0.0249\n",
      "Epoch 484: val_loss did not improve from 0.00064\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.6253e-04 - rmse: 0.0310 - val_loss: 8.5577e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 485/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0393\n",
      "Epoch 485: val_loss did not improve from 0.00064\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0017 - rmse: 0.0411 - val_loss: 0.0015 - val_rmse: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 486/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0379\n",
      "Epoch 486: val_loss did not improve from 0.00064\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0016 - rmse: 0.0398 - val_loss: 9.3076e-04 - val_rmse: 0.0305 - lr: 1.0000e-04\n",
      "Epoch 487/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0449\n",
      "Epoch 487: val_loss improved from 0.00064 to 0.00057, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0012 - rmse: 0.0343 - val_loss: 5.6901e-04 - val_rmse: 0.0239 - lr: 1.0000e-04\n",
      "Epoch 488/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2710e-04 - rmse: 0.0181\n",
      "Epoch 488: val_loss did not improve from 0.00057\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0329 - val_loss: 7.5241e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 489/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9158e-04 - rmse: 0.0222\n",
      "Epoch 489: val_loss did not improve from 0.00057\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0010 - rmse: 0.0318 - val_loss: 6.5105e-04 - val_rmse: 0.0255 - lr: 1.0000e-04\n",
      "Epoch 490/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7022e-04 - rmse: 0.0130\n",
      "Epoch 490: val_loss did not improve from 0.00057\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0340 - val_loss: 6.4545e-04 - val_rmse: 0.0254 - lr: 1.0000e-04\n",
      "Epoch 491/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8171e-04 - rmse: 0.0297\n",
      "Epoch 491: val_loss did not improve from 0.00057\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.5141e-04 - rmse: 0.0308 - val_loss: 9.3669e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 492/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7980e-04 - rmse: 0.0195\n",
      "Epoch 492: val_loss did not improve from 0.00057\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.5666e-04 - rmse: 0.0309 - val_loss: 5.7363e-04 - val_rmse: 0.0240 - lr: 1.0000e-04\n",
      "Epoch 493/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0343\n",
      "Epoch 493: val_loss did not improve from 0.00057\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0331 - val_loss: 9.3785e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 494/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0396\n",
      "Epoch 494: val_loss did not improve from 0.00057\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0012 - val_rmse: 0.0344 - lr: 1.0000e-04\n",
      "Epoch 495/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0426\n",
      "Epoch 495: val_loss improved from 0.00057 to 0.00056, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0010 - rmse: 0.0318 - val_loss: 5.5807e-04 - val_rmse: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 496/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0389\n",
      "Epoch 496: val_loss did not improve from 0.00056\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.8729e-04 - rmse: 0.0298 - val_loss: 6.7725e-04 - val_rmse: 0.0260 - lr: 1.0000e-04\n",
      "Epoch 497/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5595e-04 - rmse: 0.0293\n",
      "Epoch 497: val_loss did not improve from 0.00056\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.2553e-04 - rmse: 0.0304 - val_loss: 8.1438e-04 - val_rmse: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 498/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9952e-04 - rmse: 0.0173\n",
      "Epoch 498: val_loss did not improve from 0.00056\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0216e-04 - rmse: 0.0283 - val_loss: 5.9989e-04 - val_rmse: 0.0245 - lr: 1.0000e-04\n",
      "Epoch 499/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6193e-04 - rmse: 0.0127\n",
      "Epoch 499: val_loss did not improve from 0.00056\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9833e-04 - rmse: 0.0300 - val_loss: 7.1218e-04 - val_rmse: 0.0267 - lr: 1.0000e-04\n",
      "Epoch 500/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5954e-04 - rmse: 0.0237\n",
      "Epoch 500: val_loss did not improve from 0.00056\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0010 - rmse: 0.0318 - val_loss: 6.2660e-04 - val_rmse: 0.0250 - lr: 1.0000e-04\n",
      "Epoch 501/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0418\n",
      "Epoch 501: val_loss improved from 0.00056 to 0.00054, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.5412e-04 - rmse: 0.0275 - val_loss: 5.4213e-04 - val_rmse: 0.0233 - lr: 1.0000e-04\n",
      "Epoch 502/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8213e-04 - rmse: 0.0241\n",
      "Epoch 502: val_loss improved from 0.00054 to 0.00049, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 7.4414e-04 - rmse: 0.0273 - val_loss: 4.9491e-04 - val_rmse: 0.0222 - lr: 1.0000e-04\n",
      "Epoch 503/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0363\n",
      "Epoch 503: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1557e-04 - rmse: 0.0268 - val_loss: 5.3122e-04 - val_rmse: 0.0230 - lr: 1.0000e-04\n",
      "Epoch 504/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0356\n",
      "Epoch 504: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3452e-04 - rmse: 0.0289 - val_loss: 7.0144e-04 - val_rmse: 0.0265 - lr: 1.0000e-04\n",
      "Epoch 505/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1724e-04 - rmse: 0.0178\n",
      "Epoch 505: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6747e-04 - rmse: 0.0277 - val_loss: 5.0637e-04 - val_rmse: 0.0225 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 506/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7895e-04 - rmse: 0.0167\n",
      "Epoch 506: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7370e-04 - rmse: 0.0278 - val_loss: 6.0483e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 507/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0377\n",
      "Epoch 507: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.9967e-04 - rmse: 0.0283 - val_loss: 5.2733e-04 - val_rmse: 0.0230 - lr: 1.0000e-04\n",
      "Epoch 508/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0340\n",
      "Epoch 508: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8598e-04 - rmse: 0.0314 - val_loss: 0.0012 - val_rmse: 0.0352 - lr: 1.0000e-04\n",
      "Epoch 509/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0448\n",
      "Epoch 509: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0337 - val_loss: 6.2239e-04 - val_rmse: 0.0249 - lr: 1.0000e-04\n",
      "Epoch 510/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1909e-04 - rmse: 0.0228\n",
      "Epoch 510: val_loss improved from 0.00049 to 0.00047, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 8.2077e-04 - rmse: 0.0286 - val_loss: 4.6903e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 511/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1817e-04 - rmse: 0.0178\n",
      "Epoch 511: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7016e-04 - rmse: 0.0278 - val_loss: 7.1025e-04 - val_rmse: 0.0267 - lr: 1.0000e-04\n",
      "Epoch 512/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4797e-04 - rmse: 0.0234\n",
      "Epoch 512: val_loss improved from 0.00047 to 0.00043, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 8.5641e-04 - rmse: 0.0293 - val_loss: 4.2998e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 513/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3848e-04 - rmse: 0.0154\n",
      "Epoch 513: val_loss did not improve from 0.00043\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3773e-04 - rmse: 0.0289 - val_loss: 6.7826e-04 - val_rmse: 0.0260 - lr: 1.0000e-04\n",
      "Epoch 514/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0330e-04 - rmse: 0.0265\n",
      "Epoch 514: val_loss did not improve from 0.00043\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8004e-04 - rmse: 0.0297 - val_loss: 5.7274e-04 - val_rmse: 0.0239 - lr: 1.0000e-04\n",
      "Epoch 515/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4636e-04 - rmse: 0.0186\n",
      "Epoch 515: val_loss improved from 0.00043 to 0.00042, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 7.1201e-04 - rmse: 0.0267 - val_loss: 4.1747e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 516/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0591e-04 - rmse: 0.0201\n",
      "Epoch 516: val_loss did not improve from 0.00042\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1237e-04 - rmse: 0.0247 - val_loss: 4.3862e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 517/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9109e-04 - rmse: 0.0198\n",
      "Epoch 517: val_loss did not improve from 0.00042\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2189e-04 - rmse: 0.0269 - val_loss: 8.7208e-04 - val_rmse: 0.0295 - lr: 1.0000e-04\n",
      "Epoch 518/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7850e-04 - rmse: 0.0313\n",
      "Epoch 518: val_loss improved from 0.00042 to 0.00041, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 7.3171e-04 - rmse: 0.0271 - val_loss: 4.1369e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 519/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0329\n",
      "Epoch 519: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3092e-04 - rmse: 0.0251 - val_loss: 4.9681e-04 - val_rmse: 0.0223 - lr: 1.0000e-04\n",
      "Epoch 520/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0939e-04 - rmse: 0.0284\n",
      "Epoch 520: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.6471e-04 - rmse: 0.0294 - val_loss: 7.0905e-04 - val_rmse: 0.0266 - lr: 1.0000e-04\n",
      "Epoch 521/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7011e-04 - rmse: 0.0239\n",
      "Epoch 521: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.6628e-04 - rmse: 0.0277 - val_loss: 4.6698e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 522/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9039e-04 - rmse: 0.0198\n",
      "Epoch 522: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4135e-04 - rmse: 0.0272 - val_loss: 7.6531e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 523/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3728e-04 - rmse: 0.0272\n",
      "Epoch 523: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0338 - val_loss: 6.7270e-04 - val_rmse: 0.0259 - lr: 1.0000e-04\n",
      "Epoch 524/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6332e-04 - rmse: 0.0237\n",
      "Epoch 524: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0334 - val_loss: 4.2806e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 525/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0717e-04 - rmse: 0.0284\n",
      "Epoch 525: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7149e-04 - rmse: 0.0278 - val_loss: 0.0011 - val_rmse: 0.0334 - lr: 1.0000e-04\n",
      "Epoch 526/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0354\n",
      "Epoch 526: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0010 - rmse: 0.0318 - val_loss: 4.3287e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 527/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7253e-04 - rmse: 0.0131\n",
      "Epoch 527: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5957e-04 - rmse: 0.0310 - val_loss: 7.0947e-04 - val_rmse: 0.0266 - lr: 1.0000e-04\n",
      "Epoch 528/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7743e-04 - rmse: 0.0279\n",
      "Epoch 528: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8413e-04 - rmse: 0.0297 - val_loss: 6.2369e-04 - val_rmse: 0.0250 - lr: 1.0000e-04\n",
      "Epoch 529/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6650e-04 - rmse: 0.0216\n",
      "Epoch 529: val_loss did not improve from 0.00041\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1035e-04 - rmse: 0.0302 - val_loss: 9.7154e-04 - val_rmse: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 530/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0383\n",
      "Epoch 530: val_loss improved from 0.00041 to 0.00040, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 8.4792e-04 - rmse: 0.0291 - val_loss: 3.9926e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 531/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1510e-04 - rmse: 0.0147\n",
      "Epoch 531: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6465e-04 - rmse: 0.0258 - val_loss: 5.8661e-04 - val_rmse: 0.0242 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4054e-04 - rmse: 0.0253\n",
      "Epoch 532: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7611e-04 - rmse: 0.0260 - val_loss: 6.0704e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 533/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6039e-04 - rmse: 0.0215\n",
      "Epoch 533: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9692e-04 - rmse: 0.0244 - val_loss: 4.0317e-04 - val_rmse: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 534/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7811e-04 - rmse: 0.0167\n",
      "Epoch 534: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8211e-04 - rmse: 0.0241 - val_loss: 4.3494e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 535/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1173e-04 - rmse: 0.0146\n",
      "Epoch 535: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4135e-04 - rmse: 0.0233 - val_loss: 4.2631e-04 - val_rmse: 0.0206 - lr: 1.0000e-04\n",
      "Epoch 536/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9384e-04 - rmse: 0.0171\n",
      "Epoch 536: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3208e-04 - rmse: 0.0231 - val_loss: 4.3736e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 537/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3957e-04 - rmse: 0.0210\n",
      "Epoch 537: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5194e-04 - rmse: 0.0235 - val_loss: 4.4705e-04 - val_rmse: 0.0211 - lr: 1.0000e-04\n",
      "Epoch 538/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6020e-04 - rmse: 0.0161\n",
      "Epoch 538: val_loss improved from 0.00040 to 0.00033, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 5.5326e-04 - rmse: 0.0235 - val_loss: 3.2950e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 539/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8682e-04 - rmse: 0.0298\n",
      "Epoch 539: val_loss did not improve from 0.00033\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9379e-04 - rmse: 0.0222 - val_loss: 3.6232e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 540/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5265e-04 - rmse: 0.0159\n",
      "Epoch 540: val_loss did not improve from 0.00033\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4938e-04 - rmse: 0.0212 - val_loss: 3.5945e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 541/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0367\n",
      "Epoch 541: val_loss improved from 0.00033 to 0.00033, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.5758e-04 - rmse: 0.0214 - val_loss: 3.2942e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 542/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1047e-04 - rmse: 0.0302\n",
      "Epoch 542: val_loss did not improve from 0.00033\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.5516e-04 - rmse: 0.0256 - val_loss: 5.9591e-04 - val_rmse: 0.0244 - lr: 1.0000e-04\n",
      "Epoch 543/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0391\n",
      "Epoch 543: val_loss improved from 0.00033 to 0.00030, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 6.3913e-04 - rmse: 0.0253 - val_loss: 2.9717e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 544/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5303e-04 - rmse: 0.0159\n",
      "Epoch 544: val_loss did not improve from 0.00030\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5197e-04 - rmse: 0.0213 - val_loss: 5.5824e-04 - val_rmse: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 545/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3181e-04 - rmse: 0.0271\n",
      "Epoch 545: val_loss did not improve from 0.00030\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2447e-04 - rmse: 0.0229 - val_loss: 3.5071e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 546/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2864e-04 - rmse: 0.0270\n",
      "Epoch 546: val_loss did not improve from 0.00030\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7741e-04 - rmse: 0.0260 - val_loss: 4.9529e-04 - val_rmse: 0.0223 - lr: 1.0000e-04\n",
      "Epoch 547/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9198e-04 - rmse: 0.0281\n",
      "Epoch 547: val_loss did not improve from 0.00030\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0545e-04 - rmse: 0.0266 - val_loss: 5.8257e-04 - val_rmse: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 548/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0043e-04 - rmse: 0.0142\n",
      "Epoch 548: val_loss did not improve from 0.00030\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5541e-04 - rmse: 0.0213 - val_loss: 3.9571e-04 - val_rmse: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 549/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3744e-04 - rmse: 0.0289\n",
      "Epoch 549: val_loss did not improve from 0.00030\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9433e-04 - rmse: 0.0222 - val_loss: 4.3813e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 550/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1491e-04 - rmse: 0.0177\n",
      "Epoch 550: val_loss did not improve from 0.00030\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7949e-04 - rmse: 0.0219 - val_loss: 3.0954e-04 - val_rmse: 0.0176 - lr: 1.0000e-04\n",
      "Epoch 551/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4751e-04 - rmse: 0.0234\n",
      "Epoch 551: val_loss improved from 0.00030 to 0.00027, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 4.0607e-04 - rmse: 0.0202 - val_loss: 2.6676e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 552/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5147e-04 - rmse: 0.0159\n",
      "Epoch 552: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8830e-04 - rmse: 0.0221 - val_loss: 4.9903e-04 - val_rmse: 0.0223 - lr: 1.0000e-04\n",
      "Epoch 553/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7506e-04 - rmse: 0.0312\n",
      "Epoch 553: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7846e-04 - rmse: 0.0219 - val_loss: 3.5900e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 554/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3058e-04 - rmse: 0.0152\n",
      "Epoch 554: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2151e-04 - rmse: 0.0205 - val_loss: 3.4402e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 555/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3112e-04 - rmse: 0.0152\n",
      "Epoch 555: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3299e-04 - rmse: 0.0231 - val_loss: 7.6834e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 556/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7526e-04 - rmse: 0.0312\n",
      "Epoch 556: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9388e-04 - rmse: 0.0244 - val_loss: 4.2798e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 557/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3967e-04 - rmse: 0.0118\n",
      "Epoch 557: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3116e-04 - rmse: 0.0251 - val_loss: 5.6572e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 558/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0954e-04 - rmse: 0.0247\n",
      "Epoch 558: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1561e-04 - rmse: 0.0268 - val_loss: 5.9963e-04 - val_rmse: 0.0245 - lr: 1.0000e-04\n",
      "Epoch 559/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7029e-04 - rmse: 0.0192\n",
      "Epoch 559: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0366e-04 - rmse: 0.0246 - val_loss: 3.0311e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 560/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7185e-04 - rmse: 0.0131\n",
      "Epoch 560: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.1703e-04 - rmse: 0.0227 - val_loss: 8.5566e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 561/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0460\n",
      "Epoch 561: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6234e-04 - rmse: 0.0276 - val_loss: 5.0702e-04 - val_rmse: 0.0225 - lr: 1.0000e-04\n",
      "Epoch 562/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3693e-04 - rmse: 0.0184\n",
      "Epoch 562: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4658e-04 - rmse: 0.0234 - val_loss: 0.0010 - val_rmse: 0.0323 - lr: 1.0000e-04\n",
      "Epoch 563/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7742e-04 - rmse: 0.0279\n",
      "Epoch 563: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9881e-04 - rmse: 0.0316 - val_loss: 4.5059e-04 - val_rmse: 0.0212 - lr: 1.0000e-04\n",
      "Epoch 564/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8808e-04 - rmse: 0.0170\n",
      "Epoch 564: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4334e-04 - rmse: 0.0273 - val_loss: 6.6960e-04 - val_rmse: 0.0259 - lr: 1.0000e-04\n",
      "Epoch 565/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0398\n",
      "Epoch 565: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.6556e-04 - rmse: 0.0311 - val_loss: 2.6819e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 566/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4624e-04 - rmse: 0.0186\n",
      "Epoch 566: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9905e-04 - rmse: 0.0223 - val_loss: 5.8226e-04 - val_rmse: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 567/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9366e-04 - rmse: 0.0198\n",
      "Epoch 567: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0031e-04 - rmse: 0.0224 - val_loss: 3.9182e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 568/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2439e-04 - rmse: 0.0180\n",
      "Epoch 568: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4722e-04 - rmse: 0.0254 - val_loss: 5.8348e-04 - val_rmse: 0.0242 - lr: 1.0000e-04\n",
      "Epoch 569/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3259e-04 - rmse: 0.0231\n",
      "Epoch 569: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3830e-04 - rmse: 0.0232 - val_loss: 6.3785e-04 - val_rmse: 0.0253 - lr: 1.0000e-04\n",
      "Epoch 570/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8690e-04 - rmse: 0.0197\n",
      "Epoch 570: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2207e-04 - rmse: 0.0228 - val_loss: 2.8838e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 571/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1195e-04 - rmse: 0.0247\n",
      "Epoch 571: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8427e-04 - rmse: 0.0196 - val_loss: 2.9201e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 572/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5810e-04 - rmse: 0.0236\n",
      "Epoch 572: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1241e-04 - rmse: 0.0177 - val_loss: 2.9702e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 573/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7558e-04 - rmse: 0.0133\n",
      "Epoch 573: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1062e-04 - rmse: 0.0176 - val_loss: 2.7591e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 574/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2357e-04 - rmse: 0.0111\n",
      "Epoch 574: val_loss improved from 0.00027 to 0.00024, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.5460e-04 - rmse: 0.0188 - val_loss: 2.3872e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 575/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8583e-05 - rmse: 0.0077\n",
      "Epoch 575: val_loss did not improve from 0.00024\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3137e-04 - rmse: 0.0182 - val_loss: 3.1183e-04 - val_rmse: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 576/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6034e-04 - rmse: 0.0127\n",
      "Epoch 576: val_loss improved from 0.00024 to 0.00023, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 3.2161e-04 - rmse: 0.0179 - val_loss: 2.2649e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 577/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8833e-05 - rmse: 0.0099\n",
      "Epoch 577: val_loss did not improve from 0.00023\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8258e-04 - rmse: 0.0196 - val_loss: 5.3907e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 578/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2241e-04 - rmse: 0.0229\n",
      "Epoch 578: val_loss did not improve from 0.00023\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9491e-04 - rmse: 0.0199 - val_loss: 2.6064e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 579/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3117e-04 - rmse: 0.0251\n",
      "Epoch 579: val_loss did not improve from 0.00023\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1119e-04 - rmse: 0.0176 - val_loss: 2.3906e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 580/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9231e-04 - rmse: 0.0139\n",
      "Epoch 580: val_loss did not improve from 0.00023\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.0468e-04 - rmse: 0.0175 - val_loss: 2.5769e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 581/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5726e-04 - rmse: 0.0275\n",
      "Epoch 581: val_loss improved from 0.00023 to 0.00022, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 2.7010e-04 - rmse: 0.0164 - val_loss: 2.2129e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 582/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4371e-05 - rmse: 0.0092\n",
      "Epoch 582: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3499e-04 - rmse: 0.0183 - val_loss: 2.8579e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 583/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3465e-04 - rmse: 0.0153\n",
      "Epoch 583: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5872e-04 - rmse: 0.0189 - val_loss: 3.0878e-04 - val_rmse: 0.0176 - lr: 1.0000e-04\n",
      "Epoch 584/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0606e-04 - rmse: 0.0175\n",
      "Epoch 584: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9993e-04 - rmse: 0.0200 - val_loss: 6.8303e-04 - val_rmse: 0.0261 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 585/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6626e-04 - rmse: 0.0191\n",
      "Epoch 585: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4052e-04 - rmse: 0.0210 - val_loss: 2.5355e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 586/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9666e-04 - rmse: 0.0140\n",
      "Epoch 586: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6449e-04 - rmse: 0.0216 - val_loss: 2.7886e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 587/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1709e-04 - rmse: 0.0108\n",
      "Epoch 587: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5777e-04 - rmse: 0.0189 - val_loss: 2.8667e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 588/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1111e-04 - rmse: 0.0226\n",
      "Epoch 588: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8261e-04 - rmse: 0.0168 - val_loss: 3.4562e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 589/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4605e-04 - rmse: 0.0121\n",
      "Epoch 589: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3470e-04 - rmse: 0.0183 - val_loss: 2.5050e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 590/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5033e-04 - rmse: 0.0123\n",
      "Epoch 590: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3419e-04 - rmse: 0.0183 - val_loss: 2.3899e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 591/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0584e-04 - rmse: 0.0225\n",
      "Epoch 591: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5802e-04 - rmse: 0.0161 - val_loss: 3.1141e-04 - val_rmse: 0.0176 - lr: 1.0000e-04\n",
      "Epoch 592/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3403e-04 - rmse: 0.0183\n",
      "Epoch 592: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4926e-04 - rmse: 0.0212 - val_loss: 3.3403e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 593/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0432e-04 - rmse: 0.0143\n",
      "Epoch 593: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4916e-04 - rmse: 0.0212 - val_loss: 2.6157e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 594/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7103e-04 - rmse: 0.0165\n",
      "Epoch 594: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9416e-04 - rmse: 0.0172 - val_loss: 3.2339e-04 - val_rmse: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 595/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1852e-05 - rmse: 0.0085\n",
      "Epoch 595: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3159e-04 - rmse: 0.0182 - val_loss: 3.4842e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 596/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3489e-04 - rmse: 0.0209\n",
      "Epoch 596: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6204e-04 - rmse: 0.0190 - val_loss: 2.4513e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 597/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0199e-04 - rmse: 0.0224\n",
      "Epoch 597: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3871e-04 - rmse: 0.0184 - val_loss: 2.7816e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 598/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1783e-04 - rmse: 0.0148\n",
      "Epoch 598: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6842e-04 - rmse: 0.0164 - val_loss: 2.3223e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 599/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9030e-04 - rmse: 0.0198\n",
      "Epoch 599: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.8490e-04 - rmse: 0.0169 - val_loss: 2.5587e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 600/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0008e-04 - rmse: 0.0173\n",
      "Epoch 600: val_loss improved from 0.00022 to 0.00022, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 2.4268e-04 - rmse: 0.0156 - val_loss: 2.1753e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 601/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8240e-04 - rmse: 0.0135\n",
      "Epoch 601: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3120e-04 - rmse: 0.0152 - val_loss: 2.2782e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 602/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4433e-04 - rmse: 0.0120\n",
      "Epoch 602: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4376e-04 - rmse: 0.0156 - val_loss: 2.3555e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 603/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5609e-04 - rmse: 0.0125\n",
      "Epoch 603: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5501e-04 - rmse: 0.0160 - val_loss: 2.2994e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 604/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4623e-04 - rmse: 0.0121\n",
      "Epoch 604: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3944e-04 - rmse: 0.0155 - val_loss: 2.5385e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 605/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4435e-04 - rmse: 0.0120\n",
      "Epoch 605: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1584e-04 - rmse: 0.0147 - val_loss: 2.1852e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 606/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6457e-05 - rmse: 0.0075\n",
      "Epoch 606: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5864e-04 - rmse: 0.0161 - val_loss: 2.7047e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 607/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7630e-04 - rmse: 0.0218\n",
      "Epoch 607: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5103e-04 - rmse: 0.0187 - val_loss: 2.6630e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 608/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1248e-04 - rmse: 0.0203\n",
      "Epoch 608: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7013e-04 - rmse: 0.0164 - val_loss: 3.5430e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 609/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9558e-04 - rmse: 0.0172\n",
      "Epoch 609: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1853e-04 - rmse: 0.0178 - val_loss: 2.6746e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 610/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4083e-04 - rmse: 0.0119\n",
      "Epoch 610: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1905e-04 - rmse: 0.0148 - val_loss: 2.6402e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 611/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2740e-04 - rmse: 0.0113\n",
      "Epoch 611: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4910e-04 - rmse: 0.0158 - val_loss: 3.1282e-04 - val_rmse: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 612/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4741e-04 - rmse: 0.0186\n",
      "Epoch 612: val_loss improved from 0.00022 to 0.00019, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 32ms/step - loss: 3.1312e-04 - rmse: 0.0177 - val_loss: 1.8957e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 613/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3058e-05 - rmse: 0.0091\n",
      "Epoch 613: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.7172e-04 - rmse: 0.0165 - val_loss: 2.5779e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 614/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3427e-04 - rmse: 0.0231\n",
      "Epoch 614: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.8411e-04 - rmse: 0.0196 - val_loss: 4.1260e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 615/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3772e-04 - rmse: 0.0253\n",
      "Epoch 615: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9385e-04 - rmse: 0.0222 - val_loss: 4.7670e-04 - val_rmse: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 616/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7103e-04 - rmse: 0.0193\n",
      "Epoch 616: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9404e-04 - rmse: 0.0199 - val_loss: 5.4166e-04 - val_rmse: 0.0233 - lr: 1.0000e-04\n",
      "Epoch 617/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8307e-04 - rmse: 0.0168\n",
      "Epoch 617: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7012e-04 - rmse: 0.0192 - val_loss: 2.9094e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 618/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8433e-04 - rmse: 0.0169\n",
      "Epoch 618: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4689e-04 - rmse: 0.0186 - val_loss: 4.1550e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 619/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7097e-04 - rmse: 0.0312\n",
      "Epoch 619: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2156e-04 - rmse: 0.0228 - val_loss: 3.8741e-04 - val_rmse: 0.0197 - lr: 1.0000e-04\n",
      "Epoch 620/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6096e-04 - rmse: 0.0190\n",
      "Epoch 620: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5692e-04 - rmse: 0.0189 - val_loss: 2.7918e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 621/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3155e-04 - rmse: 0.0115\n",
      "Epoch 621: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3198e-04 - rmse: 0.0182 - val_loss: 3.0166e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 622/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3230e-04 - rmse: 0.0271\n",
      "Epoch 622: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2153e-04 - rmse: 0.0179 - val_loss: 3.7590e-04 - val_rmse: 0.0194 - lr: 1.0000e-04\n",
      "Epoch 623/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9892e-04 - rmse: 0.0173\n",
      "Epoch 623: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2977e-04 - rmse: 0.0152 - val_loss: 2.2645e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 624/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4301e-04 - rmse: 0.0156\n",
      "Epoch 624: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6861e-04 - rmse: 0.0164 - val_loss: 3.5588e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 625/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2735e-04 - rmse: 0.0151\n",
      "Epoch 625: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1658e-04 - rmse: 0.0178 - val_loss: 1.9716e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 626/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9116e-04 - rmse: 0.0138\n",
      "Epoch 626: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7392e-04 - rmse: 0.0166 - val_loss: 3.3332e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 627/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1966e-04 - rmse: 0.0205\n",
      "Epoch 627: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2247e-04 - rmse: 0.0180 - val_loss: 2.9926e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 628/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2364e-04 - rmse: 0.0111\n",
      "Epoch 628: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7743e-04 - rmse: 0.0167 - val_loss: 3.4673e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 629/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1128e-04 - rmse: 0.0203\n",
      "Epoch 629: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9419e-04 - rmse: 0.0172 - val_loss: 4.6553e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 630/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4945e-04 - rmse: 0.0212\n",
      "Epoch 630: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1501e-04 - rmse: 0.0177 - val_loss: 2.4899e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 631/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7980e-04 - rmse: 0.0279\n",
      "Epoch 631: val_loss did not improve from 0.00019\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7561e-04 - rmse: 0.0166 - val_loss: 2.9928e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 632/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7978e-05 - rmse: 0.0094\n",
      "Epoch 632: val_loss improved from 0.00019 to 0.00018, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 2.4692e-04 - rmse: 0.0157 - val_loss: 1.8054e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 633/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0541e-04 - rmse: 0.0103\n",
      "Epoch 633: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1799e-04 - rmse: 0.0148 - val_loss: 2.5395e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 634/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7448e-04 - rmse: 0.0166\n",
      "Epoch 634: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5378e-04 - rmse: 0.0124 - val_loss: 2.5938e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 635/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2777e-05 - rmse: 0.0091\n",
      "Epoch 635: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.9688e-04 - rmse: 0.0140 - val_loss: 1.9991e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 636/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6718e-04 - rmse: 0.0129\n",
      "Epoch 636: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2735e-04 - rmse: 0.0151 - val_loss: 1.9008e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 637/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0164e-05 - rmse: 0.0095\n",
      "Epoch 637: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.6925e-04 - rmse: 0.0130 - val_loss: 2.4369e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 638/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2420e-05 - rmse: 0.0072\n",
      "Epoch 638: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.8397e-04 - rmse: 0.0136 - val_loss: 1.8549e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 639/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5064e-05 - rmse: 0.0081\n",
      "Epoch 639: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4502e-04 - rmse: 0.0120 - val_loss: 1.8492e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 640/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7968e-04 - rmse: 0.0134\n",
      "Epoch 640: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3732e-04 - rmse: 0.0117 - val_loss: 1.9335e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 641/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0308e-04 - rmse: 0.0102\n",
      "Epoch 641: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4635e-04 - rmse: 0.0121 - val_loss: 2.0221e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 642/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0066e-04 - rmse: 0.0100\n",
      "Epoch 642: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.5344e-04 - rmse: 0.0124 - val_loss: 1.9171e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 643/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3450e-05 - rmse: 0.0091\n",
      "Epoch 643: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3760e-04 - rmse: 0.0117 - val_loss: 2.0591e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 644/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1600e-05 - rmse: 0.0064\n",
      "Epoch 644: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9887e-04 - rmse: 0.0141 - val_loss: 1.8610e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 645/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1238e-04 - rmse: 0.0106\n",
      "Epoch 645: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6993e-04 - rmse: 0.0130 - val_loss: 2.8099e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 646/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9773e-04 - rmse: 0.0141\n",
      "Epoch 646: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.9798e-04 - rmse: 0.0141 - val_loss: 2.5510e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 647/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3907e-04 - rmse: 0.0118\n",
      "Epoch 647: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0808e-04 - rmse: 0.0144 - val_loss: 2.2267e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 648/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8117e-04 - rmse: 0.0168\n",
      "Epoch 648: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8652e-04 - rmse: 0.0137 - val_loss: 2.4246e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 649/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5140e-05 - rmse: 0.0074\n",
      "Epoch 649: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4212e-04 - rmse: 0.0119 - val_loss: 2.0653e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 650/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0689e-04 - rmse: 0.0103\n",
      "Epoch 650: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5786e-04 - rmse: 0.0126 - val_loss: 2.7854e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 651/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0764e-04 - rmse: 0.0175\n",
      "Epoch 651: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8269e-04 - rmse: 0.0135 - val_loss: 1.9287e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 652/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0126e-05 - rmse: 0.0078\n",
      "Epoch 652: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3494e-04 - rmse: 0.0116 - val_loss: 1.9749e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 653/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7572e-04 - rmse: 0.0133\n",
      "Epoch 653: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4542e-04 - rmse: 0.0121 - val_loss: 2.1140e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 654/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3243e-05 - rmse: 0.0097\n",
      "Epoch 654: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3935e-04 - rmse: 0.0118 - val_loss: 1.8265e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 655/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7131e-04 - rmse: 0.0131\n",
      "Epoch 655: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4436e-04 - rmse: 0.0120 - val_loss: 2.2316e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 656/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8797e-04 - rmse: 0.0137\n",
      "Epoch 656: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9435e-04 - rmse: 0.0139 - val_loss: 1.8710e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 657/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5137e-04 - rmse: 0.0123\n",
      "Epoch 657: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2756e-04 - rmse: 0.0113 - val_loss: 1.9766e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 658/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4780e-04 - rmse: 0.0157\n",
      "Epoch 658: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.6531e-04 - rmse: 0.0129 - val_loss: 2.7219e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 659/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5164e-04 - rmse: 0.0123\n",
      "Epoch 659: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5066e-04 - rmse: 0.0123 - val_loss: 2.3742e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 660/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5186e-04 - rmse: 0.0123\n",
      "Epoch 660: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5118e-04 - rmse: 0.0123 - val_loss: 1.8648e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 661/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7800e-04 - rmse: 0.0167\n",
      "Epoch 661: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5900e-04 - rmse: 0.0126 - val_loss: 1.9769e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 662/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7651e-04 - rmse: 0.0133\n",
      "Epoch 662: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3888e-04 - rmse: 0.0118 - val_loss: 2.8824e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 663/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6571e-04 - rmse: 0.0129\n",
      "Epoch 663: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9653e-04 - rmse: 0.0140 - val_loss: 1.8827e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 664/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7482e-05 - rmse: 0.0088\n",
      "Epoch 664: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0432e-04 - rmse: 0.0143 - val_loss: 1.9639e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 665/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5893e-05 - rmse: 0.0068\n",
      "Epoch 665: val_loss did not improve from 0.00018\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3440e-04 - rmse: 0.0116 - val_loss: 1.8681e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 666/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5841e-05 - rmse: 0.0075\n",
      "Epoch 666: val_loss improved from 0.00018 to 0.00017, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.1492e-04 - rmse: 0.0107 - val_loss: 1.7467e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 667/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6688e-05 - rmse: 0.0052\n",
      "Epoch 667: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1638e-04 - rmse: 0.0108 - val_loss: 1.7624e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 668/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4484e-04 - rmse: 0.0120\n",
      "Epoch 668: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3091e-04 - rmse: 0.0114 - val_loss: 1.9577e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 669/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4317e-04 - rmse: 0.0120\n",
      "Epoch 669: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4188e-04 - rmse: 0.0119 - val_loss: 1.7593e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 670/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3761e-04 - rmse: 0.0117\n",
      "Epoch 670: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3240e-04 - rmse: 0.0115 - val_loss: 1.7783e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 671/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5729e-04 - rmse: 0.0125\n",
      "Epoch 671: val_loss improved from 0.00017 to 0.00016, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 1.1064e-04 - rmse: 0.0105 - val_loss: 1.6125e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 672/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6275e-05 - rmse: 0.0075\n",
      "Epoch 672: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1652e-04 - rmse: 0.0108 - val_loss: 2.4928e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 673/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0424e-04 - rmse: 0.0143\n",
      "Epoch 673: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5201e-04 - rmse: 0.0123 - val_loss: 3.0013e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 674/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8756e-04 - rmse: 0.0170\n",
      "Epoch 674: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5800e-04 - rmse: 0.0126 - val_loss: 2.7945e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 675/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7104e-04 - rmse: 0.0165\n",
      "Epoch 675: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8557e-04 - rmse: 0.0136 - val_loss: 1.8836e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 676/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3578e-05 - rmse: 0.0058\n",
      "Epoch 676: val_loss improved from 0.00016 to 0.00015, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 1.4347e-04 - rmse: 0.0120 - val_loss: 1.4956e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 677/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8938e-05 - rmse: 0.0070\n",
      "Epoch 677: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0318e-04 - rmse: 0.0102 - val_loss: 2.1514e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 678/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0076e-04 - rmse: 0.0100\n",
      "Epoch 678: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5391e-04 - rmse: 0.0124 - val_loss: 2.2948e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 679/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5829e-04 - rmse: 0.0161\n",
      "Epoch 679: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.1265e-04 - rmse: 0.0146 - val_loss: 1.6698e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 680/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1507e-05 - rmse: 0.0090\n",
      "Epoch 680: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4532e-04 - rmse: 0.0121 - val_loss: 1.9270e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 681/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0990e-04 - rmse: 0.0105\n",
      "Epoch 681: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8332e-04 - rmse: 0.0135 - val_loss: 2.8978e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 682/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6821e-05 - rmse: 0.0088\n",
      "Epoch 682: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3813e-04 - rmse: 0.0118 - val_loss: 3.4230e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 683/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8965e-04 - rmse: 0.0170\n",
      "Epoch 683: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6429e-04 - rmse: 0.0163 - val_loss: 3.3797e-04 - val_rmse: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 684/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9996e-04 - rmse: 0.0141\n",
      "Epoch 684: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8439e-04 - rmse: 0.0169 - val_loss: 2.2046e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 685/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7520e-04 - rmse: 0.0132\n",
      "Epoch 685: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7393e-04 - rmse: 0.0166 - val_loss: 3.0131e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 686/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6365e-04 - rmse: 0.0162\n",
      "Epoch 686: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8069e-04 - rmse: 0.0134 - val_loss: 2.2957e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 687/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0234e-04 - rmse: 0.0142\n",
      "Epoch 687: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.3017e-04 - rmse: 0.0114 - val_loss: 1.4849e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 688/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0222e-04 - rmse: 0.0101\n",
      "Epoch 688: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0393e-04 - rmse: 0.0102 - val_loss: 1.6507e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 689/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3548e-04 - rmse: 0.0116\n",
      "Epoch 689: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8140e-05 - rmse: 0.0099 - val_loss: 1.7278e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 690/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2159e-04 - rmse: 0.0110\n",
      "Epoch 690: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9935e-05 - rmse: 0.0100 - val_loss: 1.8741e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 691/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0922e-04 - rmse: 0.0105\n",
      "Epoch 691: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9230e-05 - rmse: 0.0094 - val_loss: 1.5735e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 692/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3361e-05 - rmse: 0.0048\n",
      "Epoch 692: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0882e-04 - rmse: 0.0104 - val_loss: 2.1469e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 693/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8429e-04 - rmse: 0.0136\n",
      "Epoch 693: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0104e-04 - rmse: 0.0101 - val_loss: 2.0420e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 694/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8241e-05 - rmse: 0.0062\n",
      "Epoch 694: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8367e-05 - rmse: 0.0094 - val_loss: 1.7242e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 695/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3903e-05 - rmse: 0.0086\n",
      "Epoch 695: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0056e-04 - rmse: 0.0100 - val_loss: 2.4406e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 696/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3375e-04 - rmse: 0.0116\n",
      "Epoch 696: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4098e-05 - rmse: 0.0097 - val_loss: 1.7726e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 697/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5057e-05 - rmse: 0.0081\n",
      "Epoch 697: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.4198e-05 - rmse: 0.0092 - val_loss: 1.7138e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 698/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7842e-05 - rmse: 0.0082\n",
      "Epoch 698: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3579e-05 - rmse: 0.0091 - val_loss: 1.8935e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 699/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2749e-05 - rmse: 0.0073\n",
      "Epoch 699: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9807e-05 - rmse: 0.0089 - val_loss: 1.6852e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 700/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1660e-05 - rmse: 0.0085\n",
      "Epoch 700: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.4827e-05 - rmse: 0.0092 - val_loss: 1.6903e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 701/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1692e-04 - rmse: 0.0108\n",
      "Epoch 701: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0876e-04 - rmse: 0.0104 - val_loss: 1.7729e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 702/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1208e-04 - rmse: 0.0106\n",
      "Epoch 702: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3460e-04 - rmse: 0.0116 - val_loss: 2.1856e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 703/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4613e-05 - rmse: 0.0092\n",
      "Epoch 703: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3950e-04 - rmse: 0.0118 - val_loss: 2.3109e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 704/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1510e-04 - rmse: 0.0107\n",
      "Epoch 704: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1194e-04 - rmse: 0.0106 - val_loss: 2.2462e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 705/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1362e-05 - rmse: 0.0084\n",
      "Epoch 705: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1215e-04 - rmse: 0.0106 - val_loss: 2.2302e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 706/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1973e-05 - rmse: 0.0096\n",
      "Epoch 706: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1861e-04 - rmse: 0.0109 - val_loss: 2.0829e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 707/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5105e-05 - rmse: 0.0081\n",
      "Epoch 707: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 9.8324e-05 - rmse: 0.0099 - val_loss: 1.4591e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 708/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2179e-04 - rmse: 0.0110\n",
      "Epoch 708: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0055e-04 - rmse: 0.0100 - val_loss: 2.1228e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 709/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1167e-04 - rmse: 0.0106\n",
      "Epoch 709: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2909e-04 - rmse: 0.0114 - val_loss: 2.0646e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 710/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6325e-05 - rmse: 0.0087\n",
      "Epoch 710: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1833e-04 - rmse: 0.0109 - val_loss: 1.9057e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 711/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4921e-05 - rmse: 0.0081\n",
      "Epoch 711: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1632e-04 - rmse: 0.0108 - val_loss: 1.9526e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 712/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0439e-05 - rmse: 0.0064\n",
      "Epoch 712: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0004e-05 - rmse: 0.0089 - val_loss: 2.0083e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 713/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2757e-05 - rmse: 0.0073\n",
      "Epoch 713: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.8429e-05 - rmse: 0.0089 - val_loss: 1.7630e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 714/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2026e-05 - rmse: 0.0085\n",
      "Epoch 714: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6756e-05 - rmse: 0.0088 - val_loss: 2.1724e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 715/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7339e-04 - rmse: 0.0132\n",
      "Epoch 715: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9234e-05 - rmse: 0.0089 - val_loss: 2.0198e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 716/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7731e-05 - rmse: 0.0061\n",
      "Epoch 716: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0849e-05 - rmse: 0.0084 - val_loss: 1.9868e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 717/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0581e-04 - rmse: 0.0103\n",
      "Epoch 717: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0731e-04 - rmse: 0.0104 - val_loss: 2.0397e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 718/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1577e-04 - rmse: 0.0108\n",
      "Epoch 718: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7142e-05 - rmse: 0.0099 - val_loss: 1.7770e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 719/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3272e-05 - rmse: 0.0080\n",
      "Epoch 719: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3414e-05 - rmse: 0.0091 - val_loss: 1.7827e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 720/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1791e-05 - rmse: 0.0072\n",
      "Epoch 720: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.3699e-05 - rmse: 0.0097 - val_loss: 1.8297e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 721/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1131e-04 - rmse: 0.0106\n",
      "Epoch 721: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3326e-04 - rmse: 0.0115 - val_loss: 1.9475e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 722/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7516e-05 - rmse: 0.0082\n",
      "Epoch 722: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3826e-05 - rmse: 0.0092 - val_loss: 2.0465e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 723/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6923e-05 - rmse: 0.0041\n",
      "Epoch 723: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7379e-05 - rmse: 0.0093 - val_loss: 1.9487e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 724/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6075e-04 - rmse: 0.0127\n",
      "Epoch 724: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.3191e-05 - rmse: 0.0097 - val_loss: 2.3869e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 725/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8577e-05 - rmse: 0.0070\n",
      "Epoch 725: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5148e-05 - rmse: 0.0092 - val_loss: 1.6099e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 726/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7249e-05 - rmse: 0.0061\n",
      "Epoch 726: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2145e-04 - rmse: 0.0110 - val_loss: 1.6995e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 727/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9693e-05 - rmse: 0.0054\n",
      "Epoch 727: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3754e-04 - rmse: 0.0117 - val_loss: 1.8492e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 728/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2588e-05 - rmse: 0.0091\n",
      "Epoch 728: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2774e-04 - rmse: 0.0113 - val_loss: 2.6386e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 729/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7062e-05 - rmse: 0.0069\n",
      "Epoch 729: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0613e-04 - rmse: 0.0175 - val_loss: 4.2127e-04 - val_rmse: 0.0205 - lr: 1.0000e-04\n",
      "Epoch 730/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2591e-04 - rmse: 0.0150\n",
      "Epoch 730: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0779e-04 - rmse: 0.0202 - val_loss: 3.4443e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 731/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2967e-04 - rmse: 0.0152\n",
      "Epoch 731: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.6088e-04 - rmse: 0.0162 - val_loss: 3.3264e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 732/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4394e-04 - rmse: 0.0156\n",
      "Epoch 732: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8814e-04 - rmse: 0.0137 - val_loss: 3.3118e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 733/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3700e-04 - rmse: 0.0154\n",
      "Epoch 733: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1526e-04 - rmse: 0.0107 - val_loss: 2.8262e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 734/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2495e-04 - rmse: 0.0150\n",
      "Epoch 734: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4361e-04 - rmse: 0.0156 - val_loss: 3.3512e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 735/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2346e-05 - rmse: 0.0079\n",
      "Epoch 735: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6134e-04 - rmse: 0.0162 - val_loss: 1.7048e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 736/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6736e-05 - rmse: 0.0082\n",
      "Epoch 736: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8801e-04 - rmse: 0.0170 - val_loss: 1.8407e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 737/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6725e-04 - rmse: 0.0129\n",
      "Epoch 737: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4252e-04 - rmse: 0.0156 - val_loss: 2.4143e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 738/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4716e-04 - rmse: 0.0157\n",
      "Epoch 738: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8319e-04 - rmse: 0.0135 - val_loss: 2.6107e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 739/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6059e-04 - rmse: 0.0161\n",
      "Epoch 739: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7027e-04 - rmse: 0.0130 - val_loss: 2.9118e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 740/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6448e-04 - rmse: 0.0128\n",
      "Epoch 740: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0994e-04 - rmse: 0.0105 - val_loss: 1.8348e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 741/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1361e-05 - rmse: 0.0096\n",
      "Epoch 741: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1535e-05 - rmse: 0.0096 - val_loss: 1.8388e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 742/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6775e-05 - rmse: 0.0082\n",
      "Epoch 742: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9793e-05 - rmse: 0.0100 - val_loss: 2.1210e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 743/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1461e-05 - rmse: 0.0096\n",
      "Epoch 743: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2385e-04 - rmse: 0.0111 - val_loss: 3.6028e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 744/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6507e-04 - rmse: 0.0163\n",
      "Epoch 744: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6936e-04 - rmse: 0.0130 - val_loss: 3.6757e-04 - val_rmse: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 745/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6654e-04 - rmse: 0.0163\n",
      "Epoch 745: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.5588e-04 - rmse: 0.0125 - val_loss: 2.1316e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 746/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0842e-04 - rmse: 0.0104\n",
      "Epoch 746: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2476e-04 - rmse: 0.0112 - val_loss: 3.0215e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 747/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2094e-04 - rmse: 0.0110\n",
      "Epoch 747: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4596e-04 - rmse: 0.0121 - val_loss: 4.3733e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 748/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6220e-04 - rmse: 0.0162\n",
      "Epoch 748: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5683e-04 - rmse: 0.0125 - val_loss: 2.4821e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 749/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9344e-05 - rmse: 0.0100\n",
      "Epoch 749: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0484e-04 - rmse: 0.0102 - val_loss: 2.3335e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 750/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9053e-05 - rmse: 0.0100\n",
      "Epoch 750: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.7894e-05 - rmse: 0.0082 - val_loss: 1.8529e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 751/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2537e-05 - rmse: 0.0072\n",
      "Epoch 751: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0972e-05 - rmse: 0.0084 - val_loss: 2.1270e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 752/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1521e-04 - rmse: 0.0107\n",
      "Epoch 752: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0849e-04 - rmse: 0.0104 - val_loss: 2.1702e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 753/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4359e-04 - rmse: 0.0120\n",
      "Epoch 753: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3864e-04 - rmse: 0.0118 - val_loss: 1.9110e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 754/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1338e-05 - rmse: 0.0078\n",
      "Epoch 754: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.4992e-04 - rmse: 0.0158 - val_loss: 1.8425e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 755/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5573e-05 - rmse: 0.0068\n",
      "Epoch 755: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9821e-04 - rmse: 0.0141 - val_loss: 2.4709e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 756/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1714e-04 - rmse: 0.0108\n",
      "Epoch 756: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7220e-04 - rmse: 0.0165 - val_loss: 6.6429e-04 - val_rmse: 0.0258 - lr: 1.0000e-04\n",
      "Epoch 757/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9300e-04 - rmse: 0.0244\n",
      "Epoch 757: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7727e-04 - rmse: 0.0194 - val_loss: 5.0945e-04 - val_rmse: 0.0226 - lr: 1.0000e-04\n",
      "Epoch 758/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2460e-04 - rmse: 0.0150\n",
      "Epoch 758: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6008e-04 - rmse: 0.0127 - val_loss: 2.4008e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 759/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4024e-04 - rmse: 0.0118\n",
      "Epoch 759: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1675e-04 - rmse: 0.0108 - val_loss: 3.1982e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 760/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2859e-04 - rmse: 0.0113\n",
      "Epoch 760: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5872e-04 - rmse: 0.0126 - val_loss: 3.1354e-04 - val_rmse: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 761/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6887e-04 - rmse: 0.0164\n",
      "Epoch 761: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6479e-04 - rmse: 0.0128 - val_loss: 3.1870e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 762/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6256e-04 - rmse: 0.0127\n",
      "Epoch 762: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5000e-04 - rmse: 0.0122 - val_loss: 3.6527e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 763/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6810e-04 - rmse: 0.0130\n",
      "Epoch 763: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4427e-04 - rmse: 0.0120 - val_loss: 2.1812e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 764/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2959e-05 - rmse: 0.0066\n",
      "Epoch 764: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0663e-05 - rmse: 0.0090 - val_loss: 1.6908e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 765/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1660e-05 - rmse: 0.0096\n",
      "Epoch 765: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1665e-05 - rmse: 0.0090 - val_loss: 1.7983e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 766/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7561e-05 - rmse: 0.0082\n",
      "Epoch 766: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3879e-05 - rmse: 0.0080 - val_loss: 1.6369e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 767/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7503e-05 - rmse: 0.0069\n",
      "Epoch 767: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4648e-05 - rmse: 0.0080 - val_loss: 1.8801e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 768/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0927e-04 - rmse: 0.0105\n",
      "Epoch 768: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7132e-05 - rmse: 0.0088 - val_loss: 1.9896e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 769/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7540e-05 - rmse: 0.0088\n",
      "Epoch 769: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0893e-05 - rmse: 0.0084 - val_loss: 1.7227e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 770/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2584e-04 - rmse: 0.0112\n",
      "Epoch 770: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8885e-05 - rmse: 0.0077 - val_loss: 1.7368e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 771/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9569e-05 - rmse: 0.0054\n",
      "Epoch 771: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5295e-05 - rmse: 0.0074 - val_loss: 1.5724e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 772/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9119e-05 - rmse: 0.0070\n",
      "Epoch 772: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2051e-05 - rmse: 0.0072 - val_loss: 1.5932e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 773/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4296e-05 - rmse: 0.0049\n",
      "Epoch 773: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2704e-05 - rmse: 0.0073 - val_loss: 1.5876e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 774/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9711e-05 - rmse: 0.0044\n",
      "Epoch 774: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1794e-05 - rmse: 0.0072 - val_loss: 1.6875e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 775/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7998e-05 - rmse: 0.0069\n",
      "Epoch 775: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.2326e-05 - rmse: 0.0091 - val_loss: 1.6141e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 776/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0280e-05 - rmse: 0.0078\n",
      "Epoch 776: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7219e-05 - rmse: 0.0093 - val_loss: 2.1274e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 777/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6097e-05 - rmse: 0.0040\n",
      "Epoch 777: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6827e-05 - rmse: 0.0088 - val_loss: 1.8239e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 778/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8617e-05 - rmse: 0.0062\n",
      "Epoch 778: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8468e-05 - rmse: 0.0083 - val_loss: 1.8814e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 779/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5423e-05 - rmse: 0.0067\n",
      "Epoch 779: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0684e-05 - rmse: 0.0078 - val_loss: 1.8265e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 780/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4751e-05 - rmse: 0.0067\n",
      "Epoch 780: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.2674e-05 - rmse: 0.0073 - val_loss: 1.6452e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 781/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4590e-05 - rmse: 0.0059\n",
      "Epoch 781: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6827e-05 - rmse: 0.0075 - val_loss: 1.9699e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 782/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5569e-05 - rmse: 0.0081\n",
      "Epoch 782: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1965e-05 - rmse: 0.0085 - val_loss: 1.4744e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 783/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3993e-05 - rmse: 0.0058\n",
      "Epoch 783: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7983e-05 - rmse: 0.0082 - val_loss: 1.6597e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 784/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0932e-05 - rmse: 0.0064\n",
      "Epoch 784: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7316e-05 - rmse: 0.0076 - val_loss: 1.8655e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 785/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0859e-05 - rmse: 0.0095\n",
      "Epoch 785: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.8050e-05 - rmse: 0.0099 - val_loss: 2.4313e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 786/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3930e-04 - rmse: 0.0118\n",
      "Epoch 786: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.5124e-05 - rmse: 0.0092 - val_loss: 2.2287e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 787/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1430e-05 - rmse: 0.0096\n",
      "Epoch 787: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8412e-05 - rmse: 0.0089 - val_loss: 1.6528e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 788/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9056e-05 - rmse: 0.0062\n",
      "Epoch 788: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8610e-05 - rmse: 0.0077 - val_loss: 1.6293e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 789/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2371e-05 - rmse: 0.0065\n",
      "Epoch 789: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1831e-05 - rmse: 0.0079 - val_loss: 1.5919e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 790/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8857e-05 - rmse: 0.0070\n",
      "Epoch 790: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4515e-05 - rmse: 0.0080 - val_loss: 1.8832e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 791/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9121e-05 - rmse: 0.0070\n",
      "Epoch 791: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4246e-05 - rmse: 0.0080 - val_loss: 1.6229e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 792/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8856e-05 - rmse: 0.0054\n",
      "Epoch 792: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.4453e-05 - rmse: 0.0086 - val_loss: 1.8625e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 793/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3772e-05 - rmse: 0.0092\n",
      "Epoch 793: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.1388e-05 - rmse: 0.0084 - val_loss: 1.7971e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 794/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9990e-05 - rmse: 0.0077\n",
      "Epoch 794: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2050e-05 - rmse: 0.0079 - val_loss: 1.8598e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 795/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5550e-05 - rmse: 0.0087\n",
      "Epoch 795: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9657e-05 - rmse: 0.0083 - val_loss: 1.7212e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 796/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3341e-05 - rmse: 0.0058\n",
      "Epoch 796: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7938e-05 - rmse: 0.0076 - val_loss: 1.7503e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 797/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1798e-05 - rmse: 0.0065\n",
      "Epoch 797: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5590e-05 - rmse: 0.0068 - val_loss: 1.7415e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 798/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2352e-05 - rmse: 0.0072\n",
      "Epoch 798: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3306e-05 - rmse: 0.0080 - val_loss: 2.0149e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 799/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7285e-05 - rmse: 0.0052\n",
      "Epoch 799: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5525e-05 - rmse: 0.0087 - val_loss: 1.5012e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 800/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0570e-05 - rmse: 0.0055\n",
      "Epoch 800: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6498e-05 - rmse: 0.0087 - val_loss: 1.8021e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 801/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4043e-05 - rmse: 0.0092\n",
      "Epoch 801: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.2611e-05 - rmse: 0.0096 - val_loss: 1.7045e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 802/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9295e-05 - rmse: 0.0063\n",
      "Epoch 802: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0106e-04 - rmse: 0.0101 - val_loss: 2.0106e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 803/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2626e-05 - rmse: 0.0065\n",
      "Epoch 803: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0238e-04 - rmse: 0.0101 - val_loss: 1.7653e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 804/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0264e-04 - rmse: 0.0101\n",
      "Epoch 804: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7608e-05 - rmse: 0.0088 - val_loss: 1.7030e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 805/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0414e-04 - rmse: 0.0102\n",
      "Epoch 805: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9992e-05 - rmse: 0.0084 - val_loss: 2.1458e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 806/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6838e-05 - rmse: 0.0093\n",
      "Epoch 806: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3484e-05 - rmse: 0.0080 - val_loss: 1.6398e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 807/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6439e-04 - rmse: 0.0128\n",
      "Epoch 807: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0155e-04 - rmse: 0.0101 - val_loss: 2.7223e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 808/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9217e-04 - rmse: 0.0139\n",
      "Epoch 808: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3172e-05 - rmse: 0.0091 - val_loss: 1.5318e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 809/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9011e-05 - rmse: 0.0062\n",
      "Epoch 809: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.6582e-05 - rmse: 0.0098 - val_loss: 2.1302e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 810/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6726e-05 - rmse: 0.0061\n",
      "Epoch 810: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5019e-05 - rmse: 0.0081 - val_loss: 1.9001e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 811/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2448e-05 - rmse: 0.0085\n",
      "Epoch 811: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0857e-04 - rmse: 0.0104 - val_loss: 2.2945e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 812/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0540e-04 - rmse: 0.0103\n",
      "Epoch 812: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5974e-05 - rmse: 0.0093 - val_loss: 1.9572e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 813/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0049e-05 - rmse: 0.0084\n",
      "Epoch 813: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.4390e-05 - rmse: 0.0092 - val_loss: 1.5493e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 814/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4725e-05 - rmse: 0.0067\n",
      "Epoch 814: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2388e-05 - rmse: 0.0072 - val_loss: 2.2872e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 815/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9509e-05 - rmse: 0.0095\n",
      "Epoch 815: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4994e-05 - rmse: 0.0067 - val_loss: 1.5346e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 816/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3331e-05 - rmse: 0.0086\n",
      "Epoch 816: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8866e-05 - rmse: 0.0070 - val_loss: 1.8970e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 817/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3427e-05 - rmse: 0.0066\n",
      "Epoch 817: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4665e-05 - rmse: 0.0067 - val_loss: 1.4930e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 818/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6876e-05 - rmse: 0.0093\n",
      "Epoch 818: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.0291e-05 - rmse: 0.0071 - val_loss: 1.5311e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 819/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6129e-05 - rmse: 0.0060\n",
      "Epoch 819: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0238e-05 - rmse: 0.0063 - val_loss: 1.5686e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 820/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3245e-05 - rmse: 0.0066\n",
      "Epoch 820: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8779e-05 - rmse: 0.0062 - val_loss: 1.5004e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 821/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9054e-05 - rmse: 0.0077\n",
      "Epoch 821: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5938e-05 - rmse: 0.0068 - val_loss: 1.7043e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 822/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3498e-05 - rmse: 0.0073\n",
      "Epoch 822: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1085e-05 - rmse: 0.0078 - val_loss: 1.8194e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 823/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9776e-05 - rmse: 0.0071\n",
      "Epoch 823: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7215e-05 - rmse: 0.0088 - val_loss: 1.7616e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 824/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2152e-05 - rmse: 0.0065\n",
      "Epoch 824: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.2126e-05 - rmse: 0.0079 - val_loss: 1.7809e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 825/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1946e-05 - rmse: 0.0085\n",
      "Epoch 825: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 7.7145e-05 - rmse: 0.0088 - val_loss: 1.4536e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 826/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1559e-05 - rmse: 0.0056\n",
      "Epoch 826: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9952e-05 - rmse: 0.0071 - val_loss: 2.0165e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 827/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6554e-05 - rmse: 0.0093\n",
      "Epoch 827: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5298e-05 - rmse: 0.0092 - val_loss: 1.9844e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 828/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4914e-04 - rmse: 0.0122\n",
      "Epoch 828: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8029e-05 - rmse: 0.0099 - val_loss: 1.6726e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 829/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3500e-05 - rmse: 0.0048\n",
      "Epoch 829: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6525e-05 - rmse: 0.0075 - val_loss: 1.4807e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 830/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4196e-05 - rmse: 0.0049\n",
      "Epoch 830: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3649e-05 - rmse: 0.0080 - val_loss: 1.5349e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 831/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9733e-05 - rmse: 0.0055\n",
      "Epoch 831: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0182e-05 - rmse: 0.0071 - val_loss: 1.5367e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 832/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2355e-05 - rmse: 0.0085\n",
      "Epoch 832: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9159e-05 - rmse: 0.0070 - val_loss: 1.6187e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 833/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0523e-05 - rmse: 0.0095\n",
      "Epoch 833: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6973e-05 - rmse: 0.0069 - val_loss: 1.7076e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 834/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2645e-05 - rmse: 0.0079\n",
      "Epoch 834: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1279e-05 - rmse: 0.0064 - val_loss: 1.4790e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 835/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5969e-05 - rmse: 0.0068\n",
      "Epoch 835: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7028e-05 - rmse: 0.0061 - val_loss: 1.4805e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 836/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2663e-05 - rmse: 0.0048\n",
      "Epoch 836: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2288e-05 - rmse: 0.0065 - val_loss: 1.9344e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 837/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0326e-05 - rmse: 0.0090\n",
      "Epoch 837: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5163e-05 - rmse: 0.0067 - val_loss: 1.5322e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 838/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0389e-05 - rmse: 0.0071\n",
      "Epoch 838: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3710e-05 - rmse: 0.0073 - val_loss: 1.5662e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 839/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2846e-05 - rmse: 0.0057\n",
      "Epoch 839: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3561e-05 - rmse: 0.0066 - val_loss: 1.4559e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 840/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0131e-05 - rmse: 0.0078\n",
      "Epoch 840: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1116e-05 - rmse: 0.0064 - val_loss: 1.8145e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 841/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6156e-05 - rmse: 0.0051\n",
      "Epoch 841: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.9877e-05 - rmse: 0.0077 - val_loss: 1.8291e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 842/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0609e-05 - rmse: 0.0090\n",
      "Epoch 842: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1264e-05 - rmse: 0.0084 - val_loss: 2.1962e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 843/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5074e-05 - rmse: 0.0059\n",
      "Epoch 843: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.8424e-05 - rmse: 0.0094 - val_loss: 2.5507e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 844/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4484e-04 - rmse: 0.0120\n",
      "Epoch 844: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1412e-04 - rmse: 0.0107 - val_loss: 3.0052e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 845/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9381e-04 - rmse: 0.0139\n",
      "Epoch 845: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0798e-04 - rmse: 0.0104 - val_loss: 1.7580e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 846/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2160e-05 - rmse: 0.0096\n",
      "Epoch 846: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0248e-04 - rmse: 0.0101 - val_loss: 3.4069e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 847/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6889e-04 - rmse: 0.0164\n",
      "Epoch 847: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4591e-04 - rmse: 0.0121 - val_loss: 1.7122e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 848/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0266e-04 - rmse: 0.0101\n",
      "Epoch 848: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0818e-04 - rmse: 0.0104 - val_loss: 1.7267e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 849/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1723e-05 - rmse: 0.0047\n",
      "Epoch 849: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8153e-05 - rmse: 0.0099 - val_loss: 4.1485e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 850/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3984e-04 - rmse: 0.0184\n",
      "Epoch 850: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4440e-04 - rmse: 0.0156 - val_loss: 2.0994e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 851/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5231e-05 - rmse: 0.0067\n",
      "Epoch 851: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5467e-05 - rmse: 0.0092 - val_loss: 2.0872e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 852/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1208e-04 - rmse: 0.0106\n",
      "Epoch 852: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9745e-05 - rmse: 0.0084 - val_loss: 2.9646e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 853/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3041e-04 - rmse: 0.0114\n",
      "Epoch 853: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3453e-05 - rmse: 0.0080 - val_loss: 1.5182e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 854/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7578e-05 - rmse: 0.0053\n",
      "Epoch 854: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.3334e-05 - rmse: 0.0073 - val_loss: 1.9134e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 855/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4013e-05 - rmse: 0.0073\n",
      "Epoch 855: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.1498e-05 - rmse: 0.0072 - val_loss: 1.6824e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 856/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3411e-05 - rmse: 0.0058\n",
      "Epoch 856: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1654e-05 - rmse: 0.0072 - val_loss: 1.9791e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 857/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4097e-05 - rmse: 0.0092\n",
      "Epoch 857: val_loss improved from 0.00015 to 0.00014, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 6.0296e-05 - rmse: 0.0078 - val_loss: 1.4258e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 858/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2142e-05 - rmse: 0.0091\n",
      "Epoch 858: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6506e-05 - rmse: 0.0075 - val_loss: 1.4835e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 859/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6480e-05 - rmse: 0.0060\n",
      "Epoch 859: val_loss improved from 0.00014 to 0.00014, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 4.1477e-05 - rmse: 0.0064 - val_loss: 1.3558e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 860/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2060e-05 - rmse: 0.0065\n",
      "Epoch 860: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0932e-05 - rmse: 0.0071 - val_loss: 1.5880e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 861/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2183e-05 - rmse: 0.0057\n",
      "Epoch 861: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3702e-05 - rmse: 0.0066 - val_loss: 1.5431e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 862/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3838e-05 - rmse: 0.0066\n",
      "Epoch 862: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3587e-05 - rmse: 0.0066 - val_loss: 1.5072e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 863/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3875e-05 - rmse: 0.0049\n",
      "Epoch 863: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9839e-05 - rmse: 0.0063 - val_loss: 1.5463e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 864/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4743e-05 - rmse: 0.0050\n",
      "Epoch 864: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4659e-05 - rmse: 0.0059 - val_loss: 1.4104e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 865/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2886e-05 - rmse: 0.0048\n",
      "Epoch 865: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6560e-05 - rmse: 0.0060 - val_loss: 1.7009e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 866/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8508e-05 - rmse: 0.0053\n",
      "Epoch 866: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1501e-05 - rmse: 0.0072 - val_loss: 1.6967e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 867/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0439e-05 - rmse: 0.0078\n",
      "Epoch 867: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.3912e-05 - rmse: 0.0086 - val_loss: 2.4055e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 868/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2462e-05 - rmse: 0.0079\n",
      "Epoch 868: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6159e-05 - rmse: 0.0081 - val_loss: 1.4601e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 869/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5802e-05 - rmse: 0.0087\n",
      "Epoch 869: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8247e-05 - rmse: 0.0083 - val_loss: 2.3013e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 870/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8482e-05 - rmse: 0.0053\n",
      "Epoch 870: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2450e-04 - rmse: 0.0112 - val_loss: 1.5298e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 871/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2581e-05 - rmse: 0.0096\n",
      "Epoch 871: val_loss did not improve from 0.00014\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4256e-05 - rmse: 0.0080 - val_loss: 1.9440e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 872/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4728e-05 - rmse: 0.0086\n",
      "Epoch 872: val_loss improved from 0.00014 to 0.00013, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.9406e-05 - rmse: 0.0077 - val_loss: 1.3196e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 873/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3284e-05 - rmse: 0.0073\n",
      "Epoch 873: val_loss did not improve from 0.00013\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0393e-05 - rmse: 0.0084 - val_loss: 2.1979e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 874/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0297e-05 - rmse: 0.0055\n",
      "Epoch 874: val_loss improved from 0.00013 to 0.00012, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.3225e-05 - rmse: 0.0080 - val_loss: 1.2277e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 875/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1280e-05 - rmse: 0.0072\n",
      "Epoch 875: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5507e-05 - rmse: 0.0087 - val_loss: 1.8851e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 876/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9962e-05 - rmse: 0.0077\n",
      "Epoch 876: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9223e-05 - rmse: 0.0083 - val_loss: 1.4943e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 877/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0221e-05 - rmse: 0.0055\n",
      "Epoch 877: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6163e-05 - rmse: 0.0060 - val_loss: 2.1404e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 878/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2020e-05 - rmse: 0.0079\n",
      "Epoch 878: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5424e-05 - rmse: 0.0067 - val_loss: 1.3446e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 879/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0575e-05 - rmse: 0.0033\n",
      "Epoch 879: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0651e-05 - rmse: 0.0071 - val_loss: 2.0499e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 880/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8188e-05 - rmse: 0.0053\n",
      "Epoch 880: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9802e-05 - rmse: 0.0063 - val_loss: 1.4697e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 881/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1318e-05 - rmse: 0.0090\n",
      "Epoch 881: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1962e-05 - rmse: 0.0072 - val_loss: 1.7088e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 882/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6353e-05 - rmse: 0.0060\n",
      "Epoch 882: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4445e-05 - rmse: 0.0067 - val_loss: 1.3873e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 883/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3863e-05 - rmse: 0.0049\n",
      "Epoch 883: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7984e-05 - rmse: 0.0062 - val_loss: 1.7045e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 884/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2480e-05 - rmse: 0.0057\n",
      "Epoch 884: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5543e-05 - rmse: 0.0060 - val_loss: 1.3506e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 885/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5200e-05 - rmse: 0.0050\n",
      "Epoch 885: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9581e-05 - rmse: 0.0070 - val_loss: 1.3923e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 886/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7689e-05 - rmse: 0.0061\n",
      "Epoch 886: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5435e-05 - rmse: 0.0081 - val_loss: 1.5681e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 887/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8959e-05 - rmse: 0.0070\n",
      "Epoch 887: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3452e-05 - rmse: 0.0080 - val_loss: 2.0584e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 888/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7022e-05 - rmse: 0.0061\n",
      "Epoch 888: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0502e-05 - rmse: 0.0078 - val_loss: 1.2639e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 889/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0654e-05 - rmse: 0.0071\n",
      "Epoch 889: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3177e-05 - rmse: 0.0079 - val_loss: 1.8543e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 890/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4427e-05 - rmse: 0.0086\n",
      "Epoch 890: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9095e-05 - rmse: 0.0070 - val_loss: 1.4873e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 891/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3422e-05 - rmse: 0.0048\n",
      "Epoch 891: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1382e-05 - rmse: 0.0072 - val_loss: 1.4194e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 892/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8608e-05 - rmse: 0.0053\n",
      "Epoch 892: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8754e-05 - rmse: 0.0062 - val_loss: 1.7272e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 893/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4796e-05 - rmse: 0.0067\n",
      "Epoch 893: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 3.3778e-05 - rmse: 0.0058 - val_loss: 1.2169e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 894/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4961e-05 - rmse: 0.0059\n",
      "Epoch 894: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3688e-05 - rmse: 0.0058 - val_loss: 1.6012e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 895/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0393e-05 - rmse: 0.0071\n",
      "Epoch 895: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4910e-05 - rmse: 0.0059 - val_loss: 1.4642e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 896/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8899e-05 - rmse: 0.0070\n",
      "Epoch 896: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7026e-05 - rmse: 0.0061 - val_loss: 1.9203e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 897/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6836e-05 - rmse: 0.0061\n",
      "Epoch 897: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5025e-05 - rmse: 0.0067 - val_loss: 1.4986e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 898/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6293e-05 - rmse: 0.0075\n",
      "Epoch 898: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4170e-05 - rmse: 0.0066 - val_loss: 1.8999e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 899/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9204e-05 - rmse: 0.0083\n",
      "Epoch 899: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7481e-05 - rmse: 0.0069 - val_loss: 1.9289e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 900/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6040e-05 - rmse: 0.0081\n",
      "Epoch 900: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3257e-05 - rmse: 0.0073 - val_loss: 1.7626e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 901/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0870e-05 - rmse: 0.0046\n",
      "Epoch 901: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0902e-05 - rmse: 0.0056 - val_loss: 1.2569e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 902/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9144e-05 - rmse: 0.0044\n",
      "Epoch 902: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4345e-05 - rmse: 0.0059 - val_loss: 1.7392e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 903/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2102e-05 - rmse: 0.0057\n",
      "Epoch 903: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 3.4130e-05 - rmse: 0.0058 - val_loss: 1.2135e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 904/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4156e-05 - rmse: 0.0066\n",
      "Epoch 904: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9173e-05 - rmse: 0.0063 - val_loss: 1.9030e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 905/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0779e-05 - rmse: 0.0055\n",
      "Epoch 905: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0208e-05 - rmse: 0.0071 - val_loss: 1.3563e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 906/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1821e-05 - rmse: 0.0047\n",
      "Epoch 906: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0277e-05 - rmse: 0.0071 - val_loss: 1.7880e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 907/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4668e-05 - rmse: 0.0038\n",
      "Epoch 907: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2798e-05 - rmse: 0.0085 - val_loss: 1.5772e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 908/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7126e-05 - rmse: 0.0082\n",
      "Epoch 908: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4312e-05 - rmse: 0.0080 - val_loss: 2.3684e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 909/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7392e-05 - rmse: 0.0069\n",
      "Epoch 909: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9387e-05 - rmse: 0.0077 - val_loss: 1.2885e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 910/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8258e-05 - rmse: 0.0043\n",
      "Epoch 910: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0209e-05 - rmse: 0.0084 - val_loss: 1.7320e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 911/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0067e-05 - rmse: 0.0078\n",
      "Epoch 911: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.5239e-05 - rmse: 0.0067 - val_loss: 1.5484e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 912/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8589e-05 - rmse: 0.0062\n",
      "Epoch 912: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7508e-05 - rmse: 0.0061 - val_loss: 1.5781e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 913/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7196e-05 - rmse: 0.0093\n",
      "Epoch 913: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5526e-05 - rmse: 0.0067 - val_loss: 1.6410e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 914/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3353e-05 - rmse: 0.0048\n",
      "Epoch 914: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6715e-05 - rmse: 0.0061 - val_loss: 1.5719e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 915/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9775e-05 - rmse: 0.0055\n",
      "Epoch 915: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0401e-05 - rmse: 0.0064 - val_loss: 1.2970e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 916/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9676e-05 - rmse: 0.0063\n",
      "Epoch 916: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3337e-05 - rmse: 0.0066 - val_loss: 1.8842e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 917/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0044e-05 - rmse: 0.0084\n",
      "Epoch 917: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0307e-05 - rmse: 0.0090 - val_loss: 1.6714e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 918/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5177e-05 - rmse: 0.0059\n",
      "Epoch 918: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8686e-05 - rmse: 0.0083 - val_loss: 1.4463e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 919/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5282e-05 - rmse: 0.0050\n",
      "Epoch 919: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3553e-05 - rmse: 0.0086 - val_loss: 1.8674e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 920/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4872e-05 - rmse: 0.0039\n",
      "Epoch 920: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0959e-05 - rmse: 0.0078 - val_loss: 1.3458e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 921/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8788e-05 - rmse: 0.0083\n",
      "Epoch 921: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1699e-05 - rmse: 0.0085 - val_loss: 3.4260e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 922/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1369e-04 - rmse: 0.0107\n",
      "Epoch 922: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3494e-04 - rmse: 0.0116 - val_loss: 1.4435e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 923/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1091e-04 - rmse: 0.0105\n",
      "Epoch 923: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1051e-04 - rmse: 0.0105 - val_loss: 1.9832e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 924/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4353e-05 - rmse: 0.0086\n",
      "Epoch 924: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3813e-04 - rmse: 0.0118 - val_loss: 1.3249e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 925/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4423e-05 - rmse: 0.0059\n",
      "Epoch 925: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4510e-04 - rmse: 0.0120 - val_loss: 3.0353e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 926/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8671e-04 - rmse: 0.0137\n",
      "Epoch 926: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8112e-04 - rmse: 0.0135 - val_loss: 1.9025e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 927/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2891e-05 - rmse: 0.0091\n",
      "Epoch 927: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3410e-04 - rmse: 0.0116 - val_loss: 2.3673e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 928/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1683e-04 - rmse: 0.0108\n",
      "Epoch 928: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6267e-04 - rmse: 0.0128 - val_loss: 4.2106e-04 - val_rmse: 0.0205 - lr: 1.0000e-04\n",
      "Epoch 929/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4822e-04 - rmse: 0.0187\n",
      "Epoch 929: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9557e-04 - rmse: 0.0140 - val_loss: 3.8163e-04 - val_rmse: 0.0195 - lr: 1.0000e-04\n",
      "Epoch 930/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5515e-04 - rmse: 0.0160\n",
      "Epoch 930: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.8352e-04 - rmse: 0.0135 - val_loss: 2.0624e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 931/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8596e-04 - rmse: 0.0136\n",
      "Epoch 931: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7920e-04 - rmse: 0.0134 - val_loss: 2.7041e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 932/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5501e-05 - rmse: 0.0087\n",
      "Epoch 932: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3845e-04 - rmse: 0.0154 - val_loss: 1.8905e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 933/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4349e-04 - rmse: 0.0120\n",
      "Epoch 933: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9764e-04 - rmse: 0.0141 - val_loss: 3.4870e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 934/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0297e-04 - rmse: 0.0101\n",
      "Epoch 934: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4779e-04 - rmse: 0.0122 - val_loss: 3.2904e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 935/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2386e-04 - rmse: 0.0180\n",
      "Epoch 935: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0357e-04 - rmse: 0.0143 - val_loss: 4.3123e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 936/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7016e-04 - rmse: 0.0164\n",
      "Epoch 936: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5745e-04 - rmse: 0.0125 - val_loss: 2.2166e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 937/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2287e-04 - rmse: 0.0149\n",
      "Epoch 937: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0029e-04 - rmse: 0.0100 - val_loss: 3.2265e-04 - val_rmse: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 938/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0211e-04 - rmse: 0.0101\n",
      "Epoch 938: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7206e-05 - rmse: 0.0088 - val_loss: 1.4760e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 939/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5565e-04 - rmse: 0.0125\n",
      "Epoch 939: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8004e-05 - rmse: 0.0088 - val_loss: 2.7904e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 940/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0012e-04 - rmse: 0.0100\n",
      "Epoch 940: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0709e-05 - rmse: 0.0095 - val_loss: 1.5587e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 941/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9802e-05 - rmse: 0.0077\n",
      "Epoch 941: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7031e-05 - rmse: 0.0088 - val_loss: 3.1968e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 942/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0039e-04 - rmse: 0.0100\n",
      "Epoch 942: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1241e-04 - rmse: 0.0106 - val_loss: 1.3564e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 943/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9426e-05 - rmse: 0.0070\n",
      "Epoch 943: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.2997e-04 - rmse: 0.0152 - val_loss: 3.3874e-04 - val_rmse: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 944/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2780e-04 - rmse: 0.0151\n",
      "Epoch 944: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4217e-04 - rmse: 0.0119 - val_loss: 1.7204e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 945/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1699e-05 - rmse: 0.0090\n",
      "Epoch 945: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0082e-04 - rmse: 0.0100 - val_loss: 1.9762e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 946/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1117e-05 - rmse: 0.0046\n",
      "Epoch 946: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0871e-04 - rmse: 0.0104 - val_loss: 1.7686e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 947/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9403e-04 - rmse: 0.0171\n",
      "Epoch 947: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1841e-04 - rmse: 0.0109 - val_loss: 2.2959e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 948/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3431e-05 - rmse: 0.0048\n",
      "Epoch 948: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0210e-04 - rmse: 0.0101 - val_loss: 1.8851e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 949/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1997e-04 - rmse: 0.0110\n",
      "Epoch 949: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3241e-04 - rmse: 0.0182 - val_loss: 3.6062e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 950/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3434e-04 - rmse: 0.0153\n",
      "Epoch 950: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6705e-04 - rmse: 0.0163 - val_loss: 1.7475e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 951/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2753e-05 - rmse: 0.0079\n",
      "Epoch 951: val_loss improved from 0.00012 to 0.00012, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 2.2696e-04 - rmse: 0.0151 - val_loss: 1.1811e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 952/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8173e-05 - rmse: 0.0053\n",
      "Epoch 952: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8138e-04 - rmse: 0.0135 - val_loss: 1.5736e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 953/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3844e-05 - rmse: 0.0058\n",
      "Epoch 953: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6987e-05 - rmse: 0.0093 - val_loss: 2.1187e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 954/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5480e-05 - rmse: 0.0087\n",
      "Epoch 954: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9592e-05 - rmse: 0.0083 - val_loss: 1.2164e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 955/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9150e-05 - rmse: 0.0054\n",
      "Epoch 955: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0383e-05 - rmse: 0.0078 - val_loss: 1.7866e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 956/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1171e-05 - rmse: 0.0072\n",
      "Epoch 956: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0155e-05 - rmse: 0.0071 - val_loss: 1.3113e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 957/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4215e-05 - rmse: 0.0074\n",
      "Epoch 957: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6346e-05 - rmse: 0.0075 - val_loss: 1.9409e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 958/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0822e-05 - rmse: 0.0046\n",
      "Epoch 958: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4370e-05 - rmse: 0.0067 - val_loss: 1.5813e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 959/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9667e-05 - rmse: 0.0095\n",
      "Epoch 959: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7286e-05 - rmse: 0.0076 - val_loss: 1.8008e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 960/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1809e-05 - rmse: 0.0079\n",
      "Epoch 960: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6046e-05 - rmse: 0.0068 - val_loss: 1.3638e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 961/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4520e-05 - rmse: 0.0080\n",
      "Epoch 961: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4231e-05 - rmse: 0.0067 - val_loss: 1.5968e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 962/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9797e-05 - rmse: 0.0055\n",
      "Epoch 962: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6006e-05 - rmse: 0.0060 - val_loss: 1.4236e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 963/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0895e-05 - rmse: 0.0056\n",
      "Epoch 963: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6463e-05 - rmse: 0.0060 - val_loss: 1.5623e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 964/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3668e-05 - rmse: 0.0058\n",
      "Epoch 964: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.8826e-05 - rmse: 0.0054 - val_loss: 1.3756e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 965/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5292e-05 - rmse: 0.0050\n",
      "Epoch 965: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9576e-05 - rmse: 0.0054 - val_loss: 1.2975e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 966/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8620e-05 - rmse: 0.0043\n",
      "Epoch 966: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2628e-05 - rmse: 0.0065 - val_loss: 1.4991e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 967/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4550e-05 - rmse: 0.0067\n",
      "Epoch 967: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9340e-05 - rmse: 0.0070 - val_loss: 1.4592e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 968/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4160e-05 - rmse: 0.0049\n",
      "Epoch 968: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1660e-05 - rmse: 0.0072 - val_loss: 1.8207e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 969/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2144e-05 - rmse: 0.0057\n",
      "Epoch 969: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4102e-05 - rmse: 0.0058 - val_loss: 1.3966e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 970/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9398e-05 - rmse: 0.0054\n",
      "Epoch 970: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7126e-05 - rmse: 0.0052 - val_loss: 1.8627e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 971/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7420e-05 - rmse: 0.0061\n",
      "Epoch 971: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.8079e-05 - rmse: 0.0053 - val_loss: 1.2163e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 972/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5408e-05 - rmse: 0.0074\n",
      "Epoch 972: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4963e-05 - rmse: 0.0059 - val_loss: 1.3145e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 973/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7629e-05 - rmse: 0.0042\n",
      "Epoch 973: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1747e-05 - rmse: 0.0056 - val_loss: 1.3059e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 974/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7568e-05 - rmse: 0.0053\n",
      "Epoch 974: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1160e-05 - rmse: 0.0064 - val_loss: 1.2476e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 975/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7890e-05 - rmse: 0.0042\n",
      "Epoch 975: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2905e-05 - rmse: 0.0057 - val_loss: 2.1833e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 976/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3330e-05 - rmse: 0.0073\n",
      "Epoch 976: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3481e-05 - rmse: 0.0073 - val_loss: 1.2577e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 977/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6235e-05 - rmse: 0.0081\n",
      "Epoch 977: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.0434e-05 - rmse: 0.0078 - val_loss: 1.9983e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 978/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7711e-05 - rmse: 0.0053\n",
      "Epoch 978: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8052e-05 - rmse: 0.0069 - val_loss: 1.2250e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 979/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9224e-05 - rmse: 0.0044\n",
      "Epoch 979: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0713e-05 - rmse: 0.0084 - val_loss: 2.1006e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 980/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9727e-05 - rmse: 0.0055\n",
      "Epoch 980: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.6364e-05 - rmse: 0.0098 - val_loss: 1.4565e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 981/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7584e-05 - rmse: 0.0053\n",
      "Epoch 981: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9603e-04 - rmse: 0.0140 - val_loss: 3.3288e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 982/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3509e-04 - rmse: 0.0153\n",
      "Epoch 982: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7482e-04 - rmse: 0.0166 - val_loss: 3.5416e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 983/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6509e-04 - rmse: 0.0128\n",
      "Epoch 983: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6933e-04 - rmse: 0.0192 - val_loss: 3.7713e-04 - val_rmse: 0.0194 - lr: 1.0000e-04\n",
      "Epoch 984/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0742e-04 - rmse: 0.0144\n",
      "Epoch 984: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7525e-04 - rmse: 0.0166 - val_loss: 3.5134e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 985/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6102e-04 - rmse: 0.0162\n",
      "Epoch 985: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2027e-04 - rmse: 0.0228 - val_loss: 4.8455e-04 - val_rmse: 0.0220 - lr: 1.0000e-04\n",
      "Epoch 986/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8065e-04 - rmse: 0.0195\n",
      "Epoch 986: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1675e-04 - rmse: 0.0147 - val_loss: 1.5785e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 987/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8244e-04 - rmse: 0.0135\n",
      "Epoch 987: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8256e-04 - rmse: 0.0135 - val_loss: 4.6768e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 988/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4562e-04 - rmse: 0.0157\n",
      "Epoch 988: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3249e-04 - rmse: 0.0152 - val_loss: 1.9807e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 989/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0769e-04 - rmse: 0.0104\n",
      "Epoch 989: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2297e-04 - rmse: 0.0111 - val_loss: 2.1337e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 990/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0525e-05 - rmse: 0.0071\n",
      "Epoch 990: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4373e-05 - rmse: 0.0074 - val_loss: 1.2974e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 991/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4151e-05 - rmse: 0.0058\n",
      "Epoch 991: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0766e-05 - rmse: 0.0090 - val_loss: 3.2392e-04 - val_rmse: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 992/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3433e-04 - rmse: 0.0116\n",
      "Epoch 992: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1067e-04 - rmse: 0.0105 - val_loss: 2.3200e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 993/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1911e-04 - rmse: 0.0179\n",
      "Epoch 993: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3301e-04 - rmse: 0.0115 - val_loss: 2.2037e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 994/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0769e-05 - rmse: 0.0090\n",
      "Epoch 994: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1950e-04 - rmse: 0.0109 - val_loss: 1.5961e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 995/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0979e-05 - rmse: 0.0056\n",
      "Epoch 995: val_loss did not improve from 0.00012\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1792e-04 - rmse: 0.0109 - val_loss: 1.9177e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 996/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8775e-05 - rmse: 0.0043\n",
      "Epoch 996: val_loss improved from 0.00012 to 0.00011, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 9.8270e-05 - rmse: 0.0099 - val_loss: 1.1024e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 997/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7055e-05 - rmse: 0.0061\n",
      "Epoch 997: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0296e-04 - rmse: 0.0101 - val_loss: 1.8389e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 998/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1425e-05 - rmse: 0.0078\n",
      "Epoch 998: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0682e-04 - rmse: 0.0103 - val_loss: 1.5164e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 999/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4597e-05 - rmse: 0.0067\n",
      "Epoch 999: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0584e-04 - rmse: 0.0103 - val_loss: 1.3906e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9506e-05 - rmse: 0.0044\n",
      "Epoch 1000: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6623e-05 - rmse: 0.0088 - val_loss: 1.3568e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1001/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5126e-05 - rmse: 0.0039\n",
      "Epoch 1001: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3657e-05 - rmse: 0.0086 - val_loss: 1.3929e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1002/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7566e-05 - rmse: 0.0053\n",
      "Epoch 1002: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0802e-05 - rmse: 0.0071 - val_loss: 1.5220e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1003/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4339e-05 - rmse: 0.0080\n",
      "Epoch 1003: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8047e-05 - rmse: 0.0082 - val_loss: 1.6350e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1004/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3928e-05 - rmse: 0.0080\n",
      "Epoch 1004: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1186e-05 - rmse: 0.0078 - val_loss: 1.9961e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 1005/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8254e-05 - rmse: 0.0069\n",
      "Epoch 1005: val_loss improved from 0.00011 to 0.00011, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 3.6558e-05 - rmse: 0.0060 - val_loss: 1.0583e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1006/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7572e-05 - rmse: 0.0053\n",
      "Epoch 1006: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5966e-05 - rmse: 0.0060 - val_loss: 1.5050e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1007/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3535e-05 - rmse: 0.0037\n",
      "Epoch 1007: val_loss improved from 0.00011 to 0.00010, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 1.9360e-05 - rmse: 0.0044 - val_loss: 1.0434e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1008/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1156e-05 - rmse: 0.0033\n",
      "Epoch 1008: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2385e-05 - rmse: 0.0057 - val_loss: 1.8705e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1009/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4749e-05 - rmse: 0.0038\n",
      "Epoch 1009: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0788e-05 - rmse: 0.0071 - val_loss: 1.2709e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1010/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0526e-05 - rmse: 0.0055\n",
      "Epoch 1010: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 5.9535e-05 - rmse: 0.0077 - val_loss: 1.0073e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1011/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7760e-05 - rmse: 0.0042\n",
      "Epoch 1011: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8250e-05 - rmse: 0.0069 - val_loss: 1.5187e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1012/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4889e-05 - rmse: 0.0050\n",
      "Epoch 1012: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1580e-05 - rmse: 0.0072 - val_loss: 1.4557e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1013/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9263e-05 - rmse: 0.0044\n",
      "Epoch 1013: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8867e-05 - rmse: 0.0054 - val_loss: 1.4868e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1014/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5737e-05 - rmse: 0.0068\n",
      "Epoch 1014: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5211e-05 - rmse: 0.0050 - val_loss: 1.3611e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1015/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4288e-05 - rmse: 0.0038\n",
      "Epoch 1015: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2626e-05 - rmse: 0.0048 - val_loss: 1.2109e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1016/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7614e-06 - rmse: 0.0030\n",
      "Epoch 1016: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0397e-05 - rmse: 0.0055 - val_loss: 1.7398e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1017/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1396e-05 - rmse: 0.0090\n",
      "Epoch 1017: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3013e-05 - rmse: 0.0085 - val_loss: 1.2118e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1018/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5810e-05 - rmse: 0.0040\n",
      "Epoch 1018: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7383e-05 - rmse: 0.0069 - val_loss: 1.8078e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1019/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4260e-05 - rmse: 0.0038\n",
      "Epoch 1019: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7327e-05 - rmse: 0.0088 - val_loss: 1.6938e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1020/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1190e-05 - rmse: 0.0084\n",
      "Epoch 1020: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8295e-05 - rmse: 0.0088 - val_loss: 2.3664e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 1021/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7189e-05 - rmse: 0.0093\n",
      "Epoch 1021: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0507e-05 - rmse: 0.0090 - val_loss: 1.8124e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1022/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0387e-04 - rmse: 0.0102\n",
      "Epoch 1022: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9651e-05 - rmse: 0.0070 - val_loss: 1.5484e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1023/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2236e-05 - rmse: 0.0035\n",
      "Epoch 1023: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8207e-05 - rmse: 0.0053 - val_loss: 1.2119e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1024/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6446e-05 - rmse: 0.0051\n",
      "Epoch 1024: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9202e-05 - rmse: 0.0054 - val_loss: 1.6954e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1025/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1728e-05 - rmse: 0.0072\n",
      "Epoch 1025: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8955e-05 - rmse: 0.0062 - val_loss: 1.6118e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1026/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4689e-05 - rmse: 0.0050\n",
      "Epoch 1026: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0134e-05 - rmse: 0.0045 - val_loss: 1.4242e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1027/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4965e-05 - rmse: 0.0059\n",
      "Epoch 1027: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5591e-05 - rmse: 0.0051 - val_loss: 1.2187e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1028/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6055e-06 - rmse: 0.0028\n",
      "Epoch 1028: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2108e-05 - rmse: 0.0057 - val_loss: 1.1677e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1029/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2383e-05 - rmse: 0.0035\n",
      "Epoch 1029: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3205e-05 - rmse: 0.0058 - val_loss: 1.8887e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1030/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1782e-04 - rmse: 0.0109\n",
      "Epoch 1030: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1754e-05 - rmse: 0.0072 - val_loss: 1.5551e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1031/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4227e-05 - rmse: 0.0067\n",
      "Epoch 1031: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6843e-05 - rmse: 0.0068 - val_loss: 1.1820e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1032/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9161e-05 - rmse: 0.0044\n",
      "Epoch 1032: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6633e-05 - rmse: 0.0088 - val_loss: 1.9417e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1033/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1452e-05 - rmse: 0.0072\n",
      "Epoch 1033: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2611e-04 - rmse: 0.0112 - val_loss: 1.5214e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1034/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8810e-05 - rmse: 0.0062\n",
      "Epoch 1034: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5548e-04 - rmse: 0.0125 - val_loss: 4.1183e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 1035/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3039e-04 - rmse: 0.0152\n",
      "Epoch 1035: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6145e-04 - rmse: 0.0127 - val_loss: 1.7858e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1036/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1671e-04 - rmse: 0.0108\n",
      "Epoch 1036: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1391e-04 - rmse: 0.0107 - val_loss: 3.2082e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 1037/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5106e-04 - rmse: 0.0123\n",
      "Epoch 1037: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0794e-04 - rmse: 0.0104 - val_loss: 2.6142e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1038/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0815e-04 - rmse: 0.0144\n",
      "Epoch 1038: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3278e-04 - rmse: 0.0115 - val_loss: 2.2485e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1039/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8090e-04 - rmse: 0.0134\n",
      "Epoch 1039: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9386e-04 - rmse: 0.0139 - val_loss: 3.8907e-04 - val_rmse: 0.0197 - lr: 1.0000e-04\n",
      "Epoch 1040/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2291e-04 - rmse: 0.0149\n",
      "Epoch 1040: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9978e-04 - rmse: 0.0141 - val_loss: 2.4084e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1041/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7130e-04 - rmse: 0.0131\n",
      "Epoch 1041: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1526e-04 - rmse: 0.0107 - val_loss: 2.5017e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1042/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4256e-05 - rmse: 0.0074\n",
      "Epoch 1042: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2567e-04 - rmse: 0.0112 - val_loss: 2.5917e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 1043/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0669e-04 - rmse: 0.0103\n",
      "Epoch 1043: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1358e-04 - rmse: 0.0107 - val_loss: 1.3067e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1044/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8381e-05 - rmse: 0.0062\n",
      "Epoch 1044: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1384e-04 - rmse: 0.0107 - val_loss: 2.2372e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1045/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5314e-05 - rmse: 0.0067\n",
      "Epoch 1045: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2858e-04 - rmse: 0.0113 - val_loss: 2.0944e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1046/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4240e-04 - rmse: 0.0119\n",
      "Epoch 1046: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8409e-04 - rmse: 0.0169 - val_loss: 5.0094e-04 - val_rmse: 0.0224 - lr: 1.0000e-04\n",
      "Epoch 1047/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4282e-04 - rmse: 0.0156\n",
      "Epoch 1047: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0172e-04 - rmse: 0.0174 - val_loss: 2.0588e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1048/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3724e-05 - rmse: 0.0097\n",
      "Epoch 1048: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3019e-04 - rmse: 0.0152 - val_loss: 2.6247e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1049/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1181e-04 - rmse: 0.0106\n",
      "Epoch 1049: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3533e-04 - rmse: 0.0153 - val_loss: 1.6603e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1050/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1649e-04 - rmse: 0.0108\n",
      "Epoch 1050: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4460e-04 - rmse: 0.0156 - val_loss: 2.4535e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1051/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7226e-05 - rmse: 0.0069\n",
      "Epoch 1051: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7365e-04 - rmse: 0.0132 - val_loss: 1.8856e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1052/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0285e-04 - rmse: 0.0101\n",
      "Epoch 1052: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2642e-04 - rmse: 0.0112 - val_loss: 2.2154e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1053/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3512e-04 - rmse: 0.0116\n",
      "Epoch 1053: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1406e-04 - rmse: 0.0107 - val_loss: 1.9365e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1054/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8415e-05 - rmse: 0.0070\n",
      "Epoch 1054: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 27ms/step - loss: 7.0479e-05 - rmse: 0.0084 - val_loss: 1.0013e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1055/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5440e-05 - rmse: 0.0050\n",
      "Epoch 1055: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1007e-05 - rmse: 0.0090 - val_loss: 2.1881e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1056/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1283e-04 - rmse: 0.0106\n",
      "Epoch 1056: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8894e-05 - rmse: 0.0094 - val_loss: 1.9101e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1057/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6120e-04 - rmse: 0.0127\n",
      "Epoch 1057: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3349e-05 - rmse: 0.0091 - val_loss: 2.5187e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1058/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5909e-04 - rmse: 0.0126\n",
      "Epoch 1058: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1119e-05 - rmse: 0.0084 - val_loss: 1.5420e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1059/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7330e-04 - rmse: 0.0132\n",
      "Epoch 1059: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0939e-05 - rmse: 0.0090 - val_loss: 2.9891e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 1060/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0667e-04 - rmse: 0.0103\n",
      "Epoch 1060: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0423e-05 - rmse: 0.0078 - val_loss: 1.8277e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1061/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8246e-05 - rmse: 0.0094\n",
      "Epoch 1061: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0781e-04 - rmse: 0.0104 - val_loss: 3.3181e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 1062/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6782e-04 - rmse: 0.0130\n",
      "Epoch 1062: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2700e-04 - rmse: 0.0113 - val_loss: 1.7389e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1063/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3820e-05 - rmse: 0.0080\n",
      "Epoch 1063: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2261e-04 - rmse: 0.0180 - val_loss: 7.9258e-04 - val_rmse: 0.0282 - lr: 1.0000e-04\n",
      "Epoch 1064/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8830e-04 - rmse: 0.0243\n",
      "Epoch 1064: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2529e-04 - rmse: 0.0180 - val_loss: 8.1422e-04 - val_rmse: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 1065/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0452\n",
      "Epoch 1065: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0337 - val_loss: 7.2790e-04 - val_rmse: 0.0270 - lr: 1.0000e-04\n",
      "Epoch 1066/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7709e-04 - rmse: 0.0240\n",
      "Epoch 1066: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0554e-04 - rmse: 0.0246 - val_loss: 3.5384e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 1067/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5014e-04 - rmse: 0.0123\n",
      "Epoch 1067: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8178e-04 - rmse: 0.0195 - val_loss: 2.9719e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 1068/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2675e-04 - rmse: 0.0113\n",
      "Epoch 1068: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.8982e-04 - rmse: 0.0197 - val_loss: 2.7912e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 1069/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2139e-04 - rmse: 0.0110\n",
      "Epoch 1069: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3173e-04 - rmse: 0.0115 - val_loss: 1.1966e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1070/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2974e-05 - rmse: 0.0096\n",
      "Epoch 1070: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.9153e-05 - rmse: 0.0083 - val_loss: 1.0487e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1071/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0631e-05 - rmse: 0.0064\n",
      "Epoch 1071: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1998e-05 - rmse: 0.0072 - val_loss: 1.6338e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1072/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8127e-05 - rmse: 0.0069\n",
      "Epoch 1072: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2173e-05 - rmse: 0.0079 - val_loss: 1.0973e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1073/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5982e-05 - rmse: 0.0060\n",
      "Epoch 1073: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2532e-05 - rmse: 0.0057 - val_loss: 2.0560e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1074/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1830e-05 - rmse: 0.0065\n",
      "Epoch 1074: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3966e-05 - rmse: 0.0086 - val_loss: 1.2627e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1075/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2536e-05 - rmse: 0.0065\n",
      "Epoch 1075: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4577e-05 - rmse: 0.0067 - val_loss: 1.5743e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1076/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1948e-05 - rmse: 0.0057\n",
      "Epoch 1076: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8833e-05 - rmse: 0.0070 - val_loss: 1.4479e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1077/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1230e-05 - rmse: 0.0046\n",
      "Epoch 1077: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3640e-05 - rmse: 0.0058 - val_loss: 1.3613e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1078/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3492e-05 - rmse: 0.0058\n",
      "Epoch 1078: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4731e-05 - rmse: 0.0050 - val_loss: 1.3487e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1079/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5615e-05 - rmse: 0.0051\n",
      "Epoch 1079: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1273e-05 - rmse: 0.0056 - val_loss: 1.4343e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1080/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1489e-05 - rmse: 0.0034\n",
      "Epoch 1080: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1410e-05 - rmse: 0.0046 - val_loss: 1.2167e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1081/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3103e-05 - rmse: 0.0048\n",
      "Epoch 1081: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1490e-05 - rmse: 0.0056 - val_loss: 1.3297e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1082/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0090e-05 - rmse: 0.0032\n",
      "Epoch 1082: val_loss did not improve from 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1949e-05 - rmse: 0.0047 - val_loss: 1.1983e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1083/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4945e-05 - rmse: 0.0039\n",
      "Epoch 1083: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4744e-05 - rmse: 0.0059 - val_loss: 1.2308e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1084/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6627e-05 - rmse: 0.0041\n",
      "Epoch 1084: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3331e-05 - rmse: 0.0058 - val_loss: 1.3154e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1085/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9303e-05 - rmse: 0.0044\n",
      "Epoch 1085: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1522e-05 - rmse: 0.0046 - val_loss: 1.6157e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1086/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4704e-05 - rmse: 0.0059\n",
      "Epoch 1086: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8169e-05 - rmse: 0.0053 - val_loss: 1.2380e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1087/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1343e-05 - rmse: 0.0056\n",
      "Epoch 1087: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5031e-05 - rmse: 0.0050 - val_loss: 1.5249e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1088/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0303e-05 - rmse: 0.0045\n",
      "Epoch 1088: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0356e-05 - rmse: 0.0045 - val_loss: 1.3002e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1089/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3747e-05 - rmse: 0.0037\n",
      "Epoch 1089: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5325e-05 - rmse: 0.0050 - val_loss: 1.2991e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1090/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5081e-05 - rmse: 0.0039\n",
      "Epoch 1090: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9303e-05 - rmse: 0.0054 - val_loss: 1.1819e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1091/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1029e-05 - rmse: 0.0033\n",
      "Epoch 1091: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8490e-05 - rmse: 0.0053 - val_loss: 1.3815e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1092/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6039e-05 - rmse: 0.0051\n",
      "Epoch 1092: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9680e-05 - rmse: 0.0044 - val_loss: 1.5038e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1093/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0458e-05 - rmse: 0.0032\n",
      "Epoch 1093: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6958e-05 - rmse: 0.0052 - val_loss: 1.8057e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1094/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1522e-05 - rmse: 0.0046\n",
      "Epoch 1094: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2447e-05 - rmse: 0.0057 - val_loss: 1.2571e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1095/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2462e-05 - rmse: 0.0065\n",
      "Epoch 1095: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7673e-05 - rmse: 0.0053 - val_loss: 1.3129e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1096/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0413e-05 - rmse: 0.0045\n",
      "Epoch 1096: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0489e-05 - rmse: 0.0045 - val_loss: 1.3639e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1097/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8865e-05 - rmse: 0.0043\n",
      "Epoch 1097: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8878e-05 - rmse: 0.0043 - val_loss: 1.3959e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1098/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5056e-05 - rmse: 0.0039\n",
      "Epoch 1098: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0686e-05 - rmse: 0.0055 - val_loss: 1.1578e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1099/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4937e-05 - rmse: 0.0039\n",
      "Epoch 1099: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1253e-05 - rmse: 0.0046 - val_loss: 1.4774e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1100/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6038e-05 - rmse: 0.0068\n",
      "Epoch 1100: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9825e-05 - rmse: 0.0045 - val_loss: 1.2206e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1101/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9485e-06 - rmse: 0.0032\n",
      "Epoch 1101: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7965e-05 - rmse: 0.0042 - val_loss: 1.5452e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1102/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0744e-06 - rmse: 0.0025\n",
      "Epoch 1102: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9470e-05 - rmse: 0.0044 - val_loss: 1.4596e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1103/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8014e-05 - rmse: 0.0053\n",
      "Epoch 1103: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8259e-05 - rmse: 0.0043 - val_loss: 1.4478e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1104/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4571e-06 - rmse: 0.0027\n",
      "Epoch 1104: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6652e-05 - rmse: 0.0041 - val_loss: 1.2373e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1105/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9714e-05 - rmse: 0.0044\n",
      "Epoch 1105: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7406e-05 - rmse: 0.0042 - val_loss: 1.3413e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1106/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3312e-05 - rmse: 0.0036\n",
      "Epoch 1106: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0033e-05 - rmse: 0.0045 - val_loss: 1.4330e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1107/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3453e-05 - rmse: 0.0037\n",
      "Epoch 1107: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6413e-05 - rmse: 0.0041 - val_loss: 1.3525e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1108/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4359e-05 - rmse: 0.0038\n",
      "Epoch 1108: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3853e-05 - rmse: 0.0037 - val_loss: 1.1319e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1109/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0273e-05 - rmse: 0.0032\n",
      "Epoch 1109: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5156e-05 - rmse: 0.0039 - val_loss: 1.3721e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1110/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5827e-05 - rmse: 0.0051\n",
      "Epoch 1110: val_loss did not improve from 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4985e-05 - rmse: 0.0039 - val_loss: 1.2002e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1111/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9977e-05 - rmse: 0.0045\n",
      "Epoch 1111: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9954e-05 - rmse: 0.0045 - val_loss: 1.4664e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1112/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1038e-05 - rmse: 0.0033\n",
      "Epoch 1112: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9338e-05 - rmse: 0.0044 - val_loss: 1.2221e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1113/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4536e-05 - rmse: 0.0067\n",
      "Epoch 1113: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3035e-05 - rmse: 0.0057 - val_loss: 1.5263e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1114/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5101e-05 - rmse: 0.0039\n",
      "Epoch 1114: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0687e-05 - rmse: 0.0064 - val_loss: 1.1108e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1115/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0612e-05 - rmse: 0.0055\n",
      "Epoch 1115: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0653e-05 - rmse: 0.0071 - val_loss: 1.7581e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1116/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5753e-05 - rmse: 0.0068\n",
      "Epoch 1116: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5146e-05 - rmse: 0.0087 - val_loss: 1.6148e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1117/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1873e-05 - rmse: 0.0056\n",
      "Epoch 1117: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9487e-05 - rmse: 0.0095 - val_loss: 1.0408e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1118/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2963e-05 - rmse: 0.0091\n",
      "Epoch 1118: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7956e-05 - rmse: 0.0088 - val_loss: 1.8757e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1119/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9949e-05 - rmse: 0.0084\n",
      "Epoch 1119: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8565e-05 - rmse: 0.0070 - val_loss: 1.0980e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1120/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1888e-05 - rmse: 0.0072\n",
      "Epoch 1120: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2833e-05 - rmse: 0.0065 - val_loss: 1.1683e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1121/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6015e-06 - rmse: 0.0031\n",
      "Epoch 1121: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9764e-05 - rmse: 0.0063 - val_loss: 1.0143e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1122/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7041e-05 - rmse: 0.0052\n",
      "Epoch 1122: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1615e-05 - rmse: 0.0056 - val_loss: 1.3120e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1123/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2702e-05 - rmse: 0.0036\n",
      "Epoch 1123: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4577e-05 - rmse: 0.0059 - val_loss: 1.3545e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1124/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3938e-05 - rmse: 0.0058\n",
      "Epoch 1124: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9722e-05 - rmse: 0.0063 - val_loss: 1.7802e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1125/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3296e-05 - rmse: 0.0091\n",
      "Epoch 1125: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1000e-05 - rmse: 0.0064 - val_loss: 1.9683e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1126/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4216e-05 - rmse: 0.0080\n",
      "Epoch 1126: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1577e-05 - rmse: 0.0072 - val_loss: 1.8986e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1127/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7075e-05 - rmse: 0.0088\n",
      "Epoch 1127: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5749e-05 - rmse: 0.0068 - val_loss: 1.7449e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1128/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8811e-05 - rmse: 0.0077\n",
      "Epoch 1128: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6255e-05 - rmse: 0.0068 - val_loss: 1.4806e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1129/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3142e-05 - rmse: 0.0066\n",
      "Epoch 1129: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3886e-05 - rmse: 0.0058 - val_loss: 1.4744e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1130/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5390e-05 - rmse: 0.0039\n",
      "Epoch 1130: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8023e-05 - rmse: 0.0062 - val_loss: 1.6462e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1131/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1032e-05 - rmse: 0.0078\n",
      "Epoch 1131: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3410e-05 - rmse: 0.0066 - val_loss: 1.5630e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1132/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7503e-05 - rmse: 0.0069\n",
      "Epoch 1132: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2401e-05 - rmse: 0.0072 - val_loss: 1.6964e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1133/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9466e-05 - rmse: 0.0063\n",
      "Epoch 1133: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4355e-05 - rmse: 0.0067 - val_loss: 2.1071e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1134/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1621e-04 - rmse: 0.0108\n",
      "Epoch 1134: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4947e-05 - rmse: 0.0087 - val_loss: 2.3141e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1135/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1773e-04 - rmse: 0.0109\n",
      "Epoch 1135: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0592e-04 - rmse: 0.0103 - val_loss: 1.9502e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1136/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6919e-05 - rmse: 0.0075\n",
      "Epoch 1136: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0589e-04 - rmse: 0.0103 - val_loss: 2.7285e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 1137/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1228e-04 - rmse: 0.0106\n",
      "Epoch 1137: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3287e-04 - rmse: 0.0115 - val_loss: 1.9548e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1138/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9610e-05 - rmse: 0.0095\n",
      "Epoch 1138: val_loss did not improve from 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0889e-04 - rmse: 0.0104 - val_loss: 2.5236e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1139/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1272e-04 - rmse: 0.0106\n",
      "Epoch 1139: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2433e-04 - rmse: 0.0112 - val_loss: 1.2122e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1140/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9844e-05 - rmse: 0.0055\n",
      "Epoch 1140: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.9669e-05 - rmse: 0.0083 - val_loss: 1.8871e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1141/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8403e-05 - rmse: 0.0053\n",
      "Epoch 1141: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.1080e-05 - rmse: 0.0071 - val_loss: 1.5026e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1142/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7557e-05 - rmse: 0.0061\n",
      "Epoch 1142: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0832e-05 - rmse: 0.0064 - val_loss: 1.2419e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1143/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6783e-05 - rmse: 0.0052\n",
      "Epoch 1143: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1752e-05 - rmse: 0.0056 - val_loss: 1.1373e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1144/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7238e-05 - rmse: 0.0052\n",
      "Epoch 1144: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0434e-05 - rmse: 0.0071 - val_loss: 1.8761e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1145/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4221e-05 - rmse: 0.0092\n",
      "Epoch 1145: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0335e-05 - rmse: 0.0090 - val_loss: 1.5920e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1146/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5731e-05 - rmse: 0.0051\n",
      "Epoch 1146: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4165e-05 - rmse: 0.0049 - val_loss: 1.7691e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1147/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3143e-05 - rmse: 0.0066\n",
      "Epoch 1147: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0651e-05 - rmse: 0.0064 - val_loss: 1.1983e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1148/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6633e-05 - rmse: 0.0093\n",
      "Epoch 1148: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4709e-05 - rmse: 0.0067 - val_loss: 1.2614e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1149/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4519e-05 - rmse: 0.0038\n",
      "Epoch 1149: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7032e-05 - rmse: 0.0052 - val_loss: 1.0140e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1150/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4746e-05 - rmse: 0.0038\n",
      "Epoch 1150: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2159e-05 - rmse: 0.0057 - val_loss: 1.8505e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1151/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1981e-05 - rmse: 0.0079\n",
      "Epoch 1151: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4942e-05 - rmse: 0.0050 - val_loss: 1.0510e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1152/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6142e-05 - rmse: 0.0040\n",
      "Epoch 1152: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9720e-05 - rmse: 0.0044 - val_loss: 1.2539e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1153/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0186e-05 - rmse: 0.0032\n",
      "Epoch 1153: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 1.5415e-05 - rmse: 0.0039 - val_loss: 1.0000e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1154/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2145e-05 - rmse: 0.0047\n",
      "Epoch 1154: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1602e-05 - rmse: 0.0046 - val_loss: 1.5744e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1155/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8039e-05 - rmse: 0.0082\n",
      "Epoch 1155: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4035e-05 - rmse: 0.0049 - val_loss: 1.2919e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1156/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1831e-05 - rmse: 0.0047\n",
      "Epoch 1156: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6438e-05 - rmse: 0.0041 - val_loss: 1.3301e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1157/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2898e-06 - rmse: 0.0027\n",
      "Epoch 1157: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9031e-05 - rmse: 0.0054 - val_loss: 1.3838e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1158/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9952e-05 - rmse: 0.0045\n",
      "Epoch 1158: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7851e-05 - rmse: 0.0062 - val_loss: 1.3489e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1159/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1297e-05 - rmse: 0.0034\n",
      "Epoch 1159: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5399e-05 - rmse: 0.0059 - val_loss: 1.2649e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1160/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6709e-05 - rmse: 0.0041\n",
      "Epoch 1160: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1439e-05 - rmse: 0.0056 - val_loss: 2.0388e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1161/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1994e-05 - rmse: 0.0065\n",
      "Epoch 1161: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.8974e-05 - rmse: 0.0054 - val_loss: 1.4400e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1162/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4657e-05 - rmse: 0.0074\n",
      "Epoch 1162: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7194e-05 - rmse: 0.0061 - val_loss: 1.5981e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1163/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3454e-05 - rmse: 0.0048\n",
      "Epoch 1163: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9386e-05 - rmse: 0.0070 - val_loss: 1.4139e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1164/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9912e-05 - rmse: 0.0055\n",
      "Epoch 1164: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4700e-04 - rmse: 0.0157 - val_loss: 5.9091e-04 - val_rmse: 0.0243 - lr: 1.0000e-04\n",
      "Epoch 1165/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8985e-04 - rmse: 0.0221\n",
      "Epoch 1165: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2416e-04 - rmse: 0.0180 - val_loss: 1.5897e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1166/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3494e-05 - rmse: 0.0073\n",
      "Epoch 1166: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3910e-04 - rmse: 0.0155 - val_loss: 2.4220e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 1167/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1606e-05 - rmse: 0.0085\n",
      "Epoch 1167: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8354e-04 - rmse: 0.0135 - val_loss: 2.3452e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 1168/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5580e-05 - rmse: 0.0075\n",
      "Epoch 1168: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5473e-04 - rmse: 0.0256 - val_loss: 7.9425e-04 - val_rmse: 0.0282 - lr: 1.0000e-04\n",
      "Epoch 1169/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8170e-04 - rmse: 0.0261\n",
      "Epoch 1169: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0323 - val_loss: 0.0016 - val_rmse: 0.0405 - lr: 1.0000e-04\n",
      "Epoch 1170/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7491e-04 - rmse: 0.0312\n",
      "Epoch 1170: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0408 - val_loss: 8.2587e-04 - val_rmse: 0.0287 - lr: 1.0000e-04\n",
      "Epoch 1171/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3407e-04 - rmse: 0.0231\n",
      "Epoch 1171: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0013 - rmse: 0.0362 - val_loss: 5.5010e-04 - val_rmse: 0.0235 - lr: 1.0000e-04\n",
      "Epoch 1172/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8705e-04 - rmse: 0.0197\n",
      "Epoch 1172: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0407 - val_loss: 3.4160e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 1173/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6856e-04 - rmse: 0.0164\n",
      "Epoch 1173: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0390 - val_loss: 2.8198e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 1174/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2208e-04 - rmse: 0.0205\n",
      "Epoch 1174: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0011 - rmse: 0.0333 - val_loss: 3.2746e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 1175/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5564e-04 - rmse: 0.0125\n",
      "Epoch 1175: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0397e-04 - rmse: 0.0174 - val_loss: 7.3417e-04 - val_rmse: 0.0271 - lr: 1.0000e-04\n",
      "Epoch 1176/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6235e-04 - rmse: 0.0162\n",
      "Epoch 1176: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5238e-04 - rmse: 0.0188 - val_loss: 2.4633e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1177/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8053e-04 - rmse: 0.0195\n",
      "Epoch 1177: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0566e-04 - rmse: 0.0143 - val_loss: 2.6557e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1178/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3902e-04 - rmse: 0.0118\n",
      "Epoch 1178: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1694e-04 - rmse: 0.0108 - val_loss: 1.8124e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1179/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6604e-05 - rmse: 0.0052\n",
      "Epoch 1179: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3111e-04 - rmse: 0.0115 - val_loss: 4.1092e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 1180/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8376e-04 - rmse: 0.0136\n",
      "Epoch 1180: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3069e-04 - rmse: 0.0114 - val_loss: 1.8863e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1181/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8088e-05 - rmse: 0.0076\n",
      "Epoch 1181: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3141e-05 - rmse: 0.0079 - val_loss: 1.7872e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1182/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9857e-05 - rmse: 0.0071\n",
      "Epoch 1182: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3401e-05 - rmse: 0.0073 - val_loss: 1.8725e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1183/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6848e-05 - rmse: 0.0052\n",
      "Epoch 1183: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9925e-05 - rmse: 0.0071 - val_loss: 1.7119e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1184/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5324e-05 - rmse: 0.0081\n",
      "Epoch 1184: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1372e-05 - rmse: 0.0078 - val_loss: 1.9085e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1185/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6727e-05 - rmse: 0.0061\n",
      "Epoch 1185: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1015e-05 - rmse: 0.0071 - val_loss: 1.4331e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1186/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6582e-05 - rmse: 0.0041\n",
      "Epoch 1186: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0038e-05 - rmse: 0.0071 - val_loss: 1.5347e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1187/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5655e-05 - rmse: 0.0040\n",
      "Epoch 1187: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1980e-05 - rmse: 0.0065 - val_loss: 1.5735e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1188/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4841e-05 - rmse: 0.0050\n",
      "Epoch 1188: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9972e-05 - rmse: 0.0063 - val_loss: 1.6024e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1189/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8125e-05 - rmse: 0.0069\n",
      "Epoch 1189: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9683e-05 - rmse: 0.0054 - val_loss: 1.4281e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1190/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2649e-05 - rmse: 0.0065\n",
      "Epoch 1190: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4268e-05 - rmse: 0.0059 - val_loss: 1.3780e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1191/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9210e-05 - rmse: 0.0054\n",
      "Epoch 1191: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7997e-05 - rmse: 0.0053 - val_loss: 1.4471e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1192/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2953e-05 - rmse: 0.0057\n",
      "Epoch 1192: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5291e-05 - rmse: 0.0059 - val_loss: 1.4171e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1193/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2052e-05 - rmse: 0.0047\n",
      "Epoch 1193: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2618e-05 - rmse: 0.0065 - val_loss: 1.8407e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1194/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1231e-05 - rmse: 0.0056\n",
      "Epoch 1194: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8840e-05 - rmse: 0.0054 - val_loss: 1.6237e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1195/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0612e-05 - rmse: 0.0045\n",
      "Epoch 1195: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4400e-05 - rmse: 0.0049 - val_loss: 1.8328e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1196/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7131e-05 - rmse: 0.0041\n",
      "Epoch 1196: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5588e-05 - rmse: 0.0060 - val_loss: 1.4180e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1197/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1499e-05 - rmse: 0.0046\n",
      "Epoch 1197: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8707e-05 - rmse: 0.0062 - val_loss: 1.3729e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1198/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4777e-05 - rmse: 0.0038\n",
      "Epoch 1198: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2394e-05 - rmse: 0.0057 - val_loss: 1.5067e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1199/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8537e-05 - rmse: 0.0043\n",
      "Epoch 1199: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3564e-05 - rmse: 0.0049 - val_loss: 1.5062e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1200/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3562e-05 - rmse: 0.0037\n",
      "Epoch 1200: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9973e-05 - rmse: 0.0055 - val_loss: 1.7414e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1201/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9433e-05 - rmse: 0.0070\n",
      "Epoch 1201: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0162e-05 - rmse: 0.0071 - val_loss: 1.9692e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1202/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8274e-05 - rmse: 0.0053\n",
      "Epoch 1202: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5828e-05 - rmse: 0.0068 - val_loss: 1.4167e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1203/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2747e-05 - rmse: 0.0057\n",
      "Epoch 1203: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4852e-05 - rmse: 0.0059 - val_loss: 1.6126e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1204/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0707e-05 - rmse: 0.0033\n",
      "Epoch 1204: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7921e-05 - rmse: 0.0069 - val_loss: 1.7934e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1205/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0052e-04 - rmse: 0.0100\n",
      "Epoch 1205: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2877e-05 - rmse: 0.0065 - val_loss: 1.6681e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1206/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6236e-05 - rmse: 0.0060\n",
      "Epoch 1206: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1658e-05 - rmse: 0.0056 - val_loss: 1.6695e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1207/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1219e-05 - rmse: 0.0064\n",
      "Epoch 1207: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1721e-05 - rmse: 0.0065 - val_loss: 1.8579e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1208/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5728e-05 - rmse: 0.0051\n",
      "Epoch 1208: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2084e-05 - rmse: 0.0065 - val_loss: 1.1413e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1209/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8223e-05 - rmse: 0.0043\n",
      "Epoch 1209: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5375e-05 - rmse: 0.0059 - val_loss: 1.7205e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1210/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5826e-05 - rmse: 0.0075\n",
      "Epoch 1210: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4436e-05 - rmse: 0.0049 - val_loss: 1.6935e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1211/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7096e-05 - rmse: 0.0041\n",
      "Epoch 1211: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7149e-05 - rmse: 0.0052 - val_loss: 1.9107e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1212/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4413e-05 - rmse: 0.0049\n",
      "Epoch 1212: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0678e-05 - rmse: 0.0045 - val_loss: 1.5556e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1213/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7362e-05 - rmse: 0.0052\n",
      "Epoch 1213: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9901e-05 - rmse: 0.0045 - val_loss: 1.3469e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1214/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9351e-05 - rmse: 0.0054\n",
      "Epoch 1214: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6132e-05 - rmse: 0.0040 - val_loss: 1.6961e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1215/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3351e-05 - rmse: 0.0066\n",
      "Epoch 1215: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2408e-05 - rmse: 0.0047 - val_loss: 1.3839e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1216/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0374e-05 - rmse: 0.0032\n",
      "Epoch 1216: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1088e-05 - rmse: 0.0046 - val_loss: 1.5455e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1217/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2716e-05 - rmse: 0.0036\n",
      "Epoch 1217: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6342e-05 - rmse: 0.0040 - val_loss: 1.4267e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1218/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7406e-06 - rmse: 0.0028\n",
      "Epoch 1218: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0539e-05 - rmse: 0.0045 - val_loss: 1.7425e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1219/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9371e-05 - rmse: 0.0054\n",
      "Epoch 1219: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5713e-05 - rmse: 0.0060 - val_loss: 1.0915e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1220/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7168e-05 - rmse: 0.0041\n",
      "Epoch 1220: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3071e-05 - rmse: 0.0058 - val_loss: 1.7744e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1221/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4009e-05 - rmse: 0.0037\n",
      "Epoch 1221: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7707e-05 - rmse: 0.0061 - val_loss: 1.2064e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1222/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7299e-05 - rmse: 0.0042\n",
      "Epoch 1222: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5979e-05 - rmse: 0.0040 - val_loss: 2.1220e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1223/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4398e-05 - rmse: 0.0067\n",
      "Epoch 1223: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6876e-05 - rmse: 0.0061 - val_loss: 1.2778e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1224/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7801e-05 - rmse: 0.0088\n",
      "Epoch 1224: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4481e-05 - rmse: 0.0067 - val_loss: 2.0151e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1225/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2962e-05 - rmse: 0.0073\n",
      "Epoch 1225: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7215e-05 - rmse: 0.0061 - val_loss: 1.0298e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1226/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5603e-05 - rmse: 0.0040\n",
      "Epoch 1226: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1153e-05 - rmse: 0.0056 - val_loss: 1.4955e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1227/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1967e-05 - rmse: 0.0035\n",
      "Epoch 1227: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9845e-05 - rmse: 0.0063 - val_loss: 1.4821e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1228/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7940e-05 - rmse: 0.0069\n",
      "Epoch 1228: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6171e-05 - rmse: 0.0051 - val_loss: 1.3644e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1229/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0929e-05 - rmse: 0.0033\n",
      "Epoch 1229: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8881e-05 - rmse: 0.0043 - val_loss: 1.4860e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1230/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8265e-05 - rmse: 0.0053\n",
      "Epoch 1230: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9899e-05 - rmse: 0.0055 - val_loss: 1.5726e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1231/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8156e-05 - rmse: 0.0069\n",
      "Epoch 1231: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8806e-05 - rmse: 0.0070 - val_loss: 1.4444e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1232/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7646e-05 - rmse: 0.0042\n",
      "Epoch 1232: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0183e-05 - rmse: 0.0063 - val_loss: 1.6488e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1233/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0658e-05 - rmse: 0.0078\n",
      "Epoch 1233: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8928e-05 - rmse: 0.0070 - val_loss: 1.5532e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1234/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1006e-05 - rmse: 0.0056\n",
      "Epoch 1234: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7331e-05 - rmse: 0.0052 - val_loss: 1.1159e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1235/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9625e-05 - rmse: 0.0054\n",
      "Epoch 1235: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1054e-05 - rmse: 0.0046 - val_loss: 1.4504e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1236/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3011e-05 - rmse: 0.0036\n",
      "Epoch 1236: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9597e-05 - rmse: 0.0044 - val_loss: 1.3502e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1237/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2056e-05 - rmse: 0.0057\n",
      "Epoch 1237: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0116e-05 - rmse: 0.0045 - val_loss: 1.5829e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1238/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0830e-05 - rmse: 0.0033\n",
      "Epoch 1238: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4154e-05 - rmse: 0.0049 - val_loss: 1.7872e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1239/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0952e-05 - rmse: 0.0090\n",
      "Epoch 1239: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7458e-05 - rmse: 0.0069 - val_loss: 1.9558e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1240/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3329e-05 - rmse: 0.0097\n",
      "Epoch 1240: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0614e-05 - rmse: 0.0078 - val_loss: 1.7120e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1241/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9797e-05 - rmse: 0.0077\n",
      "Epoch 1241: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2473e-05 - rmse: 0.0072 - val_loss: 1.7594e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1242/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1512e-05 - rmse: 0.0056\n",
      "Epoch 1242: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1527e-05 - rmse: 0.0056 - val_loss: 1.3420e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1243/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6386e-05 - rmse: 0.0051\n",
      "Epoch 1243: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2861e-05 - rmse: 0.0048 - val_loss: 1.4614e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1244/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7747e-05 - rmse: 0.0061\n",
      "Epoch 1244: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0372e-05 - rmse: 0.0045 - val_loss: 1.2324e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1245/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7530e-05 - rmse: 0.0042\n",
      "Epoch 1245: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5304e-05 - rmse: 0.0039 - val_loss: 1.1946e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1246/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5044e-05 - rmse: 0.0039\n",
      "Epoch 1246: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9250e-05 - rmse: 0.0044 - val_loss: 1.3988e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1247/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7488e-06 - rmse: 0.0022\n",
      "Epoch 1247: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8446e-05 - rmse: 0.0043 - val_loss: 1.2405e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1248/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9958e-06 - rmse: 0.0020\n",
      "Epoch 1248: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7599e-05 - rmse: 0.0042 - val_loss: 1.4559e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1249/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1496e-05 - rmse: 0.0056\n",
      "Epoch 1249: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0683e-05 - rmse: 0.0064 - val_loss: 1.3206e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1250/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3006e-05 - rmse: 0.0057\n",
      "Epoch 1250: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0857e-05 - rmse: 0.0095 - val_loss: 1.5807e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1251/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1116e-05 - rmse: 0.0046\n",
      "Epoch 1251: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1098e-05 - rmse: 0.0095 - val_loss: 1.3235e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1252/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9840e-05 - rmse: 0.0045\n",
      "Epoch 1252: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5679e-04 - rmse: 0.0125 - val_loss: 1.7956e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1253/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7933e-05 - rmse: 0.0076\n",
      "Epoch 1253: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5515e-04 - rmse: 0.0125 - val_loss: 1.8749e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1254/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4250e-05 - rmse: 0.0074\n",
      "Epoch 1254: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1129e-04 - rmse: 0.0105 - val_loss: 1.3623e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1255/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1174e-05 - rmse: 0.0033\n",
      "Epoch 1255: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8079e-05 - rmse: 0.0088 - val_loss: 2.5972e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 1256/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3189e-04 - rmse: 0.0115\n",
      "Epoch 1256: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1681e-04 - rmse: 0.0147 - val_loss: 1.4509e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1257/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2095e-05 - rmse: 0.0035\n",
      "Epoch 1257: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9372e-04 - rmse: 0.0171 - val_loss: 4.3316e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 1258/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0478e-04 - rmse: 0.0143\n",
      "Epoch 1258: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7252e-04 - rmse: 0.0193 - val_loss: 2.0168e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1259/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9391e-04 - rmse: 0.0139\n",
      "Epoch 1259: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1004e-04 - rmse: 0.0176 - val_loss: 1.2411e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1260/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7338e-05 - rmse: 0.0088\n",
      "Epoch 1260: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5382e-04 - rmse: 0.0124 - val_loss: 1.3089e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1261/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8792e-05 - rmse: 0.0070\n",
      "Epoch 1261: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.2414e-05 - rmse: 0.0096 - val_loss: 1.1919e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1262/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2512e-05 - rmse: 0.0047\n",
      "Epoch 1262: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8615e-05 - rmse: 0.0062 - val_loss: 1.3697e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1263/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3179e-05 - rmse: 0.0066\n",
      "Epoch 1263: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8793e-05 - rmse: 0.0062 - val_loss: 1.5784e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1264/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9952e-05 - rmse: 0.0045\n",
      "Epoch 1264: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2326e-05 - rmse: 0.0057 - val_loss: 1.3235e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1265/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9098e-05 - rmse: 0.0044\n",
      "Epoch 1265: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1377e-05 - rmse: 0.0056 - val_loss: 1.1292e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1266/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4072e-05 - rmse: 0.0038\n",
      "Epoch 1266: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4044e-05 - rmse: 0.0058 - val_loss: 1.2777e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1267/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4273e-05 - rmse: 0.0059\n",
      "Epoch 1267: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0080e-05 - rmse: 0.0055 - val_loss: 1.2746e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1268/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5006e-05 - rmse: 0.0039\n",
      "Epoch 1268: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9974e-05 - rmse: 0.0055 - val_loss: 1.5419e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1269/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8280e-05 - rmse: 0.0076\n",
      "Epoch 1269: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4001e-05 - rmse: 0.0058 - val_loss: 1.1893e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1270/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2852e-05 - rmse: 0.0036\n",
      "Epoch 1270: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7032e-05 - rmse: 0.0041 - val_loss: 1.1496e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1271/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4937e-05 - rmse: 0.0039\n",
      "Epoch 1271: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6365e-05 - rmse: 0.0051 - val_loss: 1.1747e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1272/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6769e-05 - rmse: 0.0052\n",
      "Epoch 1272: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3792e-05 - rmse: 0.0058 - val_loss: 1.3412e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1273/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3677e-05 - rmse: 0.0037\n",
      "Epoch 1273: val_loss improved from 0.00010 to 0.00010, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 4.3274e-05 - rmse: 0.0066 - val_loss: 9.6475e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1274/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0480e-05 - rmse: 0.0032\n",
      "Epoch 1274: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7972e-05 - rmse: 0.0062 - val_loss: 1.6235e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1275/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1657e-05 - rmse: 0.0072\n",
      "Epoch 1275: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7594e-05 - rmse: 0.0061 - val_loss: 1.2792e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1276/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3194e-05 - rmse: 0.0036\n",
      "Epoch 1276: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0108e-05 - rmse: 0.0078 - val_loss: 2.6294e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1277/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2245e-04 - rmse: 0.0111\n",
      "Epoch 1277: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.0695e-05 - rmse: 0.0090 - val_loss: 1.9254e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1278/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8059e-04 - rmse: 0.0168\n",
      "Epoch 1278: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7302e-04 - rmse: 0.0132 - val_loss: 2.5386e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1279/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1097e-04 - rmse: 0.0105\n",
      "Epoch 1279: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2291e-04 - rmse: 0.0111 - val_loss: 2.0135e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1280/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1246e-05 - rmse: 0.0056\n",
      "Epoch 1280: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3808e-05 - rmse: 0.0097 - val_loss: 2.8957e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 1281/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1767e-04 - rmse: 0.0108\n",
      "Epoch 1281: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8811e-04 - rmse: 0.0137 - val_loss: 1.7595e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1282/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9044e-04 - rmse: 0.0170\n",
      "Epoch 1282: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9431e-04 - rmse: 0.0139 - val_loss: 2.8094e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 1283/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3298e-04 - rmse: 0.0153\n",
      "Epoch 1283: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0472e-04 - rmse: 0.0102 - val_loss: 1.2250e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1284/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0288e-05 - rmse: 0.0055\n",
      "Epoch 1284: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3605e-05 - rmse: 0.0091 - val_loss: 2.4473e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 1285/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1212e-04 - rmse: 0.0106\n",
      "Epoch 1285: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8854e-05 - rmse: 0.0077 - val_loss: 1.1355e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1286/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2872e-05 - rmse: 0.0079\n",
      "Epoch 1286: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0090e-05 - rmse: 0.0063 - val_loss: 1.4140e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1287/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1229e-05 - rmse: 0.0056\n",
      "Epoch 1287: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3644e-05 - rmse: 0.0058 - val_loss: 1.1188e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1288/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9822e-05 - rmse: 0.0063\n",
      "Epoch 1288: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0488e-05 - rmse: 0.0045 - val_loss: 1.5284e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1289/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0424e-05 - rmse: 0.0045\n",
      "Epoch 1289: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9758e-05 - rmse: 0.0044 - val_loss: 1.7372e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1290/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3462e-05 - rmse: 0.0066\n",
      "Epoch 1290: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2671e-05 - rmse: 0.0057 - val_loss: 1.0712e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1291/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3746e-06 - rmse: 0.0029\n",
      "Epoch 1291: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2414e-05 - rmse: 0.0047 - val_loss: 1.1688e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1292/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6027e-05 - rmse: 0.0040\n",
      "Epoch 1292: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0875e-05 - rmse: 0.0056 - val_loss: 1.0358e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1293/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7384e-06 - rmse: 0.0028\n",
      "Epoch 1293: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1396e-05 - rmse: 0.0046 - val_loss: 1.4593e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1294/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3547e-05 - rmse: 0.0049\n",
      "Epoch 1294: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3803e-05 - rmse: 0.0049 - val_loss: 1.3359e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1295/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7378e-06 - rmse: 0.0031\n",
      "Epoch 1295: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1569e-05 - rmse: 0.0046 - val_loss: 1.2732e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1296/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2498e-05 - rmse: 0.0035\n",
      "Epoch 1296: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9066e-05 - rmse: 0.0044 - val_loss: 1.0818e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1297/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0723e-05 - rmse: 0.0033\n",
      "Epoch 1297: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0393e-05 - rmse: 0.0045 - val_loss: 1.2327e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1298/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0451e-06 - rmse: 0.0030\n",
      "Epoch 1298: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9901e-05 - rmse: 0.0045 - val_loss: 1.4360e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1299/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3148e-05 - rmse: 0.0036\n",
      "Epoch 1299: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0748e-05 - rmse: 0.0046 - val_loss: 1.4938e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1300/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6863e-05 - rmse: 0.0052\n",
      "Epoch 1300: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3072e-05 - rmse: 0.0066 - val_loss: 1.5892e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1301/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2236e-05 - rmse: 0.0057\n",
      "Epoch 1301: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7540e-05 - rmse: 0.0052 - val_loss: 1.2968e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1302/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3729e-05 - rmse: 0.0066\n",
      "Epoch 1302: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0718e-05 - rmse: 0.0046 - val_loss: 1.2040e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1303/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2785e-06 - rmse: 0.0027\n",
      "Epoch 1303: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3859e-05 - rmse: 0.0037 - val_loss: 1.1146e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1304/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5606e-05 - rmse: 0.0051\n",
      "Epoch 1304: val_loss did not improve from 0.00010\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4977e-05 - rmse: 0.0039 - val_loss: 1.2839e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1305/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5373e-05 - rmse: 0.0039\n",
      "Epoch 1305: val_loss improved from 0.00010 to 0.00009, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 41ms/step - loss: 1.2984e-05 - rmse: 0.0036 - val_loss: 9.4403e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1306/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8207e-06 - rmse: 0.0026\n",
      "Epoch 1306: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7203e-05 - rmse: 0.0041 - val_loss: 1.3302e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1307/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2226e-06 - rmse: 0.0025\n",
      "Epoch 1307: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7265e-05 - rmse: 0.0042 - val_loss: 1.0520e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1308/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1396e-05 - rmse: 0.0034\n",
      "Epoch 1308: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5937e-05 - rmse: 0.0040 - val_loss: 1.1478e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1309/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3527e-05 - rmse: 0.0037\n",
      "Epoch 1309: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2907e-05 - rmse: 0.0036 - val_loss: 1.1921e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1310/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5374e-05 - rmse: 0.0059\n",
      "Epoch 1310: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0922e-05 - rmse: 0.0046 - val_loss: 1.2128e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1311/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3757e-05 - rmse: 0.0049\n",
      "Epoch 1311: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1756e-05 - rmse: 0.0034 - val_loss: 1.1907e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1312/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2346e-05 - rmse: 0.0035\n",
      "Epoch 1312: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5978e-05 - rmse: 0.0040 - val_loss: 1.2556e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1313/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0145e-05 - rmse: 0.0055\n",
      "Epoch 1313: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1532e-05 - rmse: 0.0046 - val_loss: 1.2528e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1314/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5146e-05 - rmse: 0.0039\n",
      "Epoch 1314: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5413e-05 - rmse: 0.0039 - val_loss: 1.1622e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1315/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4359e-05 - rmse: 0.0038\n",
      "Epoch 1315: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4187e-05 - rmse: 0.0038 - val_loss: 1.2433e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1316/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2827e-05 - rmse: 0.0036\n",
      "Epoch 1316: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2757e-05 - rmse: 0.0036 - val_loss: 1.3021e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1317/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8608e-05 - rmse: 0.0043\n",
      "Epoch 1317: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2664e-05 - rmse: 0.0048 - val_loss: 1.2553e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1318/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2694e-06 - rmse: 0.0027\n",
      "Epoch 1318: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5991e-05 - rmse: 0.0051 - val_loss: 1.0437e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1319/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3119e-05 - rmse: 0.0079\n",
      "Epoch 1319: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8976e-05 - rmse: 0.0083 - val_loss: 1.5398e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1320/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7821e-05 - rmse: 0.0053\n",
      "Epoch 1320: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5335e-05 - rmse: 0.0059 - val_loss: 1.2787e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1321/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3902e-05 - rmse: 0.0073\n",
      "Epoch 1321: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5193e-05 - rmse: 0.0050 - val_loss: 1.3764e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1322/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8317e-05 - rmse: 0.0043\n",
      "Epoch 1322: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2404e-05 - rmse: 0.0047 - val_loss: 1.3965e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1323/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9095e-05 - rmse: 0.0044\n",
      "Epoch 1323: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8596e-05 - rmse: 0.0043 - val_loss: 1.3678e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1324/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7848e-05 - rmse: 0.0042\n",
      "Epoch 1324: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6115e-05 - rmse: 0.0040 - val_loss: 1.3036e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1325/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4793e-06 - rmse: 0.0023\n",
      "Epoch 1325: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4334e-05 - rmse: 0.0038 - val_loss: 1.1458e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1326/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7682e-06 - rmse: 0.0031\n",
      "Epoch 1326: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1108e-05 - rmse: 0.0033 - val_loss: 1.3056e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1327/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1708e-06 - rmse: 0.0029\n",
      "Epoch 1327: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3554e-05 - rmse: 0.0037 - val_loss: 1.1735e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1328/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2753e-05 - rmse: 0.0057\n",
      "Epoch 1328: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0038e-05 - rmse: 0.0045 - val_loss: 1.5823e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1329/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2000e-05 - rmse: 0.0035\n",
      "Epoch 1329: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6662e-05 - rmse: 0.0041 - val_loss: 9.5668e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1330/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4465e-05 - rmse: 0.0049\n",
      "Epoch 1330: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6004e-05 - rmse: 0.0040 - val_loss: 1.3602e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1331/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0089e-05 - rmse: 0.0032\n",
      "Epoch 1331: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2454e-05 - rmse: 0.0035 - val_loss: 1.0180e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1332/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0618e-05 - rmse: 0.0033\n",
      "Epoch 1332: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2121e-05 - rmse: 0.0035 - val_loss: 1.4960e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1333/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3181e-05 - rmse: 0.0048\n",
      "Epoch 1333: val_loss did not improve from 0.00009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7088e-05 - rmse: 0.0041 - val_loss: 1.1376e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1334/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3627e-05 - rmse: 0.0037\n",
      "Epoch 1334: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0744e-05 - rmse: 0.0046 - val_loss: 1.5745e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1335/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0742e-05 - rmse: 0.0055\n",
      "Epoch 1335: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2970e-05 - rmse: 0.0048 - val_loss: 1.6038e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1336/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0163e-04 - rmse: 0.0101\n",
      "Epoch 1336: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8838e-05 - rmse: 0.0077 - val_loss: 1.5740e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1337/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0868e-05 - rmse: 0.0033\n",
      "Epoch 1337: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7845e-05 - rmse: 0.0062 - val_loss: 1.1989e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1338/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6048e-05 - rmse: 0.0040\n",
      "Epoch 1338: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0615e-05 - rmse: 0.0064 - val_loss: 1.3905e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1339/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0479e-05 - rmse: 0.0064\n",
      "Epoch 1339: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1583e-05 - rmse: 0.0064 - val_loss: 2.1360e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1340/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0827e-05 - rmse: 0.0095\n",
      "Epoch 1340: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5635e-05 - rmse: 0.0068 - val_loss: 1.3559e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1341/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4835e-05 - rmse: 0.0050\n",
      "Epoch 1341: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8342e-05 - rmse: 0.0053 - val_loss: 1.4489e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1342/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4784e-05 - rmse: 0.0038\n",
      "Epoch 1342: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3567e-05 - rmse: 0.0058 - val_loss: 1.2036e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1343/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8955e-05 - rmse: 0.0044\n",
      "Epoch 1343: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5263e-05 - rmse: 0.0074 - val_loss: 2.1702e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1344/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7325e-05 - rmse: 0.0076\n",
      "Epoch 1344: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6644e-05 - rmse: 0.0061 - val_loss: 1.8048e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1345/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4288e-05 - rmse: 0.0086\n",
      "Epoch 1345: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7110e-05 - rmse: 0.0076 - val_loss: 1.8958e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1346/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4828e-05 - rmse: 0.0087\n",
      "Epoch 1346: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8831e-05 - rmse: 0.0062 - val_loss: 1.6894e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1347/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9230e-05 - rmse: 0.0077\n",
      "Epoch 1347: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9122e-05 - rmse: 0.0054 - val_loss: 1.6092e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1348/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2891e-05 - rmse: 0.0057\n",
      "Epoch 1348: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7722e-05 - rmse: 0.0053 - val_loss: 9.9847e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1349/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2317e-05 - rmse: 0.0035\n",
      "Epoch 1349: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8183e-05 - rmse: 0.0043 - val_loss: 1.7364e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1350/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8878e-05 - rmse: 0.0043\n",
      "Epoch 1350: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7930e-05 - rmse: 0.0053 - val_loss: 1.0860e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1351/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8637e-05 - rmse: 0.0089\n",
      "Epoch 1351: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1685e-05 - rmse: 0.0065 - val_loss: 1.9681e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1352/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2980e-05 - rmse: 0.0066\n",
      "Epoch 1352: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0349e-05 - rmse: 0.0064 - val_loss: 1.1604e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1353/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1305e-05 - rmse: 0.0064\n",
      "Epoch 1353: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3344e-05 - rmse: 0.0066 - val_loss: 1.2275e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1354/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9889e-06 - rmse: 0.0030\n",
      "Epoch 1354: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.8911e-05 - rmse: 0.0054 - val_loss: 1.4485e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1355/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2782e-05 - rmse: 0.0073\n",
      "Epoch 1355: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4538e-05 - rmse: 0.0050 - val_loss: 1.2427e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1356/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5393e-05 - rmse: 0.0039\n",
      "Epoch 1356: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8170e-05 - rmse: 0.0043 - val_loss: 1.0376e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1357/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0012e-05 - rmse: 0.0045\n",
      "Epoch 1357: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1207e-05 - rmse: 0.0033 - val_loss: 1.2207e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1358/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6703e-05 - rmse: 0.0052\n",
      "Epoch 1358: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 2.2367e-05 - rmse: 0.0047 - val_loss: 9.3063e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1359/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2824e-05 - rmse: 0.0036\n",
      "Epoch 1359: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3081e-05 - rmse: 0.0048 - val_loss: 1.2032e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1360/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1149e-05 - rmse: 0.0046\n",
      "Epoch 1360: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9003e-05 - rmse: 0.0054 - val_loss: 1.7093e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1361/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6715e-05 - rmse: 0.0082\n",
      "Epoch 1361: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4089e-05 - rmse: 0.0074 - val_loss: 9.4365e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1362/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1756e-05 - rmse: 0.0047\n",
      "Epoch 1362: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2392e-05 - rmse: 0.0057 - val_loss: 2.0732e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1363/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4121e-05 - rmse: 0.0074\n",
      "Epoch 1363: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8093e-05 - rmse: 0.0069 - val_loss: 1.8945e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1364/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1980e-04 - rmse: 0.0109\n",
      "Epoch 1364: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.3638e-05 - rmse: 0.0073 - val_loss: 1.5312e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1365/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5885e-05 - rmse: 0.0051\n",
      "Epoch 1365: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6647e-05 - rmse: 0.0052 - val_loss: 9.7119e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1366/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2071e-06 - rmse: 0.0027\n",
      "Epoch 1366: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8018e-05 - rmse: 0.0053 - val_loss: 1.2960e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1367/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7645e-06 - rmse: 0.0031\n",
      "Epoch 1367: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0726e-05 - rmse: 0.0055 - val_loss: 1.1532e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1368/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8852e-05 - rmse: 0.0062\n",
      "Epoch 1368: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7246e-05 - rmse: 0.0061 - val_loss: 1.3390e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1369/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0889e-05 - rmse: 0.0033\n",
      "Epoch 1369: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2507e-05 - rmse: 0.0035 - val_loss: 9.3793e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1370/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4182e-05 - rmse: 0.0038\n",
      "Epoch 1370: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3759e-05 - rmse: 0.0037 - val_loss: 1.2806e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1371/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2213e-05 - rmse: 0.0035\n",
      "Epoch 1371: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8604e-05 - rmse: 0.0053 - val_loss: 1.0648e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1372/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0154e-06 - rmse: 0.0026\n",
      "Epoch 1372: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3449e-05 - rmse: 0.0048 - val_loss: 1.1952e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1373/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1426e-06 - rmse: 0.0027\n",
      "Epoch 1373: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0836e-05 - rmse: 0.0046 - val_loss: 1.1672e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1374/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2362e-05 - rmse: 0.0035\n",
      "Epoch 1374: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2821e-05 - rmse: 0.0048 - val_loss: 1.3259e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1375/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3042e-05 - rmse: 0.0073\n",
      "Epoch 1375: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4946e-05 - rmse: 0.0050 - val_loss: 1.3491e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1376/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5992e-05 - rmse: 0.0051\n",
      "Epoch 1376: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7143e-05 - rmse: 0.0041 - val_loss: 1.5117e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1377/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5925e-05 - rmse: 0.0060\n",
      "Epoch 1377: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3216e-05 - rmse: 0.0058 - val_loss: 1.3432e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1378/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9758e-05 - rmse: 0.0044\n",
      "Epoch 1378: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 3.0296e-05 - rmse: 0.0055 - val_loss: 9.2324e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1379/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5247e-06 - rmse: 0.0024\n",
      "Epoch 1379: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7505e-05 - rmse: 0.0052 - val_loss: 1.9835e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 1380/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7554e-05 - rmse: 0.0052\n",
      "Epoch 1380: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6133e-05 - rmse: 0.0075 - val_loss: 1.2205e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1381/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4731e-04 - rmse: 0.0121\n",
      "Epoch 1381: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8374e-05 - rmse: 0.0089 - val_loss: 2.0009e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 1382/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1915e-04 - rmse: 0.0109\n",
      "Epoch 1382: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7383e-05 - rmse: 0.0088 - val_loss: 9.9067e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1383/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0650e-05 - rmse: 0.0064\n",
      "Epoch 1383: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7001e-05 - rmse: 0.0061 - val_loss: 1.6039e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1384/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5771e-05 - rmse: 0.0051\n",
      "Epoch 1384: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9197e-05 - rmse: 0.0054 - val_loss: 1.1274e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1385/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5883e-05 - rmse: 0.0081\n",
      "Epoch 1385: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1313e-05 - rmse: 0.0064 - val_loss: 1.5925e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1386/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5187e-05 - rmse: 0.0050\n",
      "Epoch 1386: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0621e-05 - rmse: 0.0045 - val_loss: 1.0049e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1387/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9654e-05 - rmse: 0.0044\n",
      "Epoch 1387: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7629e-05 - rmse: 0.0042 - val_loss: 1.4762e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1388/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0895e-05 - rmse: 0.0056\n",
      "Epoch 1388: val_loss improved from 0.00009 to 0.00009, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 30ms/step - loss: 2.3609e-05 - rmse: 0.0049 - val_loss: 8.6212e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1389/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2061e-05 - rmse: 0.0057\n",
      "Epoch 1389: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0175e-05 - rmse: 0.0055 - val_loss: 1.4441e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1390/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9347e-05 - rmse: 0.0044\n",
      "Epoch 1390: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3371e-05 - rmse: 0.0058 - val_loss: 1.4110e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1391/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8777e-05 - rmse: 0.0089\n",
      "Epoch 1391: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4095e-05 - rmse: 0.0058 - val_loss: 1.4133e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1392/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0987e-05 - rmse: 0.0046\n",
      "Epoch 1392: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2701e-05 - rmse: 0.0048 - val_loss: 8.8987e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1393/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4040e-05 - rmse: 0.0037\n",
      "Epoch 1393: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3516e-05 - rmse: 0.0048 - val_loss: 1.3864e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1394/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0524e-05 - rmse: 0.0032\n",
      "Epoch 1394: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0810e-05 - rmse: 0.0033 - val_loss: 9.0591e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1395/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2202e-05 - rmse: 0.0035\n",
      "Epoch 1395: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8646e-05 - rmse: 0.0043 - val_loss: 1.1331e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1396/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1078e-06 - rmse: 0.0023\n",
      "Epoch 1396: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9815e-05 - rmse: 0.0045 - val_loss: 9.4215e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1397/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6553e-05 - rmse: 0.0041\n",
      "Epoch 1397: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7378e-05 - rmse: 0.0042 - val_loss: 1.3016e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1398/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5464e-06 - rmse: 0.0026\n",
      "Epoch 1398: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7207e-05 - rmse: 0.0041 - val_loss: 1.3760e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1399/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8724e-05 - rmse: 0.0043\n",
      "Epoch 1399: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4587e-05 - rmse: 0.0038 - val_loss: 1.1170e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1400/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2623e-05 - rmse: 0.0036\n",
      "Epoch 1400: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4083e-06 - rmse: 0.0031 - val_loss: 1.1098e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1401/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4377e-05 - rmse: 0.0038\n",
      "Epoch 1401: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3249e-05 - rmse: 0.0036 - val_loss: 1.0033e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1402/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0647e-05 - rmse: 0.0033\n",
      "Epoch 1402: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9912e-06 - rmse: 0.0028 - val_loss: 1.1616e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1403/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3308e-06 - rmse: 0.0023\n",
      "Epoch 1403: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5398e-06 - rmse: 0.0031 - val_loss: 1.1322e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1404/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6444e-06 - rmse: 0.0029\n",
      "Epoch 1404: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1528e-05 - rmse: 0.0034 - val_loss: 1.0764e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1405/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7375e-06 - rmse: 0.0031\n",
      "Epoch 1405: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2320e-05 - rmse: 0.0035 - val_loss: 1.0834e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1406/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4399e-06 - rmse: 0.0021\n",
      "Epoch 1406: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0127e-05 - rmse: 0.0032 - val_loss: 1.0240e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1407/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5130e-05 - rmse: 0.0039\n",
      "Epoch 1407: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.2374e-06 - rmse: 0.0029 - val_loss: 1.1806e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1408/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3287e-06 - rmse: 0.0023\n",
      "Epoch 1408: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.4881e-06 - rmse: 0.0029 - val_loss: 8.6944e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1409/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5187e-05 - rmse: 0.0039\n",
      "Epoch 1409: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1575e-06 - rmse: 0.0029 - val_loss: 1.1281e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1410/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2371e-05 - rmse: 0.0035\n",
      "Epoch 1410: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1040e-05 - rmse: 0.0033 - val_loss: 1.0328e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1411/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2056e-06 - rmse: 0.0027\n",
      "Epoch 1411: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0854e-05 - rmse: 0.0033 - val_loss: 1.1962e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1412/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0411e-05 - rmse: 0.0045\n",
      "Epoch 1412: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6161e-05 - rmse: 0.0040 - val_loss: 1.0240e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1413/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5676e-05 - rmse: 0.0040\n",
      "Epoch 1413: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9160e-05 - rmse: 0.0044 - val_loss: 1.2175e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1414/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2930e-06 - rmse: 0.0025\n",
      "Epoch 1414: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4704e-05 - rmse: 0.0038 - val_loss: 1.4533e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1415/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0997e-05 - rmse: 0.0078\n",
      "Epoch 1415: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.5948e-05 - rmse: 0.0051 - val_loss: 1.1606e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1416/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1224e-06 - rmse: 0.0028\n",
      "Epoch 1416: val_loss did not improve from 0.00009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4059e-05 - rmse: 0.0049 - val_loss: 1.1170e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1417/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9086e-05 - rmse: 0.0054\n",
      "Epoch 1417: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7931e-05 - rmse: 0.0053 - val_loss: 1.3767e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1418/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2037e-05 - rmse: 0.0057\n",
      "Epoch 1418: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2575e-05 - rmse: 0.0048 - val_loss: 1.2073e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1419/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1268e-06 - rmse: 0.0020\n",
      "Epoch 1419: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4334e-05 - rmse: 0.0049 - val_loss: 1.2491e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1420/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2063e-05 - rmse: 0.0047\n",
      "Epoch 1420: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5485e-05 - rmse: 0.0067 - val_loss: 1.2565e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1421/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6471e-05 - rmse: 0.0041\n",
      "Epoch 1421: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6505e-05 - rmse: 0.0060 - val_loss: 1.1052e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1422/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5083e-05 - rmse: 0.0059\n",
      "Epoch 1422: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9545e-05 - rmse: 0.0054 - val_loss: 1.0529e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1423/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2572e-06 - rmse: 0.0029\n",
      "Epoch 1423: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8850e-05 - rmse: 0.0043 - val_loss: 1.0891e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1424/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2176e-05 - rmse: 0.0035\n",
      "Epoch 1424: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5978e-06 - rmse: 0.0026 - val_loss: 1.2190e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1425/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7080e-05 - rmse: 0.0041\n",
      "Epoch 1425: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6069e-06 - rmse: 0.0029 - val_loss: 1.0299e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1426/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1612e-06 - rmse: 0.0018\n",
      "Epoch 1426: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5452e-06 - rmse: 0.0027 - val_loss: 1.2869e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1427/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4485e-06 - rmse: 0.0027\n",
      "Epoch 1427: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7989e-06 - rmse: 0.0031 - val_loss: 9.8216e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1428/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0031e-05 - rmse: 0.0045\n",
      "Epoch 1428: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2068e-06 - rmse: 0.0029 - val_loss: 1.3068e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1429/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6446e-05 - rmse: 0.0051\n",
      "Epoch 1429: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7295e-06 - rmse: 0.0030 - val_loss: 9.9981e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1430/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0042e-06 - rmse: 0.0022\n",
      "Epoch 1430: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7036e-06 - rmse: 0.0030 - val_loss: 1.4232e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1431/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9011e-06 - rmse: 0.0028\n",
      "Epoch 1431: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1959e-05 - rmse: 0.0035 - val_loss: 8.9949e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1432/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0928e-05 - rmse: 0.0033\n",
      "Epoch 1432: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4933e-05 - rmse: 0.0039 - val_loss: 1.1114e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1433/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1957e-06 - rmse: 0.0020\n",
      "Epoch 1433: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5988e-06 - rmse: 0.0028 - val_loss: 1.1299e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1434/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3245e-06 - rmse: 0.0021\n",
      "Epoch 1434: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9927e-06 - rmse: 0.0028 - val_loss: 1.1652e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1435/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7778e-05 - rmse: 0.0042\n",
      "Epoch 1435: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2844e-05 - rmse: 0.0036 - val_loss: 1.2586e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1436/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2691e-05 - rmse: 0.0036\n",
      "Epoch 1436: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1976e-05 - rmse: 0.0035 - val_loss: 1.0568e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1437/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2221e-06 - rmse: 0.0023\n",
      "Epoch 1437: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7846e-05 - rmse: 0.0042 - val_loss: 1.5644e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1438/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0779e-05 - rmse: 0.0033\n",
      "Epoch 1438: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2915e-05 - rmse: 0.0048 - val_loss: 9.5444e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1439/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7458e-05 - rmse: 0.0069\n",
      "Epoch 1439: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2037e-05 - rmse: 0.0047 - val_loss: 1.4866e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1440/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4616e-06 - rmse: 0.0029\n",
      "Epoch 1440: val_loss improved from 0.00009 to 0.00008, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 1.4496e-05 - rmse: 0.0038 - val_loss: 8.0835e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1441/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2891e-05 - rmse: 0.0048\n",
      "Epoch 1441: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5417e-05 - rmse: 0.0039 - val_loss: 1.6955e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1442/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1255e-05 - rmse: 0.0046\n",
      "Epoch 1442: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6064e-05 - rmse: 0.0040 - val_loss: 8.8328e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1443/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4967e-05 - rmse: 0.0039\n",
      "Epoch 1443: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9525e-05 - rmse: 0.0077 - val_loss: 1.8394e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1444/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3607e-05 - rmse: 0.0049\n",
      "Epoch 1444: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2381e-05 - rmse: 0.0079 - val_loss: 8.7429e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1445/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5617e-05 - rmse: 0.0051\n",
      "Epoch 1445: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5001e-05 - rmse: 0.0067 - val_loss: 2.2117e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1446/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5884e-05 - rmse: 0.0068\n",
      "Epoch 1446: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4298e-05 - rmse: 0.0080 - val_loss: 1.2553e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1447/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8424e-05 - rmse: 0.0053\n",
      "Epoch 1447: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5530e-05 - rmse: 0.0051 - val_loss: 1.2391e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1448/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1670e-06 - rmse: 0.0023\n",
      "Epoch 1448: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6820e-05 - rmse: 0.0041 - val_loss: 1.1076e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1449/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2611e-05 - rmse: 0.0036\n",
      "Epoch 1449: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3446e-05 - rmse: 0.0037 - val_loss: 1.1110e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1450/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2044e-05 - rmse: 0.0035\n",
      "Epoch 1450: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3598e-05 - rmse: 0.0037 - val_loss: 8.7119e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1451/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5617e-05 - rmse: 0.0040\n",
      "Epoch 1451: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0499e-05 - rmse: 0.0045 - val_loss: 1.3283e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1452/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6071e-05 - rmse: 0.0060\n",
      "Epoch 1452: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5407e-05 - rmse: 0.0060 - val_loss: 1.1446e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1453/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0523e-05 - rmse: 0.0032\n",
      "Epoch 1453: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7430e-05 - rmse: 0.0042 - val_loss: 1.3681e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1454/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3273e-05 - rmse: 0.0048\n",
      "Epoch 1454: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3423e-05 - rmse: 0.0048 - val_loss: 1.0570e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1455/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1355e-05 - rmse: 0.0034\n",
      "Epoch 1455: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0816e-05 - rmse: 0.0056 - val_loss: 1.2387e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1456/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1423e-05 - rmse: 0.0078\n",
      "Epoch 1456: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2877e-05 - rmse: 0.0065 - val_loss: 1.5755e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1457/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6502e-05 - rmse: 0.0068\n",
      "Epoch 1457: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2882e-05 - rmse: 0.0057 - val_loss: 1.1478e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1458/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2777e-05 - rmse: 0.0065\n",
      "Epoch 1458: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0266e-05 - rmse: 0.0045 - val_loss: 1.3467e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1459/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4369e-06 - rmse: 0.0029\n",
      "Epoch 1459: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5820e-05 - rmse: 0.0040 - val_loss: 1.4047e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1460/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4080e-05 - rmse: 0.0049\n",
      "Epoch 1460: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7379e-05 - rmse: 0.0042 - val_loss: 1.1719e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1461/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3397e-05 - rmse: 0.0048\n",
      "Epoch 1461: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8157e-05 - rmse: 0.0043 - val_loss: 1.3020e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1462/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5715e-05 - rmse: 0.0040\n",
      "Epoch 1462: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0623e-05 - rmse: 0.0064 - val_loss: 1.9624e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1463/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8288e-05 - rmse: 0.0062\n",
      "Epoch 1463: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8729e-05 - rmse: 0.0070 - val_loss: 1.0292e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1464/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8205e-05 - rmse: 0.0088\n",
      "Epoch 1464: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2905e-05 - rmse: 0.0096 - val_loss: 3.0153e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 1465/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1318e-04 - rmse: 0.0106\n",
      "Epoch 1465: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.7260e-05 - rmse: 0.0076 - val_loss: 8.5355e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1466/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9101e-05 - rmse: 0.0054\n",
      "Epoch 1466: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4131e-05 - rmse: 0.0058 - val_loss: 1.7769e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1467/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5262e-05 - rmse: 0.0039\n",
      "Epoch 1467: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3917e-05 - rmse: 0.0049 - val_loss: 1.1349e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1468/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0343e-05 - rmse: 0.0032\n",
      "Epoch 1468: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5521e-05 - rmse: 0.0051 - val_loss: 1.3636e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1469/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1759e-06 - rmse: 0.0027\n",
      "Epoch 1469: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.9988e-05 - rmse: 0.0045 - val_loss: 1.1920e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1470/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7562e-05 - rmse: 0.0042\n",
      "Epoch 1470: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2831e-05 - rmse: 0.0036 - val_loss: 1.1492e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1471/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3860e-06 - rmse: 0.0021\n",
      "Epoch 1471: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0637e-05 - rmse: 0.0033 - val_loss: 1.1994e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1472/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9932e-06 - rmse: 0.0024\n",
      "Epoch 1472: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2652e-06 - rmse: 0.0027 - val_loss: 1.0640e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1473/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4968e-05 - rmse: 0.0039\n",
      "Epoch 1473: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8150e-06 - rmse: 0.0028 - val_loss: 1.2909e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1474/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3354e-05 - rmse: 0.0058\n",
      "Epoch 1474: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5729e-05 - rmse: 0.0040 - val_loss: 1.2478e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1475/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6497e-05 - rmse: 0.0041\n",
      "Epoch 1475: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5179e-05 - rmse: 0.0039 - val_loss: 1.1032e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1476/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1230e-05 - rmse: 0.0034\n",
      "Epoch 1476: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2624e-05 - rmse: 0.0048 - val_loss: 1.4553e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1477/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5320e-05 - rmse: 0.0050\n",
      "Epoch 1477: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5384e-05 - rmse: 0.0039 - val_loss: 9.4730e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1478/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1900e-05 - rmse: 0.0047\n",
      "Epoch 1478: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4190e-05 - rmse: 0.0038 - val_loss: 1.2015e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1479/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4983e-06 - rmse: 0.0027\n",
      "Epoch 1479: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0652e-06 - rmse: 0.0028 - val_loss: 1.0376e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1480/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7716e-06 - rmse: 0.0017\n",
      "Epoch 1480: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2706e-05 - rmse: 0.0036 - val_loss: 1.2641e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1481/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3707e-05 - rmse: 0.0037\n",
      "Epoch 1481: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7852e-06 - rmse: 0.0026 - val_loss: 1.0661e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1482/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7100e-06 - rmse: 0.0024\n",
      "Epoch 1482: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7312e-06 - rmse: 0.0031 - val_loss: 1.1368e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1483/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6226e-05 - rmse: 0.0040\n",
      "Epoch 1483: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1926e-05 - rmse: 0.0035 - val_loss: 1.2675e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1484/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6139e-05 - rmse: 0.0040\n",
      "Epoch 1484: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2405e-05 - rmse: 0.0047 - val_loss: 9.3999e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1485/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9347e-05 - rmse: 0.0054\n",
      "Epoch 1485: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9110e-05 - rmse: 0.0044 - val_loss: 1.2139e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1486/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0662e-06 - rmse: 0.0020\n",
      "Epoch 1486: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1865e-06 - rmse: 0.0030 - val_loss: 1.1186e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1487/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6502e-06 - rmse: 0.0031\n",
      "Epoch 1487: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3620e-05 - rmse: 0.0037 - val_loss: 1.2150e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1488/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6150e-06 - rmse: 0.0029\n",
      "Epoch 1488: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1536e-05 - rmse: 0.0034 - val_loss: 1.1071e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1489/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2128e-06 - rmse: 0.0018\n",
      "Epoch 1489: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9002e-06 - rmse: 0.0026 - val_loss: 1.3618e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1490/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3700e-05 - rmse: 0.0037\n",
      "Epoch 1490: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.6035e-06 - rmse: 0.0031 - val_loss: 1.0343e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1491/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3389e-06 - rmse: 0.0021\n",
      "Epoch 1491: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4714e-05 - rmse: 0.0038 - val_loss: 1.3171e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1492/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4424e-05 - rmse: 0.0038\n",
      "Epoch 1492: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6529e-05 - rmse: 0.0041 - val_loss: 1.1364e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1493/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9906e-06 - rmse: 0.0032\n",
      "Epoch 1493: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8271e-06 - rmse: 0.0030 - val_loss: 1.0953e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1494/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3332e-05 - rmse: 0.0037\n",
      "Epoch 1494: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5440e-05 - rmse: 0.0039 - val_loss: 9.9992e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1495/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0885e-05 - rmse: 0.0033\n",
      "Epoch 1495: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3080e-05 - rmse: 0.0036 - val_loss: 1.5696e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1496/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6968e-05 - rmse: 0.0052\n",
      "Epoch 1496: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2525e-05 - rmse: 0.0047 - val_loss: 9.9408e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1497/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8165e-05 - rmse: 0.0053\n",
      "Epoch 1497: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4945e-05 - rmse: 0.0039 - val_loss: 1.8454e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1498/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8249e-05 - rmse: 0.0083\n",
      "Epoch 1498: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9728e-05 - rmse: 0.0077 - val_loss: 1.3998e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1499/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7385e-05 - rmse: 0.0042\n",
      "Epoch 1499: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4939e-05 - rmse: 0.0097 - val_loss: 1.4369e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1500/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3830e-04 - rmse: 0.0154\n",
      "Epoch 1500: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2419e-04 - rmse: 0.0150 - val_loss: 8.8965e-04 - val_rmse: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 1501/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3272e-04 - rmse: 0.0271\n",
      "Epoch 1501: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5093e-04 - rmse: 0.0187 - val_loss: 2.1576e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1502/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5314e-04 - rmse: 0.0124\n",
      "Epoch 1502: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9259e-04 - rmse: 0.0171 - val_loss: 3.9255e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 1503/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0713e-04 - rmse: 0.0202\n",
      "Epoch 1503: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6241e-04 - rmse: 0.0162 - val_loss: 1.1068e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1504/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5890e-05 - rmse: 0.0075\n",
      "Epoch 1504: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6801e-04 - rmse: 0.0130 - val_loss: 2.4910e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1505/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4120e-04 - rmse: 0.0155\n",
      "Epoch 1505: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0186e-04 - rmse: 0.0142 - val_loss: 2.4274e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 1506/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4833e-04 - rmse: 0.0122\n",
      "Epoch 1506: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3868e-04 - rmse: 0.0118 - val_loss: 8.5350e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1507/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6715e-05 - rmse: 0.0041\n",
      "Epoch 1507: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1018e-04 - rmse: 0.0105 - val_loss: 4.0618e-04 - val_rmse: 0.0202 - lr: 1.0000e-04\n",
      "Epoch 1508/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5645e-04 - rmse: 0.0160\n",
      "Epoch 1508: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2468e-04 - rmse: 0.0112 - val_loss: 3.0163e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 1509/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8122e-04 - rmse: 0.0195\n",
      "Epoch 1509: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9452e-04 - rmse: 0.0139 - val_loss: 3.6700e-04 - val_rmse: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 1510/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6096e-04 - rmse: 0.0127\n",
      "Epoch 1510: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2572e-04 - rmse: 0.0112 - val_loss: 3.6399e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 1511/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4181e-04 - rmse: 0.0156\n",
      "Epoch 1511: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1515e-04 - rmse: 0.0147 - val_loss: 1.4999e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1512/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5480e-05 - rmse: 0.0092\n",
      "Epoch 1512: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5267e-05 - rmse: 0.0098 - val_loss: 1.5272e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1513/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4920e-05 - rmse: 0.0081\n",
      "Epoch 1513: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1571e-05 - rmse: 0.0085 - val_loss: 1.3579e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1514/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4024e-05 - rmse: 0.0058\n",
      "Epoch 1514: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5421e-05 - rmse: 0.0067 - val_loss: 1.4770e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1515/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9356e-05 - rmse: 0.0044\n",
      "Epoch 1515: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2610e-05 - rmse: 0.0065 - val_loss: 9.1442e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1516/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4034e-05 - rmse: 0.0058\n",
      "Epoch 1516: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9702e-05 - rmse: 0.0054 - val_loss: 1.0893e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1517/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4962e-05 - rmse: 0.0039\n",
      "Epoch 1517: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0098e-05 - rmse: 0.0045 - val_loss: 1.6400e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1518/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0688e-05 - rmse: 0.0078\n",
      "Epoch 1518: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2200e-05 - rmse: 0.0072 - val_loss: 1.1011e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1519/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0219e-05 - rmse: 0.0071\n",
      "Epoch 1519: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1603e-05 - rmse: 0.0072 - val_loss: 1.3398e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1520/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3393e-05 - rmse: 0.0058\n",
      "Epoch 1520: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4789e-05 - rmse: 0.0067 - val_loss: 1.8081e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1521/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1061e-04 - rmse: 0.0105\n",
      "Epoch 1521: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6411e-05 - rmse: 0.0075 - val_loss: 9.8175e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1522/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7224e-05 - rmse: 0.0052\n",
      "Epoch 1522: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2835e-05 - rmse: 0.0065 - val_loss: 1.4209e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1523/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4308e-05 - rmse: 0.0074\n",
      "Epoch 1523: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5826e-05 - rmse: 0.0060 - val_loss: 1.1687e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1524/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1584e-05 - rmse: 0.0064\n",
      "Epoch 1524: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2444e-05 - rmse: 0.0057 - val_loss: 1.2366e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1525/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5542e-06 - rmse: 0.0031\n",
      "Epoch 1525: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0498e-05 - rmse: 0.0045 - val_loss: 1.1961e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1526/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3290e-05 - rmse: 0.0048\n",
      "Epoch 1526: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2631e-05 - rmse: 0.0048 - val_loss: 1.2125e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1527/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3222e-05 - rmse: 0.0048\n",
      "Epoch 1527: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4105e-05 - rmse: 0.0038 - val_loss: 9.7509e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1528/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4295e-06 - rmse: 0.0025\n",
      "Epoch 1528: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0882e-05 - rmse: 0.0033 - val_loss: 9.4166e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1529/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4896e-06 - rmse: 0.0025\n",
      "Epoch 1529: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4634e-05 - rmse: 0.0050 - val_loss: 2.0925e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1530/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9237e-05 - rmse: 0.0094\n",
      "Epoch 1530: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1983e-04 - rmse: 0.0109 - val_loss: 9.8818e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1531/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6294e-05 - rmse: 0.0060\n",
      "Epoch 1531: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.4777e-05 - rmse: 0.0086 - val_loss: 1.2195e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1532/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9273e-05 - rmse: 0.0044\n",
      "Epoch 1532: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4858e-05 - rmse: 0.0081 - val_loss: 1.2000e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1533/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0948e-05 - rmse: 0.0090\n",
      "Epoch 1533: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1651e-04 - rmse: 0.0108 - val_loss: 2.0480e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1534/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3803e-05 - rmse: 0.0097\n",
      "Epoch 1534: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8841e-05 - rmse: 0.0089 - val_loss: 1.6443e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1535/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8227e-05 - rmse: 0.0088\n",
      "Epoch 1535: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7222e-05 - rmse: 0.0082 - val_loss: 1.1625e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1536/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2621e-05 - rmse: 0.0057\n",
      "Epoch 1536: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.4370e-05 - rmse: 0.0074 - val_loss: 1.4992e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1537/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2372e-05 - rmse: 0.0035\n",
      "Epoch 1537: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0981e-05 - rmse: 0.0064 - val_loss: 9.2350e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1538/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5458e-05 - rmse: 0.0087\n",
      "Epoch 1538: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4305e-05 - rmse: 0.0074 - val_loss: 2.5572e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 1539/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5003e-04 - rmse: 0.0122\n",
      "Epoch 1539: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9452e-05 - rmse: 0.0089 - val_loss: 1.4419e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1540/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2493e-04 - rmse: 0.0150\n",
      "Epoch 1540: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1405e-04 - rmse: 0.0107 - val_loss: 4.0459e-04 - val_rmse: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 1541/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0497e-04 - rmse: 0.0143\n",
      "Epoch 1541: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2170e-04 - rmse: 0.0110 - val_loss: 1.1188e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1542/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3765e-04 - rmse: 0.0117\n",
      "Epoch 1542: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.1377e-05 - rmse: 0.0084 - val_loss: 2.2002e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1543/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5008e-05 - rmse: 0.0039\n",
      "Epoch 1543: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3091e-04 - rmse: 0.0114 - val_loss: 2.6286e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1544/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7620e-04 - rmse: 0.0166\n",
      "Epoch 1544: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2933e-04 - rmse: 0.0151 - val_loss: 2.6999e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1545/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3638e-04 - rmse: 0.0117\n",
      "Epoch 1545: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9909e-04 - rmse: 0.0200 - val_loss: 1.2212e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1546/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4617e-05 - rmse: 0.0050\n",
      "Epoch 1546: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4511e-04 - rmse: 0.0186 - val_loss: 3.7549e-04 - val_rmse: 0.0194 - lr: 1.0000e-04\n",
      "Epoch 1547/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4033e-04 - rmse: 0.0118\n",
      "Epoch 1547: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3478e-04 - rmse: 0.0271 - val_loss: 0.0013 - val_rmse: 0.0359 - lr: 1.0000e-04\n",
      "Epoch 1548/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0352\n",
      "Epoch 1548: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8560e-04 - rmse: 0.0314 - val_loss: 0.0012 - val_rmse: 0.0348 - lr: 1.0000e-04\n",
      "Epoch 1549/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0433\n",
      "Epoch 1549: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8623e-04 - rmse: 0.0298 - val_loss: 8.0549e-04 - val_rmse: 0.0284 - lr: 1.0000e-04\n",
      "Epoch 1550/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8965e-04 - rmse: 0.0298\n",
      "Epoch 1550: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8240e-04 - rmse: 0.0261 - val_loss: 2.3247e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1551/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0377e-04 - rmse: 0.0143\n",
      "Epoch 1551: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2125e-04 - rmse: 0.0205 - val_loss: 2.0873e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1552/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5292e-04 - rmse: 0.0124\n",
      "Epoch 1552: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8297e-04 - rmse: 0.0220 - val_loss: 2.3874e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1553/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1071e-04 - rmse: 0.0105\n",
      "Epoch 1553: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3975e-04 - rmse: 0.0184 - val_loss: 4.3081e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 1554/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1763e-04 - rmse: 0.0148\n",
      "Epoch 1554: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3375e-04 - rmse: 0.0153 - val_loss: 5.0323e-04 - val_rmse: 0.0224 - lr: 1.0000e-04\n",
      "Epoch 1555/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9538e-04 - rmse: 0.0172\n",
      "Epoch 1555: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3768e-04 - rmse: 0.0117 - val_loss: 1.5947e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1556/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2477e-05 - rmse: 0.0065\n",
      "Epoch 1556: val_loss improved from 0.00008 to 0.00008, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 5.1117e-05 - rmse: 0.0071 - val_loss: 7.7641e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1557/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3729e-05 - rmse: 0.0049\n",
      "Epoch 1557: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2529e-05 - rmse: 0.0057 - val_loss: 1.4067e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1558/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0516e-05 - rmse: 0.0071\n",
      "Epoch 1558: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7920e-05 - rmse: 0.0062 - val_loss: 1.1466e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1559/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2075e-05 - rmse: 0.0065\n",
      "Epoch 1559: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7660e-05 - rmse: 0.0061 - val_loss: 1.0330e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1560/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9808e-05 - rmse: 0.0055\n",
      "Epoch 1560: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6581e-05 - rmse: 0.0060 - val_loss: 1.3721e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1561/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1905e-05 - rmse: 0.0079\n",
      "Epoch 1561: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0658e-05 - rmse: 0.0064 - val_loss: 8.8941e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1562/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7628e-06 - rmse: 0.0030\n",
      "Epoch 1562: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6740e-05 - rmse: 0.0052 - val_loss: 1.1079e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1563/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6533e-05 - rmse: 0.0041\n",
      "Epoch 1563: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5749e-05 - rmse: 0.0060 - val_loss: 8.1448e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1564/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1172e-05 - rmse: 0.0033\n",
      "Epoch 1564: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7252e-05 - rmse: 0.0069 - val_loss: 1.8713e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1565/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7871e-05 - rmse: 0.0082\n",
      "Epoch 1565: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7730e-05 - rmse: 0.0061 - val_loss: 8.8456e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1566/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5377e-05 - rmse: 0.0074\n",
      "Epoch 1566: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8110e-05 - rmse: 0.0076 - val_loss: 2.6658e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1567/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2769e-05 - rmse: 0.0091\n",
      "Epoch 1567: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0574e-05 - rmse: 0.0095 - val_loss: 9.9671e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1568/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9521e-05 - rmse: 0.0100\n",
      "Epoch 1568: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9892e-05 - rmse: 0.0084 - val_loss: 1.7189e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1569/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9046e-05 - rmse: 0.0054\n",
      "Epoch 1569: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6835e-05 - rmse: 0.0082 - val_loss: 1.6604e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1570/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4707e-05 - rmse: 0.0092\n",
      "Epoch 1570: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0768e-04 - rmse: 0.0104 - val_loss: 2.2686e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1571/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3430e-04 - rmse: 0.0116\n",
      "Epoch 1571: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1287e-05 - rmse: 0.0072 - val_loss: 8.5588e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1572/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1154e-05 - rmse: 0.0056\n",
      "Epoch 1572: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1391e-05 - rmse: 0.0056 - val_loss: 1.7818e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1573/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3495e-05 - rmse: 0.0066\n",
      "Epoch 1573: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1492e-05 - rmse: 0.0056 - val_loss: 7.8935e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1574/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2144e-05 - rmse: 0.0057\n",
      "Epoch 1574: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0316e-05 - rmse: 0.0055 - val_loss: 1.0435e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1575/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6572e-05 - rmse: 0.0052\n",
      "Epoch 1575: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3215e-05 - rmse: 0.0048 - val_loss: 7.9046e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1576/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2471e-05 - rmse: 0.0035\n",
      "Epoch 1576: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6235e-05 - rmse: 0.0040 - val_loss: 1.1105e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1577/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7114e-05 - rmse: 0.0041\n",
      "Epoch 1577: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3649e-05 - rmse: 0.0037 - val_loss: 9.4696e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1578/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9676e-06 - rmse: 0.0026\n",
      "Epoch 1578: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2338e-05 - rmse: 0.0035 - val_loss: 9.4015e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1579/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4754e-06 - rmse: 0.0023\n",
      "Epoch 1579: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2304e-05 - rmse: 0.0035 - val_loss: 9.2134e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1580/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9873e-06 - rmse: 0.0028\n",
      "Epoch 1580: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5677e-06 - rmse: 0.0026 - val_loss: 1.1715e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1581/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0397e-06 - rmse: 0.0025\n",
      "Epoch 1581: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3729e-05 - rmse: 0.0037 - val_loss: 8.7771e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1582/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5028e-05 - rmse: 0.0039\n",
      "Epoch 1582: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4656e-05 - rmse: 0.0038 - val_loss: 1.2972e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1583/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4276e-05 - rmse: 0.0059\n",
      "Epoch 1583: val_loss improved from 0.00008 to 0.00008, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 31ms/step - loss: 1.5924e-05 - rmse: 0.0040 - val_loss: 7.7493e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1584/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8598e-06 - rmse: 0.0028\n",
      "Epoch 1584: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1475e-05 - rmse: 0.0046 - val_loss: 1.5488e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1585/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6708e-05 - rmse: 0.0075\n",
      "Epoch 1585: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2841e-05 - rmse: 0.0057 - val_loss: 8.9927e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1586/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5963e-05 - rmse: 0.0051\n",
      "Epoch 1586: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2301e-05 - rmse: 0.0047 - val_loss: 9.0565e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1587/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5171e-05 - rmse: 0.0039\n",
      "Epoch 1587: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4031e-05 - rmse: 0.0049 - val_loss: 1.1850e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1588/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2452e-05 - rmse: 0.0057\n",
      "Epoch 1588: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7834e-05 - rmse: 0.0069 - val_loss: 1.9236e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1589/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5252e-05 - rmse: 0.0098\n",
      "Epoch 1589: val_loss improved from 0.00008 to 0.00007, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 5.0889e-05 - rmse: 0.0071 - val_loss: 7.2245e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1590/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9576e-05 - rmse: 0.0054\n",
      "Epoch 1590: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.5922e-05 - rmse: 0.0068 - val_loss: 1.5548e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1591/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2685e-05 - rmse: 0.0065\n",
      "Epoch 1591: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3583e-05 - rmse: 0.0073 - val_loss: 1.1764e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1592/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7215e-05 - rmse: 0.0082\n",
      "Epoch 1592: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6433e-05 - rmse: 0.0075 - val_loss: 1.1660e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1593/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7193e-06 - rmse: 0.0030\n",
      "Epoch 1593: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2817e-05 - rmse: 0.0057 - val_loss: 8.8494e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1594/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5475e-06 - rmse: 0.0027\n",
      "Epoch 1594: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7010e-05 - rmse: 0.0061 - val_loss: 1.5936e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1595/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4534e-05 - rmse: 0.0074\n",
      "Epoch 1595: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6092e-05 - rmse: 0.0060 - val_loss: 1.9272e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1596/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5370e-04 - rmse: 0.0124\n",
      "Epoch 1596: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0374e-04 - rmse: 0.0102 - val_loss: 2.0144e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1597/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1350e-05 - rmse: 0.0084\n",
      "Epoch 1597: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6863e-05 - rmse: 0.0093 - val_loss: 1.0433e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1598/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1659e-05 - rmse: 0.0056\n",
      "Epoch 1598: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8920e-05 - rmse: 0.0089 - val_loss: 1.5930e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1599/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3185e-05 - rmse: 0.0086\n",
      "Epoch 1599: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1386e-05 - rmse: 0.0078 - val_loss: 1.3534e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1600/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8143e-05 - rmse: 0.0076\n",
      "Epoch 1600: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3313e-05 - rmse: 0.0066 - val_loss: 1.0522e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1601/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0165e-05 - rmse: 0.0045\n",
      "Epoch 1601: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5042e-05 - rmse: 0.0050 - val_loss: 8.5598e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1602/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6413e-05 - rmse: 0.0041\n",
      "Epoch 1602: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7159e-05 - rmse: 0.0041 - val_loss: 1.1091e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1603/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1748e-05 - rmse: 0.0034\n",
      "Epoch 1603: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3569e-05 - rmse: 0.0037 - val_loss: 8.5659e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1604/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6240e-05 - rmse: 0.0040\n",
      "Epoch 1604: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0647e-05 - rmse: 0.0033 - val_loss: 9.9136e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1605/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2473e-05 - rmse: 0.0047\n",
      "Epoch 1605: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0764e-05 - rmse: 0.0046 - val_loss: 7.6979e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1606/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0863e-06 - rmse: 0.0030\n",
      "Epoch 1606: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4102e-05 - rmse: 0.0038 - val_loss: 1.0403e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1607/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0956e-05 - rmse: 0.0033\n",
      "Epoch 1607: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.6162e-06 - rmse: 0.0031 - val_loss: 8.1130e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1608/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6522e-06 - rmse: 0.0029\n",
      "Epoch 1608: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3956e-05 - rmse: 0.0037 - val_loss: 1.5303e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1609/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4335e-05 - rmse: 0.0038\n",
      "Epoch 1609: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 4.3532e-05 - rmse: 0.0066 - val_loss: 6.8732e-05 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1610/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5203e-06 - rmse: 0.0029\n",
      "Epoch 1610: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1357e-05 - rmse: 0.0096 - val_loss: 2.7732e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1611/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0419e-04 - rmse: 0.0102\n",
      "Epoch 1611: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3209e-05 - rmse: 0.0086 - val_loss: 9.2089e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1612/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4780e-05 - rmse: 0.0059\n",
      "Epoch 1612: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1604e-05 - rmse: 0.0078 - val_loss: 1.7523e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1613/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9490e-05 - rmse: 0.0063\n",
      "Epoch 1613: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8273e-05 - rmse: 0.0083 - val_loss: 1.8690e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1614/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4820e-04 - rmse: 0.0187\n",
      "Epoch 1614: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4525e-04 - rmse: 0.0121 - val_loss: 3.5729e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 1615/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6655e-04 - rmse: 0.0191\n",
      "Epoch 1615: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1555e-04 - rmse: 0.0147 - val_loss: 1.9977e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 1616/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1455e-04 - rmse: 0.0107\n",
      "Epoch 1616: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2210e-04 - rmse: 0.0110 - val_loss: 1.3954e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1617/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2884e-05 - rmse: 0.0057\n",
      "Epoch 1617: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8373e-05 - rmse: 0.0094 - val_loss: 1.6378e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1618/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7590e-04 - rmse: 0.0133\n",
      "Epoch 1618: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2473e-04 - rmse: 0.0112 - val_loss: 4.8100e-04 - val_rmse: 0.0219 - lr: 1.0000e-04\n",
      "Epoch 1619/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2994e-04 - rmse: 0.0152\n",
      "Epoch 1619: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5295e-04 - rmse: 0.0124 - val_loss: 1.5869e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1620/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1534e-04 - rmse: 0.0147\n",
      "Epoch 1620: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1985e-04 - rmse: 0.0109 - val_loss: 2.6233e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1621/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2565e-04 - rmse: 0.0112\n",
      "Epoch 1621: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3821e-05 - rmse: 0.0097 - val_loss: 1.4952e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1622/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4133e-04 - rmse: 0.0119\n",
      "Epoch 1622: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6348e-05 - rmse: 0.0087 - val_loss: 1.8620e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1623/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6045e-05 - rmse: 0.0060\n",
      "Epoch 1623: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3935e-05 - rmse: 0.0066 - val_loss: 7.3096e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1624/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5271e-05 - rmse: 0.0074\n",
      "Epoch 1624: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3333e-05 - rmse: 0.0091 - val_loss: 1.6158e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1625/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6590e-05 - rmse: 0.0068\n",
      "Epoch 1625: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9157e-05 - rmse: 0.0070 - val_loss: 8.4134e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1626/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0371e-05 - rmse: 0.0084\n",
      "Epoch 1626: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1668e-05 - rmse: 0.0079 - val_loss: 2.0303e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1627/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3117e-05 - rmse: 0.0066\n",
      "Epoch 1627: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.5434e-05 - rmse: 0.0067 - val_loss: 9.1570e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1628/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0309e-05 - rmse: 0.0063\n",
      "Epoch 1628: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7708e-05 - rmse: 0.0061 - val_loss: 1.4826e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1629/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6590e-05 - rmse: 0.0068\n",
      "Epoch 1629: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7481e-05 - rmse: 0.0069 - val_loss: 9.7682e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1630/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0853e-05 - rmse: 0.0056\n",
      "Epoch 1630: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2303e-05 - rmse: 0.0091 - val_loss: 2.2354e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1631/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2744e-05 - rmse: 0.0085\n",
      "Epoch 1631: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9706e-05 - rmse: 0.0083 - val_loss: 1.1240e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1632/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0110e-05 - rmse: 0.0063\n",
      "Epoch 1632: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9900e-05 - rmse: 0.0084 - val_loss: 1.0955e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1633/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8432e-05 - rmse: 0.0076\n",
      "Epoch 1633: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8089e-05 - rmse: 0.0083 - val_loss: 1.4116e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1634/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4281e-05 - rmse: 0.0080\n",
      "Epoch 1634: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6626e-05 - rmse: 0.0075 - val_loss: 1.9114e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1635/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8769e-05 - rmse: 0.0083\n",
      "Epoch 1635: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9185e-05 - rmse: 0.0083 - val_loss: 1.6048e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1636/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4061e-04 - rmse: 0.0119\n",
      "Epoch 1636: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4840e-04 - rmse: 0.0122 - val_loss: 1.3466e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1637/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0587e-05 - rmse: 0.0033\n",
      "Epoch 1637: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6675e-05 - rmse: 0.0093 - val_loss: 1.1701e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1638/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6604e-05 - rmse: 0.0061\n",
      "Epoch 1638: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6721e-05 - rmse: 0.0061 - val_loss: 1.2279e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1639/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8607e-05 - rmse: 0.0043\n",
      "Epoch 1639: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1638e-05 - rmse: 0.0065 - val_loss: 1.0874e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1640/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9486e-05 - rmse: 0.0054\n",
      "Epoch 1640: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2086e-05 - rmse: 0.0091 - val_loss: 2.7885e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 1641/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3684e-04 - rmse: 0.0117\n",
      "Epoch 1641: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0473e-05 - rmse: 0.0090 - val_loss: 1.6611e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1642/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2217e-04 - rmse: 0.0111\n",
      "Epoch 1642: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.7973e-05 - rmse: 0.0099 - val_loss: 3.3464e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 1643/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6492e-04 - rmse: 0.0128\n",
      "Epoch 1643: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0955e-04 - rmse: 0.0105 - val_loss: 1.3598e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1644/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4876e-05 - rmse: 0.0067\n",
      "Epoch 1644: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8962e-05 - rmse: 0.0099 - val_loss: 2.4358e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 1645/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4516e-05 - rmse: 0.0080\n",
      "Epoch 1645: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3684e-04 - rmse: 0.0117 - val_loss: 1.8788e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1646/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0455e-04 - rmse: 0.0175\n",
      "Epoch 1646: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2432e-04 - rmse: 0.0150 - val_loss: 2.4494e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1647/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1704e-04 - rmse: 0.0108\n",
      "Epoch 1647: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2822e-04 - rmse: 0.0113 - val_loss: 1.4238e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1648/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0422e-05 - rmse: 0.0055\n",
      "Epoch 1648: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6905e-05 - rmse: 0.0061 - val_loss: 1.7159e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1649/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5904e-05 - rmse: 0.0081\n",
      "Epoch 1649: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8516e-05 - rmse: 0.0070 - val_loss: 8.7210e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1650/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8583e-05 - rmse: 0.0070\n",
      "Epoch 1650: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0351e-05 - rmse: 0.0084 - val_loss: 2.5089e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1651/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0328e-04 - rmse: 0.0102\n",
      "Epoch 1651: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5182e-04 - rmse: 0.0123 - val_loss: 9.8226e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1652/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0020e-05 - rmse: 0.0084\n",
      "Epoch 1652: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5703e-04 - rmse: 0.0125 - val_loss: 3.3765e-04 - val_rmse: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 1653/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8814e-04 - rmse: 0.0137\n",
      "Epoch 1653: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5423e-04 - rmse: 0.0124 - val_loss: 3.4125e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 1654/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1195e-05 - rmse: 0.0064\n",
      "Epoch 1654: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4369e-04 - rmse: 0.0307 - val_loss: 0.0016 - val_rmse: 0.0405 - lr: 1.0000e-04\n",
      "Epoch 1655/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0389\n",
      "Epoch 1655: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0012 - rmse: 0.0341 - val_loss: 0.0015 - val_rmse: 0.0393 - lr: 1.0000e-04\n",
      "Epoch 1656/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0534\n",
      "Epoch 1656: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0011 - rmse: 0.0326 - val_loss: 0.0015 - val_rmse: 0.0385 - lr: 1.0000e-04\n",
      "Epoch 1657/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8624e-04 - rmse: 0.0314\n",
      "Epoch 1657: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0561e-04 - rmse: 0.0284 - val_loss: 0.0012 - val_rmse: 0.0350 - lr: 1.0000e-04\n",
      "Epoch 1658/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7323e-04 - rmse: 0.0278\n",
      "Epoch 1658: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.2853e-04 - rmse: 0.0305 - val_loss: 0.0010 - val_rmse: 0.0317 - lr: 1.0000e-04\n",
      "Epoch 1659/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0357\n",
      "Epoch 1659: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2904e-04 - rmse: 0.0230 - val_loss: 1.8563e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1660/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5453e-04 - rmse: 0.0124\n",
      "Epoch 1660: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2113e-04 - rmse: 0.0179 - val_loss: 1.3595e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1661/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2063e-05 - rmse: 0.0096\n",
      "Epoch 1661: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.7139e-04 - rmse: 0.0193 - val_loss: 1.8723e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1662/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4182e-04 - rmse: 0.0119\n",
      "Epoch 1662: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4940e-04 - rmse: 0.0187 - val_loss: 1.1716e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1663/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7170e-05 - rmse: 0.0076\n",
      "Epoch 1663: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0349e-04 - rmse: 0.0143 - val_loss: 9.6400e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1664/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7372e-05 - rmse: 0.0069\n",
      "Epoch 1664: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6183e-04 - rmse: 0.0162 - val_loss: 9.6230e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1665/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8713e-05 - rmse: 0.0077\n",
      "Epoch 1665: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9399e-04 - rmse: 0.0139 - val_loss: 1.1025e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1666/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1171e-05 - rmse: 0.0046\n",
      "Epoch 1666: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3551e-05 - rmse: 0.0097 - val_loss: 1.3249e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1667/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0860e-04 - rmse: 0.0104\n",
      "Epoch 1667: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5935e-05 - rmse: 0.0098 - val_loss: 1.8148e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1668/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2279e-05 - rmse: 0.0085\n",
      "Epoch 1668: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5013e-05 - rmse: 0.0067 - val_loss: 8.1814e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1669/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9599e-05 - rmse: 0.0089\n",
      "Epoch 1669: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2404e-05 - rmse: 0.0072 - val_loss: 1.8744e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1670/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3294e-05 - rmse: 0.0091\n",
      "Epoch 1670: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3955e-05 - rmse: 0.0066 - val_loss: 1.0104e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1671/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6882e-05 - rmse: 0.0082\n",
      "Epoch 1671: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4208e-05 - rmse: 0.0066 - val_loss: 1.9214e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1672/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2348e-05 - rmse: 0.0065\n",
      "Epoch 1672: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3212e-05 - rmse: 0.0086 - val_loss: 1.4210e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1673/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8970e-05 - rmse: 0.0089\n",
      "Epoch 1673: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6184e-04 - rmse: 0.0127 - val_loss: 4.2180e-04 - val_rmse: 0.0205 - lr: 1.0000e-04\n",
      "Epoch 1674/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9963e-04 - rmse: 0.0141\n",
      "Epoch 1674: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5993e-04 - rmse: 0.0126 - val_loss: 1.1717e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1675/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4496e-04 - rmse: 0.0120\n",
      "Epoch 1675: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8545e-05 - rmse: 0.0099 - val_loss: 1.8395e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1676/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0573e-04 - rmse: 0.0103\n",
      "Epoch 1676: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9863e-05 - rmse: 0.0077 - val_loss: 9.4071e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1677/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4199e-05 - rmse: 0.0066\n",
      "Epoch 1677: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3823e-05 - rmse: 0.0066 - val_loss: 9.0878e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1678/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0172e-05 - rmse: 0.0032\n",
      "Epoch 1678: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7058e-05 - rmse: 0.0069 - val_loss: 1.2512e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1679/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1421e-05 - rmse: 0.0034\n",
      "Epoch 1679: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5717e-05 - rmse: 0.0040 - val_loss: 8.4678e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1680/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0239e-05 - rmse: 0.0078\n",
      "Epoch 1680: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9871e-05 - rmse: 0.0055 - val_loss: 1.3870e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1681/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1397e-05 - rmse: 0.0072\n",
      "Epoch 1681: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8467e-05 - rmse: 0.0043 - val_loss: 7.7153e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1682/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2850e-06 - rmse: 0.0030\n",
      "Epoch 1682: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5178e-05 - rmse: 0.0039 - val_loss: 7.6440e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1683/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0327e-05 - rmse: 0.0032\n",
      "Epoch 1683: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7627e-05 - rmse: 0.0053 - val_loss: 1.2693e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1684/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6143e-05 - rmse: 0.0051\n",
      "Epoch 1684: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0551e-05 - rmse: 0.0064 - val_loss: 7.4571e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1685/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9506e-05 - rmse: 0.0044\n",
      "Epoch 1685: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6417e-05 - rmse: 0.0060 - val_loss: 1.1090e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1686/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5987e-05 - rmse: 0.0040\n",
      "Epoch 1686: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5602e-05 - rmse: 0.0060 - val_loss: 7.5193e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1687/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5465e-05 - rmse: 0.0060\n",
      "Epoch 1687: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9291e-05 - rmse: 0.0063 - val_loss: 1.7151e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1688/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4117e-05 - rmse: 0.0058\n",
      "Epoch 1688: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5867e-05 - rmse: 0.0081 - val_loss: 1.5945e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1689/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9441e-04 - rmse: 0.0139\n",
      "Epoch 1689: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6567e-05 - rmse: 0.0093 - val_loss: 1.5423e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1690/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8610e-05 - rmse: 0.0043\n",
      "Epoch 1690: val_loss improved from 0.00007 to 0.00007, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.2841e-05 - rmse: 0.0048 - val_loss: 6.6907e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1691/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9007e-05 - rmse: 0.0062\n",
      "Epoch 1691: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9948e-05 - rmse: 0.0055 - val_loss: 1.0511e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1692/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6759e-05 - rmse: 0.0041\n",
      "Epoch 1692: val_loss improved from 0.00007 to 0.00005, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 1.7353e-05 - rmse: 0.0042 - val_loss: 5.0828e-05 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 1693/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4124e-05 - rmse: 0.0049\n",
      "Epoch 1693: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6155e-05 - rmse: 0.0040 - val_loss: 7.5670e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1694/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9436e-06 - rmse: 0.0028\n",
      "Epoch 1694: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4531e-05 - rmse: 0.0038 - val_loss: 7.6227e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1695/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7023e-05 - rmse: 0.0041\n",
      "Epoch 1695: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2448e-05 - rmse: 0.0047 - val_loss: 7.9196e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1696/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2513e-06 - rmse: 0.0027\n",
      "Epoch 1696: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0422e-05 - rmse: 0.0055 - val_loss: 7.6073e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1697/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0103e-05 - rmse: 0.0032\n",
      "Epoch 1697: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5628e-05 - rmse: 0.0060 - val_loss: 6.8039e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1698/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7668e-06 - rmse: 0.0024\n",
      "Epoch 1698: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2105e-05 - rmse: 0.0072 - val_loss: 6.4294e-05 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 1699/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3984e-05 - rmse: 0.0037\n",
      "Epoch 1699: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2436e-05 - rmse: 0.0079 - val_loss: 8.8440e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1700/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0075e-06 - rmse: 0.0026\n",
      "Epoch 1700: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0678e-05 - rmse: 0.0084 - val_loss: 6.9711e-05 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1701/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3659e-05 - rmse: 0.0037\n",
      "Epoch 1701: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2689e-05 - rmse: 0.0073 - val_loss: 8.8917e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1702/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2879e-05 - rmse: 0.0048\n",
      "Epoch 1702: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6538e-05 - rmse: 0.0060 - val_loss: 1.2435e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1703/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3406e-06 - rmse: 0.0029\n",
      "Epoch 1703: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2830e-05 - rmse: 0.0079 - val_loss: 9.0041e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1704/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5549e-05 - rmse: 0.0060\n",
      "Epoch 1704: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9641e-05 - rmse: 0.0083 - val_loss: 1.3607e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1705/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7771e-05 - rmse: 0.0053\n",
      "Epoch 1705: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9409e-05 - rmse: 0.0063 - val_loss: 7.6347e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1706/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2392e-05 - rmse: 0.0035\n",
      "Epoch 1706: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7038e-05 - rmse: 0.0041 - val_loss: 9.8928e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1707/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8701e-06 - rmse: 0.0026\n",
      "Epoch 1707: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1019e-05 - rmse: 0.0033 - val_loss: 7.7104e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1708/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6809e-06 - rmse: 0.0031\n",
      "Epoch 1708: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6734e-05 - rmse: 0.0041 - val_loss: 9.7404e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1709/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8354e-05 - rmse: 0.0043\n",
      "Epoch 1709: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5450e-05 - rmse: 0.0039 - val_loss: 8.7152e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1710/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7947e-06 - rmse: 0.0030\n",
      "Epoch 1710: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5154e-06 - rmse: 0.0026 - val_loss: 8.0404e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1711/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0618e-06 - rmse: 0.0025\n",
      "Epoch 1711: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1218e-05 - rmse: 0.0033 - val_loss: 8.2694e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1712/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1357e-05 - rmse: 0.0046\n",
      "Epoch 1712: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8386e-05 - rmse: 0.0043 - val_loss: 7.9379e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1713/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8041e-06 - rmse: 0.0024\n",
      "Epoch 1713: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2628e-05 - rmse: 0.0036 - val_loss: 8.2485e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1714/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2933e-05 - rmse: 0.0036\n",
      "Epoch 1714: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4980e-06 - rmse: 0.0031 - val_loss: 9.4075e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1715/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3413e-06 - rmse: 0.0031\n",
      "Epoch 1715: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8346e-06 - rmse: 0.0028 - val_loss: 8.5677e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1716/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8681e-05 - rmse: 0.0043\n",
      "Epoch 1716: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0137e-05 - rmse: 0.0032 - val_loss: 8.1631e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1717/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8063e-05 - rmse: 0.0043\n",
      "Epoch 1717: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5057e-06 - rmse: 0.0031 - val_loss: 9.2925e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1718/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7089e-06 - rmse: 0.0024\n",
      "Epoch 1718: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.7821e-06 - rmse: 0.0026 - val_loss: 5.9879e-05 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1719/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0683e-05 - rmse: 0.0033\n",
      "Epoch 1719: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3149e-06 - rmse: 0.0029 - val_loss: 8.8860e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1720/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6693e-06 - rmse: 0.0022\n",
      "Epoch 1720: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3584e-06 - rmse: 0.0025 - val_loss: 7.0164e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1721/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9432e-06 - rmse: 0.0030\n",
      "Epoch 1721: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1740e-06 - rmse: 0.0027 - val_loss: 8.7840e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1722/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1007e-06 - rmse: 0.0028\n",
      "Epoch 1722: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7782e-06 - rmse: 0.0028 - val_loss: 7.6858e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1723/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0554e-05 - rmse: 0.0032\n",
      "Epoch 1723: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2632e-05 - rmse: 0.0036 - val_loss: 9.6850e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1724/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0442e-05 - rmse: 0.0032\n",
      "Epoch 1724: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1201e-05 - rmse: 0.0033 - val_loss: 6.8125e-05 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1725/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7537e-06 - rmse: 0.0024\n",
      "Epoch 1725: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1542e-05 - rmse: 0.0034 - val_loss: 9.0141e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1726/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9892e-06 - rmse: 0.0030\n",
      "Epoch 1726: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.0285e-06 - rmse: 0.0027 - val_loss: 7.1873e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1727/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3424e-06 - rmse: 0.0025\n",
      "Epoch 1727: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2639e-06 - rmse: 0.0023 - val_loss: 8.3575e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1728/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2630e-06 - rmse: 0.0027\n",
      "Epoch 1728: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.6542e-06 - rmse: 0.0026 - val_loss: 8.0438e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1729/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3873e-06 - rmse: 0.0031\n",
      "Epoch 1729: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6769e-06 - rmse: 0.0028 - val_loss: 7.6916e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1730/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5996e-05 - rmse: 0.0040\n",
      "Epoch 1730: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2502e-05 - rmse: 0.0035 - val_loss: 1.0181e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1731/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6393e-05 - rmse: 0.0051\n",
      "Epoch 1731: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5379e-05 - rmse: 0.0039 - val_loss: 8.8472e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1732/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7529e-06 - rmse: 0.0030\n",
      "Epoch 1732: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8435e-06 - rmse: 0.0028 - val_loss: 7.7970e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1733/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2696e-06 - rmse: 0.0030\n",
      "Epoch 1733: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.2667e-06 - rmse: 0.0027 - val_loss: 7.4321e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1734/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3891e-06 - rmse: 0.0021\n",
      "Epoch 1734: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2548e-06 - rmse: 0.0023 - val_loss: 7.4368e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1735/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3715e-06 - rmse: 0.0018\n",
      "Epoch 1735: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1316e-06 - rmse: 0.0023 - val_loss: 6.3876e-05 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 1736/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2306e-06 - rmse: 0.0030\n",
      "Epoch 1736: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2877e-06 - rmse: 0.0027 - val_loss: 9.2496e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1737/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6911e-05 - rmse: 0.0041\n",
      "Epoch 1737: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5372e-06 - rmse: 0.0029 - val_loss: 7.0225e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1738/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0797e-06 - rmse: 0.0020\n",
      "Epoch 1738: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5158e-06 - rmse: 0.0027 - val_loss: 9.9969e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1739/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2314e-06 - rmse: 0.0023\n",
      "Epoch 1739: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5787e-06 - rmse: 0.0024 - val_loss: 6.3098e-05 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 1740/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6302e-06 - rmse: 0.0028\n",
      "Epoch 1740: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6325e-06 - rmse: 0.0024 - val_loss: 8.9709e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1741/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1048e-06 - rmse: 0.0023\n",
      "Epoch 1741: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8068e-06 - rmse: 0.0030 - val_loss: 6.3307e-05 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 1742/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8127e-06 - rmse: 0.0028\n",
      "Epoch 1742: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3519e-05 - rmse: 0.0037 - val_loss: 9.0349e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1743/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6948e-06 - rmse: 0.0019\n",
      "Epoch 1743: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0161e-05 - rmse: 0.0032 - val_loss: 7.2301e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1744/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1125e-06 - rmse: 0.0023\n",
      "Epoch 1744: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9806e-06 - rmse: 0.0030 - val_loss: 7.5452e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1745/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0771e-06 - rmse: 0.0018\n",
      "Epoch 1745: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8979e-06 - rmse: 0.0031 - val_loss: 7.6959e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1746/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0842e-06 - rmse: 0.0025\n",
      "Epoch 1746: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0107e-05 - rmse: 0.0032 - val_loss: 7.2706e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1747/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3360e-05 - rmse: 0.0037\n",
      "Epoch 1747: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3011e-06 - rmse: 0.0027 - val_loss: 9.1598e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1748/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8714e-06 - rmse: 0.0022\n",
      "Epoch 1748: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2003e-05 - rmse: 0.0035 - val_loss: 8.1496e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1749/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0167e-05 - rmse: 0.0055\n",
      "Epoch 1749: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7072e-05 - rmse: 0.0041 - val_loss: 8.9655e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1750/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6806e-06 - rmse: 0.0024\n",
      "Epoch 1750: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1425e-06 - rmse: 0.0030 - val_loss: 6.4851e-05 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 1751/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7533e-05 - rmse: 0.0042\n",
      "Epoch 1751: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7335e-06 - rmse: 0.0028 - val_loss: 8.1419e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1752/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2627e-06 - rmse: 0.0029\n",
      "Epoch 1752: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3913e-06 - rmse: 0.0025 - val_loss: 6.3322e-05 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 1753/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9134e-06 - rmse: 0.0026\n",
      "Epoch 1753: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0927e-05 - rmse: 0.0033 - val_loss: 1.0612e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1754/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7551e-06 - rmse: 0.0028\n",
      "Epoch 1754: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6230e-05 - rmse: 0.0040 - val_loss: 6.0894e-05 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 1755/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0096e-05 - rmse: 0.0032\n",
      "Epoch 1755: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3724e-05 - rmse: 0.0049 - val_loss: 1.0120e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1756/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2261e-06 - rmse: 0.0023\n",
      "Epoch 1756: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8698e-05 - rmse: 0.0062 - val_loss: 6.5177e-05 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 1757/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7038e-06 - rmse: 0.0019\n",
      "Epoch 1757: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4222e-05 - rmse: 0.0080 - val_loss: 1.1581e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1758/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0345e-05 - rmse: 0.0064\n",
      "Epoch 1758: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.2568e-05 - rmse: 0.0079 - val_loss: 9.0439e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1759/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1117e-05 - rmse: 0.0071\n",
      "Epoch 1759: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3829e-05 - rmse: 0.0058 - val_loss: 9.3252e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1760/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8792e-05 - rmse: 0.0043\n",
      "Epoch 1760: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0001e-05 - rmse: 0.0055 - val_loss: 1.2281e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1761/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9690e-05 - rmse: 0.0054\n",
      "Epoch 1761: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6760e-05 - rmse: 0.0068 - val_loss: 8.2738e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1762/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1861e-06 - rmse: 0.0025\n",
      "Epoch 1762: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8814e-05 - rmse: 0.0054 - val_loss: 8.4552e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1763/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0161e-05 - rmse: 0.0032\n",
      "Epoch 1763: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9993e-05 - rmse: 0.0063 - val_loss: 1.5063e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1764/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5884e-05 - rmse: 0.0075\n",
      "Epoch 1764: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4474e-05 - rmse: 0.0059 - val_loss: 1.0596e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1765/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1201e-05 - rmse: 0.0056\n",
      "Epoch 1765: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5391e-05 - rmse: 0.0050 - val_loss: 1.0010e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1766/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8139e-05 - rmse: 0.0043\n",
      "Epoch 1766: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4472e-05 - rmse: 0.0038 - val_loss: 9.4850e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1767/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3796e-05 - rmse: 0.0037\n",
      "Epoch 1767: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3361e-05 - rmse: 0.0037 - val_loss: 1.0345e-04 - val_rmse: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 1768/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7409e-05 - rmse: 0.0052\n",
      "Epoch 1768: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9665e-05 - rmse: 0.0044 - val_loss: 8.1425e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1769/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8833e-05 - rmse: 0.0043\n",
      "Epoch 1769: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7795e-05 - rmse: 0.0042 - val_loss: 7.3200e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1770/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8975e-06 - rmse: 0.0030\n",
      "Epoch 1770: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7918e-06 - rmse: 0.0030 - val_loss: 9.0323e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1771/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9717e-06 - rmse: 0.0028\n",
      "Epoch 1771: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2800e-05 - rmse: 0.0036 - val_loss: 7.5337e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1772/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6236e-06 - rmse: 0.0022\n",
      "Epoch 1772: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5930e-05 - rmse: 0.0040 - val_loss: 7.9986e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1773/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0475e-05 - rmse: 0.0032\n",
      "Epoch 1773: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5675e-05 - rmse: 0.0040 - val_loss: 7.8338e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1774/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9397e-05 - rmse: 0.0044\n",
      "Epoch 1774: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4331e-05 - rmse: 0.0038 - val_loss: 9.2218e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1775/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6445e-05 - rmse: 0.0041\n",
      "Epoch 1775: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2622e-05 - rmse: 0.0036 - val_loss: 6.7889e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1776/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1753e-06 - rmse: 0.0023\n",
      "Epoch 1776: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9636e-06 - rmse: 0.0028 - val_loss: 8.1592e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1777/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7211e-06 - rmse: 0.0016\n",
      "Epoch 1777: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3109e-06 - rmse: 0.0021 - val_loss: 7.6011e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1778/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1556e-06 - rmse: 0.0023\n",
      "Epoch 1778: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.4986e-06 - rmse: 0.0021 - val_loss: 8.1568e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1779/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3210e-06 - rmse: 0.0023\n",
      "Epoch 1779: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2801e-06 - rmse: 0.0027 - val_loss: 8.3929e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1780/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0545e-06 - rmse: 0.0020\n",
      "Epoch 1780: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6314e-06 - rmse: 0.0028 - val_loss: 8.3800e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1781/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6142e-06 - rmse: 0.0016\n",
      "Epoch 1781: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0317e-05 - rmse: 0.0032 - val_loss: 8.1724e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1782/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0827e-06 - rmse: 0.0010\n",
      "Epoch 1782: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0484e-05 - rmse: 0.0032 - val_loss: 9.0706e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1783/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3704e-06 - rmse: 0.0023\n",
      "Epoch 1783: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2746e-05 - rmse: 0.0036 - val_loss: 7.7744e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1784/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8336e-05 - rmse: 0.0053\n",
      "Epoch 1784: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9884e-05 - rmse: 0.0045 - val_loss: 1.2734e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1785/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1289e-05 - rmse: 0.0034\n",
      "Epoch 1785: val_loss improved from 0.00005 to 0.00005, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 3.8898e-05 - rmse: 0.0062 - val_loss: 4.6775e-05 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 1786/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3028e-05 - rmse: 0.0073\n",
      "Epoch 1786: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1411e-05 - rmse: 0.0072 - val_loss: 1.3615e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1787/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1294e-06 - rmse: 0.0030\n",
      "Epoch 1787: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4270e-05 - rmse: 0.0067 - val_loss: 7.4955e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1788/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5640e-05 - rmse: 0.0060\n",
      "Epoch 1788: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2331e-05 - rmse: 0.0065 - val_loss: 1.6946e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1789/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5083e-05 - rmse: 0.0098\n",
      "Epoch 1789: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8710e-05 - rmse: 0.0070 - val_loss: 9.7538e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1790/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8449e-05 - rmse: 0.0062\n",
      "Epoch 1790: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7288e-05 - rmse: 0.0076 - val_loss: 1.1055e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1791/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0477e-05 - rmse: 0.0055\n",
      "Epoch 1791: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1438e-05 - rmse: 0.0056 - val_loss: 1.2731e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1792/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6004e-05 - rmse: 0.0068\n",
      "Epoch 1792: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.6902e-05 - rmse: 0.0068 - val_loss: 1.7727e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1793/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6162e-05 - rmse: 0.0068\n",
      "Epoch 1793: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2021e-05 - rmse: 0.0057 - val_loss: 7.3558e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1794/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1961e-05 - rmse: 0.0047\n",
      "Epoch 1794: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6140e-05 - rmse: 0.0040 - val_loss: 9.8124e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1795/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5138e-05 - rmse: 0.0050\n",
      "Epoch 1795: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5275e-05 - rmse: 0.0039 - val_loss: 7.0984e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1796/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0466e-05 - rmse: 0.0032\n",
      "Epoch 1796: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7780e-05 - rmse: 0.0042 - val_loss: 1.0771e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1797/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2889e-05 - rmse: 0.0036\n",
      "Epoch 1797: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7428e-05 - rmse: 0.0042 - val_loss: 8.2744e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1798/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1086e-05 - rmse: 0.0033\n",
      "Epoch 1798: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3394e-05 - rmse: 0.0048 - val_loss: 7.1340e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1799/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1720e-06 - rmse: 0.0027\n",
      "Epoch 1799: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1436e-05 - rmse: 0.0046 - val_loss: 9.4268e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1800/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7944e-05 - rmse: 0.0042\n",
      "Epoch 1800: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3397e-05 - rmse: 0.0037 - val_loss: 5.8541e-05 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1801/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6806e-05 - rmse: 0.0052\n",
      "Epoch 1801: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8096e-05 - rmse: 0.0043 - val_loss: 9.1140e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1802/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7891e-06 - rmse: 0.0026\n",
      "Epoch 1802: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5762e-06 - rmse: 0.0026 - val_loss: 6.0400e-05 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 1803/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1454e-06 - rmse: 0.0025\n",
      "Epoch 1803: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2671e-06 - rmse: 0.0027 - val_loss: 9.3860e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1804/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8412e-06 - rmse: 0.0024\n",
      "Epoch 1804: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.7360e-06 - rmse: 0.0026 - val_loss: 8.0124e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1805/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1984e-05 - rmse: 0.0035\n",
      "Epoch 1805: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0089e-05 - rmse: 0.0032 - val_loss: 8.9507e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1806/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5561e-06 - rmse: 0.0019\n",
      "Epoch 1806: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1736e-06 - rmse: 0.0030 - val_loss: 6.8010e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1807/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0812e-05 - rmse: 0.0033\n",
      "Epoch 1807: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3932e-05 - rmse: 0.0037 - val_loss: 8.9261e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1808/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8633e-05 - rmse: 0.0054\n",
      "Epoch 1808: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1849e-05 - rmse: 0.0047 - val_loss: 6.9507e-05 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1809/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5734e-05 - rmse: 0.0040\n",
      "Epoch 1809: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6367e-05 - rmse: 0.0051 - val_loss: 1.2197e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1810/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3021e-05 - rmse: 0.0048\n",
      "Epoch 1810: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7961e-05 - rmse: 0.0042 - val_loss: 7.6972e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1811/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6695e-05 - rmse: 0.0052\n",
      "Epoch 1811: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4436e-05 - rmse: 0.0038 - val_loss: 9.9497e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1812/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9954e-06 - rmse: 0.0026\n",
      "Epoch 1812: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2098e-05 - rmse: 0.0035 - val_loss: 6.6624e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1813/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8375e-06 - rmse: 0.0024\n",
      "Epoch 1813: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5430e-06 - rmse: 0.0031 - val_loss: 8.9452e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1814/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0289e-06 - rmse: 0.0028\n",
      "Epoch 1814: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0781e-05 - rmse: 0.0033 - val_loss: 9.2558e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1815/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0229e-05 - rmse: 0.0032\n",
      "Epoch 1815: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4880e-05 - rmse: 0.0039 - val_loss: 5.4955e-05 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 1816/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6744e-06 - rmse: 0.0024\n",
      "Epoch 1816: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0520e-05 - rmse: 0.0045 - val_loss: 1.1248e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1817/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8412e-06 - rmse: 0.0028\n",
      "Epoch 1817: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7660e-05 - rmse: 0.0053 - val_loss: 9.1112e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1818/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6174e-05 - rmse: 0.0075\n",
      "Epoch 1818: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7463e-05 - rmse: 0.0076 - val_loss: 2.1836e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1819/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1592e-05 - rmse: 0.0096\n",
      "Epoch 1819: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1035e-05 - rmse: 0.0071 - val_loss: 1.3106e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1820/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8690e-05 - rmse: 0.0083\n",
      "Epoch 1820: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0627e-05 - rmse: 0.0064 - val_loss: 1.3077e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1821/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2941e-05 - rmse: 0.0066\n",
      "Epoch 1821: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8687e-05 - rmse: 0.0062 - val_loss: 7.5431e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1822/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6659e-05 - rmse: 0.0041\n",
      "Epoch 1822: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1631e-05 - rmse: 0.0056 - val_loss: 1.1263e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1823/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0898e-05 - rmse: 0.0056\n",
      "Epoch 1823: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1534e-05 - rmse: 0.0046 - val_loss: 5.8523e-05 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1824/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1265e-05 - rmse: 0.0034\n",
      "Epoch 1824: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8036e-05 - rmse: 0.0042 - val_loss: 9.7553e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1825/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3327e-06 - rmse: 0.0027\n",
      "Epoch 1825: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0041e-06 - rmse: 0.0028 - val_loss: 7.0425e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1826/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8054e-06 - rmse: 0.0022\n",
      "Epoch 1826: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6192e-06 - rmse: 0.0028 - val_loss: 1.0059e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1827/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5208e-06 - rmse: 0.0031\n",
      "Epoch 1827: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5435e-06 - rmse: 0.0031 - val_loss: 6.3741e-05 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 1828/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5374e-05 - rmse: 0.0039\n",
      "Epoch 1828: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7586e-05 - rmse: 0.0042 - val_loss: 9.8071e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1829/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0208e-05 - rmse: 0.0045\n",
      "Epoch 1829: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4499e-05 - rmse: 0.0038 - val_loss: 7.2948e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1830/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8418e-05 - rmse: 0.0043\n",
      "Epoch 1830: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6361e-05 - rmse: 0.0051 - val_loss: 8.9359e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1831/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3368e-06 - rmse: 0.0025\n",
      "Epoch 1831: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8045e-05 - rmse: 0.0042 - val_loss: 6.5315e-05 - val_rmse: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 1832/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9592e-06 - rmse: 0.0020\n",
      "Epoch 1832: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9331e-05 - rmse: 0.0044 - val_loss: 8.1533e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1833/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6552e-05 - rmse: 0.0041\n",
      "Epoch 1833: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8423e-05 - rmse: 0.0043 - val_loss: 9.6368e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1834/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1083e-05 - rmse: 0.0056\n",
      "Epoch 1834: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6481e-05 - rmse: 0.0051 - val_loss: 7.8215e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1835/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1181e-05 - rmse: 0.0033\n",
      "Epoch 1835: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8668e-05 - rmse: 0.0043 - val_loss: 1.3190e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1836/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5069e-05 - rmse: 0.0039\n",
      "Epoch 1836: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6512e-05 - rmse: 0.0041 - val_loss: 5.5778e-05 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1837/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7917e-05 - rmse: 0.0053\n",
      "Epoch 1837: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7164e-05 - rmse: 0.0052 - val_loss: 1.3402e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1838/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1190e-05 - rmse: 0.0064\n",
      "Epoch 1838: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6701e-05 - rmse: 0.0061 - val_loss: 9.6317e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1839/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4273e-05 - rmse: 0.0059\n",
      "Epoch 1839: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8062e-05 - rmse: 0.0042 - val_loss: 7.3183e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1840/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0425e-05 - rmse: 0.0055\n",
      "Epoch 1840: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0275e-05 - rmse: 0.0045 - val_loss: 8.3437e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1841/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0254e-06 - rmse: 0.0027\n",
      "Epoch 1841: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0745e-05 - rmse: 0.0033 - val_loss: 8.0426e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1842/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4644e-05 - rmse: 0.0038\n",
      "Epoch 1842: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1782e-05 - rmse: 0.0034 - val_loss: 7.1387e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1843/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0144e-06 - rmse: 0.0017\n",
      "Epoch 1843: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1732e-05 - rmse: 0.0034 - val_loss: 1.0270e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1844/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0353e-05 - rmse: 0.0045\n",
      "Epoch 1844: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8445e-05 - rmse: 0.0043 - val_loss: 5.5904e-05 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1845/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5552e-05 - rmse: 0.0039\n",
      "Epoch 1845: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4004e-05 - rmse: 0.0037 - val_loss: 8.4438e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1846/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2866e-06 - rmse: 0.0030\n",
      "Epoch 1846: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8378e-06 - rmse: 0.0026 - val_loss: 7.8497e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1847/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2139e-05 - rmse: 0.0035\n",
      "Epoch 1847: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0182e-06 - rmse: 0.0025 - val_loss: 8.1408e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1848/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0238e-05 - rmse: 0.0032\n",
      "Epoch 1848: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2410e-06 - rmse: 0.0023 - val_loss: 7.4954e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1849/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1160e-06 - rmse: 0.0015\n",
      "Epoch 1849: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0313e-06 - rmse: 0.0020 - val_loss: 7.8993e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1850/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0329e-06 - rmse: 0.0028\n",
      "Epoch 1850: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9573e-06 - rmse: 0.0024 - val_loss: 8.1211e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1851/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8508e-06 - rmse: 0.0017\n",
      "Epoch 1851: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9420e-06 - rmse: 0.0024 - val_loss: 7.6192e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1852/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1398e-06 - rmse: 0.0015\n",
      "Epoch 1852: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0889e-06 - rmse: 0.0027 - val_loss: 8.5250e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1853/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5095e-05 - rmse: 0.0039\n",
      "Epoch 1853: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6015e-06 - rmse: 0.0029 - val_loss: 7.8212e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1854/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2966e-06 - rmse: 0.0023\n",
      "Epoch 1854: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1931e-06 - rmse: 0.0023 - val_loss: 8.1666e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1855/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4285e-06 - rmse: 0.0016\n",
      "Epoch 1855: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4145e-06 - rmse: 0.0023 - val_loss: 7.1121e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1856/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9389e-06 - rmse: 0.0026\n",
      "Epoch 1856: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0185e-06 - rmse: 0.0022 - val_loss: 7.8435e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1857/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2493e-05 - rmse: 0.0035\n",
      "Epoch 1857: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4576e-06 - rmse: 0.0025 - val_loss: 7.0256e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1858/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1861e-06 - rmse: 0.0018\n",
      "Epoch 1858: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4428e-06 - rmse: 0.0021 - val_loss: 7.5268e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1859/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2951e-06 - rmse: 0.0011\n",
      "Epoch 1859: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8791e-06 - rmse: 0.0022 - val_loss: 7.2750e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1860/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8623e-06 - rmse: 0.0014\n",
      "Epoch 1860: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7992e-06 - rmse: 0.0022 - val_loss: 7.7720e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1861/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1905e-06 - rmse: 0.0023\n",
      "Epoch 1861: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8227e-06 - rmse: 0.0026 - val_loss: 8.2600e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1862/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3680e-06 - rmse: 0.0029\n",
      "Epoch 1862: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6413e-06 - rmse: 0.0022 - val_loss: 7.7420e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1863/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1271e-06 - rmse: 0.0020\n",
      "Epoch 1863: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0131e-06 - rmse: 0.0022 - val_loss: 7.1747e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1864/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0480e-05 - rmse: 0.0032\n",
      "Epoch 1864: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3101e-06 - rmse: 0.0023 - val_loss: 9.6777e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1865/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4715e-06 - rmse: 0.0025\n",
      "Epoch 1865: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0704e-05 - rmse: 0.0033 - val_loss: 8.0919e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1866/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2514e-05 - rmse: 0.0035\n",
      "Epoch 1866: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8031e-06 - rmse: 0.0031 - val_loss: 7.5119e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1867/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9020e-06 - rmse: 0.0014\n",
      "Epoch 1867: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9319e-06 - rmse: 0.0020 - val_loss: 8.3834e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1868/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7443e-06 - rmse: 0.0024\n",
      "Epoch 1868: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0314e-05 - rmse: 0.0032 - val_loss: 9.2617e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1869/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9597e-05 - rmse: 0.0044\n",
      "Epoch 1869: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7382e-05 - rmse: 0.0042 - val_loss: 5.6982e-05 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1870/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2937e-06 - rmse: 0.0023\n",
      "Epoch 1870: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1570e-05 - rmse: 0.0034 - val_loss: 9.2219e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1871/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4435e-06 - rmse: 0.0019\n",
      "Epoch 1871: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4424e-05 - rmse: 0.0049 - val_loss: 8.4312e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1872/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2244e-05 - rmse: 0.0047\n",
      "Epoch 1872: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2500e-05 - rmse: 0.0065 - val_loss: 1.0532e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1873/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7882e-05 - rmse: 0.0062\n",
      "Epoch 1873: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1243e-05 - rmse: 0.0072 - val_loss: 1.6276e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1874/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0007e-04 - rmse: 0.0100\n",
      "Epoch 1874: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7531e-05 - rmse: 0.0082 - val_loss: 7.9018e-05 - val_rmse: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 1875/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5060e-05 - rmse: 0.0097\n",
      "Epoch 1875: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8822e-05 - rmse: 0.0094 - val_loss: 3.4752e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 1876/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8609e-04 - rmse: 0.0136\n",
      "Epoch 1876: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5069e-05 - rmse: 0.0098 - val_loss: 8.0285e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1877/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0721e-04 - rmse: 0.0104\n",
      "Epoch 1877: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0025e-05 - rmse: 0.0084 - val_loss: 1.7487e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1878/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5630e-05 - rmse: 0.0081\n",
      "Epoch 1878: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6955e-05 - rmse: 0.0075 - val_loss: 5.3270e-05 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 1879/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4049e-05 - rmse: 0.0037\n",
      "Epoch 1879: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0035e-05 - rmse: 0.0071 - val_loss: 8.5079e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1880/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4367e-05 - rmse: 0.0086\n",
      "Epoch 1880: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5128e-05 - rmse: 0.0067 - val_loss: 1.1546e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1881/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8194e-05 - rmse: 0.0053\n",
      "Epoch 1881: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5516e-05 - rmse: 0.0067 - val_loss: 6.4526e-05 - val_rmse: 0.0080 - lr: 1.0000e-04\n",
      "Epoch 1882/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9231e-05 - rmse: 0.0054\n",
      "Epoch 1882: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0157e-05 - rmse: 0.0045 - val_loss: 9.2283e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1883/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6141e-05 - rmse: 0.0040\n",
      "Epoch 1883: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1249e-05 - rmse: 0.0046 - val_loss: 5.8584e-05 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1884/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5448e-05 - rmse: 0.0067\n",
      "Epoch 1884: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9031e-05 - rmse: 0.0054 - val_loss: 9.6289e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1885/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4750e-05 - rmse: 0.0038\n",
      "Epoch 1885: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1115e-05 - rmse: 0.0046 - val_loss: 1.0985e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1886/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5230e-05 - rmse: 0.0059\n",
      "Epoch 1886: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9017e-05 - rmse: 0.0044 - val_loss: 6.1504e-05 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 1887/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1749e-06 - rmse: 0.0030\n",
      "Epoch 1887: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4376e-05 - rmse: 0.0038 - val_loss: 1.1258e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1888/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6981e-05 - rmse: 0.0052\n",
      "Epoch 1888: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9137e-05 - rmse: 0.0054 - val_loss: 6.6534e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1889/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1608e-05 - rmse: 0.0056\n",
      "Epoch 1889: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9265e-05 - rmse: 0.0044 - val_loss: 9.3225e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1890/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4615e-05 - rmse: 0.0038\n",
      "Epoch 1890: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1757e-05 - rmse: 0.0034 - val_loss: 7.3801e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1891/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2627e-06 - rmse: 0.0029\n",
      "Epoch 1891: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5253e-05 - rmse: 0.0039 - val_loss: 1.1504e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1892/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2535e-05 - rmse: 0.0065\n",
      "Epoch 1892: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6643e-05 - rmse: 0.0052 - val_loss: 7.1944e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1893/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9228e-05 - rmse: 0.0044\n",
      "Epoch 1893: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7412e-05 - rmse: 0.0042 - val_loss: 8.2325e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1894/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3394e-05 - rmse: 0.0048\n",
      "Epoch 1894: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0776e-05 - rmse: 0.0046 - val_loss: 6.0528e-05 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 1895/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2416e-05 - rmse: 0.0035\n",
      "Epoch 1895: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6736e-06 - rmse: 0.0031 - val_loss: 9.8803e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n",
      "Epoch 1896/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2821e-06 - rmse: 0.0030\n",
      "Epoch 1896: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1234e-05 - rmse: 0.0034 - val_loss: 4.9956e-05 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 1897/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2730e-06 - rmse: 0.0029\n",
      "Epoch 1897: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3011e-05 - rmse: 0.0036 - val_loss: 1.1826e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1898/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5846e-05 - rmse: 0.0040\n",
      "Epoch 1898: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0001e-05 - rmse: 0.0032 - val_loss: 5.9837e-05 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1899/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6955e-06 - rmse: 0.0016\n",
      "Epoch 1899: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1617e-05 - rmse: 0.0056 - val_loss: 1.7818e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1900/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4412e-05 - rmse: 0.0086\n",
      "Epoch 1900: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1497e-05 - rmse: 0.0056 - val_loss: 5.1480e-05 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 1901/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2112e-05 - rmse: 0.0047\n",
      "Epoch 1901: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9053e-05 - rmse: 0.0044 - val_loss: 9.4640e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1902/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0951e-05 - rmse: 0.0046\n",
      "Epoch 1902: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8536e-05 - rmse: 0.0053 - val_loss: 5.3573e-05 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 1903/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3453e-05 - rmse: 0.0037\n",
      "Epoch 1903: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8280e-05 - rmse: 0.0043 - val_loss: 1.0893e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1904/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4655e-05 - rmse: 0.0059\n",
      "Epoch 1904: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1287e-05 - rmse: 0.0056 - val_loss: 8.9296e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1905/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6729e-05 - rmse: 0.0052\n",
      "Epoch 1905: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9308e-05 - rmse: 0.0044 - val_loss: 7.2542e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1906/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5001e-06 - rmse: 0.0025\n",
      "Epoch 1906: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7933e-05 - rmse: 0.0042 - val_loss: 4.6785e-05 - val_rmse: 0.0068 - lr: 1.0000e-04\n",
      "Epoch 1907/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6104e-05 - rmse: 0.0040\n",
      "Epoch 1907: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0184e-05 - rmse: 0.0032 - val_loss: 9.3501e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1908/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2939e-06 - rmse: 0.0018\n",
      "Epoch 1908: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7969e-06 - rmse: 0.0026 - val_loss: 5.4314e-05 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 1909/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0854e-06 - rmse: 0.0027\n",
      "Epoch 1909: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2357e-06 - rmse: 0.0023 - val_loss: 6.8210e-05 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1910/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9257e-06 - rmse: 0.0020\n",
      "Epoch 1910: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2137e-06 - rmse: 0.0025 - val_loss: 5.2004e-05 - val_rmse: 0.0072 - lr: 1.0000e-04\n",
      "Epoch 1911/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0887e-05 - rmse: 0.0033\n",
      "Epoch 1911: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5608e-06 - rmse: 0.0026 - val_loss: 7.2417e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1912/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4529e-06 - rmse: 0.0016\n",
      "Epoch 1912: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2578e-06 - rmse: 0.0027 - val_loss: 5.6682e-05 - val_rmse: 0.0075 - lr: 1.0000e-04\n",
      "Epoch 1913/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3016e-05 - rmse: 0.0036\n",
      "Epoch 1913: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6063e-06 - rmse: 0.0029 - val_loss: 1.1455e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1914/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7568e-05 - rmse: 0.0042\n",
      "Epoch 1914: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3788e-05 - rmse: 0.0037 - val_loss: 5.4562e-05 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 1915/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8106e-05 - rmse: 0.0043\n",
      "Epoch 1915: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0303e-05 - rmse: 0.0032 - val_loss: 8.3756e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1916/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5381e-06 - rmse: 0.0021\n",
      "Epoch 1916: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6765e-06 - rmse: 0.0026 - val_loss: 5.3582e-05 - val_rmse: 0.0073 - lr: 1.0000e-04\n",
      "Epoch 1917/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2443e-06 - rmse: 0.0025\n",
      "Epoch 1917: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8459e-06 - rmse: 0.0026 - val_loss: 6.3760e-05 - val_rmse: 0.0080 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1918/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3915e-06 - rmse: 0.0025\n",
      "Epoch 1918: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0520e-06 - rmse: 0.0028 - val_loss: 8.0642e-05 - val_rmse: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 1919/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0270e-06 - rmse: 0.0010\n",
      "Epoch 1919: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0404e-05 - rmse: 0.0032 - val_loss: 4.8972e-05 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 1920/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2110e-05 - rmse: 0.0047\n",
      "Epoch 1920: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3576e-05 - rmse: 0.0049 - val_loss: 1.3849e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1921/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7088e-05 - rmse: 0.0041\n",
      "Epoch 1921: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2607e-05 - rmse: 0.0057 - val_loss: 4.9197e-05 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 1922/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9935e-05 - rmse: 0.0055\n",
      "Epoch 1922: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2103e-05 - rmse: 0.0057 - val_loss: 1.1332e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1923/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2276e-06 - rmse: 0.0029\n",
      "Epoch 1923: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3624e-05 - rmse: 0.0049 - val_loss: 5.4897e-05 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 1924/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4182e-05 - rmse: 0.0049\n",
      "Epoch 1924: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9814e-05 - rmse: 0.0063 - val_loss: 1.4439e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1925/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8065e-05 - rmse: 0.0062\n",
      "Epoch 1925: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8430e-05 - rmse: 0.0089 - val_loss: 1.1744e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1926/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6153e-04 - rmse: 0.0127\n",
      "Epoch 1926: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1264e-04 - rmse: 0.0106 - val_loss: 3.0115e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 1927/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5102e-04 - rmse: 0.0123\n",
      "Epoch 1927: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2136e-04 - rmse: 0.0110 - val_loss: 6.0975e-05 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 1928/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2851e-05 - rmse: 0.0073\n",
      "Epoch 1928: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7284e-04 - rmse: 0.0131 - val_loss: 7.6953e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1929/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2972e-05 - rmse: 0.0085\n",
      "Epoch 1929: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6005e-04 - rmse: 0.0161 - val_loss: 2.2793e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1930/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2378e-05 - rmse: 0.0079\n",
      "Epoch 1930: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4211e-04 - rmse: 0.0210 - val_loss: 2.8469e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 1931/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4390e-04 - rmse: 0.0211\n",
      "Epoch 1931: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0199e-04 - rmse: 0.0142 - val_loss: 2.2033e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1932/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3638e-05 - rmse: 0.0091\n",
      "Epoch 1932: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5938e-05 - rmse: 0.0081 - val_loss: 1.0153e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1933/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3496e-05 - rmse: 0.0073\n",
      "Epoch 1933: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8183e-05 - rmse: 0.0076 - val_loss: 1.1687e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1934/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2068e-05 - rmse: 0.0065\n",
      "Epoch 1934: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5363e-05 - rmse: 0.0074 - val_loss: 1.3845e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1935/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4279e-05 - rmse: 0.0086\n",
      "Epoch 1935: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6434e-05 - rmse: 0.0082 - val_loss: 6.2098e-05 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 1936/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6623e-05 - rmse: 0.0041\n",
      "Epoch 1936: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2757e-05 - rmse: 0.0057 - val_loss: 8.5573e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 1937/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4626e-05 - rmse: 0.0059\n",
      "Epoch 1937: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0201e-05 - rmse: 0.0055 - val_loss: 1.2469e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1938/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6287e-05 - rmse: 0.0075\n",
      "Epoch 1938: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.8997e-05 - rmse: 0.0077 - val_loss: 1.2617e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1939/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4285e-05 - rmse: 0.0049\n",
      "Epoch 1939: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.0586e-05 - rmse: 0.0064 - val_loss: 1.1222e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1940/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3383e-05 - rmse: 0.0073\n",
      "Epoch 1940: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2559e-05 - rmse: 0.0079 - val_loss: 8.2259e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1941/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9729e-05 - rmse: 0.0071\n",
      "Epoch 1941: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1874e-04 - rmse: 0.0109 - val_loss: 8.4648e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1942/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1167e-05 - rmse: 0.0056\n",
      "Epoch 1942: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2728e-04 - rmse: 0.0113 - val_loss: 2.7099e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 1943/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9309e-04 - rmse: 0.0139\n",
      "Epoch 1943: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6856e-04 - rmse: 0.0130 - val_loss: 1.9267e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1944/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6142e-04 - rmse: 0.0127\n",
      "Epoch 1944: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4438e-05 - rmse: 0.0097 - val_loss: 1.1340e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1945/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3768e-05 - rmse: 0.0037\n",
      "Epoch 1945: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3881e-05 - rmse: 0.0073 - val_loss: 8.7005e-05 - val_rmse: 0.0093 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1946/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3635e-05 - rmse: 0.0097\n",
      "Epoch 1946: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7343e-05 - rmse: 0.0069 - val_loss: 2.0390e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1947/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1999e-04 - rmse: 0.0110\n",
      "Epoch 1947: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3797e-05 - rmse: 0.0073 - val_loss: 1.1476e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1948/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4153e-05 - rmse: 0.0080\n",
      "Epoch 1948: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2675e-05 - rmse: 0.0073 - val_loss: 1.9524e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1949/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8654e-05 - rmse: 0.0077\n",
      "Epoch 1949: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5101e-05 - rmse: 0.0067 - val_loss: 8.7981e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1950/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4477e-05 - rmse: 0.0038\n",
      "Epoch 1950: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9372e-05 - rmse: 0.0063 - val_loss: 1.3834e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1951/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7413e-05 - rmse: 0.0088\n",
      "Epoch 1951: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4584e-05 - rmse: 0.0097 - val_loss: 1.5005e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1952/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4523e-05 - rmse: 0.0067\n",
      "Epoch 1952: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0072e-04 - rmse: 0.0100 - val_loss: 1.3613e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1953/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4432e-04 - rmse: 0.0120\n",
      "Epoch 1953: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.3096e-05 - rmse: 0.0091 - val_loss: 1.1720e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1954/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4028e-04 - rmse: 0.0118\n",
      "Epoch 1954: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7053e-05 - rmse: 0.0099 - val_loss: 3.3928e-04 - val_rmse: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 1955/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1495e-04 - rmse: 0.0147\n",
      "Epoch 1955: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0179e-04 - rmse: 0.0101 - val_loss: 7.3134e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1956/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5353e-05 - rmse: 0.0050\n",
      "Epoch 1956: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9410e-05 - rmse: 0.0083 - val_loss: 1.9749e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 1957/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5800e-05 - rmse: 0.0093\n",
      "Epoch 1957: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4594e-04 - rmse: 0.0121 - val_loss: 2.1365e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1958/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7229e-04 - rmse: 0.0131\n",
      "Epoch 1958: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0284e-04 - rmse: 0.0101 - val_loss: 1.8484e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1959/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0413e-04 - rmse: 0.0102\n",
      "Epoch 1959: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3583e-05 - rmse: 0.0091 - val_loss: 8.1977e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1960/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0946e-05 - rmse: 0.0064\n",
      "Epoch 1960: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3591e-05 - rmse: 0.0066 - val_loss: 8.2587e-05 - val_rmse: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 1961/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2680e-06 - rmse: 0.0023\n",
      "Epoch 1961: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7454e-05 - rmse: 0.0052 - val_loss: 9.6202e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1962/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1425e-05 - rmse: 0.0034\n",
      "Epoch 1962: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2193e-05 - rmse: 0.0047 - val_loss: 6.0580e-05 - val_rmse: 0.0078 - lr: 1.0000e-04\n",
      "Epoch 1963/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9250e-06 - rmse: 0.0030\n",
      "Epoch 1963: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1353e-05 - rmse: 0.0034 - val_loss: 5.0544e-05 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 1964/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1560e-06 - rmse: 0.0025\n",
      "Epoch 1964: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2337e-05 - rmse: 0.0035 - val_loss: 5.9528e-05 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1965/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3734e-06 - rmse: 0.0023\n",
      "Epoch 1965: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2391e-06 - rmse: 0.0029 - val_loss: 6.2523e-05 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 1966/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7557e-06 - rmse: 0.0026\n",
      "Epoch 1966: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0098e-06 - rmse: 0.0028 - val_loss: 7.6755e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1967/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8600e-06 - rmse: 0.0030\n",
      "Epoch 1967: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6720e-06 - rmse: 0.0024 - val_loss: 7.3064e-05 - val_rmse: 0.0085 - lr: 1.0000e-04\n",
      "Epoch 1968/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8748e-06 - rmse: 0.0028\n",
      "Epoch 1968: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3078e-06 - rmse: 0.0025 - val_loss: 6.9384e-05 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1969/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8614e-06 - rmse: 0.0028\n",
      "Epoch 1969: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0945e-05 - rmse: 0.0033 - val_loss: 6.8633e-05 - val_rmse: 0.0083 - lr: 1.0000e-04\n",
      "Epoch 1970/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1631e-06 - rmse: 0.0025\n",
      "Epoch 1970: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4347e-06 - rmse: 0.0031 - val_loss: 4.9073e-05 - val_rmse: 0.0070 - lr: 1.0000e-04\n",
      "Epoch 1971/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8848e-06 - rmse: 0.0028\n",
      "Epoch 1971: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1699e-06 - rmse: 0.0029 - val_loss: 9.3117e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1972/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1265e-06 - rmse: 0.0027\n",
      "Epoch 1972: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0925e-05 - rmse: 0.0033 - val_loss: 5.5162e-05 - val_rmse: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 1973/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3385e-05 - rmse: 0.0037\n",
      "Epoch 1973: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2982e-05 - rmse: 0.0036 - val_loss: 7.5590e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1974/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0824e-06 - rmse: 0.0027\n",
      "Epoch 1974: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2703e-05 - rmse: 0.0036 - val_loss: 8.7434e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1975/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9779e-05 - rmse: 0.0055\n",
      "Epoch 1975: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5610e-05 - rmse: 0.0060 - val_loss: 8.8484e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1976/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1270e-05 - rmse: 0.0034\n",
      "Epoch 1976: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6185e-05 - rmse: 0.0040 - val_loss: 5.0482e-05 - val_rmse: 0.0071 - lr: 1.0000e-04\n",
      "Epoch 1977/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5971e-05 - rmse: 0.0040\n",
      "Epoch 1977: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2986e-05 - rmse: 0.0036 - val_loss: 8.9272e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1978/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2706e-05 - rmse: 0.0036\n",
      "Epoch 1978: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5125e-05 - rmse: 0.0039 - val_loss: 6.1726e-05 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 1979/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6537e-06 - rmse: 0.0022\n",
      "Epoch 1979: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2862e-05 - rmse: 0.0036 - val_loss: 6.7434e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1980/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5277e-06 - rmse: 0.0021\n",
      "Epoch 1980: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1056e-06 - rmse: 0.0028 - val_loss: 5.9615e-05 - val_rmse: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 1981/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2150e-06 - rmse: 0.0027\n",
      "Epoch 1981: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4612e-06 - rmse: 0.0023 - val_loss: 7.6862e-05 - val_rmse: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 1982/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8948e-06 - rmse: 0.0020\n",
      "Epoch 1982: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1617e-06 - rmse: 0.0023 - val_loss: 6.7503e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1983/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8098e-06 - rmse: 0.0020\n",
      "Epoch 1983: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7156e-06 - rmse: 0.0022 - val_loss: 7.4211e-05 - val_rmse: 0.0086 - lr: 1.0000e-04\n",
      "Epoch 1984/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5285e-06 - rmse: 0.0024\n",
      "Epoch 1984: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3877e-06 - rmse: 0.0025 - val_loss: 6.2234e-05 - val_rmse: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 1985/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5833e-06 - rmse: 0.0024\n",
      "Epoch 1985: val_loss did not improve from 0.00005\n",
      "\n",
      "Epoch 1985: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3968e-06 - rmse: 0.0021 - val_loss: 6.7707e-05 - val_rmse: 0.0082 - lr: 1.0000e-04\n",
      "Epoch 1986/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6785e-06 - rmse: 0.0016\n",
      "Epoch 1986: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0955e-06 - rmse: 0.0020 - val_loss: 5.7052e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 1987/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7084e-06 - rmse: 0.0024\n",
      "Epoch 1987: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5780e-06 - rmse: 0.0019 - val_loss: 6.9461e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 1988/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1486e-06 - rmse: 0.0018\n",
      "Epoch 1988: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8750e-06 - rmse: 0.0020 - val_loss: 6.3275e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 1989/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2514e-06 - rmse: 0.0021\n",
      "Epoch 1989: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3082e-06 - rmse: 0.0018 - val_loss: 6.7186e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 1990/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6117e-06 - rmse: 0.0028\n",
      "Epoch 1990: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2622e-06 - rmse: 0.0018 - val_loss: 6.4362e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 1991/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1816e-06 - rmse: 0.0015\n",
      "Epoch 1991: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3952e-06 - rmse: 0.0015 - val_loss: 6.4711e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 1992/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3680e-06 - rmse: 0.0018\n",
      "Epoch 1992: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9587e-06 - rmse: 0.0020 - val_loss: 6.7167e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 1993/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3141e-06 - rmse: 0.0018\n",
      "Epoch 1993: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1562e-06 - rmse: 0.0020 - val_loss: 6.9367e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 1994/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1076e-06 - rmse: 0.0027\n",
      "Epoch 1994: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1823e-06 - rmse: 0.0020 - val_loss: 6.1937e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 1995/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7459e-06 - rmse: 0.0013\n",
      "Epoch 1995: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8328e-06 - rmse: 0.0017 - val_loss: 6.8426e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 1996/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6865e-06 - rmse: 0.0024\n",
      "Epoch 1996: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0419e-06 - rmse: 0.0017 - val_loss: 6.2997e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 1997/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3024e-06 - rmse: 0.0021\n",
      "Epoch 1997: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7333e-06 - rmse: 0.0017 - val_loss: 6.7060e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 1998/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6256e-06 - rmse: 0.0019\n",
      "Epoch 1998: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2484e-06 - rmse: 0.0015 - val_loss: 7.1991e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 1999/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0554e-06 - rmse: 0.0020\n",
      "Epoch 1999: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4267e-06 - rmse: 0.0019 - val_loss: 6.3728e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2000/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3074e-06 - rmse: 0.0025\n",
      "Epoch 2000: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7642e-06 - rmse: 0.0017 - val_loss: 6.7811e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2001/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8734e-06 - rmse: 0.0017\n",
      "Epoch 2001: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5376e-06 - rmse: 0.0016 - val_loss: 6.8666e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2002/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2656e-06 - rmse: 0.0011\n",
      "Epoch 2002: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5865e-06 - rmse: 0.0016 - val_loss: 6.7701e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2003/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7065e-06 - rmse: 0.0013\n",
      "Epoch 2003: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2159e-06 - rmse: 0.0015 - val_loss: 6.1878e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2004/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2682e-06 - rmse: 0.0021\n",
      "Epoch 2004: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8036e-06 - rmse: 0.0017 - val_loss: 6.8391e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2005/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3046e-06 - rmse: 0.0011\n",
      "Epoch 2005: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0490e-06 - rmse: 0.0017 - val_loss: 6.4751e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2006/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6153e-06 - rmse: 0.0021\n",
      "Epoch 2006: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9664e-06 - rmse: 0.0020 - val_loss: 6.3808e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2007/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8668e-06 - rmse: 0.0014\n",
      "Epoch 2007: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4730e-06 - rmse: 0.0019 - val_loss: 7.1981e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2008/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1044e-07 - rmse: 9.0025e-04\n",
      "Epoch 2008: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6591e-06 - rmse: 0.0016 - val_loss: 6.2341e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2009/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1672e-06 - rmse: 0.0011\n",
      "Epoch 2009: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3129e-06 - rmse: 0.0015 - val_loss: 7.1732e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2010/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7527e-07 - rmse: 9.3556e-04\n",
      "Epoch 2010: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0491e-06 - rmse: 0.0014 - val_loss: 6.3136e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2011/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1584e-06 - rmse: 0.0011\n",
      "Epoch 2011: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5304e-06 - rmse: 0.0016 - val_loss: 6.9542e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2012/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1095e-06 - rmse: 0.0025\n",
      "Epoch 2012: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1182e-06 - rmse: 0.0015 - val_loss: 6.8137e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2013/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1910e-06 - rmse: 0.0011\n",
      "Epoch 2013: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9876e-06 - rmse: 0.0014 - val_loss: 6.5420e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2014/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8077e-06 - rmse: 0.0022\n",
      "Epoch 2014: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9060e-06 - rmse: 0.0014 - val_loss: 6.8022e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2015/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2277e-06 - rmse: 0.0011\n",
      "Epoch 2015: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9835e-06 - rmse: 0.0014 - val_loss: 6.2638e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2016/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7807e-07 - rmse: 9.8897e-04\n",
      "Epoch 2016: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3129e-06 - rmse: 0.0015 - val_loss: 6.7852e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2017/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0872e-06 - rmse: 0.0010\n",
      "Epoch 2017: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9581e-06 - rmse: 0.0017 - val_loss: 6.5583e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2018/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7732e-06 - rmse: 0.0013\n",
      "Epoch 2018: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3279e-06 - rmse: 0.0018 - val_loss: 6.5376e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2019/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0446e-06 - rmse: 0.0014\n",
      "Epoch 2019: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4488e-06 - rmse: 0.0019 - val_loss: 6.9290e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2020/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7967e-06 - rmse: 0.0013\n",
      "Epoch 2020: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4945e-06 - rmse: 0.0016 - val_loss: 6.7651e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2021/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6045e-06 - rmse: 0.0013\n",
      "Epoch 2021: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0982e-06 - rmse: 0.0014 - val_loss: 7.1462e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2022/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2589e-06 - rmse: 0.0015\n",
      "Epoch 2022: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2171e-06 - rmse: 0.0015 - val_loss: 6.6737e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2023/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5709e-06 - rmse: 0.0013\n",
      "Epoch 2023: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1597e-06 - rmse: 0.0015 - val_loss: 6.3698e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2024/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5015e-06 - rmse: 0.0019\n",
      "Epoch 2024: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0796e-06 - rmse: 0.0014 - val_loss: 7.1112e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2025/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1453e-06 - rmse: 0.0011\n",
      "Epoch 2025: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2757e-06 - rmse: 0.0015 - val_loss: 6.8871e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2026/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8890e-07 - rmse: 9.4282e-04\n",
      "Epoch 2026: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0616e-06 - rmse: 0.0014 - val_loss: 6.4851e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2027/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5935e-07 - rmse: 9.7947e-04\n",
      "Epoch 2027: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4479e-06 - rmse: 0.0016 - val_loss: 7.4510e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2028/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0535e-06 - rmse: 0.0014\n",
      "Epoch 2028: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2938e-06 - rmse: 0.0015 - val_loss: 6.3478e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2029/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0397e-06 - rmse: 0.0017\n",
      "Epoch 2029: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6388e-06 - rmse: 0.0016 - val_loss: 7.0085e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2030/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4892e-06 - rmse: 0.0012\n",
      "Epoch 2030: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6742e-06 - rmse: 0.0019 - val_loss: 6.4991e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2031/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2614e-06 - rmse: 0.0011\n",
      "Epoch 2031: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5441e-06 - rmse: 0.0019 - val_loss: 6.6674e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2032/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3508e-06 - rmse: 0.0012\n",
      "Epoch 2032: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8062e-06 - rmse: 0.0017 - val_loss: 7.3362e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2033/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9199e-06 - rmse: 0.0020\n",
      "Epoch 2033: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8843e-06 - rmse: 0.0017 - val_loss: 6.4622e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2034/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0348e-06 - rmse: 0.0014\n",
      "Epoch 2034: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0001e-06 - rmse: 0.0017 - val_loss: 8.2358e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2035/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6993e-06 - rmse: 0.0013\n",
      "Epoch 2035: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6945e-06 - rmse: 0.0016 - val_loss: 6.3738e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2036/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4475e-06 - rmse: 0.0012\n",
      "Epoch 2036: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1558e-06 - rmse: 0.0015 - val_loss: 6.8149e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2037/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7441e-06 - rmse: 0.0017\n",
      "Epoch 2037: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0269e-06 - rmse: 0.0014 - val_loss: 6.9509e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2038/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1422e-06 - rmse: 0.0011\n",
      "Epoch 2038: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3590e-06 - rmse: 0.0015 - val_loss: 6.8838e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2039/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5149e-06 - rmse: 0.0016\n",
      "Epoch 2039: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0340e-06 - rmse: 0.0017 - val_loss: 7.0502e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2040/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2237e-06 - rmse: 0.0023\n",
      "Epoch 2040: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3959e-06 - rmse: 0.0018 - val_loss: 7.3444e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2041/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1034e-06 - rmse: 0.0023\n",
      "Epoch 2041: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6039e-06 - rmse: 0.0019 - val_loss: 7.1885e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2042/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1401e-06 - rmse: 0.0023\n",
      "Epoch 2042: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5109e-06 - rmse: 0.0016 - val_loss: 6.4055e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2043/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1954e-07 - rmse: 8.4826e-04\n",
      "Epoch 2043: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2814e-06 - rmse: 0.0015 - val_loss: 7.0613e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2044/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0436e-06 - rmse: 0.0014\n",
      "Epoch 2044: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4795e-06 - rmse: 0.0016 - val_loss: 6.3286e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2045/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2124e-06 - rmse: 0.0011\n",
      "Epoch 2045: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0497e-06 - rmse: 0.0014 - val_loss: 6.5895e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2046/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0449e-06 - rmse: 0.0010\n",
      "Epoch 2046: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2960e-06 - rmse: 0.0015 - val_loss: 7.0462e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2047/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8252e-07 - rmse: 9.3943e-04\n",
      "Epoch 2047: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1223e-06 - rmse: 0.0018 - val_loss: 7.0025e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2048/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3916e-06 - rmse: 0.0012\n",
      "Epoch 2048: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0366e-06 - rmse: 0.0014 - val_loss: 6.3371e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2049/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8372e-06 - rmse: 0.0017\n",
      "Epoch 2049: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1306e-06 - rmse: 0.0015 - val_loss: 7.4547e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2050/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6764e-06 - rmse: 0.0024\n",
      "Epoch 2050: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9684e-06 - rmse: 0.0017 - val_loss: 6.4516e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2051/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8408e-06 - rmse: 0.0028\n",
      "Epoch 2051: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7130e-06 - rmse: 0.0022 - val_loss: 7.5949e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2052/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2976e-06 - rmse: 0.0021\n",
      "Epoch 2052: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7358e-06 - rmse: 0.0024 - val_loss: 6.9017e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2053/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1118e-06 - rmse: 0.0018\n",
      "Epoch 2053: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6691e-06 - rmse: 0.0019 - val_loss: 6.8950e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2054/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5834e-06 - rmse: 0.0013\n",
      "Epoch 2054: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2915e-06 - rmse: 0.0018 - val_loss: 7.6784e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2055/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3456e-06 - rmse: 0.0018\n",
      "Epoch 2055: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3558e-06 - rmse: 0.0021 - val_loss: 7.8820e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2056/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4405e-06 - rmse: 0.0029\n",
      "Epoch 2056: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3761e-06 - rmse: 0.0021 - val_loss: 7.3384e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2057/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6416e-06 - rmse: 0.0013\n",
      "Epoch 2057: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2925e-06 - rmse: 0.0018 - val_loss: 6.7259e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2058/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1262e-06 - rmse: 0.0020\n",
      "Epoch 2058: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6679e-06 - rmse: 0.0016 - val_loss: 6.6227e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2059/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0476e-06 - rmse: 0.0010\n",
      "Epoch 2059: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9171e-06 - rmse: 0.0014 - val_loss: 6.8604e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2060/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2993e-06 - rmse: 0.0018\n",
      "Epoch 2060: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0809e-06 - rmse: 0.0014 - val_loss: 7.0897e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2061/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0655e-06 - rmse: 0.0010\n",
      "Epoch 2061: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1102e-06 - rmse: 0.0015 - val_loss: 6.6819e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2062/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3079e-06 - rmse: 0.0011\n",
      "Epoch 2062: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8016e-06 - rmse: 0.0013 - val_loss: 6.9595e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2063/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0545e-06 - rmse: 0.0014\n",
      "Epoch 2063: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2362e-06 - rmse: 0.0015 - val_loss: 6.6080e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2064/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4169e-06 - rmse: 0.0012\n",
      "Epoch 2064: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0313e-06 - rmse: 0.0014 - val_loss: 7.5278e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2065/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7918e-06 - rmse: 0.0013\n",
      "Epoch 2065: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7204e-06 - rmse: 0.0016 - val_loss: 6.6828e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2066/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0714e-06 - rmse: 0.0014\n",
      "Epoch 2066: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8924e-06 - rmse: 0.0022 - val_loss: 7.3823e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2067/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9979e-06 - rmse: 0.0014\n",
      "Epoch 2067: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8514e-06 - rmse: 0.0022 - val_loss: 6.6898e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2068/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7648e-06 - rmse: 0.0013\n",
      "Epoch 2068: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8136e-06 - rmse: 0.0017 - val_loss: 7.5613e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2069/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0594e-06 - rmse: 0.0010\n",
      "Epoch 2069: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4471e-06 - rmse: 0.0016 - val_loss: 6.0414e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2070/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0261e-06 - rmse: 0.0010\n",
      "Epoch 2070: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1099e-06 - rmse: 0.0020 - val_loss: 6.8543e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2071/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2381e-07 - rmse: 7.8982e-04\n",
      "Epoch 2071: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2074e-06 - rmse: 0.0025 - val_loss: 6.9784e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2072/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4029e-06 - rmse: 0.0016\n",
      "Epoch 2072: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7132e-06 - rmse: 0.0026 - val_loss: 6.6983e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2073/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6693e-06 - rmse: 0.0022\n",
      "Epoch 2073: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6586e-06 - rmse: 0.0022 - val_loss: 8.7117e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 2074/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0843e-05 - rmse: 0.0033\n",
      "Epoch 2074: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4567e-06 - rmse: 0.0023 - val_loss: 6.3870e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2075/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8124e-06 - rmse: 0.0020\n",
      "Epoch 2075: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2972e-06 - rmse: 0.0018 - val_loss: 6.7931e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2076/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2267e-06 - rmse: 0.0015\n",
      "Epoch 2076: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9987e-06 - rmse: 0.0017 - val_loss: 7.0418e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2077/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4076e-06 - rmse: 0.0016\n",
      "Epoch 2077: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3151e-06 - rmse: 0.0021 - val_loss: 6.1273e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2078/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6613e-06 - rmse: 0.0019\n",
      "Epoch 2078: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9690e-06 - rmse: 0.0017 - val_loss: 8.5229e-05 - val_rmse: 0.0092 - lr: 5.0000e-05\n",
      "Epoch 2079/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9995e-06 - rmse: 0.0024\n",
      "Epoch 2079: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6362e-06 - rmse: 0.0019 - val_loss: 5.8885e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2080/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4100e-06 - rmse: 0.0021\n",
      "Epoch 2080: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6430e-06 - rmse: 0.0019 - val_loss: 7.5470e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2081/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5793e-06 - rmse: 0.0021\n",
      "Epoch 2081: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5640e-06 - rmse: 0.0016 - val_loss: 6.3450e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2082/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0947e-06 - rmse: 0.0010\n",
      "Epoch 2082: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6647e-06 - rmse: 0.0019 - val_loss: 6.4005e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2083/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3800e-06 - rmse: 0.0021\n",
      "Epoch 2083: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2886e-06 - rmse: 0.0021 - val_loss: 7.9901e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2084/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3876e-06 - rmse: 0.0018\n",
      "Epoch 2084: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6732e-06 - rmse: 0.0016 - val_loss: 5.8054e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2085/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2888e-07 - rmse: 9.1043e-04\n",
      "Epoch 2085: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0002e-06 - rmse: 0.0017 - val_loss: 7.9028e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2086/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5856e-06 - rmse: 0.0016\n",
      "Epoch 2086: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6471e-06 - rmse: 0.0019 - val_loss: 6.3937e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2087/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3998e-06 - rmse: 0.0015\n",
      "Epoch 2087: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4231e-06 - rmse: 0.0025 - val_loss: 6.5662e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2088/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5767e-06 - rmse: 0.0021\n",
      "Epoch 2088: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3764e-06 - rmse: 0.0027 - val_loss: 7.9930e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2089/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7514e-06 - rmse: 0.0019\n",
      "Epoch 2089: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7351e-06 - rmse: 0.0019 - val_loss: 6.3653e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2090/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1800e-05 - rmse: 0.0034\n",
      "Epoch 2090: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4138e-06 - rmse: 0.0021 - val_loss: 8.4945e-05 - val_rmse: 0.0092 - lr: 5.0000e-05\n",
      "Epoch 2091/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4075e-06 - rmse: 0.0025\n",
      "Epoch 2091: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7279e-06 - rmse: 0.0022 - val_loss: 6.3975e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2092/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5318e-06 - rmse: 0.0012\n",
      "Epoch 2092: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0965e-06 - rmse: 0.0014 - val_loss: 7.2025e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2093/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6597e-07 - rmse: 9.8284e-04\n",
      "Epoch 2093: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4192e-06 - rmse: 0.0016 - val_loss: 6.4483e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2094/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6999e-07 - rmse: 7.5498e-04\n",
      "Epoch 2094: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6769e-06 - rmse: 0.0016 - val_loss: 6.8479e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2095/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2814e-07 - rmse: 9.1002e-04\n",
      "Epoch 2095: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1982e-06 - rmse: 0.0015 - val_loss: 6.8842e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2096/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2262e-06 - rmse: 0.0015\n",
      "Epoch 2096: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5599e-06 - rmse: 0.0012 - val_loss: 6.1528e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2097/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2897e-06 - rmse: 0.0018\n",
      "Epoch 2097: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0724e-06 - rmse: 0.0014 - val_loss: 7.3649e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2098/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3122e-06 - rmse: 0.0021\n",
      "Epoch 2098: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7641e-06 - rmse: 0.0017 - val_loss: 6.4566e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2099/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4985e-06 - rmse: 0.0016\n",
      "Epoch 2099: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3432e-06 - rmse: 0.0018 - val_loss: 7.5508e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2100/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0769e-06 - rmse: 0.0027\n",
      "Epoch 2100: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6406e-06 - rmse: 0.0026 - val_loss: 6.4110e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2101/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1124e-06 - rmse: 0.0015\n",
      "Epoch 2101: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2375e-06 - rmse: 0.0025 - val_loss: 7.3392e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2102/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0879e-06 - rmse: 0.0014\n",
      "Epoch 2102: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1728e-06 - rmse: 0.0027 - val_loss: 7.3823e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2103/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3032e-06 - rmse: 0.0025\n",
      "Epoch 2103: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7947e-06 - rmse: 0.0026 - val_loss: 7.1042e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2104/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5614e-05 - rmse: 0.0040\n",
      "Epoch 2104: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0149e-05 - rmse: 0.0032 - val_loss: 8.1920e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2105/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5177e-06 - rmse: 0.0031\n",
      "Epoch 2105: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6514e-06 - rmse: 0.0024 - val_loss: 6.1386e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2106/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5847e-06 - rmse: 0.0031\n",
      "Epoch 2106: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8048e-06 - rmse: 0.0022 - val_loss: 7.9483e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2107/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2599e-06 - rmse: 0.0018\n",
      "Epoch 2107: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3668e-06 - rmse: 0.0029 - val_loss: 5.4449e-05 - val_rmse: 0.0074 - lr: 5.0000e-05\n",
      "Epoch 2108/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3874e-06 - rmse: 0.0015\n",
      "Epoch 2108: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0208e-05 - rmse: 0.0032 - val_loss: 7.5255e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2109/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4054e-06 - rmse: 0.0012\n",
      "Epoch 2109: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1942e-05 - rmse: 0.0035 - val_loss: 7.8875e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2110/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5174e-06 - rmse: 0.0016\n",
      "Epoch 2110: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7000e-06 - rmse: 0.0028 - val_loss: 5.2368e-05 - val_rmse: 0.0072 - lr: 5.0000e-05\n",
      "Epoch 2111/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0596e-05 - rmse: 0.0033\n",
      "Epoch 2111: val_loss did not improve from 0.00005\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2212e-06 - rmse: 0.0027 - val_loss: 9.3688e-05 - val_rmse: 0.0097 - lr: 5.0000e-05\n",
      "Epoch 2112/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5069e-06 - rmse: 0.0026\n",
      "Epoch 2112: val_loss improved from 0.00005 to 0.00004, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 5.8222e-06 - rmse: 0.0024 - val_loss: 4.3715e-05 - val_rmse: 0.0066 - lr: 5.0000e-05\n",
      "Epoch 2113/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0931e-06 - rmse: 0.0018\n",
      "Epoch 2113: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2523e-05 - rmse: 0.0035 - val_loss: 1.0876e-04 - val_rmse: 0.0104 - lr: 5.0000e-05\n",
      "Epoch 2114/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0774e-05 - rmse: 0.0033\n",
      "Epoch 2114: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1891e-05 - rmse: 0.0034 - val_loss: 5.7593e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2115/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1122e-06 - rmse: 0.0028\n",
      "Epoch 2115: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9309e-06 - rmse: 0.0028 - val_loss: 7.1441e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2116/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0218e-06 - rmse: 0.0017\n",
      "Epoch 2116: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1861e-06 - rmse: 0.0023 - val_loss: 6.2846e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2117/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4749e-06 - rmse: 0.0012\n",
      "Epoch 2117: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9578e-06 - rmse: 0.0020 - val_loss: 7.1322e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2118/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2823e-06 - rmse: 0.0018\n",
      "Epoch 2118: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8084e-06 - rmse: 0.0022 - val_loss: 7.2146e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2119/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4236e-06 - rmse: 0.0019\n",
      "Epoch 2119: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0656e-06 - rmse: 0.0028 - val_loss: 8.1455e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2120/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0525e-06 - rmse: 0.0014\n",
      "Epoch 2120: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4970e-06 - rmse: 0.0025 - val_loss: 6.2004e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2121/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0450e-06 - rmse: 0.0014\n",
      "Epoch 2121: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8416e-06 - rmse: 0.0024 - val_loss: 6.5131e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2122/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2289e-06 - rmse: 0.0025\n",
      "Epoch 2122: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4169e-06 - rmse: 0.0023 - val_loss: 6.8762e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2123/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0141e-06 - rmse: 0.0014\n",
      "Epoch 2123: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6498e-06 - rmse: 0.0016 - val_loss: 5.5362e-05 - val_rmse: 0.0074 - lr: 5.0000e-05\n",
      "Epoch 2124/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9234e-06 - rmse: 0.0014\n",
      "Epoch 2124: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2376e-06 - rmse: 0.0018 - val_loss: 8.3019e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2125/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3992e-06 - rmse: 0.0012\n",
      "Epoch 2125: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9083e-06 - rmse: 0.0022 - val_loss: 5.7772e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2126/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4282e-06 - rmse: 0.0016\n",
      "Epoch 2126: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5579e-06 - rmse: 0.0016 - val_loss: 7.0353e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2127/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8677e-06 - rmse: 0.0014\n",
      "Epoch 2127: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2810e-06 - rmse: 0.0015 - val_loss: 6.3016e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2128/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7413e-06 - rmse: 0.0024\n",
      "Epoch 2128: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6744e-06 - rmse: 0.0019 - val_loss: 6.9559e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2129/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3930e-06 - rmse: 0.0027\n",
      "Epoch 2129: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8456e-06 - rmse: 0.0022 - val_loss: 7.1529e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2130/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2272e-06 - rmse: 0.0018\n",
      "Epoch 2130: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0572e-06 - rmse: 0.0022 - val_loss: 8.1184e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2131/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7264e-06 - rmse: 0.0024\n",
      "Epoch 2131: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8386e-06 - rmse: 0.0022 - val_loss: 6.3321e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2132/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3703e-06 - rmse: 0.0027\n",
      "Epoch 2132: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.1593e-06 - rmse: 0.0030 - val_loss: 7.6713e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2133/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2440e-05 - rmse: 0.0035\n",
      "Epoch 2133: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0748e-05 - rmse: 0.0033 - val_loss: 8.1713e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2134/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1300e-06 - rmse: 0.0030\n",
      "Epoch 2134: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3728e-06 - rmse: 0.0021 - val_loss: 6.0839e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2135/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5506e-06 - rmse: 0.0026\n",
      "Epoch 2135: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3812e-06 - rmse: 0.0023 - val_loss: 8.0506e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2136/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7049e-06 - rmse: 0.0013\n",
      "Epoch 2136: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1052e-06 - rmse: 0.0023 - val_loss: 5.9996e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2137/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7007e-06 - rmse: 0.0019\n",
      "Epoch 2137: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2902e-06 - rmse: 0.0021 - val_loss: 8.3389e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2138/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1973e-06 - rmse: 0.0025\n",
      "Epoch 2138: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8328e-06 - rmse: 0.0022 - val_loss: 5.9803e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2139/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8398e-06 - rmse: 0.0024\n",
      "Epoch 2139: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2803e-06 - rmse: 0.0023 - val_loss: 8.6532e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 2140/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6369e-06 - rmse: 0.0026\n",
      "Epoch 2140: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7011e-06 - rmse: 0.0022 - val_loss: 6.1802e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2141/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1072e-06 - rmse: 0.0025\n",
      "Epoch 2141: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0604e-06 - rmse: 0.0022 - val_loss: 7.8075e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2142/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6185e-06 - rmse: 0.0026\n",
      "Epoch 2142: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2772e-06 - rmse: 0.0023 - val_loss: 6.4581e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2143/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5048e-06 - rmse: 0.0012\n",
      "Epoch 2143: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5452e-06 - rmse: 0.0016 - val_loss: 7.2144e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2144/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5493e-07 - rmse: 8.6887e-04\n",
      "Epoch 2144: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5277e-06 - rmse: 0.0016 - val_loss: 6.3776e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2145/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6472e-06 - rmse: 0.0013\n",
      "Epoch 2145: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9050e-06 - rmse: 0.0014 - val_loss: 6.9846e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2146/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4037e-07 - rmse: 8.6045e-04\n",
      "Epoch 2146: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.9025e-06 - rmse: 0.0014 - val_loss: 6.3281e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2147/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5855e-06 - rmse: 0.0016\n",
      "Epoch 2147: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8237e-06 - rmse: 0.0014 - val_loss: 6.7912e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2148/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9051e-07 - rmse: 9.4367e-04\n",
      "Epoch 2148: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2631e-06 - rmse: 0.0015 - val_loss: 7.2171e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2149/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0459e-06 - rmse: 0.0010\n",
      "Epoch 2149: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2257e-06 - rmse: 0.0015 - val_loss: 6.5176e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2150/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1062e-06 - rmse: 0.0018\n",
      "Epoch 2150: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9916e-06 - rmse: 0.0017 - val_loss: 6.7225e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2151/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8849e-06 - rmse: 0.0017\n",
      "Epoch 2151: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4764e-06 - rmse: 0.0016 - val_loss: 7.2284e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2152/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3343e-06 - rmse: 0.0015\n",
      "Epoch 2152: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3910e-06 - rmse: 0.0018 - val_loss: 6.5014e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2153/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8540e-06 - rmse: 0.0017\n",
      "Epoch 2153: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7528e-06 - rmse: 0.0019 - val_loss: 7.8580e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2154/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5496e-06 - rmse: 0.0024\n",
      "Epoch 2154: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.0632e-06 - rmse: 0.0027 - val_loss: 7.1016e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2155/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8499e-06 - rmse: 0.0026\n",
      "Epoch 2155: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1761e-06 - rmse: 0.0023 - val_loss: 6.6568e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2156/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1974e-06 - rmse: 0.0025\n",
      "Epoch 2156: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2073e-06 - rmse: 0.0023 - val_loss: 8.1453e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2157/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1070e-06 - rmse: 0.0027\n",
      "Epoch 2157: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1162e-06 - rmse: 0.0027 - val_loss: 7.1322e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2158/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8964e-06 - rmse: 0.0028\n",
      "Epoch 2158: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2716e-06 - rmse: 0.0023 - val_loss: 7.1788e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2159/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0446e-06 - rmse: 0.0014\n",
      "Epoch 2159: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7958e-06 - rmse: 0.0022 - val_loss: 6.1943e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2160/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4033e-06 - rmse: 0.0023\n",
      "Epoch 2160: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.0655e-06 - rmse: 0.0023 - val_loss: 7.7898e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2161/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1823e-06 - rmse: 0.0025\n",
      "Epoch 2161: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5096e-06 - rmse: 0.0019 - val_loss: 5.8205e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2162/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1725e-06 - rmse: 0.0023\n",
      "Epoch 2162: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6734e-06 - rmse: 0.0019 - val_loss: 8.0705e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2163/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4547e-06 - rmse: 0.0012\n",
      "Epoch 2163: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7187e-06 - rmse: 0.0019 - val_loss: 6.6457e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2164/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8140e-06 - rmse: 0.0013\n",
      "Epoch 2164: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9447e-06 - rmse: 0.0017 - val_loss: 7.0882e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2165/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7603e-06 - rmse: 0.0013\n",
      "Epoch 2165: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1271e-06 - rmse: 0.0018 - val_loss: 6.5139e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2166/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4195e-06 - rmse: 0.0012\n",
      "Epoch 2166: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2820e-06 - rmse: 0.0018 - val_loss: 6.3269e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2167/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2128e-06 - rmse: 0.0011\n",
      "Epoch 2167: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3986e-06 - rmse: 0.0015 - val_loss: 6.7752e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2168/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3181e-06 - rmse: 0.0015\n",
      "Epoch 2168: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3444e-06 - rmse: 0.0018 - val_loss: 6.2190e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2169/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0530e-06 - rmse: 0.0010\n",
      "Epoch 2169: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0553e-06 - rmse: 0.0017 - val_loss: 6.7300e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2170/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9339e-06 - rmse: 0.0017\n",
      "Epoch 2170: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1318e-06 - rmse: 0.0020 - val_loss: 5.8547e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2171/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7308e-06 - rmse: 0.0024\n",
      "Epoch 2171: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7105e-06 - rmse: 0.0022 - val_loss: 7.7610e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2172/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0628e-06 - rmse: 0.0018\n",
      "Epoch 2172: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3517e-06 - rmse: 0.0025 - val_loss: 5.4734e-05 - val_rmse: 0.0074 - lr: 5.0000e-05\n",
      "Epoch 2173/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1242e-06 - rmse: 0.0023\n",
      "Epoch 2173: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7180e-06 - rmse: 0.0024 - val_loss: 7.2055e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2174/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6380e-06 - rmse: 0.0013\n",
      "Epoch 2174: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2092e-06 - rmse: 0.0018 - val_loss: 7.0290e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2175/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1291e-06 - rmse: 0.0018\n",
      "Epoch 2175: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9293e-06 - rmse: 0.0017 - val_loss: 7.5669e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2176/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3220e-06 - rmse: 0.0021\n",
      "Epoch 2176: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2431e-06 - rmse: 0.0015 - val_loss: 7.2805e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2177/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2387e-06 - rmse: 0.0015\n",
      "Epoch 2177: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3895e-06 - rmse: 0.0018 - val_loss: 6.6791e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2178/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8792e-06 - rmse: 0.0022\n",
      "Epoch 2178: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9364e-06 - rmse: 0.0020 - val_loss: 6.8990e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2179/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2072e-06 - rmse: 0.0011\n",
      "Epoch 2179: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8928e-06 - rmse: 0.0017 - val_loss: 6.5491e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2180/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3282e-06 - rmse: 0.0021\n",
      "Epoch 2180: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9482e-06 - rmse: 0.0020 - val_loss: 8.1071e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2181/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7536e-06 - rmse: 0.0024\n",
      "Epoch 2181: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4031e-06 - rmse: 0.0018 - val_loss: 5.9432e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2182/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4711e-06 - rmse: 0.0019\n",
      "Epoch 2182: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0731e-06 - rmse: 0.0018 - val_loss: 8.3143e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2183/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2215e-06 - rmse: 0.0011\n",
      "Epoch 2183: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0670e-06 - rmse: 0.0014 - val_loss: 6.3085e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2184/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4142e-06 - rmse: 0.0016\n",
      "Epoch 2184: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3581e-06 - rmse: 0.0021 - val_loss: 6.7442e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2185/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6904e-07 - rmse: 6.8487e-04\n",
      "Epoch 2185: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4034e-06 - rmse: 0.0018 - val_loss: 6.4988e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2186/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3656e-06 - rmse: 0.0015\n",
      "Epoch 2186: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6218e-06 - rmse: 0.0021 - val_loss: 7.5166e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2187/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9768e-07 - rmse: 9.9884e-04\n",
      "Epoch 2187: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2666e-06 - rmse: 0.0021 - val_loss: 5.8220e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2188/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0767e-06 - rmse: 0.0018\n",
      "Epoch 2188: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2536e-06 - rmse: 0.0023 - val_loss: 8.7280e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 2189/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1412e-06 - rmse: 0.0018\n",
      "Epoch 2189: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5856e-06 - rmse: 0.0016 - val_loss: 6.1615e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2190/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3588e-06 - rmse: 0.0015\n",
      "Epoch 2190: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0498e-06 - rmse: 0.0020 - val_loss: 7.9907e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2191/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0980e-06 - rmse: 0.0014\n",
      "Epoch 2191: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2819e-06 - rmse: 0.0018 - val_loss: 6.3693e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2192/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1559e-06 - rmse: 0.0018\n",
      "Epoch 2192: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2725e-06 - rmse: 0.0015 - val_loss: 7.2030e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2193/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7034e-07 - rmse: 9.8506e-04\n",
      "Epoch 2193: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7501e-06 - rmse: 0.0013 - val_loss: 6.9448e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2194/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0587e-07 - rmse: 9.5177e-04\n",
      "Epoch 2194: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1241e-06 - rmse: 0.0015 - val_loss: 7.5711e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2195/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2809e-06 - rmse: 0.0011\n",
      "Epoch 2195: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7188e-06 - rmse: 0.0016 - val_loss: 6.7909e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2196/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9187e-07 - rmse: 7.6933e-04\n",
      "Epoch 2196: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0826e-06 - rmse: 0.0014 - val_loss: 6.3999e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2197/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5628e-06 - rmse: 0.0019\n",
      "Epoch 2197: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7397e-06 - rmse: 0.0019 - val_loss: 7.1110e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2198/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8963e-06 - rmse: 0.0022\n",
      "Epoch 2198: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1946e-06 - rmse: 0.0018 - val_loss: 6.5299e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2199/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2792e-06 - rmse: 0.0015\n",
      "Epoch 2199: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2999e-06 - rmse: 0.0023 - val_loss: 7.2320e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2200/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1406e-07 - rmse: 6.4348e-04\n",
      "Epoch 2200: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4816e-06 - rmse: 0.0021 - val_loss: 5.9572e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2201/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7403e-07 - rmse: 9.3489e-04\n",
      "Epoch 2201: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9267e-06 - rmse: 0.0017 - val_loss: 7.9015e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2202/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5967e-06 - rmse: 0.0016\n",
      "Epoch 2202: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9404e-06 - rmse: 0.0017 - val_loss: 5.7681e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2203/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2176e-06 - rmse: 0.0015\n",
      "Epoch 2203: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1816e-06 - rmse: 0.0020 - val_loss: 8.7619e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2204/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4039e-06 - rmse: 0.0021\n",
      "Epoch 2204: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.9901e-06 - rmse: 0.0020 - val_loss: 6.0736e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2205/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1428e-06 - rmse: 0.0023\n",
      "Epoch 2205: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4246e-06 - rmse: 0.0021 - val_loss: 8.2876e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2206/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2591e-06 - rmse: 0.0015\n",
      "Epoch 2206: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9809e-06 - rmse: 0.0017 - val_loss: 5.9626e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2207/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7609e-06 - rmse: 0.0017\n",
      "Epoch 2207: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7603e-06 - rmse: 0.0017 - val_loss: 8.4436e-05 - val_rmse: 0.0092 - lr: 5.0000e-05\n",
      "Epoch 2208/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2113e-06 - rmse: 0.0011\n",
      "Epoch 2208: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6465e-06 - rmse: 0.0013 - val_loss: 5.1473e-05 - val_rmse: 0.0072 - lr: 5.0000e-05\n",
      "Epoch 2209/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4284e-05 - rmse: 0.0038\n",
      "Epoch 2209: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0000e-06 - rmse: 0.0026 - val_loss: 8.2048e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2210/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9568e-06 - rmse: 0.0020\n",
      "Epoch 2210: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6433e-06 - rmse: 0.0019 - val_loss: 6.5476e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2211/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7362e-06 - rmse: 0.0013\n",
      "Epoch 2211: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1980e-06 - rmse: 0.0015 - val_loss: 7.2417e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2212/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4421e-06 - rmse: 0.0012\n",
      "Epoch 2212: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8364e-06 - rmse: 0.0014 - val_loss: 6.7543e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2213/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8370e-06 - rmse: 0.0017\n",
      "Epoch 2213: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5797e-06 - rmse: 0.0013 - val_loss: 7.2054e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2214/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4872e-06 - rmse: 0.0021\n",
      "Epoch 2214: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9420e-06 - rmse: 0.0014 - val_loss: 6.9456e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2215/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7600e-06 - rmse: 0.0019\n",
      "Epoch 2215: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8074e-06 - rmse: 0.0017 - val_loss: 6.1658e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2216/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8932e-06 - rmse: 0.0014\n",
      "Epoch 2216: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2976e-06 - rmse: 0.0021 - val_loss: 7.8508e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2217/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3927e-06 - rmse: 0.0027\n",
      "Epoch 2217: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7120e-06 - rmse: 0.0019 - val_loss: 6.7474e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2218/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7195e-06 - rmse: 0.0026\n",
      "Epoch 2218: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4199e-06 - rmse: 0.0025 - val_loss: 7.7131e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2219/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8840e-06 - rmse: 0.0014\n",
      "Epoch 2219: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6403e-06 - rmse: 0.0022 - val_loss: 6.1406e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2220/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2646e-06 - rmse: 0.0015\n",
      "Epoch 2220: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2964e-06 - rmse: 0.0023 - val_loss: 9.0831e-05 - val_rmse: 0.0095 - lr: 5.0000e-05\n",
      "Epoch 2221/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5072e-06 - rmse: 0.0023\n",
      "Epoch 2221: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1948e-06 - rmse: 0.0029 - val_loss: 5.1677e-05 - val_rmse: 0.0072 - lr: 5.0000e-05\n",
      "Epoch 2222/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0480e-06 - rmse: 0.0014\n",
      "Epoch 2222: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0412e-06 - rmse: 0.0027 - val_loss: 9.6139e-05 - val_rmse: 0.0098 - lr: 5.0000e-05\n",
      "Epoch 2223/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3334e-06 - rmse: 0.0021\n",
      "Epoch 2223: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1206e-05 - rmse: 0.0033 - val_loss: 5.6403e-05 - val_rmse: 0.0075 - lr: 5.0000e-05\n",
      "Epoch 2224/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7938e-06 - rmse: 0.0019\n",
      "Epoch 2224: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0390e-05 - rmse: 0.0032 - val_loss: 1.0543e-04 - val_rmse: 0.0103 - lr: 5.0000e-05\n",
      "Epoch 2225/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3103e-06 - rmse: 0.0027\n",
      "Epoch 2225: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4393e-05 - rmse: 0.0038 - val_loss: 4.9269e-05 - val_rmse: 0.0070 - lr: 5.0000e-05\n",
      "Epoch 2226/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8350e-06 - rmse: 0.0022\n",
      "Epoch 2226: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0199e-05 - rmse: 0.0045 - val_loss: 1.5489e-04 - val_rmse: 0.0124 - lr: 5.0000e-05\n",
      "Epoch 2227/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8628e-05 - rmse: 0.0054\n",
      "Epoch 2227: val_loss improved from 0.00004 to 0.00004, saving model to D:\\TrainedModels\\2022120520221205steadyValidation_AeroCNN1_val_0.2_test0.1_128units_OptimalSettingsCl_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.0065e-05 - rmse: 0.0045 - val_loss: 4.0950e-05 - val_rmse: 0.0064 - lr: 5.0000e-05\n",
      "Epoch 2228/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4146e-05 - rmse: 0.0049\n",
      "Epoch 2228: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4309e-05 - rmse: 0.0038 - val_loss: 9.9140e-05 - val_rmse: 0.0100 - lr: 5.0000e-05\n",
      "Epoch 2229/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2953e-05 - rmse: 0.0036\n",
      "Epoch 2229: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.9466e-06 - rmse: 0.0030 - val_loss: 6.3190e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2230/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9985e-06 - rmse: 0.0022\n",
      "Epoch 2230: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1043e-06 - rmse: 0.0020 - val_loss: 8.6341e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 2231/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3674e-06 - rmse: 0.0023\n",
      "Epoch 2231: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7109e-06 - rmse: 0.0019 - val_loss: 6.6874e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2232/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1934e-06 - rmse: 0.0015\n",
      "Epoch 2232: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4626e-06 - rmse: 0.0016 - val_loss: 7.6607e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2233/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6416e-07 - rmse: 8.7416e-04\n",
      "Epoch 2233: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7350e-06 - rmse: 0.0017 - val_loss: 5.8743e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2234/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5960e-06 - rmse: 0.0016\n",
      "Epoch 2234: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7429e-06 - rmse: 0.0017 - val_loss: 8.8101e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2235/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9483e-06 - rmse: 0.0020\n",
      "Epoch 2235: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7323e-06 - rmse: 0.0022 - val_loss: 5.7044e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2236/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1691e-06 - rmse: 0.0015\n",
      "Epoch 2236: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0439e-05 - rmse: 0.0032 - val_loss: 9.5693e-05 - val_rmse: 0.0098 - lr: 5.0000e-05\n",
      "Epoch 2237/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1657e-05 - rmse: 0.0034\n",
      "Epoch 2237: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5639e-06 - rmse: 0.0029 - val_loss: 7.2800e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2238/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5913e-06 - rmse: 0.0031\n",
      "Epoch 2238: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3663e-06 - rmse: 0.0025 - val_loss: 8.3710e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2239/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3716e-06 - rmse: 0.0029\n",
      "Epoch 2239: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9820e-06 - rmse: 0.0028 - val_loss: 7.1426e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2240/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5462e-06 - rmse: 0.0019\n",
      "Epoch 2240: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3068e-06 - rmse: 0.0023 - val_loss: 8.0504e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2241/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3944e-06 - rmse: 0.0021\n",
      "Epoch 2241: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6616e-06 - rmse: 0.0024 - val_loss: 6.1585e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2242/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2220e-06 - rmse: 0.0018\n",
      "Epoch 2242: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8255e-06 - rmse: 0.0024 - val_loss: 8.6573e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 2243/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1229e-06 - rmse: 0.0018\n",
      "Epoch 2243: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9642e-06 - rmse: 0.0024 - val_loss: 6.7706e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2244/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1803e-06 - rmse: 0.0029\n",
      "Epoch 2244: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7915e-06 - rmse: 0.0026 - val_loss: 6.7159e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2245/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3854e-06 - rmse: 0.0018\n",
      "Epoch 2245: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8849e-06 - rmse: 0.0020 - val_loss: 6.3018e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2246/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7234e-06 - rmse: 0.0024\n",
      "Epoch 2246: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9573e-06 - rmse: 0.0017 - val_loss: 7.4065e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2247/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9834e-06 - rmse: 0.0014\n",
      "Epoch 2247: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3729e-06 - rmse: 0.0018 - val_loss: 6.5178e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2248/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2813e-06 - rmse: 0.0027\n",
      "Epoch 2248: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8324e-06 - rmse: 0.0022 - val_loss: 7.4729e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2249/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2176e-06 - rmse: 0.0011\n",
      "Epoch 2249: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7227e-06 - rmse: 0.0024 - val_loss: 6.7983e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2250/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5917e-06 - rmse: 0.0029\n",
      "Epoch 2250: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2592e-06 - rmse: 0.0027 - val_loss: 8.0370e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2251/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4966e-06 - rmse: 0.0021\n",
      "Epoch 2251: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1191e-06 - rmse: 0.0020 - val_loss: 6.3504e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2252/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7774e-06 - rmse: 0.0022\n",
      "Epoch 2252: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0531e-06 - rmse: 0.0025 - val_loss: 8.7106e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2253/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4281e-06 - rmse: 0.0023\n",
      "Epoch 2253: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7308e-06 - rmse: 0.0026 - val_loss: 5.9261e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2254/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1614e-06 - rmse: 0.0023\n",
      "Epoch 2254: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5467e-06 - rmse: 0.0024 - val_loss: 7.0839e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2255/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7390e-06 - rmse: 0.0017\n",
      "Epoch 2255: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1893e-06 - rmse: 0.0020 - val_loss: 5.5749e-05 - val_rmse: 0.0075 - lr: 5.0000e-05\n",
      "Epoch 2256/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9233e-06 - rmse: 0.0017\n",
      "Epoch 2256: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1244e-06 - rmse: 0.0023 - val_loss: 9.3429e-05 - val_rmse: 0.0097 - lr: 5.0000e-05\n",
      "Epoch 2257/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0099e-06 - rmse: 0.0025\n",
      "Epoch 2257: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4895e-06 - rmse: 0.0021 - val_loss: 6.3035e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2258/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1877e-06 - rmse: 0.0018\n",
      "Epoch 2258: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6771e-06 - rmse: 0.0022 - val_loss: 7.6732e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2259/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5501e-06 - rmse: 0.0012\n",
      "Epoch 2259: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2812e-06 - rmse: 0.0018 - val_loss: 6.4443e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2260/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3428e-06 - rmse: 0.0012\n",
      "Epoch 2260: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0805e-06 - rmse: 0.0014 - val_loss: 7.4294e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2261/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6666e-06 - rmse: 0.0013\n",
      "Epoch 2261: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4637e-06 - rmse: 0.0016 - val_loss: 5.9567e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2262/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5626e-06 - rmse: 0.0019\n",
      "Epoch 2262: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0641e-06 - rmse: 0.0018 - val_loss: 8.3953e-05 - val_rmse: 0.0092 - lr: 5.0000e-05\n",
      "Epoch 2263/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9289e-06 - rmse: 0.0032\n",
      "Epoch 2263: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6582e-06 - rmse: 0.0026 - val_loss: 6.4321e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2264/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9938e-06 - rmse: 0.0028\n",
      "Epoch 2264: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1882e-06 - rmse: 0.0020 - val_loss: 8.6288e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 2265/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6383e-06 - rmse: 0.0019\n",
      "Epoch 2265: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8988e-06 - rmse: 0.0026 - val_loss: 5.4103e-05 - val_rmse: 0.0074 - lr: 5.0000e-05\n",
      "Epoch 2266/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6790e-06 - rmse: 0.0019\n",
      "Epoch 2266: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.2896e-06 - rmse: 0.0030 - val_loss: 7.2730e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2267/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2464e-06 - rmse: 0.0025\n",
      "Epoch 2267: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4371e-06 - rmse: 0.0027 - val_loss: 7.9559e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2268/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5080e-06 - rmse: 0.0026\n",
      "Epoch 2268: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3966e-06 - rmse: 0.0029 - val_loss: 4.8989e-05 - val_rmse: 0.0070 - lr: 5.0000e-05\n",
      "Epoch 2269/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1895e-06 - rmse: 0.0025\n",
      "Epoch 2269: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4874e-06 - rmse: 0.0027 - val_loss: 8.7479e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2270/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5537e-06 - rmse: 0.0026\n",
      "Epoch 2270: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8453e-06 - rmse: 0.0022 - val_loss: 5.0214e-05 - val_rmse: 0.0071 - lr: 5.0000e-05\n",
      "Epoch 2271/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8087e-06 - rmse: 0.0020\n",
      "Epoch 2271: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0446e-06 - rmse: 0.0027 - val_loss: 8.4933e-05 - val_rmse: 0.0092 - lr: 5.0000e-05\n",
      "Epoch 2272/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6907e-06 - rmse: 0.0016\n",
      "Epoch 2272: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3632e-06 - rmse: 0.0025 - val_loss: 5.0136e-05 - val_rmse: 0.0071 - lr: 5.0000e-05\n",
      "Epoch 2273/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9772e-06 - rmse: 0.0026\n",
      "Epoch 2273: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8992e-06 - rmse: 0.0022 - val_loss: 8.7810e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2274/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2858e-06 - rmse: 0.0018\n",
      "Epoch 2274: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1506e-06 - rmse: 0.0023 - val_loss: 5.9371e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2275/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4034e-06 - rmse: 0.0021\n",
      "Epoch 2275: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0310e-06 - rmse: 0.0020 - val_loss: 8.6007e-05 - val_rmse: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 2276/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3348e-06 - rmse: 0.0023\n",
      "Epoch 2276: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6757e-06 - rmse: 0.0029 - val_loss: 6.4154e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2277/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8170e-06 - rmse: 0.0026\n",
      "Epoch 2277: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7114e-06 - rmse: 0.0028 - val_loss: 8.8298e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2278/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4500e-05 - rmse: 0.0038\n",
      "Epoch 2278: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0056e-05 - rmse: 0.0032 - val_loss: 6.4861e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2279/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4145e-06 - rmse: 0.0021\n",
      "Epoch 2279: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7590e-06 - rmse: 0.0024 - val_loss: 5.9841e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2280/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4555e-06 - rmse: 0.0012\n",
      "Epoch 2280: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5598e-06 - rmse: 0.0021 - val_loss: 7.6181e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2281/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1219e-06 - rmse: 0.0018\n",
      "Epoch 2281: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5118e-06 - rmse: 0.0019 - val_loss: 6.0623e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2282/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6082e-06 - rmse: 0.0013\n",
      "Epoch 2282: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8199e-06 - rmse: 0.0017 - val_loss: 7.9767e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2283/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8054e-06 - rmse: 0.0013\n",
      "Epoch 2283: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6408e-06 - rmse: 0.0016 - val_loss: 6.1313e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2284/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3907e-06 - rmse: 0.0015\n",
      "Epoch 2284: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8787e-06 - rmse: 0.0017 - val_loss: 7.8021e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2285/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6064e-06 - rmse: 0.0019\n",
      "Epoch 2285: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2373e-06 - rmse: 0.0015 - val_loss: 6.3898e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2286/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3414e-06 - rmse: 0.0015\n",
      "Epoch 2286: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0609e-06 - rmse: 0.0017 - val_loss: 7.4308e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2287/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1619e-06 - rmse: 0.0018\n",
      "Epoch 2287: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9149e-06 - rmse: 0.0028 - val_loss: 7.9802e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2288/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4707e-06 - rmse: 0.0029\n",
      "Epoch 2288: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8896e-05 - rmse: 0.0043 - val_loss: 5.7410e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2289/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6623e-05 - rmse: 0.0052\n",
      "Epoch 2289: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1839e-05 - rmse: 0.0047 - val_loss: 1.5833e-04 - val_rmse: 0.0126 - lr: 5.0000e-05\n",
      "Epoch 2290/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5127e-05 - rmse: 0.0050\n",
      "Epoch 2290: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.2338e-05 - rmse: 0.0057 - val_loss: 5.5557e-05 - val_rmse: 0.0075 - lr: 5.0000e-05\n",
      "Epoch 2291/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0631e-05 - rmse: 0.0071\n",
      "Epoch 2291: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0735e-05 - rmse: 0.0078 - val_loss: 2.2905e-04 - val_rmse: 0.0151 - lr: 5.0000e-05\n",
      "Epoch 2292/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9575e-05 - rmse: 0.0083\n",
      "Epoch 2292: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2119e-05 - rmse: 0.0072 - val_loss: 1.2020e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 2293/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0979e-04 - rmse: 0.0105\n",
      "Epoch 2293: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5485e-05 - rmse: 0.0087 - val_loss: 9.2898e-05 - val_rmse: 0.0096 - lr: 5.0000e-05\n",
      "Epoch 2294/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9888e-05 - rmse: 0.0045\n",
      "Epoch 2294: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2094e-05 - rmse: 0.0065 - val_loss: 1.4446e-04 - val_rmse: 0.0120 - lr: 5.0000e-05\n",
      "Epoch 2295/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7349e-05 - rmse: 0.0093\n",
      "Epoch 2295: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9648e-05 - rmse: 0.0070 - val_loss: 1.5998e-04 - val_rmse: 0.0126 - lr: 5.0000e-05\n",
      "Epoch 2296/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3081e-05 - rmse: 0.0091\n",
      "Epoch 2296: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4778e-05 - rmse: 0.0074 - val_loss: 7.9012e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2297/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7462e-05 - rmse: 0.0052\n",
      "Epoch 2297: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9506e-05 - rmse: 0.0063 - val_loss: 1.1567e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 2298/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4366e-06 - rmse: 0.0027\n",
      "Epoch 2298: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0230e-05 - rmse: 0.0055 - val_loss: 8.3425e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2299/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0421e-05 - rmse: 0.0055\n",
      "Epoch 2299: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9866e-05 - rmse: 0.0045 - val_loss: 8.0982e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2300/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5737e-06 - rmse: 0.0026\n",
      "Epoch 2300: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0166e-05 - rmse: 0.0032 - val_loss: 4.3498e-05 - val_rmse: 0.0066 - lr: 5.0000e-05\n",
      "Epoch 2301/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2722e-05 - rmse: 0.0036\n",
      "Epoch 2301: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4694e-05 - rmse: 0.0038 - val_loss: 8.1261e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2302/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6520e-06 - rmse: 0.0026\n",
      "Epoch 2302: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.2127e-06 - rmse: 0.0030 - val_loss: 6.3142e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2303/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2760e-06 - rmse: 0.0027\n",
      "Epoch 2303: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3038e-05 - rmse: 0.0036 - val_loss: 1.0211e-04 - val_rmse: 0.0101 - lr: 5.0000e-05\n",
      "Epoch 2304/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3938e-05 - rmse: 0.0049\n",
      "Epoch 2304: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0202e-05 - rmse: 0.0045 - val_loss: 4.9970e-05 - val_rmse: 0.0071 - lr: 5.0000e-05\n",
      "Epoch 2305/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3180e-06 - rmse: 0.0029\n",
      "Epoch 2305: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5638e-05 - rmse: 0.0040 - val_loss: 9.8769e-05 - val_rmse: 0.0099 - lr: 5.0000e-05\n",
      "Epoch 2306/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2088e-05 - rmse: 0.0035\n",
      "Epoch 2306: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6552e-05 - rmse: 0.0041 - val_loss: 6.2952e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2307/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7875e-05 - rmse: 0.0042\n",
      "Epoch 2307: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0683e-05 - rmse: 0.0033 - val_loss: 7.8856e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2308/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9016e-06 - rmse: 0.0020\n",
      "Epoch 2308: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1939e-06 - rmse: 0.0030 - val_loss: 6.3274e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2309/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3329e-05 - rmse: 0.0037\n",
      "Epoch 2309: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3144e-06 - rmse: 0.0029 - val_loss: 8.8236e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2310/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6752e-06 - rmse: 0.0019\n",
      "Epoch 2310: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0569e-05 - rmse: 0.0033 - val_loss: 5.8312e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2311/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2366e-05 - rmse: 0.0035\n",
      "Epoch 2311: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4522e-05 - rmse: 0.0038 - val_loss: 7.5793e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2312/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9352e-06 - rmse: 0.0026\n",
      "Epoch 2312: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0940e-06 - rmse: 0.0030 - val_loss: 6.8154e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2313/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8612e-06 - rmse: 0.0028\n",
      "Epoch 2313: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.6278e-06 - rmse: 0.0022 - val_loss: 7.8288e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2314/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1042e-05 - rmse: 0.0033\n",
      "Epoch 2314: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4339e-06 - rmse: 0.0021 - val_loss: 6.7587e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2315/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6908e-06 - rmse: 0.0013\n",
      "Epoch 2315: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6101e-06 - rmse: 0.0016 - val_loss: 7.3968e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2316/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8223e-06 - rmse: 0.0017\n",
      "Epoch 2316: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0071e-06 - rmse: 0.0017 - val_loss: 5.6756e-05 - val_rmse: 0.0075 - lr: 5.0000e-05\n",
      "Epoch 2317/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1573e-06 - rmse: 0.0011\n",
      "Epoch 2317: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5365e-06 - rmse: 0.0019 - val_loss: 7.5251e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2318/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6874e-06 - rmse: 0.0019\n",
      "Epoch 2318: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.1309e-06 - rmse: 0.0025 - val_loss: 5.7047e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2319/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6869e-06 - rmse: 0.0016\n",
      "Epoch 2319: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9672e-06 - rmse: 0.0030 - val_loss: 8.8124e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2320/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1963e-05 - rmse: 0.0035\n",
      "Epoch 2320: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7129e-06 - rmse: 0.0026 - val_loss: 6.6804e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2321/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0962e-05 - rmse: 0.0033\n",
      "Epoch 2321: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8214e-06 - rmse: 0.0030 - val_loss: 8.2108e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2322/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6999e-06 - rmse: 0.0016\n",
      "Epoch 2322: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9497e-06 - rmse: 0.0030 - val_loss: 6.7576e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2323/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6154e-05 - rmse: 0.0040\n",
      "Epoch 2323: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5290e-05 - rmse: 0.0039 - val_loss: 1.2354e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 2324/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1575e-05 - rmse: 0.0064\n",
      "Epoch 2324: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9729e-05 - rmse: 0.0044 - val_loss: 4.9365e-05 - val_rmse: 0.0070 - lr: 5.0000e-05\n",
      "Epoch 2325/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1152e-06 - rmse: 0.0023\n",
      "Epoch 2325: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0738e-05 - rmse: 0.0033 - val_loss: 1.1312e-04 - val_rmse: 0.0106 - lr: 5.0000e-05\n",
      "Epoch 2326/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2670e-05 - rmse: 0.0048\n",
      "Epoch 2326: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4843e-06 - rmse: 0.0031 - val_loss: 5.5175e-05 - val_rmse: 0.0074 - lr: 5.0000e-05\n",
      "Epoch 2327/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5246e-05 - rmse: 0.0039\n",
      "Epoch 2327: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7614e-06 - rmse: 0.0026 - val_loss: 8.8811e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2328/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9768e-06 - rmse: 0.0022\n",
      "Epoch 2328: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.1191e-06 - rmse: 0.0020 - val_loss: 5.8744e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2329/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5994e-06 - rmse: 0.0019\n",
      "Epoch 2329: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9423e-06 - rmse: 0.0020 - val_loss: 6.9305e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2330/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2525e-06 - rmse: 0.0015\n",
      "Epoch 2330: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9388e-06 - rmse: 0.0017 - val_loss: 5.9746e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2331/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6500e-07 - rmse: 9.8234e-04\n",
      "Epoch 2331: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0064e-06 - rmse: 0.0014 - val_loss: 7.8803e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2332/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6767e-06 - rmse: 0.0029\n",
      "Epoch 2332: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4340e-06 - rmse: 0.0019 - val_loss: 5.8555e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2333/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6310e-07 - rmse: 8.1431e-04\n",
      "Epoch 2333: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5701e-06 - rmse: 0.0021 - val_loss: 7.4685e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2334/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3223e-06 - rmse: 0.0025\n",
      "Epoch 2334: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.0704e-06 - rmse: 0.0020 - val_loss: 6.5955e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2335/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0710e-06 - rmse: 0.0010\n",
      "Epoch 2335: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9007e-06 - rmse: 0.0017 - val_loss: 7.4739e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2336/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9963e-06 - rmse: 0.0014\n",
      "Epoch 2336: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4726e-06 - rmse: 0.0019 - val_loss: 7.3432e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2337/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1118e-06 - rmse: 0.0023\n",
      "Epoch 2337: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5673e-06 - rmse: 0.0019 - val_loss: 6.9110e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2338/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1330e-05 - rmse: 0.0034\n",
      "Epoch 2338: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4310e-06 - rmse: 0.0023 - val_loss: 7.8669e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2339/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6998e-06 - rmse: 0.0024\n",
      "Epoch 2339: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0298e-06 - rmse: 0.0027 - val_loss: 6.1447e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2340/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7138e-06 - rmse: 0.0013\n",
      "Epoch 2340: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2679e-06 - rmse: 0.0025 - val_loss: 5.7708e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2341/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0537e-06 - rmse: 0.0010\n",
      "Epoch 2341: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8432e-06 - rmse: 0.0017 - val_loss: 7.0942e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2342/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4610e-06 - rmse: 0.0021\n",
      "Epoch 2342: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5970e-06 - rmse: 0.0019 - val_loss: 6.4408e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2343/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8245e-06 - rmse: 0.0020\n",
      "Epoch 2343: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2430e-06 - rmse: 0.0021 - val_loss: 7.4581e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2344/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2831e-06 - rmse: 0.0018\n",
      "Epoch 2344: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.8381e-06 - rmse: 0.0026 - val_loss: 5.5136e-05 - val_rmse: 0.0074 - lr: 5.0000e-05\n",
      "Epoch 2345/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5721e-06 - rmse: 0.0013\n",
      "Epoch 2345: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9191e-06 - rmse: 0.0020 - val_loss: 8.0197e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2346/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6583e-06 - rmse: 0.0019\n",
      "Epoch 2346: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6496e-06 - rmse: 0.0016 - val_loss: 5.8890e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2347/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6729e-06 - rmse: 0.0016\n",
      "Epoch 2347: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0017e-06 - rmse: 0.0014 - val_loss: 8.0699e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2348/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7379e-06 - rmse: 0.0017\n",
      "Epoch 2348: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2101e-06 - rmse: 0.0021 - val_loss: 5.3250e-05 - val_rmse: 0.0073 - lr: 5.0000e-05\n",
      "Epoch 2349/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4005e-06 - rmse: 0.0015\n",
      "Epoch 2349: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8469e-06 - rmse: 0.0020 - val_loss: 6.9941e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2350/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8791e-06 - rmse: 0.0017\n",
      "Epoch 2350: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5576e-06 - rmse: 0.0016 - val_loss: 6.2402e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2351/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9235e-06 - rmse: 0.0022\n",
      "Epoch 2351: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4142e-06 - rmse: 0.0016 - val_loss: 6.6949e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2352/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5992e-06 - rmse: 0.0013\n",
      "Epoch 2352: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6216e-06 - rmse: 0.0013 - val_loss: 6.2539e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2353/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7684e-07 - rmse: 6.1387e-04\n",
      "Epoch 2353: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2610e-06 - rmse: 0.0015 - val_loss: 6.3142e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2354/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8356e-07 - rmse: 8.2677e-04\n",
      "Epoch 2354: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4856e-06 - rmse: 0.0016 - val_loss: 6.4094e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2355/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2585e-06 - rmse: 0.0011\n",
      "Epoch 2355: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6020e-06 - rmse: 0.0013 - val_loss: 6.2355e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2356/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1377e-06 - rmse: 0.0015\n",
      "Epoch 2356: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8512e-06 - rmse: 0.0017 - val_loss: 7.7189e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2357/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1136e-06 - rmse: 0.0015\n",
      "Epoch 2357: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5176e-06 - rmse: 0.0016 - val_loss: 6.2572e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2358/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7889e-06 - rmse: 0.0028\n",
      "Epoch 2358: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3183e-06 - rmse: 0.0023 - val_loss: 7.2992e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2359/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4008e-06 - rmse: 0.0021\n",
      "Epoch 2359: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5964e-06 - rmse: 0.0016 - val_loss: 6.1698e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2360/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3343e-06 - rmse: 0.0012\n",
      "Epoch 2360: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8111e-06 - rmse: 0.0017 - val_loss: 6.3275e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2361/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4265e-06 - rmse: 0.0016\n",
      "Epoch 2361: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7947e-06 - rmse: 0.0017 - val_loss: 7.7692e-05 - val_rmse: 0.0088 - lr: 5.0000e-05\n",
      "Epoch 2362/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1092e-06 - rmse: 0.0011\n",
      "Epoch 2362: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0542e-06 - rmse: 0.0020 - val_loss: 5.5473e-05 - val_rmse: 0.0074 - lr: 5.0000e-05\n",
      "Epoch 2363/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4900e-06 - rmse: 0.0019\n",
      "Epoch 2363: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0926e-06 - rmse: 0.0020 - val_loss: 7.0699e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2364/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8830e-06 - rmse: 0.0014\n",
      "Epoch 2364: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6258e-06 - rmse: 0.0016 - val_loss: 6.6007e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2365/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6284e-07 - rmse: 9.2889e-04\n",
      "Epoch 2365: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3927e-06 - rmse: 0.0015 - val_loss: 7.3825e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2366/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8299e-06 - rmse: 0.0020\n",
      "Epoch 2366: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7284e-06 - rmse: 0.0017 - val_loss: 5.2583e-05 - val_rmse: 0.0073 - lr: 5.0000e-05\n",
      "Epoch 2367/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4738e-06 - rmse: 0.0012\n",
      "Epoch 2367: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0133e-06 - rmse: 0.0017 - val_loss: 7.4920e-05 - val_rmse: 0.0087 - lr: 5.0000e-05\n",
      "Epoch 2368/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5264e-06 - rmse: 0.0012\n",
      "Epoch 2368: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6582e-06 - rmse: 0.0013 - val_loss: 5.3525e-05 - val_rmse: 0.0073 - lr: 5.0000e-05\n",
      "Epoch 2369/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4283e-06 - rmse: 0.0012\n",
      "Epoch 2369: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9699e-06 - rmse: 0.0017 - val_loss: 7.9889e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2370/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1451e-06 - rmse: 0.0011\n",
      "Epoch 2370: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1846e-06 - rmse: 0.0018 - val_loss: 5.2802e-05 - val_rmse: 0.0073 - lr: 5.0000e-05\n",
      "Epoch 2371/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8779e-06 - rmse: 0.0020\n",
      "Epoch 2371: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8088e-06 - rmse: 0.0024 - val_loss: 8.2963e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2372/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0088e-06 - rmse: 0.0022\n",
      "Epoch 2372: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0382e-05 - rmse: 0.0032 - val_loss: 5.6417e-05 - val_rmse: 0.0075 - lr: 5.0000e-05\n",
      "Epoch 2373/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2168e-05 - rmse: 0.0035\n",
      "Epoch 2373: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3289e-05 - rmse: 0.0036 - val_loss: 1.1247e-04 - val_rmse: 0.0106 - lr: 5.0000e-05\n",
      "Epoch 2374/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3145e-05 - rmse: 0.0048\n",
      "Epoch 2374: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2918e-05 - rmse: 0.0036 - val_loss: 5.5828e-05 - val_rmse: 0.0075 - lr: 5.0000e-05\n",
      "Epoch 2375/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3068e-06 - rmse: 0.0031\n",
      "Epoch 2375: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2271e-05 - rmse: 0.0035 - val_loss: 8.8307e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2376/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4801e-05 - rmse: 0.0038\n",
      "Epoch 2376: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4029e-05 - rmse: 0.0037 - val_loss: 5.7258e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2377/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1921e-05 - rmse: 0.0047\n",
      "Epoch 2377: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3276e-05 - rmse: 0.0036 - val_loss: 1.2662e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 2378/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5046e-05 - rmse: 0.0039\n",
      "Epoch 2378: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0394e-05 - rmse: 0.0045 - val_loss: 5.0444e-05 - val_rmse: 0.0071 - lr: 5.0000e-05\n",
      "Epoch 2379/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0596e-05 - rmse: 0.0033\n",
      "Epoch 2379: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2160e-05 - rmse: 0.0047 - val_loss: 1.6183e-04 - val_rmse: 0.0127 - lr: 5.0000e-05\n",
      "Epoch 2380/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0623e-05 - rmse: 0.0071\n",
      "Epoch 2380: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6084e-05 - rmse: 0.0060 - val_loss: 5.0288e-05 - val_rmse: 0.0071 - lr: 5.0000e-05\n",
      "Epoch 2381/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4153e-06 - rmse: 0.0031\n",
      "Epoch 2381: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5329e-05 - rmse: 0.0050 - val_loss: 7.9938e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2382/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6598e-06 - rmse: 0.0026\n",
      "Epoch 2382: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5909e-05 - rmse: 0.0040 - val_loss: 7.4693e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2383/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2842e-05 - rmse: 0.0036\n",
      "Epoch 2383: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8307e-06 - rmse: 0.0031 - val_loss: 6.5522e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2384/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5481e-05 - rmse: 0.0039\n",
      "Epoch 2384: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9900e-06 - rmse: 0.0028 - val_loss: 7.8605e-05 - val_rmse: 0.0089 - lr: 5.0000e-05\n",
      "Epoch 2385/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3311e-06 - rmse: 0.0027\n",
      "Epoch 2385: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3394e-05 - rmse: 0.0037 - val_loss: 5.1634e-05 - val_rmse: 0.0072 - lr: 5.0000e-05\n",
      "Epoch 2386/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2434e-05 - rmse: 0.0047\n",
      "Epoch 2386: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6987e-05 - rmse: 0.0041 - val_loss: 9.1934e-05 - val_rmse: 0.0096 - lr: 5.0000e-05\n",
      "Epoch 2387/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9193e-05 - rmse: 0.0044\n",
      "Epoch 2387: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.2165e-06 - rmse: 0.0027 - val_loss: 5.8560e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2388/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0551e-06 - rmse: 0.0022\n",
      "Epoch 2388: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0015e-06 - rmse: 0.0024 - val_loss: 7.3999e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2389/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0391e-06 - rmse: 0.0014\n",
      "Epoch 2389: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7310e-06 - rmse: 0.0024 - val_loss: 5.8726e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2390/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9408e-06 - rmse: 0.0022\n",
      "Epoch 2390: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1078e-06 - rmse: 0.0018 - val_loss: 6.4457e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2391/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9231e-06 - rmse: 0.0014\n",
      "Epoch 2391: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7863e-06 - rmse: 0.0017 - val_loss: 5.8508e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2392/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4628e-06 - rmse: 0.0016\n",
      "Epoch 2392: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9443e-06 - rmse: 0.0014 - val_loss: 7.0670e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2393/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8376e-06 - rmse: 0.0017\n",
      "Epoch 2393: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2102e-06 - rmse: 0.0023 - val_loss: 5.3816e-05 - val_rmse: 0.0073 - lr: 5.0000e-05\n",
      "Epoch 2394/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1584e-06 - rmse: 0.0020\n",
      "Epoch 2394: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8158e-06 - rmse: 0.0017 - val_loss: 6.8236e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2395/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4687e-07 - rmse: 9.2026e-04\n",
      "Epoch 2395: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6581e-06 - rmse: 0.0013 - val_loss: 6.3067e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n",
      "Epoch 2396/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0783e-06 - rmse: 0.0010\n",
      "Epoch 2396: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3452e-06 - rmse: 0.0015 - val_loss: 6.4333e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2397/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6638e-06 - rmse: 0.0013\n",
      "Epoch 2397: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3062e-06 - rmse: 0.0015 - val_loss: 6.1075e-05 - val_rmse: 0.0078 - lr: 5.0000e-05\n",
      "Epoch 2398/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6686e-07 - rmse: 8.7571e-04\n",
      "Epoch 2398: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5642e-06 - rmse: 0.0013 - val_loss: 6.4856e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2399/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4033e-07 - rmse: 6.6357e-04\n",
      "Epoch 2399: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4693e-06 - rmse: 0.0012 - val_loss: 6.3887e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2400/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1645e-06 - rmse: 0.0015\n",
      "Epoch 2400: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1941e-06 - rmse: 0.0015 - val_loss: 6.8204e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2401/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7148e-07 - rmse: 8.1944e-04\n",
      "Epoch 2401: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0960e-06 - rmse: 0.0020 - val_loss: 6.3870e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2402/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5877e-05 - rmse: 0.0040\n",
      "Epoch 2402: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1446e-06 - rmse: 0.0029 - val_loss: 7.1267e-05 - val_rmse: 0.0084 - lr: 5.0000e-05\n",
      "Epoch 2403/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1737e-06 - rmse: 0.0011\n",
      "Epoch 2403: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7628e-06 - rmse: 0.0022 - val_loss: 6.5559e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2404/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1399e-05 - rmse: 0.0034\n",
      "Epoch 2404: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3515e-06 - rmse: 0.0025 - val_loss: 6.3508e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2405/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9226e-06 - rmse: 0.0017\n",
      "Epoch 2405: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4611e-06 - rmse: 0.0021 - val_loss: 8.2950e-05 - val_rmse: 0.0091 - lr: 5.0000e-05\n",
      "Epoch 2406/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3514e-06 - rmse: 0.0025\n",
      "Epoch 2406: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6305e-06 - rmse: 0.0024 - val_loss: 5.3530e-05 - val_rmse: 0.0073 - lr: 5.0000e-05\n",
      "Epoch 2407/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5620e-06 - rmse: 0.0019\n",
      "Epoch 2407: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4431e-06 - rmse: 0.0019 - val_loss: 6.5397e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2408/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2652e-06 - rmse: 0.0011\n",
      "Epoch 2408: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8225e-06 - rmse: 0.0017 - val_loss: 6.6410e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2409/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4111e-06 - rmse: 0.0012\n",
      "Epoch 2409: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2041e-06 - rmse: 0.0015 - val_loss: 7.1739e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2410/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8274e-06 - rmse: 0.0014\n",
      "Epoch 2410: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5604e-06 - rmse: 0.0019 - val_loss: 5.8365e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2411/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8220e-06 - rmse: 0.0013\n",
      "Epoch 2411: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1395e-06 - rmse: 0.0023 - val_loss: 7.3714e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2412/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9492e-06 - rmse: 0.0014\n",
      "Epoch 2412: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0463e-06 - rmse: 0.0020 - val_loss: 5.7305e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2413/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5255e-06 - rmse: 0.0024\n",
      "Epoch 2413: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8759e-06 - rmse: 0.0017 - val_loss: 6.4927e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2414/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3521e-06 - rmse: 0.0021\n",
      "Epoch 2414: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6041e-06 - rmse: 0.0016 - val_loss: 7.2958e-05 - val_rmse: 0.0085 - lr: 5.0000e-05\n",
      "Epoch 2415/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9640e-07 - rmse: 7.7227e-04\n",
      "Epoch 2415: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0210e-06 - rmse: 0.0017 - val_loss: 6.7014e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2416/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7685e-06 - rmse: 0.0028\n",
      "Epoch 2416: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4514e-06 - rmse: 0.0021 - val_loss: 8.0597e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2417/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3145e-06 - rmse: 0.0015\n",
      "Epoch 2417: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7486e-06 - rmse: 0.0022 - val_loss: 5.9139e-05 - val_rmse: 0.0077 - lr: 5.0000e-05\n",
      "Epoch 2418/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7590e-06 - rmse: 0.0013\n",
      "Epoch 2418: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8908e-06 - rmse: 0.0024 - val_loss: 8.0459e-05 - val_rmse: 0.0090 - lr: 5.0000e-05\n",
      "Epoch 2419/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6579e-06 - rmse: 0.0028\n",
      "Epoch 2419: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9751e-06 - rmse: 0.0020 - val_loss: 6.7490e-05 - val_rmse: 0.0082 - lr: 5.0000e-05\n",
      "Epoch 2420/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3176e-06 - rmse: 0.0018\n",
      "Epoch 2420: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9916e-06 - rmse: 0.0028 - val_loss: 6.3113e-05 - val_rmse: 0.0079 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2421/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9647e-06 - rmse: 0.0024\n",
      "Epoch 2421: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9404e-06 - rmse: 0.0024 - val_loss: 7.4556e-05 - val_rmse: 0.0086 - lr: 5.0000e-05\n",
      "Epoch 2422/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0520e-06 - rmse: 0.0020\n",
      "Epoch 2422: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4927e-06 - rmse: 0.0016 - val_loss: 5.8118e-05 - val_rmse: 0.0076 - lr: 5.0000e-05\n",
      "Epoch 2423/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7640e-06 - rmse: 0.0019\n",
      "Epoch 2423: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9454e-06 - rmse: 0.0014 - val_loss: 6.5824e-05 - val_rmse: 0.0081 - lr: 5.0000e-05\n",
      "Epoch 2424/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2920e-06 - rmse: 0.0011\n",
      "Epoch 2424: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1463e-06 - rmse: 0.0015 - val_loss: 5.2090e-05 - val_rmse: 0.0072 - lr: 5.0000e-05\n",
      "Epoch 2425/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1504e-06 - rmse: 0.0015\n",
      "Epoch 2425: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.9376e-06 - rmse: 0.0020 - val_loss: 8.9233e-05 - val_rmse: 0.0094 - lr: 5.0000e-05\n",
      "Epoch 2426/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4111e-06 - rmse: 0.0029\n",
      "Epoch 2426: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2396e-06 - rmse: 0.0025 - val_loss: 6.4116e-05 - val_rmse: 0.0080 - lr: 5.0000e-05\n",
      "Epoch 2427/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0122e-05 - rmse: 0.0032\n",
      "Epoch 2427: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 2427: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9467e-06 - rmse: 0.0028 - val_loss: 6.8643e-05 - val_rmse: 0.0083 - lr: 5.0000e-05\n",
      "Epoch 2428/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1600e-06 - rmse: 0.0015\n",
      "Epoch 2428: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7460e-06 - rmse: 0.0017 - val_loss: 6.2103e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2429/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6601e-06 - rmse: 0.0016\n",
      "Epoch 2429: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7957e-06 - rmse: 0.0013 - val_loss: 6.7446e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2430/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0085e-07 - rmse: 9.4913e-04\n",
      "Epoch 2430: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8411e-06 - rmse: 0.0014 - val_loss: 6.6958e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2431/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4895e-06 - rmse: 0.0012\n",
      "Epoch 2431: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0621e-06 - rmse: 0.0010 - val_loss: 5.9406e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2432/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8411e-07 - rmse: 9.9202e-04\n",
      "Epoch 2432: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1544e-06 - rmse: 0.0011 - val_loss: 6.7060e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2433/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9043e-06 - rmse: 0.0014\n",
      "Epoch 2433: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2567e-06 - rmse: 0.0011 - val_loss: 6.6052e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2434/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9426e-06 - rmse: 0.0017\n",
      "Epoch 2434: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3283e-06 - rmse: 0.0012 - val_loss: 6.4704e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2435/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2668e-07 - rmse: 6.5320e-04\n",
      "Epoch 2435: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0848e-06 - rmse: 0.0010 - val_loss: 6.6218e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2436/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0215e-07 - rmse: 5.4968e-04\n",
      "Epoch 2436: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5334e-06 - rmse: 0.0012 - val_loss: 5.8796e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2437/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2694e-06 - rmse: 0.0018\n",
      "Epoch 2437: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1399e-06 - rmse: 0.0011 - val_loss: 7.0674e-05 - val_rmse: 0.0084 - lr: 2.5000e-05\n",
      "Epoch 2438/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8719e-07 - rmse: 5.3590e-04\n",
      "Epoch 2438: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4977e-06 - rmse: 0.0012 - val_loss: 5.9915e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2439/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3732e-07 - rmse: 9.6815e-04\n",
      "Epoch 2439: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5673e-06 - rmse: 0.0013 - val_loss: 7.1660e-05 - val_rmse: 0.0085 - lr: 2.5000e-05\n",
      "Epoch 2440/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5223e-07 - rmse: 9.2316e-04\n",
      "Epoch 2440: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3618e-06 - rmse: 0.0015 - val_loss: 6.4596e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2441/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9476e-07 - rmse: 8.9149e-04\n",
      "Epoch 2441: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7289e-06 - rmse: 0.0013 - val_loss: 6.4141e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2442/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1489e-06 - rmse: 0.0011\n",
      "Epoch 2442: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2339e-06 - rmse: 0.0015 - val_loss: 6.7780e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2443/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7032e-06 - rmse: 0.0013\n",
      "Epoch 2443: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6394e-06 - rmse: 0.0016 - val_loss: 6.3103e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2444/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2269e-06 - rmse: 0.0011\n",
      "Epoch 2444: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4760e-06 - rmse: 0.0016 - val_loss: 6.7083e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2445/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3831e-07 - rmse: 8.5925e-04\n",
      "Epoch 2445: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5957e-06 - rmse: 0.0013 - val_loss: 6.4398e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2446/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6378e-06 - rmse: 0.0013\n",
      "Epoch 2446: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9615e-06 - rmse: 0.0014 - val_loss: 6.8045e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2447/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5375e-06 - rmse: 0.0012\n",
      "Epoch 2447: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8013e-06 - rmse: 0.0013 - val_loss: 6.8143e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2448/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6964e-06 - rmse: 0.0024\n",
      "Epoch 2448: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4364e-06 - rmse: 0.0016 - val_loss: 6.7923e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2449/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1169e-06 - rmse: 0.0011\n",
      "Epoch 2449: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5461e-06 - rmse: 0.0012 - val_loss: 6.3759e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2450/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0911e-06 - rmse: 0.0010\n",
      "Epoch 2450: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8435e-06 - rmse: 0.0014 - val_loss: 6.4282e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2451/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4870e-06 - rmse: 0.0019\n",
      "Epoch 2451: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6162e-06 - rmse: 0.0016 - val_loss: 6.9343e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2452/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0140e-06 - rmse: 0.0010\n",
      "Epoch 2452: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2758e-06 - rmse: 0.0011 - val_loss: 6.1551e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2453/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9696e-06 - rmse: 0.0017\n",
      "Epoch 2453: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8227e-06 - rmse: 0.0017 - val_loss: 6.5172e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2454/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9915e-06 - rmse: 0.0014\n",
      "Epoch 2454: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0661e-06 - rmse: 0.0023 - val_loss: 6.7169e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2455/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4372e-06 - rmse: 0.0012\n",
      "Epoch 2455: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7868e-06 - rmse: 0.0019 - val_loss: 5.2368e-05 - val_rmse: 0.0072 - lr: 2.5000e-05\n",
      "Epoch 2456/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7598e-06 - rmse: 0.0022\n",
      "Epoch 2456: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1249e-06 - rmse: 0.0018 - val_loss: 7.8130e-05 - val_rmse: 0.0088 - lr: 2.5000e-05\n",
      "Epoch 2457/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0275e-06 - rmse: 0.0010\n",
      "Epoch 2457: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3022e-06 - rmse: 0.0021 - val_loss: 5.5969e-05 - val_rmse: 0.0075 - lr: 2.5000e-05\n",
      "Epoch 2458/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1602e-06 - rmse: 0.0011\n",
      "Epoch 2458: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.8384e-06 - rmse: 0.0020 - val_loss: 6.7549e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2459/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1215e-06 - rmse: 0.0015\n",
      "Epoch 2459: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0930e-06 - rmse: 0.0014 - val_loss: 6.8586e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2460/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3561e-07 - rmse: 9.6727e-04\n",
      "Epoch 2460: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7603e-06 - rmse: 0.0013 - val_loss: 5.3731e-05 - val_rmse: 0.0073 - lr: 2.5000e-05\n",
      "Epoch 2461/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1461e-06 - rmse: 0.0011\n",
      "Epoch 2461: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6513e-06 - rmse: 0.0016 - val_loss: 7.5902e-05 - val_rmse: 0.0087 - lr: 2.5000e-05\n",
      "Epoch 2462/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4317e-06 - rmse: 0.0021\n",
      "Epoch 2462: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2354e-06 - rmse: 0.0015 - val_loss: 6.0166e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2463/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2253e-07 - rmse: 8.5002e-04\n",
      "Epoch 2463: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7026e-06 - rmse: 0.0013 - val_loss: 6.8956e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2464/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5390e-07 - rmse: 9.2407e-04\n",
      "Epoch 2464: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7931e-06 - rmse: 0.0013 - val_loss: 6.1377e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2465/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3281e-06 - rmse: 0.0015\n",
      "Epoch 2465: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8448e-06 - rmse: 0.0014 - val_loss: 6.1868e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2466/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1360e-06 - rmse: 0.0011\n",
      "Epoch 2466: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1137e-06 - rmse: 0.0015 - val_loss: 6.7426e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2467/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4699e-06 - rmse: 0.0012\n",
      "Epoch 2467: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0002e-06 - rmse: 0.0014 - val_loss: 6.8293e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2468/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0454e-06 - rmse: 0.0017\n",
      "Epoch 2468: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3189e-06 - rmse: 0.0018 - val_loss: 6.5973e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2469/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8959e-06 - rmse: 0.0014\n",
      "Epoch 2469: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8813e-06 - rmse: 0.0017 - val_loss: 6.6326e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2470/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1404e-07 - rmse: 6.4346e-04\n",
      "Epoch 2470: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5824e-06 - rmse: 0.0013 - val_loss: 6.2069e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2471/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3882e-06 - rmse: 0.0018\n",
      "Epoch 2471: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5552e-06 - rmse: 0.0016 - val_loss: 6.5920e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2472/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9498e-07 - rmse: 8.9162e-04\n",
      "Epoch 2472: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3007e-06 - rmse: 0.0011 - val_loss: 6.2815e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2473/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2261e-06 - rmse: 0.0015\n",
      "Epoch 2473: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9252e-06 - rmse: 0.0014 - val_loss: 6.8475e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2474/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5369e-06 - rmse: 0.0016\n",
      "Epoch 2474: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9653e-06 - rmse: 0.0014 - val_loss: 6.1036e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2475/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9628e-07 - rmse: 4.4304e-04\n",
      "Epoch 2475: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8166e-06 - rmse: 0.0013 - val_loss: 6.9876e-05 - val_rmse: 0.0084 - lr: 2.5000e-05\n",
      "Epoch 2476/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1169e-06 - rmse: 0.0018\n",
      "Epoch 2476: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6453e-06 - rmse: 0.0016 - val_loss: 6.2119e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2477/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6700e-06 - rmse: 0.0019\n",
      "Epoch 2477: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2931e-06 - rmse: 0.0015 - val_loss: 6.7064e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2478/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0112e-06 - rmse: 0.0020\n",
      "Epoch 2478: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7847e-06 - rmse: 0.0013 - val_loss: 5.8379e-05 - val_rmse: 0.0076 - lr: 2.5000e-05\n",
      "Epoch 2479/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0587e-06 - rmse: 0.0010\n",
      "Epoch 2479: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2925e-06 - rmse: 0.0011 - val_loss: 6.6787e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2480/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0257e-07 - rmse: 8.9586e-04\n",
      "Epoch 2480: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5656e-06 - rmse: 0.0013 - val_loss: 6.2643e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2481/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2994e-06 - rmse: 0.0015\n",
      "Epoch 2481: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3070e-06 - rmse: 0.0011 - val_loss: 6.5430e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2482/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2267e-06 - rmse: 0.0011\n",
      "Epoch 2482: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7108e-06 - rmse: 0.0013 - val_loss: 6.1968e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2483/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6150e-07 - rmse: 7.4933e-04\n",
      "Epoch 2483: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5759e-06 - rmse: 0.0013 - val_loss: 6.4801e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2484/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0395e-07 - rmse: 6.3557e-04\n",
      "Epoch 2484: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7505e-06 - rmse: 0.0013 - val_loss: 7.1109e-05 - val_rmse: 0.0084 - lr: 2.5000e-05\n",
      "Epoch 2485/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7944e-06 - rmse: 0.0024\n",
      "Epoch 2485: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5253e-06 - rmse: 0.0016 - val_loss: 6.4205e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2486/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0991e-06 - rmse: 0.0018\n",
      "Epoch 2486: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9190e-06 - rmse: 0.0014 - val_loss: 6.9135e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2487/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2266e-07 - rmse: 6.5013e-04\n",
      "Epoch 2487: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0098e-06 - rmse: 0.0010 - val_loss: 5.8853e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2488/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4181e-07 - rmse: 7.3607e-04\n",
      "Epoch 2488: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4567e-06 - rmse: 0.0012 - val_loss: 6.4591e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2489/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8936e-06 - rmse: 0.0017\n",
      "Epoch 2489: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1836e-06 - rmse: 0.0011 - val_loss: 6.1882e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2490/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3865e-06 - rmse: 0.0012\n",
      "Epoch 2490: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5542e-06 - rmse: 0.0012 - val_loss: 6.4640e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2491/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8486e-07 - rmse: 4.2995e-04\n",
      "Epoch 2491: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.4269e-07 - rmse: 9.1798e-04 - val_loss: 6.1493e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2492/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1672e-07 - rmse: 5.6278e-04\n",
      "Epoch 2492: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3271e-07 - rmse: 9.6577e-04 - val_loss: 6.3171e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2493/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9990e-07 - rmse: 6.3237e-04\n",
      "Epoch 2493: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7628e-07 - rmse: 9.3610e-04 - val_loss: 6.1936e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2494/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6221e-07 - rmse: 5.1207e-04\n",
      "Epoch 2494: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8369e-07 - rmse: 8.8526e-04 - val_loss: 6.2459e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2495/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4603e-06 - rmse: 0.0012\n",
      "Epoch 2495: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0038e-06 - rmse: 0.0010 - val_loss: 6.3049e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2496/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1408e-07 - rmse: 5.6043e-04\n",
      "Epoch 2496: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9754e-07 - rmse: 9.9877e-04 - val_loss: 6.3017e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2497/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2556e-06 - rmse: 0.0018\n",
      "Epoch 2497: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5294e-06 - rmse: 0.0012 - val_loss: 6.0771e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2498/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9309e-07 - rmse: 8.3252e-04\n",
      "Epoch 2498: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5249e-06 - rmse: 0.0012 - val_loss: 6.5308e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2499/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9794e-06 - rmse: 0.0014\n",
      "Epoch 2499: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7547e-06 - rmse: 0.0013 - val_loss: 6.0945e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2500/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1194e-06 - rmse: 0.0011\n",
      "Epoch 2500: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7280e-06 - rmse: 0.0013 - val_loss: 6.4554e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2501/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3229e-06 - rmse: 0.0012\n",
      "Epoch 2501: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7299e-06 - rmse: 0.0013 - val_loss: 6.0707e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2502/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4523e-07 - rmse: 7.3839e-04\n",
      "Epoch 2502: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4176e-06 - rmse: 0.0012 - val_loss: 6.3466e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2503/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9387e-06 - rmse: 0.0017\n",
      "Epoch 2503: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5921e-06 - rmse: 0.0013 - val_loss: 6.6091e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2504/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3807e-06 - rmse: 0.0012\n",
      "Epoch 2504: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5340e-06 - rmse: 0.0012 - val_loss: 6.1174e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2505/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4863e-06 - rmse: 0.0016\n",
      "Epoch 2505: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7504e-07 - rmse: 9.8744e-04 - val_loss: 6.8080e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2506/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4960e-07 - rmse: 8.0598e-04\n",
      "Epoch 2506: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3048e-06 - rmse: 0.0011 - val_loss: 5.4475e-05 - val_rmse: 0.0074 - lr: 2.5000e-05\n",
      "Epoch 2507/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9542e-06 - rmse: 0.0014\n",
      "Epoch 2507: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4814e-06 - rmse: 0.0012 - val_loss: 6.7530e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2508/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9270e-07 - rmse: 8.3228e-04\n",
      "Epoch 2508: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0380e-06 - rmse: 0.0010 - val_loss: 5.9553e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2509/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3393e-06 - rmse: 0.0012\n",
      "Epoch 2509: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3917e-07 - rmse: 9.6911e-04 - val_loss: 6.3341e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2510/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5040e-07 - rmse: 5.9195e-04\n",
      "Epoch 2510: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9054e-07 - rmse: 9.4368e-04 - val_loss: 6.2017e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2511/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6886e-07 - rmse: 5.1851e-04\n",
      "Epoch 2511: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0546e-06 - rmse: 0.0010 - val_loss: 6.1392e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2512/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9910e-07 - rmse: 7.7402e-04\n",
      "Epoch 2512: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2501e-06 - rmse: 0.0011 - val_loss: 6.3584e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2513/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8916e-06 - rmse: 0.0020\n",
      "Epoch 2513: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2106e-06 - rmse: 0.0015 - val_loss: 6.4988e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2514/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3898e-06 - rmse: 0.0015\n",
      "Epoch 2514: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9034e-06 - rmse: 0.0014 - val_loss: 6.2242e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2515/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8171e-06 - rmse: 0.0013\n",
      "Epoch 2515: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3183e-06 - rmse: 0.0015 - val_loss: 6.3491e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2516/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6259e-06 - rmse: 0.0016\n",
      "Epoch 2516: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1913e-06 - rmse: 0.0011 - val_loss: 6.6948e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2517/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5048e-06 - rmse: 0.0012\n",
      "Epoch 2517: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2181e-06 - rmse: 0.0011 - val_loss: 6.3009e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2518/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8427e-06 - rmse: 0.0014\n",
      "Epoch 2518: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9916e-06 - rmse: 0.0014 - val_loss: 7.0189e-05 - val_rmse: 0.0084 - lr: 2.5000e-05\n",
      "Epoch 2519/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9006e-06 - rmse: 0.0017\n",
      "Epoch 2519: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0043e-06 - rmse: 0.0014 - val_loss: 6.2264e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2520/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1830e-06 - rmse: 0.0015\n",
      "Epoch 2520: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0422e-06 - rmse: 0.0014 - val_loss: 6.5809e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2521/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0624e-06 - rmse: 0.0010\n",
      "Epoch 2521: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3850e-06 - rmse: 0.0012 - val_loss: 6.2719e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2522/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6034e-07 - rmse: 9.7997e-04\n",
      "Epoch 2522: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3288e-06 - rmse: 0.0012 - val_loss: 6.2752e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2523/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1715e-06 - rmse: 0.0011\n",
      "Epoch 2523: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1422e-06 - rmse: 0.0015 - val_loss: 6.0003e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2524/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6565e-06 - rmse: 0.0013\n",
      "Epoch 2524: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4477e-06 - rmse: 0.0012 - val_loss: 6.5690e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2525/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3401e-07 - rmse: 6.5879e-04\n",
      "Epoch 2525: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2173e-06 - rmse: 0.0011 - val_loss: 5.9821e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2526/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7222e-07 - rmse: 8.7876e-04\n",
      "Epoch 2526: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0760e-06 - rmse: 0.0010 - val_loss: 6.7587e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2527/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5119e-07 - rmse: 6.7171e-04\n",
      "Epoch 2527: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2802e-06 - rmse: 0.0011 - val_loss: 5.8277e-05 - val_rmse: 0.0076 - lr: 2.5000e-05\n",
      "Epoch 2528/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3705e-06 - rmse: 0.0015\n",
      "Epoch 2528: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2271e-06 - rmse: 0.0011 - val_loss: 6.9093e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2529/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0764e-06 - rmse: 0.0018\n",
      "Epoch 2529: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0136e-06 - rmse: 0.0010 - val_loss: 6.0213e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2530/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6781e-07 - rmse: 7.5353e-04\n",
      "Epoch 2530: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9716e-07 - rmse: 9.9858e-04 - val_loss: 6.9395e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2531/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0345e-06 - rmse: 0.0014\n",
      "Epoch 2531: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4803e-06 - rmse: 0.0012 - val_loss: 5.8735e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2532/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0949e-06 - rmse: 0.0014\n",
      "Epoch 2532: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5481e-07 - rmse: 9.7714e-04 - val_loss: 6.6764e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2533/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0476e-07 - rmse: 8.9709e-04\n",
      "Epoch 2533: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0749e-07 - rmse: 9.5263e-04 - val_loss: 6.0793e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2534/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2034e-07 - rmse: 6.4833e-04\n",
      "Epoch 2534: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8548e-07 - rmse: 9.4100e-04 - val_loss: 6.3804e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2535/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2987e-07 - rmse: 6.5565e-04\n",
      "Epoch 2535: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0109e-06 - rmse: 0.0010 - val_loss: 6.4611e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2536/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3337e-06 - rmse: 0.0012\n",
      "Epoch 2536: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4442e-06 - rmse: 0.0012 - val_loss: 6.4697e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2537/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6271e-07 - rmse: 8.7333e-04\n",
      "Epoch 2537: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9184e-06 - rmse: 0.0014 - val_loss: 6.2011e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2538/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0510e-06 - rmse: 0.0014\n",
      "Epoch 2538: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.3503e-06 - rmse: 0.0015 - val_loss: 6.8574e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2539/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3566e-06 - rmse: 0.0012\n",
      "Epoch 2539: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0119e-06 - rmse: 0.0017 - val_loss: 5.6040e-05 - val_rmse: 0.0075 - lr: 2.5000e-05\n",
      "Epoch 2540/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3045e-06 - rmse: 0.0015\n",
      "Epoch 2540: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1341e-06 - rmse: 0.0015 - val_loss: 7.2440e-05 - val_rmse: 0.0085 - lr: 2.5000e-05\n",
      "Epoch 2541/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8245e-06 - rmse: 0.0022\n",
      "Epoch 2541: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2677e-06 - rmse: 0.0015 - val_loss: 5.9280e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2542/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8214e-06 - rmse: 0.0013\n",
      "Epoch 2542: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5261e-06 - rmse: 0.0012 - val_loss: 7.3500e-05 - val_rmse: 0.0086 - lr: 2.5000e-05\n",
      "Epoch 2543/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8725e-06 - rmse: 0.0020\n",
      "Epoch 2543: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1350e-06 - rmse: 0.0015 - val_loss: 5.6464e-05 - val_rmse: 0.0075 - lr: 2.5000e-05\n",
      "Epoch 2544/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5789e-06 - rmse: 0.0013\n",
      "Epoch 2544: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6771e-06 - rmse: 0.0013 - val_loss: 6.6496e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2545/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1236e-06 - rmse: 0.0015\n",
      "Epoch 2545: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2666e-06 - rmse: 0.0015 - val_loss: 6.2758e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2546/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0389e-06 - rmse: 0.0014\n",
      "Epoch 2546: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4689e-06 - rmse: 0.0016 - val_loss: 7.5675e-05 - val_rmse: 0.0087 - lr: 2.5000e-05\n",
      "Epoch 2547/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9859e-06 - rmse: 0.0017\n",
      "Epoch 2547: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0422e-06 - rmse: 0.0020 - val_loss: 5.6750e-05 - val_rmse: 0.0075 - lr: 2.5000e-05\n",
      "Epoch 2548/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0080e-06 - rmse: 0.0014\n",
      "Epoch 2548: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.2314e-06 - rmse: 0.0025 - val_loss: 6.5188e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2549/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1050e-06 - rmse: 0.0011\n",
      "Epoch 2549: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5271e-06 - rmse: 0.0024 - val_loss: 6.8335e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2550/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4892e-06 - rmse: 0.0021\n",
      "Epoch 2550: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9320e-06 - rmse: 0.0030 - val_loss: 6.0196e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2551/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2403e-06 - rmse: 0.0027\n",
      "Epoch 2551: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9987e-06 - rmse: 0.0026 - val_loss: 7.8708e-05 - val_rmse: 0.0089 - lr: 2.5000e-05\n",
      "Epoch 2552/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4586e-06 - rmse: 0.0023\n",
      "Epoch 2552: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5167e-06 - rmse: 0.0021 - val_loss: 6.4950e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2553/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5914e-06 - rmse: 0.0029\n",
      "Epoch 2553: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8179e-06 - rmse: 0.0024 - val_loss: 9.2367e-05 - val_rmse: 0.0096 - lr: 2.5000e-05\n",
      "Epoch 2554/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3339e-05 - rmse: 0.0037\n",
      "Epoch 2554: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.3798e-06 - rmse: 0.0023 - val_loss: 5.6077e-05 - val_rmse: 0.0075 - lr: 2.5000e-05\n",
      "Epoch 2555/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7078e-06 - rmse: 0.0013\n",
      "Epoch 2555: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.7724e-06 - rmse: 0.0019 - val_loss: 5.7171e-05 - val_rmse: 0.0076 - lr: 2.5000e-05\n",
      "Epoch 2556/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8902e-07 - rmse: 9.4288e-04\n",
      "Epoch 2556: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6785e-06 - rmse: 0.0019 - val_loss: 6.6290e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2557/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1106e-06 - rmse: 0.0015\n",
      "Epoch 2557: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.0444e-06 - rmse: 0.0014 - val_loss: 5.8233e-05 - val_rmse: 0.0076 - lr: 2.5000e-05\n",
      "Epoch 2558/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6012e-06 - rmse: 0.0013\n",
      "Epoch 2558: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5090e-06 - rmse: 0.0012 - val_loss: 6.4177e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2559/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6715e-06 - rmse: 0.0013\n",
      "Epoch 2559: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0876e-06 - rmse: 0.0010 - val_loss: 6.4924e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2560/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3272e-06 - rmse: 0.0015\n",
      "Epoch 2560: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0787e-06 - rmse: 0.0010 - val_loss: 6.2946e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2561/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9314e-07 - rmse: 7.7016e-04\n",
      "Epoch 2561: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0421e-06 - rmse: 0.0010 - val_loss: 6.1186e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2562/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6979e-07 - rmse: 9.8478e-04\n",
      "Epoch 2562: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1216e-06 - rmse: 0.0011 - val_loss: 6.1425e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2563/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6147e-07 - rmse: 5.1134e-04\n",
      "Epoch 2563: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0653e-06 - rmse: 0.0010 - val_loss: 6.4269e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2564/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2974e-06 - rmse: 0.0011\n",
      "Epoch 2564: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3424e-07 - rmse: 8.5688e-04 - val_loss: 6.5509e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2565/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6943e-07 - rmse: 6.8515e-04\n",
      "Epoch 2565: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0991e-06 - rmse: 0.0010 - val_loss: 6.1682e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2566/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0774e-07 - rmse: 7.1256e-04\n",
      "Epoch 2566: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0908e-06 - rmse: 0.0010 - val_loss: 6.7342e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2567/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3918e-06 - rmse: 0.0012\n",
      "Epoch 2567: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3603e-06 - rmse: 0.0012 - val_loss: 5.8610e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2568/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6470e-07 - rmse: 9.8219e-04\n",
      "Epoch 2568: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2747e-06 - rmse: 0.0011 - val_loss: 6.5509e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2569/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1545e-06 - rmse: 0.0015\n",
      "Epoch 2569: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0852e-06 - rmse: 0.0014 - val_loss: 6.5453e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2570/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9503e-06 - rmse: 0.0020\n",
      "Epoch 2570: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.9099e-06 - rmse: 0.0017 - val_loss: 6.1352e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2571/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2110e-06 - rmse: 0.0011\n",
      "Epoch 2571: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4168e-06 - rmse: 0.0012 - val_loss: 6.4177e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2572/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4419e-06 - rmse: 0.0012\n",
      "Epoch 2572: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4667e-06 - rmse: 0.0012 - val_loss: 6.1342e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2573/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9904e-07 - rmse: 5.4684e-04\n",
      "Epoch 2573: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1812e-06 - rmse: 0.0011 - val_loss: 6.5064e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2574/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1855e-06 - rmse: 0.0011\n",
      "Epoch 2574: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1260e-06 - rmse: 0.0011 - val_loss: 6.2083e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2575/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5106e-07 - rmse: 9.2253e-04\n",
      "Epoch 2575: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3628e-06 - rmse: 0.0012 - val_loss: 6.6610e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2576/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3999e-07 - rmse: 8.6023e-04\n",
      "Epoch 2576: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3226e-06 - rmse: 0.0012 - val_loss: 6.2960e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2577/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0855e-06 - rmse: 0.0010\n",
      "Epoch 2577: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0412e-06 - rmse: 0.0014 - val_loss: 6.2749e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2578/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1201e-06 - rmse: 0.0011\n",
      "Epoch 2578: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7074e-06 - rmse: 0.0013 - val_loss: 5.9157e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2579/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1727e-06 - rmse: 0.0011\n",
      "Epoch 2579: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7588e-06 - rmse: 0.0013 - val_loss: 6.4131e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2580/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7084e-06 - rmse: 0.0013\n",
      "Epoch 2580: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4157e-06 - rmse: 0.0018 - val_loss: 6.6299e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2581/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0115e-07 - rmse: 7.0792e-04\n",
      "Epoch 2581: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9492e-06 - rmse: 0.0017 - val_loss: 6.0708e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2582/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7199e-06 - rmse: 0.0013\n",
      "Epoch 2582: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.7868e-06 - rmse: 0.0013 - val_loss: 6.6910e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2583/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8823e-07 - rmse: 4.3386e-04\n",
      "Epoch 2583: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9038e-06 - rmse: 0.0014 - val_loss: 6.2739e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2584/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3944e-06 - rmse: 0.0015\n",
      "Epoch 2584: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.6359e-06 - rmse: 0.0019 - val_loss: 5.9207e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2585/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3353e-07 - rmse: 6.5843e-04\n",
      "Epoch 2585: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0090e-06 - rmse: 0.0020 - val_loss: 7.0647e-05 - val_rmse: 0.0084 - lr: 2.5000e-05\n",
      "Epoch 2586/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2828e-07 - rmse: 9.1010e-04\n",
      "Epoch 2586: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.2649e-06 - rmse: 0.0015 - val_loss: 5.9459e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2587/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7038e-06 - rmse: 0.0013\n",
      "Epoch 2587: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2979e-06 - rmse: 0.0011 - val_loss: 6.8769e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2588/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1146e-06 - rmse: 0.0015\n",
      "Epoch 2588: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5742e-06 - rmse: 0.0013 - val_loss: 5.9813e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2589/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9902e-07 - rmse: 7.0641e-04\n",
      "Epoch 2589: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6200e-06 - rmse: 0.0013 - val_loss: 6.8021e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2590/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4244e-06 - rmse: 0.0019\n",
      "Epoch 2590: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7209e-06 - rmse: 0.0013 - val_loss: 6.0586e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2591/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3303e-06 - rmse: 0.0012\n",
      "Epoch 2591: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6169e-06 - rmse: 0.0013 - val_loss: 6.4693e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2592/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0186e-06 - rmse: 0.0014\n",
      "Epoch 2592: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8912e-06 - rmse: 0.0014 - val_loss: 6.2062e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2593/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4578e-07 - rmse: 8.0361e-04\n",
      "Epoch 2593: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6130e-06 - rmse: 0.0013 - val_loss: 6.5653e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2594/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2043e-07 - rmse: 5.6607e-04\n",
      "Epoch 2594: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1510e-06 - rmse: 0.0011 - val_loss: 6.1885e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2595/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9921e-07 - rmse: 9.9961e-04\n",
      "Epoch 2595: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7096e-07 - rmse: 9.3326e-04 - val_loss: 6.1523e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2596/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4512e-07 - rmse: 7.3832e-04\n",
      "Epoch 2596: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5745e-07 - rmse: 9.7849e-04 - val_loss: 6.3241e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2597/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8649e-07 - rmse: 6.2168e-04\n",
      "Epoch 2597: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2369e-07 - rmse: 9.0757e-04 - val_loss: 6.0651e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2598/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7993e-07 - rmse: 5.2909e-04\n",
      "Epoch 2598: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5582e-07 - rmse: 9.2510e-04 - val_loss: 6.4055e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2599/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6612e-06 - rmse: 0.0013\n",
      "Epoch 2599: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2524e-06 - rmse: 0.0011 - val_loss: 6.4765e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2600/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2025e-06 - rmse: 0.0011\n",
      "Epoch 2600: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3049e-06 - rmse: 0.0011 - val_loss: 6.3438e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2601/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4108e-07 - rmse: 5.8402e-04\n",
      "Epoch 2601: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1718e-06 - rmse: 0.0011 - val_loss: 5.9561e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2602/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6319e-07 - rmse: 8.1436e-04\n",
      "Epoch 2602: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0235e-06 - rmse: 0.0010 - val_loss: 6.6833e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2603/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2212e-06 - rmse: 0.0011\n",
      "Epoch 2603: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7173e-06 - rmse: 0.0013 - val_loss: 6.0353e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2604/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9156e-07 - rmse: 7.0111e-04\n",
      "Epoch 2604: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3737e-06 - rmse: 0.0012 - val_loss: 6.5462e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2605/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2598e-07 - rmse: 7.9119e-04\n",
      "Epoch 2605: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2553e-06 - rmse: 0.0011 - val_loss: 6.3448e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2606/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1332e-06 - rmse: 0.0011\n",
      "Epoch 2606: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3198e-06 - rmse: 0.0011 - val_loss: 6.5950e-05 - val_rmse: 0.0081 - lr: 2.5000e-05\n",
      "Epoch 2607/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3148e-06 - rmse: 0.0011\n",
      "Epoch 2607: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4494e-06 - rmse: 0.0012 - val_loss: 6.0950e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2608/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5946e-06 - rmse: 0.0019\n",
      "Epoch 2608: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9310e-06 - rmse: 0.0014 - val_loss: 6.8759e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2609/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1714e-06 - rmse: 0.0011\n",
      "Epoch 2609: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7750e-06 - rmse: 0.0013 - val_loss: 6.0911e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2610/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7122e-06 - rmse: 0.0013\n",
      "Epoch 2610: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7058e-06 - rmse: 0.0013 - val_loss: 6.9651e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2611/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0883e-06 - rmse: 0.0010\n",
      "Epoch 2611: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8754e-06 - rmse: 0.0014 - val_loss: 6.0144e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2612/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5454e-07 - rmse: 9.2441e-04\n",
      "Epoch 2612: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0651e-06 - rmse: 0.0010 - val_loss: 6.4435e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2613/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7446e-07 - rmse: 8.8004e-04\n",
      "Epoch 2613: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7992e-06 - rmse: 0.0013 - val_loss: 6.2778e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2614/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1582e-06 - rmse: 0.0020\n",
      "Epoch 2614: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.5418e-06 - rmse: 0.0016 - val_loss: 6.6655e-05 - val_rmse: 0.0082 - lr: 2.5000e-05\n",
      "Epoch 2615/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8161e-06 - rmse: 0.0013\n",
      "Epoch 2615: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5405e-06 - rmse: 0.0012 - val_loss: 6.1176e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2616/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6159e-07 - rmse: 8.1338e-04\n",
      "Epoch 2616: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1877e-06 - rmse: 0.0011 - val_loss: 5.9868e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2617/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5134e-06 - rmse: 0.0012\n",
      "Epoch 2617: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4688e-06 - rmse: 0.0012 - val_loss: 6.4366e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2618/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3717e-07 - rmse: 9.6808e-04\n",
      "Epoch 2618: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4725e-06 - rmse: 0.0012 - val_loss: 6.3317e-05 - val_rmse: 0.0080 - lr: 2.5000e-05\n",
      "Epoch 2619/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3650e-07 - rmse: 9.6773e-04\n",
      "Epoch 2619: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2078e-06 - rmse: 0.0011 - val_loss: 6.8683e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2620/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8886e-06 - rmse: 0.0014\n",
      "Epoch 2620: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6687e-06 - rmse: 0.0016 - val_loss: 6.2271e-05 - val_rmse: 0.0079 - lr: 2.5000e-05\n",
      "Epoch 2621/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0897e-07 - rmse: 6.3951e-04\n",
      "Epoch 2621: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1390e-06 - rmse: 0.0015 - val_loss: 6.1434e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2622/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5341e-06 - rmse: 0.0016\n",
      "Epoch 2622: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.3294e-06 - rmse: 0.0015 - val_loss: 6.1518e-05 - val_rmse: 0.0078 - lr: 2.5000e-05\n",
      "Epoch 2623/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2681e-06 - rmse: 0.0015\n",
      "Epoch 2623: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4187e-06 - rmse: 0.0016 - val_loss: 5.9991e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2624/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2518e-06 - rmse: 0.0015\n",
      "Epoch 2624: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8993e-06 - rmse: 0.0014 - val_loss: 5.8798e-05 - val_rmse: 0.0077 - lr: 2.5000e-05\n",
      "Epoch 2625/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6412e-06 - rmse: 0.0013\n",
      "Epoch 2625: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1891e-06 - rmse: 0.0015 - val_loss: 7.8442e-05 - val_rmse: 0.0089 - lr: 2.5000e-05\n",
      "Epoch 2626/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7977e-06 - rmse: 0.0026\n",
      "Epoch 2626: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.6648e-06 - rmse: 0.0019 - val_loss: 5.3371e-05 - val_rmse: 0.0073 - lr: 2.5000e-05\n",
      "Epoch 2627/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9519e-06 - rmse: 0.0020\n",
      "Epoch 2627: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 2627: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.4628e-06 - rmse: 0.0019 - val_loss: 6.8605e-05 - val_rmse: 0.0083 - lr: 2.5000e-05\n",
      "Epoch 2628/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3149e-06 - rmse: 0.0015\n",
      "Epoch 2628: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4665e-06 - rmse: 0.0016 - val_loss: 6.4666e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2629/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3775e-06 - rmse: 0.0012\n",
      "Epoch 2629: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6348e-06 - rmse: 0.0013 - val_loss: 5.6987e-05 - val_rmse: 0.0075 - lr: 1.2500e-05\n",
      "Epoch 2630/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5049e-06 - rmse: 0.0012\n",
      "Epoch 2630: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0669e-06 - rmse: 0.0010 - val_loss: 6.6029e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2631/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4672e-07 - rmse: 5.8883e-04\n",
      "Epoch 2631: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4707e-07 - rmse: 9.7318e-04 - val_loss: 6.3890e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2632/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6215e-07 - rmse: 8.7301e-04\n",
      "Epoch 2632: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0168e-06 - rmse: 0.0010 - val_loss: 5.5771e-05 - val_rmse: 0.0075 - lr: 1.2500e-05\n",
      "Epoch 2633/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1015e-06 - rmse: 0.0010\n",
      "Epoch 2633: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2533e-06 - rmse: 0.0011 - val_loss: 6.2474e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2634/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1266e-06 - rmse: 0.0011\n",
      "Epoch 2634: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6557e-06 - rmse: 0.0013 - val_loss: 6.6940e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2635/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9732e-06 - rmse: 0.0017\n",
      "Epoch 2635: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3905e-06 - rmse: 0.0012 - val_loss: 5.5415e-05 - val_rmse: 0.0074 - lr: 1.2500e-05\n",
      "Epoch 2636/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5197e-07 - rmse: 6.7229e-04\n",
      "Epoch 2636: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2789e-06 - rmse: 0.0011 - val_loss: 6.4735e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2637/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2141e-06 - rmse: 0.0011\n",
      "Epoch 2637: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4612e-06 - rmse: 0.0012 - val_loss: 6.2475e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2638/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5044e-07 - rmse: 5.9198e-04\n",
      "Epoch 2638: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9854e-07 - rmse: 9.9927e-04 - val_loss: 5.9901e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2639/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0686e-07 - rmse: 4.5482e-04\n",
      "Epoch 2639: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3879e-07 - rmse: 8.5953e-04 - val_loss: 6.5126e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2640/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7567e-07 - rmse: 5.2504e-04\n",
      "Epoch 2640: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3142e-07 - rmse: 8.5523e-04 - val_loss: 6.1210e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2641/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7296e-07 - rmse: 6.1070e-04\n",
      "Epoch 2641: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.7463e-07 - rmse: 8.2136e-04 - val_loss: 6.1318e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2642/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9168e-07 - rmse: 4.3781e-04\n",
      "Epoch 2642: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9267e-07 - rmse: 8.3227e-04 - val_loss: 6.3512e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2643/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4304e-07 - rmse: 6.6561e-04\n",
      "Epoch 2643: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9697e-07 - rmse: 9.4709e-04 - val_loss: 6.4891e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2644/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8961e-07 - rmse: 5.3815e-04\n",
      "Epoch 2644: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0613e-07 - rmse: 9.5191e-04 - val_loss: 6.0520e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2645/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6973e-07 - rmse: 6.0806e-04\n",
      "Epoch 2645: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3033e-06 - rmse: 0.0011 - val_loss: 6.1284e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2646/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8441e-06 - rmse: 0.0014\n",
      "Epoch 2646: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0085e-06 - rmse: 0.0010 - val_loss: 6.4489e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2647/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3913e-07 - rmse: 4.8900e-04\n",
      "Epoch 2647: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7228e-07 - rmse: 8.7879e-04 - val_loss: 6.0552e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2648/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6013e-07 - rmse: 5.1003e-04\n",
      "Epoch 2648: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3086e-07 - rmse: 9.1151e-04 - val_loss: 6.1028e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2649/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0444e-07 - rmse: 7.1024e-04\n",
      "Epoch 2649: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0609e-07 - rmse: 8.9782e-04 - val_loss: 6.4035e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2650/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5886e-07 - rmse: 6.7739e-04\n",
      "Epoch 2650: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1066e-07 - rmse: 9.0037e-04 - val_loss: 6.2573e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2651/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5757e-06 - rmse: 0.0013\n",
      "Epoch 2651: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3286e-06 - rmse: 0.0012 - val_loss: 6.0061e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2652/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8110e-07 - rmse: 9.3867e-04\n",
      "Epoch 2652: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3257e-06 - rmse: 0.0012 - val_loss: 6.4267e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2653/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8461e-07 - rmse: 9.4054e-04\n",
      "Epoch 2653: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7990e-07 - rmse: 8.8312e-04 - val_loss: 6.2402e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2654/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5235e-06 - rmse: 0.0012\n",
      "Epoch 2654: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7204e-07 - rmse: 9.3383e-04 - val_loss: 6.3892e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2655/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1948e-07 - rmse: 4.6848e-04\n",
      "Epoch 2655: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0627e-06 - rmse: 0.0010 - val_loss: 6.4164e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2656/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7872e-07 - rmse: 5.2794e-04\n",
      "Epoch 2656: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5876e-07 - rmse: 8.7107e-04 - val_loss: 5.9856e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2657/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4440e-07 - rmse: 7.3783e-04\n",
      "Epoch 2657: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9047e-07 - rmse: 8.8908e-04 - val_loss: 6.4862e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2658/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1294e-06 - rmse: 0.0011\n",
      "Epoch 2658: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0437e-06 - rmse: 0.0010 - val_loss: 6.5365e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2659/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7037e-06 - rmse: 0.0013\n",
      "Epoch 2659: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4727e-06 - rmse: 0.0012 - val_loss: 6.0048e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2660/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1160e-06 - rmse: 0.0011\n",
      "Epoch 2660: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2075e-06 - rmse: 0.0011 - val_loss: 6.4997e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2661/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3419e-07 - rmse: 6.5893e-04\n",
      "Epoch 2661: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3330e-06 - rmse: 0.0012 - val_loss: 5.8940e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2662/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0773e-06 - rmse: 0.0010\n",
      "Epoch 2662: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2951e-06 - rmse: 0.0011 - val_loss: 6.3460e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2663/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5191e-06 - rmse: 0.0012\n",
      "Epoch 2663: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0201e-06 - rmse: 0.0010 - val_loss: 6.3584e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2664/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9632e-06 - rmse: 0.0017\n",
      "Epoch 2664: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0109e-06 - rmse: 0.0010 - val_loss: 6.0395e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2665/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0076e-06 - rmse: 0.0010\n",
      "Epoch 2665: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.5278e-07 - rmse: 9.7611e-04 - val_loss: 6.3148e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2666/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2208e-06 - rmse: 0.0015\n",
      "Epoch 2666: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1168e-07 - rmse: 8.4361e-04 - val_loss: 6.3911e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2667/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2567e-07 - rmse: 5.7068e-04\n",
      "Epoch 2667: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4482e-07 - rmse: 8.0301e-04 - val_loss: 6.2130e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2668/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1166e-07 - rmse: 5.5827e-04\n",
      "Epoch 2668: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8868e-07 - rmse: 8.8808e-04 - val_loss: 6.1017e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2669/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9807e-07 - rmse: 9.4767e-04\n",
      "Epoch 2669: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0398e-06 - rmse: 0.0010 - val_loss: 6.4316e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2670/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8784e-06 - rmse: 0.0014\n",
      "Epoch 2670: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0187e-07 - rmse: 8.9547e-04 - val_loss: 6.4099e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2671/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1157e-07 - rmse: 7.1524e-04\n",
      "Epoch 2671: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6181e-07 - rmse: 9.2833e-04 - val_loss: 6.2422e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2672/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9195e-07 - rmse: 4.3812e-04\n",
      "Epoch 2672: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.9408e-07 - rmse: 9.4556e-04 - val_loss: 6.1092e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2673/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4780e-07 - rmse: 8.6476e-04\n",
      "Epoch 2673: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0022e-06 - rmse: 0.0010 - val_loss: 6.5037e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2674/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9927e-07 - rmse: 7.7413e-04\n",
      "Epoch 2674: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1002e-07 - rmse: 9.5395e-04 - val_loss: 6.1590e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2675/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2808e-06 - rmse: 0.0011\n",
      "Epoch 2675: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.1433e-07 - rmse: 8.4518e-04 - val_loss: 6.1910e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2676/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9692e-07 - rmse: 6.3002e-04\n",
      "Epoch 2676: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9485e-07 - rmse: 8.9154e-04 - val_loss: 6.1583e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2677/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8898e-07 - rmse: 6.9927e-04\n",
      "Epoch 2677: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6761e-07 - rmse: 9.3145e-04 - val_loss: 6.3994e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2678/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1617e-07 - rmse: 5.6229e-04\n",
      "Epoch 2678: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8904e-07 - rmse: 8.3008e-04 - val_loss: 6.2149e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2679/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7225e-07 - rmse: 6.8721e-04\n",
      "Epoch 2679: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7451e-07 - rmse: 8.2129e-04 - val_loss: 6.1389e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2680/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1889e-06 - rmse: 0.0011\n",
      "Epoch 2680: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0859e-06 - rmse: 0.0010 - val_loss: 6.4019e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2681/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4054e-06 - rmse: 0.0012\n",
      "Epoch 2681: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2451e-06 - rmse: 0.0011 - val_loss: 6.3301e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2682/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5885e-07 - rmse: 8.1169e-04\n",
      "Epoch 2682: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6884e-07 - rmse: 9.3211e-04 - val_loss: 6.0449e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2683/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5074e-07 - rmse: 5.0073e-04\n",
      "Epoch 2683: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2137e-06 - rmse: 0.0011 - val_loss: 6.2756e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2684/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8610e-06 - rmse: 0.0014\n",
      "Epoch 2684: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4702e-06 - rmse: 0.0012 - val_loss: 6.0369e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2685/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9195e-06 - rmse: 0.0014\n",
      "Epoch 2685: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8267e-06 - rmse: 0.0014 - val_loss: 6.3755e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2686/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6280e-07 - rmse: 7.5020e-04\n",
      "Epoch 2686: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3596e-06 - rmse: 0.0012 - val_loss: 6.1267e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2687/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5086e-07 - rmse: 9.7512e-04\n",
      "Epoch 2687: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8578e-07 - rmse: 9.9287e-04 - val_loss: 6.1236e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2688/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9757e-07 - rmse: 9.9878e-04\n",
      "Epoch 2688: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0778e-06 - rmse: 0.0010 - val_loss: 6.4683e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2689/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2164e-06 - rmse: 0.0015\n",
      "Epoch 2689: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4386e-07 - rmse: 9.7153e-04 - val_loss: 6.1185e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2690/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5260e-07 - rmse: 6.7276e-04\n",
      "Epoch 2690: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2047e-07 - rmse: 8.4881e-04 - val_loss: 6.3096e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2691/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8662e-07 - rmse: 5.3537e-04\n",
      "Epoch 2691: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3133e-07 - rmse: 8.5518e-04 - val_loss: 6.1409e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2692/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7771e-06 - rmse: 0.0013\n",
      "Epoch 2692: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0063e-07 - rmse: 8.9478e-04 - val_loss: 6.2395e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2693/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3061e-07 - rmse: 7.9411e-04\n",
      "Epoch 2693: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4706e-07 - rmse: 8.6433e-04 - val_loss: 6.1863e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2694/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8662e-07 - rmse: 5.3537e-04\n",
      "Epoch 2694: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1473e-07 - rmse: 8.4541e-04 - val_loss: 6.2606e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2695/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2129e-07 - rmse: 7.8822e-04\n",
      "Epoch 2695: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2947e-07 - rmse: 8.5409e-04 - val_loss: 6.2781e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2696/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8318e-07 - rmse: 5.3214e-04\n",
      "Epoch 2696: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4041e-07 - rmse: 8.0026e-04 - val_loss: 6.1037e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2697/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5890e-07 - rmse: 9.7923e-04\n",
      "Epoch 2697: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5782e-07 - rmse: 9.7868e-04 - val_loss: 6.5444e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2698/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8703e-06 - rmse: 0.0014\n",
      "Epoch 2698: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0226e-07 - rmse: 9.4987e-04 - val_loss: 5.9515e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2699/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3566e-07 - rmse: 4.8545e-04\n",
      "Epoch 2699: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9256e-07 - rmse: 8.3220e-04 - val_loss: 6.4915e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2700/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9735e-07 - rmse: 7.0523e-04\n",
      "Epoch 2700: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9206e-07 - rmse: 8.8998e-04 - val_loss: 6.1252e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2701/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6438e-07 - rmse: 5.1418e-04\n",
      "Epoch 2701: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.8988e-07 - rmse: 8.8875e-04 - val_loss: 6.3820e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2702/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1657e-07 - rmse: 6.4542e-04\n",
      "Epoch 2702: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0257e-07 - rmse: 8.9586e-04 - val_loss: 6.1373e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2703/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8926e-07 - rmse: 6.2391e-04\n",
      "Epoch 2703: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.9232e-07 - rmse: 8.3206e-04 - val_loss: 6.1917e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2704/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5361e-06 - rmse: 0.0012\n",
      "Epoch 2704: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4252e-07 - rmse: 8.0157e-04 - val_loss: 6.3823e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2705/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2144e-07 - rmse: 4.7057e-04\n",
      "Epoch 2705: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1233e-07 - rmse: 7.8252e-04 - val_loss: 6.1454e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2706/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2271e-06 - rmse: 0.0011\n",
      "Epoch 2706: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5451e-07 - rmse: 9.2440e-04 - val_loss: 6.1914e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2707/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8794e-06 - rmse: 0.0014\n",
      "Epoch 2707: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5078e-07 - rmse: 9.2238e-04 - val_loss: 6.4743e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2708/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6772e-07 - rmse: 6.0640e-04\n",
      "Epoch 2708: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1999e-07 - rmse: 9.5916e-04 - val_loss: 6.1514e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2709/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8504e-07 - rmse: 4.3017e-04\n",
      "Epoch 2709: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8305e-07 - rmse: 8.2647e-04 - val_loss: 6.1754e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2710/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4275e-07 - rmse: 4.9270e-04\n",
      "Epoch 2710: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8905e-07 - rmse: 8.8828e-04 - val_loss: 6.2126e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2711/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8580e-07 - rmse: 4.3104e-04\n",
      "Epoch 2711: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4518e-07 - rmse: 8.0323e-04 - val_loss: 6.0506e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2712/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3619e-07 - rmse: 4.8599e-04\n",
      "Epoch 2712: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1953e-07 - rmse: 8.4825e-04 - val_loss: 6.4860e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2713/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6203e-06 - rmse: 0.0013\n",
      "Epoch 2713: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2293e-06 - rmse: 0.0011 - val_loss: 5.9122e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2714/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3208e-07 - rmse: 7.9504e-04\n",
      "Epoch 2714: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8601e-07 - rmse: 8.8657e-04 - val_loss: 6.7128e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2715/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2127e-07 - rmse: 7.2199e-04\n",
      "Epoch 2715: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2004e-06 - rmse: 0.0011 - val_loss: 5.8196e-05 - val_rmse: 0.0076 - lr: 1.2500e-05\n",
      "Epoch 2716/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3533e-07 - rmse: 7.3166e-04\n",
      "Epoch 2716: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1017e-06 - rmse: 0.0010 - val_loss: 6.3572e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2717/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2897e-07 - rmse: 8.5379e-04\n",
      "Epoch 2717: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5399e-07 - rmse: 9.7672e-04 - val_loss: 6.1504e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2718/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2205e-07 - rmse: 6.4965e-04\n",
      "Epoch 2718: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0025e-07 - rmse: 8.9457e-04 - val_loss: 6.2087e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2719/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9334e-07 - rmse: 9.4517e-04\n",
      "Epoch 2719: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8952e-07 - rmse: 8.8855e-04 - val_loss: 6.2867e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2720/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3498e-06 - rmse: 0.0012\n",
      "Epoch 2720: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7848e-07 - rmse: 8.8232e-04 - val_loss: 6.2307e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2721/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5753e-07 - rmse: 3.9691e-04\n",
      "Epoch 2721: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0024e-07 - rmse: 8.3680e-04 - val_loss: 6.5498e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2722/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7149e-07 - rmse: 6.0950e-04\n",
      "Epoch 2722: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2930e-07 - rmse: 7.9329e-04 - val_loss: 6.1106e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2723/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7622e-07 - rmse: 5.2557e-04\n",
      "Epoch 2723: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3965e-07 - rmse: 9.6936e-04 - val_loss: 6.1621e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2724/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2299e-07 - rmse: 4.7222e-04\n",
      "Epoch 2724: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0602e-07 - rmse: 8.4025e-04 - val_loss: 6.3349e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2725/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6079e-07 - rmse: 6.7882e-04\n",
      "Epoch 2725: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5519e-07 - rmse: 9.2476e-04 - val_loss: 6.4149e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2726/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0680e-06 - rmse: 0.0010\n",
      "Epoch 2726: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0227e-06 - rmse: 0.0010 - val_loss: 5.9670e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2727/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0344e-07 - rmse: 6.3517e-04\n",
      "Epoch 2727: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3460e-07 - rmse: 9.6675e-04 - val_loss: 6.4339e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2728/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0587e-06 - rmse: 0.0010\n",
      "Epoch 2728: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4806e-06 - rmse: 0.0012 - val_loss: 6.2810e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2729/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6601e-07 - rmse: 8.1609e-04\n",
      "Epoch 2729: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0662e-06 - rmse: 0.0010 - val_loss: 6.0424e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2730/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2142e-07 - rmse: 9.5991e-04\n",
      "Epoch 2730: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2014e-06 - rmse: 0.0011 - val_loss: 6.2330e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2731/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3372e-06 - rmse: 0.0012\n",
      "Epoch 2731: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0012e-06 - rmse: 0.0010 - val_loss: 6.3570e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2732/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3453e-06 - rmse: 0.0012\n",
      "Epoch 2732: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4238e-06 - rmse: 0.0012 - val_loss: 6.0199e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2733/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0911e-07 - rmse: 8.4209e-04\n",
      "Epoch 2733: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.5052e-06 - rmse: 0.0012 - val_loss: 6.2827e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2734/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3330e-07 - rmse: 6.5826e-04\n",
      "Epoch 2734: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7840e-07 - rmse: 9.8914e-04 - val_loss: 6.6174e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2735/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8163e-06 - rmse: 0.0013\n",
      "Epoch 2735: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0518e-06 - rmse: 0.0010 - val_loss: 6.4866e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2736/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0453e-07 - rmse: 7.7752e-04\n",
      "Epoch 2736: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7248e-07 - rmse: 9.3406e-04 - val_loss: 6.1408e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2737/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3992e-07 - rmse: 6.6326e-04\n",
      "Epoch 2737: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6098e-07 - rmse: 8.7234e-04 - val_loss: 6.2800e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2738/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6264e-07 - rmse: 5.1248e-04\n",
      "Epoch 2738: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9797e-07 - rmse: 8.3545e-04 - val_loss: 6.0968e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2739/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5856e-07 - rmse: 5.9880e-04\n",
      "Epoch 2739: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2488e-07 - rmse: 8.5140e-04 - val_loss: 6.5459e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2740/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2136e-06 - rmse: 0.0011\n",
      "Epoch 2740: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8411e-07 - rmse: 8.8550e-04 - val_loss: 6.0806e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2741/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2314e-06 - rmse: 0.0011\n",
      "Epoch 2741: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.0464e-07 - rmse: 8.9702e-04 - val_loss: 6.5079e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2742/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3147e-07 - rmse: 6.5686e-04\n",
      "Epoch 2742: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1939e-07 - rmse: 7.8701e-04 - val_loss: 6.1383e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2743/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4141e-07 - rmse: 6.6438e-04\n",
      "Epoch 2743: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7158e-07 - rmse: 9.8569e-04 - val_loss: 6.3016e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2744/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0928e-07 - rmse: 7.1364e-04\n",
      "Epoch 2744: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6085e-07 - rmse: 9.2782e-04 - val_loss: 6.2156e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2745/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5476e-07 - rmse: 8.6877e-04\n",
      "Epoch 2745: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1763e-06 - rmse: 0.0011 - val_loss: 6.6310e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2746/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3895e-07 - rmse: 5.8219e-04\n",
      "Epoch 2746: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.6831e-07 - rmse: 9.8403e-04 - val_loss: 6.0162e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2747/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9497e-07 - rmse: 7.0354e-04\n",
      "Epoch 2747: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7748e-07 - rmse: 9.3674e-04 - val_loss: 6.2758e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2748/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3246e-07 - rmse: 6.5761e-04\n",
      "Epoch 2748: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.0808e-07 - rmse: 8.4148e-04 - val_loss: 6.0937e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2749/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2294e-06 - rmse: 0.0011\n",
      "Epoch 2749: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5943e-07 - rmse: 8.1205e-04 - val_loss: 6.3708e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2750/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2691e-06 - rmse: 0.0011\n",
      "Epoch 2750: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5027e-07 - rmse: 8.6618e-04 - val_loss: 6.3794e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2751/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2886e-07 - rmse: 7.9301e-04\n",
      "Epoch 2751: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3365e-07 - rmse: 7.9602e-04 - val_loss: 6.0872e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2752/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7434e-07 - rmse: 6.8872e-04\n",
      "Epoch 2752: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.8309e-07 - rmse: 9.3973e-04 - val_loss: 6.1150e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2753/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2495e-07 - rmse: 5.7005e-04\n",
      "Epoch 2753: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1892e-06 - rmse: 0.0011 - val_loss: 6.3384e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2754/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0671e-07 - rmse: 3.2667e-04\n",
      "Epoch 2754: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1980e-07 - rmse: 9.0543e-04 - val_loss: 6.2403e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2755/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6873e-07 - rmse: 5.1839e-04\n",
      "Epoch 2755: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8171e-07 - rmse: 8.8414e-04 - val_loss: 6.2864e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2756/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3261e-06 - rmse: 0.0012\n",
      "Epoch 2756: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.4851e-07 - rmse: 9.2115e-04 - val_loss: 6.3228e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2757/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4773e-07 - rmse: 6.6913e-04\n",
      "Epoch 2757: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.0564e-07 - rmse: 8.4003e-04 - val_loss: 6.2513e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2758/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5278e-07 - rmse: 5.9395e-04\n",
      "Epoch 2758: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1763e-07 - rmse: 8.4713e-04 - val_loss: 6.1401e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2759/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3082e-07 - rmse: 3.6169e-04\n",
      "Epoch 2759: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7959e-07 - rmse: 7.6131e-04 - val_loss: 6.1678e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2760/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9589e-07 - rmse: 4.4259e-04\n",
      "Epoch 2760: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.9644e-07 - rmse: 9.4680e-04 - val_loss: 6.4631e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2761/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7229e-07 - rmse: 6.1015e-04\n",
      "Epoch 2761: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6761e-07 - rmse: 8.1707e-04 - val_loss: 6.0017e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2762/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3033e-07 - rmse: 9.1123e-04\n",
      "Epoch 2762: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6463e-07 - rmse: 9.2985e-04 - val_loss: 6.1562e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2763/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0165e-07 - rmse: 8.9535e-04\n",
      "Epoch 2763: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4629e-06 - rmse: 0.0012 - val_loss: 6.3121e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2764/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6898e-07 - rmse: 6.0744e-04\n",
      "Epoch 2764: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4287e-06 - rmse: 0.0012 - val_loss: 5.9318e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2765/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8577e-07 - rmse: 9.9286e-04\n",
      "Epoch 2765: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.6606e-07 - rmse: 9.3063e-04 - val_loss: 6.5074e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2766/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3817e-07 - rmse: 4.8803e-04\n",
      "Epoch 2766: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1727e-07 - rmse: 9.5774e-04 - val_loss: 6.3441e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2767/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0846e-07 - rmse: 4.5657e-04\n",
      "Epoch 2767: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8104e-07 - rmse: 8.8376e-04 - val_loss: 6.0989e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2768/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4945e-06 - rmse: 0.0012\n",
      "Epoch 2768: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2252e-06 - rmse: 0.0011 - val_loss: 6.7207e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2769/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2677e-06 - rmse: 0.0015\n",
      "Epoch 2769: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9925e-07 - rmse: 9.9963e-04 - val_loss: 6.1393e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2770/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2261e-06 - rmse: 0.0011\n",
      "Epoch 2770: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3883e-06 - rmse: 0.0012 - val_loss: 6.2854e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2771/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3326e-06 - rmse: 0.0012\n",
      "Epoch 2771: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1779e-06 - rmse: 0.0011 - val_loss: 6.5961e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2772/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0161e-06 - rmse: 0.0010\n",
      "Epoch 2772: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8823e-07 - rmse: 9.9410e-04 - val_loss: 6.4399e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2773/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1021e-06 - rmse: 0.0010\n",
      "Epoch 2773: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.4599e-07 - rmse: 9.7262e-04 - val_loss: 6.2202e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2774/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0349e-06 - rmse: 0.0014\n",
      "Epoch 2774: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3669e-07 - rmse: 9.6783e-04 - val_loss: 6.4763e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2775/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5685e-07 - rmse: 5.9737e-04\n",
      "Epoch 2775: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.7247e-07 - rmse: 8.2004e-04 - val_loss: 6.4086e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2776/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6529e-06 - rmse: 0.0013\n",
      "Epoch 2776: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9276e-07 - rmse: 8.9037e-04 - val_loss: 6.2771e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2777/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3850e-07 - rmse: 7.9906e-04\n",
      "Epoch 2777: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.3173e-07 - rmse: 9.6526e-04 - val_loss: 6.4019e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2778/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4664e-07 - rmse: 7.3935e-04\n",
      "Epoch 2778: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0290e-06 - rmse: 0.0010 - val_loss: 6.2451e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2779/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2097e-06 - rmse: 0.0011\n",
      "Epoch 2779: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8419e-07 - rmse: 8.8554e-04 - val_loss: 6.4628e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2780/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9301e-07 - rmse: 9.9650e-04\n",
      "Epoch 2780: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9574e-07 - rmse: 8.9204e-04 - val_loss: 6.3220e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2781/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4757e-07 - rmse: 4.9757e-04\n",
      "Epoch 2781: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.7558e-07 - rmse: 8.2194e-04 - val_loss: 6.1890e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2782/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0447e-07 - rmse: 4.5218e-04\n",
      "Epoch 2782: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.4485e-07 - rmse: 9.1916e-04 - val_loss: 6.3952e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2783/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1133e-07 - rmse: 5.5797e-04\n",
      "Epoch 2783: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6952e-07 - rmse: 9.3248e-04 - val_loss: 6.6534e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2784/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7033e-06 - rmse: 0.0016\n",
      "Epoch 2784: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5933e-06 - rmse: 0.0013 - val_loss: 6.0087e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2785/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4001e-07 - rmse: 6.6333e-04\n",
      "Epoch 2785: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0121e-06 - rmse: 0.0010 - val_loss: 6.7253e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2786/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1336e-07 - rmse: 7.8318e-04\n",
      "Epoch 2786: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0607e-07 - rmse: 9.5188e-04 - val_loss: 6.1158e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2787/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3003e-07 - rmse: 4.7961e-04\n",
      "Epoch 2787: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1432e-06 - rmse: 0.0011 - val_loss: 6.3189e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2788/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1939e-06 - rmse: 0.0011\n",
      "Epoch 2788: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3137e-07 - rmse: 7.9459e-04 - val_loss: 6.3628e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2789/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0602e-07 - rmse: 7.1135e-04\n",
      "Epoch 2789: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.8729e-07 - rmse: 9.9363e-04 - val_loss: 6.2436e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2790/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3728e-07 - rmse: 8.5865e-04\n",
      "Epoch 2790: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4398e-07 - rmse: 8.0249e-04 - val_loss: 6.1186e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2791/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1699e-06 - rmse: 0.0011\n",
      "Epoch 2791: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0783e-06 - rmse: 0.0010 - val_loss: 6.4175e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2792/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6325e-07 - rmse: 8.1440e-04\n",
      "Epoch 2792: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3155e-06 - rmse: 0.0011 - val_loss: 6.2235e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2793/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8971e-07 - rmse: 9.9484e-04\n",
      "Epoch 2793: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0350e-06 - rmse: 0.0010 - val_loss: 6.2225e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2794/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7478e-07 - rmse: 6.1219e-04\n",
      "Epoch 2794: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.6139e-07 - rmse: 9.2811e-04 - val_loss: 6.2739e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2795/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2652e-06 - rmse: 0.0015\n",
      "Epoch 2795: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1325e-06 - rmse: 0.0011 - val_loss: 6.2411e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2796/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7175e-06 - rmse: 0.0013\n",
      "Epoch 2796: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5146e-06 - rmse: 0.0012 - val_loss: 6.3067e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2797/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6388e-07 - rmse: 8.1479e-04\n",
      "Epoch 2797: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2017e-06 - rmse: 0.0011 - val_loss: 5.7410e-05 - val_rmse: 0.0076 - lr: 1.2500e-05\n",
      "Epoch 2798/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1034e-06 - rmse: 0.0011\n",
      "Epoch 2798: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0084e-06 - rmse: 0.0010 - val_loss: 6.4365e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2799/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3567e-07 - rmse: 7.3190e-04\n",
      "Epoch 2799: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2939e-06 - rmse: 0.0011 - val_loss: 6.2544e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2800/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1766e-07 - rmse: 7.1948e-04\n",
      "Epoch 2800: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1619e-06 - rmse: 0.0011 - val_loss: 6.0821e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2801/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8659e-07 - rmse: 6.9756e-04\n",
      "Epoch 2801: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9344e-07 - rmse: 9.9672e-04 - val_loss: 6.5187e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2802/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1688e-06 - rmse: 0.0015\n",
      "Epoch 2802: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1418e-06 - rmse: 0.0011 - val_loss: 6.0516e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2803/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3421e-06 - rmse: 0.0012\n",
      "Epoch 2803: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1060e-07 - rmse: 9.0033e-04 - val_loss: 6.6866e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2804/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8420e-07 - rmse: 6.9585e-04\n",
      "Epoch 2804: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1003e-07 - rmse: 9.0002e-04 - val_loss: 6.0719e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2805/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2738e-07 - rmse: 7.9207e-04\n",
      "Epoch 2805: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0283e-07 - rmse: 8.9601e-04 - val_loss: 6.4679e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2806/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7855e-07 - rmse: 8.8236e-04\n",
      "Epoch 2806: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1492e-07 - rmse: 9.0273e-04 - val_loss: 6.3782e-05 - val_rmse: 0.0080 - lr: 1.2500e-05\n",
      "Epoch 2807/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4542e-07 - rmse: 6.6739e-04\n",
      "Epoch 2807: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.7050e-07 - rmse: 9.8514e-04 - val_loss: 6.6509e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2808/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2762e-07 - rmse: 8.5301e-04\n",
      "Epoch 2808: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3034e-06 - rmse: 0.0011 - val_loss: 6.2308e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2809/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7830e-07 - rmse: 4.2226e-04\n",
      "Epoch 2809: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2203e-07 - rmse: 9.6022e-04 - val_loss: 6.6463e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2810/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7716e-06 - rmse: 0.0013\n",
      "Epoch 2810: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0616e-06 - rmse: 0.0014 - val_loss: 6.2219e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2811/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4494e-06 - rmse: 0.0012\n",
      "Epoch 2811: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0703e-06 - rmse: 0.0014 - val_loss: 6.0917e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2812/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3056e-07 - rmse: 9.6466e-04\n",
      "Epoch 2812: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1145e-06 - rmse: 0.0015 - val_loss: 6.7001e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2813/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6612e-06 - rmse: 0.0016\n",
      "Epoch 2813: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4846e-06 - rmse: 0.0012 - val_loss: 6.1377e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2814/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8163e-06 - rmse: 0.0017\n",
      "Epoch 2814: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.6206e-06 - rmse: 0.0013 - val_loss: 6.7042e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2815/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3638e-07 - rmse: 8.5812e-04\n",
      "Epoch 2815: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1217e-06 - rmse: 0.0011 - val_loss: 6.0191e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2816/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9124e-07 - rmse: 7.0089e-04\n",
      "Epoch 2816: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3690e-06 - rmse: 0.0012 - val_loss: 6.6059e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2817/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8377e-07 - rmse: 7.6405e-04\n",
      "Epoch 2817: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.4178e-06 - rmse: 0.0012 - val_loss: 6.2345e-05 - val_rmse: 0.0079 - lr: 1.2500e-05\n",
      "Epoch 2818/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1854e-07 - rmse: 9.5841e-04\n",
      "Epoch 2818: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0345e-06 - rmse: 0.0010 - val_loss: 5.8834e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2819/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9141e-07 - rmse: 6.2562e-04\n",
      "Epoch 2819: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2472e-06 - rmse: 0.0011 - val_loss: 6.6945e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2820/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1189e-07 - rmse: 7.8223e-04\n",
      "Epoch 2820: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3900e-07 - rmse: 9.1597e-04 - val_loss: 5.9410e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2821/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8117e-06 - rmse: 0.0013\n",
      "Epoch 2821: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0682e-06 - rmse: 0.0010 - val_loss: 6.6800e-05 - val_rmse: 0.0082 - lr: 1.2500e-05\n",
      "Epoch 2822/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1463e-07 - rmse: 7.8398e-04\n",
      "Epoch 2822: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2355e-06 - rmse: 0.0011 - val_loss: 6.0610e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2823/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2478e-07 - rmse: 5.6989e-04\n",
      "Epoch 2823: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.3000e-06 - rmse: 0.0011 - val_loss: 6.5921e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2824/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7628e-07 - rmse: 5.2563e-04\n",
      "Epoch 2824: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0894e-06 - rmse: 0.0010 - val_loss: 6.0779e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2825/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7399e-07 - rmse: 8.7977e-04\n",
      "Epoch 2825: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3305e-07 - rmse: 9.1271e-04 - val_loss: 5.8851e-05 - val_rmse: 0.0077 - lr: 1.2500e-05\n",
      "Epoch 2826/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1063e-07 - rmse: 5.5734e-04\n",
      "Epoch 2826: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3251e-07 - rmse: 7.9530e-04 - val_loss: 6.5770e-05 - val_rmse: 0.0081 - lr: 1.2500e-05\n",
      "Epoch 2827/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1366e-07 - rmse: 5.6006e-04\n",
      "Epoch 2827: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 2827: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9757e-07 - rmse: 8.9307e-04 - val_loss: 6.0763e-05 - val_rmse: 0.0078 - lr: 1.2500e-05\n",
      "Epoch 2828/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8118e-07 - rmse: 5.3027e-04\n",
      "Epoch 2828: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2090e-07 - rmse: 7.8797e-04 - val_loss: 6.2069e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2829/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1320e-07 - rmse: 5.5965e-04\n",
      "Epoch 2829: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3786e-07 - rmse: 7.9866e-04 - val_loss: 6.3834e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2830/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1571e-06 - rmse: 0.0011\n",
      "Epoch 2830: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4243e-07 - rmse: 8.6164e-04 - val_loss: 6.0927e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2831/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3554e-07 - rmse: 5.7926e-04\n",
      "Epoch 2831: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5775e-07 - rmse: 8.1102e-04 - val_loss: 6.2455e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2832/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3258e-07 - rmse: 5.7670e-04\n",
      "Epoch 2832: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1051e-07 - rmse: 8.4292e-04 - val_loss: 6.5088e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2833/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8854e-06 - rmse: 0.0014\n",
      "Epoch 2833: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8786e-07 - rmse: 8.2938e-04 - val_loss: 6.1710e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2834/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4494e-07 - rmse: 3.8071e-04\n",
      "Epoch 2834: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3066e-07 - rmse: 8.5479e-04 - val_loss: 6.1031e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2835/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1880e-06 - rmse: 0.0011\n",
      "Epoch 2835: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3612e-07 - rmse: 7.9757e-04 - val_loss: 6.3965e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2836/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6826e-06 - rmse: 0.0013\n",
      "Epoch 2836: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0450e-07 - rmse: 7.7750e-04 - val_loss: 6.1848e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2837/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9501e-07 - rmse: 8.3368e-04\n",
      "Epoch 2837: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1739e-07 - rmse: 7.8574e-04 - val_loss: 6.4503e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2838/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7250e-07 - rmse: 8.7892e-04\n",
      "Epoch 2838: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0027e-07 - rmse: 8.3682e-04 - val_loss: 6.1255e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2839/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3048e-07 - rmse: 9.1131e-04\n",
      "Epoch 2839: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9540e-07 - rmse: 7.7162e-04 - val_loss: 6.2265e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2840/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0393e-07 - rmse: 6.3556e-04\n",
      "Epoch 2840: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7059e-07 - rmse: 7.5537e-04 - val_loss: 6.1188e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2841/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8625e-07 - rmse: 4.3157e-04\n",
      "Epoch 2841: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5240e-07 - rmse: 7.4324e-04 - val_loss: 6.3473e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2842/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6082e-07 - rmse: 4.0102e-04\n",
      "Epoch 2842: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8518e-07 - rmse: 8.2775e-04 - val_loss: 6.2755e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2843/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4215e-06 - rmse: 0.0012\n",
      "Epoch 2843: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2956e-07 - rmse: 8.5414e-04 - val_loss: 6.0411e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2844/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0786e-07 - rmse: 5.5485e-04\n",
      "Epoch 2844: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1143e-07 - rmse: 8.4346e-04 - val_loss: 6.2369e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2845/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7332e-07 - rmse: 4.1632e-04\n",
      "Epoch 2845: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0689e-07 - rmse: 7.1196e-04 - val_loss: 6.2574e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2846/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1300e-07 - rmse: 6.4265e-04\n",
      "Epoch 2846: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0319e-07 - rmse: 7.7665e-04 - val_loss: 6.3237e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2847/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4747e-06 - rmse: 0.0012\n",
      "Epoch 2847: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4233e-07 - rmse: 8.0146e-04 - val_loss: 6.0958e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2848/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2000e-07 - rmse: 5.6568e-04\n",
      "Epoch 2848: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2877e-07 - rmse: 8.5368e-04 - val_loss: 6.2998e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2849/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6999e-07 - rmse: 5.1961e-04\n",
      "Epoch 2849: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2929e-07 - rmse: 7.9328e-04 - val_loss: 6.3212e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2850/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2586e-07 - rmse: 7.9112e-04\n",
      "Epoch 2850: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0437e-07 - rmse: 7.7741e-04 - val_loss: 6.1907e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2851/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1000e-07 - rmse: 5.5677e-04\n",
      "Epoch 2851: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6356e-07 - rmse: 7.5071e-04 - val_loss: 6.0573e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2852/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0534e-06 - rmse: 0.0010\n",
      "Epoch 2852: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5596e-07 - rmse: 8.0991e-04 - val_loss: 6.4176e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2853/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6119e-07 - rmse: 4.0148e-04\n",
      "Epoch 2853: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1160e-07 - rmse: 8.4356e-04 - val_loss: 6.3108e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2854/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0102e-07 - rmse: 7.7525e-04\n",
      "Epoch 2854: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2206e-07 - rmse: 7.2253e-04 - val_loss: 5.9871e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2855/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9083e-07 - rmse: 7.6865e-04\n",
      "Epoch 2855: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.4434e-07 - rmse: 8.0271e-04 - val_loss: 6.2470e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2856/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1177e-06 - rmse: 0.0011\n",
      "Epoch 2856: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8106e-07 - rmse: 7.6227e-04 - val_loss: 6.2875e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2857/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7106e-07 - rmse: 4.1360e-04\n",
      "Epoch 2857: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1070e-07 - rmse: 7.8147e-04 - val_loss: 6.4088e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2858/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9158e-07 - rmse: 8.3161e-04\n",
      "Epoch 2858: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6239e-07 - rmse: 8.7315e-04 - val_loss: 6.0089e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2859/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0571e-06 - rmse: 0.0010\n",
      "Epoch 2859: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7604e-07 - rmse: 8.8093e-04 - val_loss: 6.5044e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2860/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2693e-06 - rmse: 0.0011\n",
      "Epoch 2860: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6844e-07 - rmse: 8.7661e-04 - val_loss: 6.0945e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2861/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3747e-06 - rmse: 0.0012\n",
      "Epoch 2861: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8517e-07 - rmse: 8.2775e-04 - val_loss: 6.1162e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2862/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8661e-07 - rmse: 5.3536e-04\n",
      "Epoch 2862: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9270e-07 - rmse: 7.6987e-04 - val_loss: 6.3645e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2863/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3763e-07 - rmse: 5.8106e-04\n",
      "Epoch 2863: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8498e-07 - rmse: 8.2764e-04 - val_loss: 6.3255e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2864/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3045e-06 - rmse: 0.0011\n",
      "Epoch 2864: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9736e-07 - rmse: 8.9295e-04 - val_loss: 5.9293e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2865/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7022e-07 - rmse: 6.0846e-04\n",
      "Epoch 2865: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3412e-07 - rmse: 7.9632e-04 - val_loss: 6.4775e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2866/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4284e-07 - rmse: 5.8552e-04\n",
      "Epoch 2866: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4328e-07 - rmse: 8.0205e-04 - val_loss: 6.2636e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2867/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5406e-06 - rmse: 0.0012\n",
      "Epoch 2867: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7253e-07 - rmse: 8.7893e-04 - val_loss: 6.0902e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2868/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7198e-07 - rmse: 6.8701e-04\n",
      "Epoch 2868: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6325e-07 - rmse: 8.1440e-04 - val_loss: 6.5640e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2869/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8130e-07 - rmse: 6.1749e-04\n",
      "Epoch 2869: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8675e-07 - rmse: 8.2870e-04 - val_loss: 6.1166e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2870/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7848e-07 - rmse: 4.2247e-04\n",
      "Epoch 2870: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6137e-07 - rmse: 7.4925e-04 - val_loss: 6.1723e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2871/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0639e-07 - rmse: 5.5353e-04\n",
      "Epoch 2871: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2626e-07 - rmse: 7.9137e-04 - val_loss: 6.3887e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2872/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7219e-07 - rmse: 5.2172e-04\n",
      "Epoch 2872: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1797e-07 - rmse: 7.8611e-04 - val_loss: 6.2009e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2873/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6666e-07 - rmse: 6.0553e-04\n",
      "Epoch 2873: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5586e-07 - rmse: 7.4556e-04 - val_loss: 6.1085e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2874/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4387e-07 - rmse: 9.1862e-04\n",
      "Epoch 2874: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8060e-07 - rmse: 8.2498e-04 - val_loss: 6.5036e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2875/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0592e-06 - rmse: 0.0014\n",
      "Epoch 2875: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6892e-07 - rmse: 8.1787e-04 - val_loss: 6.0460e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2876/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8706e-07 - rmse: 5.3578e-04\n",
      "Epoch 2876: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4638e-07 - rmse: 7.3917e-04 - val_loss: 6.2407e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2877/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6741e-07 - rmse: 6.8368e-04\n",
      "Epoch 2877: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.9900e-07 - rmse: 8.9387e-04 - val_loss: 6.4206e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2878/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9146e-07 - rmse: 5.3987e-04\n",
      "Epoch 2878: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.8113e-07 - rmse: 8.2530e-04 - val_loss: 5.9896e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2879/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4061e-07 - rmse: 5.8362e-04\n",
      "Epoch 2879: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4379e-07 - rmse: 8.0236e-04 - val_loss: 6.3229e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2880/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9947e-07 - rmse: 9.9973e-04\n",
      "Epoch 2880: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4662e-07 - rmse: 8.0413e-04 - val_loss: 6.2608e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2881/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7015e-07 - rmse: 5.1976e-04\n",
      "Epoch 2881: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8021e-07 - rmse: 7.6171e-04 - val_loss: 6.0929e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2882/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8721e-07 - rmse: 5.3592e-04\n",
      "Epoch 2882: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7029e-07 - rmse: 7.5517e-04 - val_loss: 6.3785e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2883/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1925e-07 - rmse: 5.6502e-04\n",
      "Epoch 2883: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0393e-07 - rmse: 7.7713e-04 - val_loss: 6.0634e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2884/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5177e-07 - rmse: 6.7214e-04\n",
      "Epoch 2884: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5658e-07 - rmse: 8.1029e-04 - val_loss: 6.2808e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2885/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0562e-07 - rmse: 7.7822e-04\n",
      "Epoch 2885: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4145e-07 - rmse: 8.0090e-04 - val_loss: 6.2772e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2886/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5288e-07 - rmse: 6.7296e-04\n",
      "Epoch 2886: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8867e-07 - rmse: 8.8807e-04 - val_loss: 6.3331e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2887/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6240e-06 - rmse: 0.0013\n",
      "Epoch 2887: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5569e-07 - rmse: 7.4544e-04 - val_loss: 5.9481e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2888/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2638e-06 - rmse: 0.0011\n",
      "Epoch 2888: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3476e-07 - rmse: 7.9672e-04 - val_loss: 6.1447e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2889/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7432e-07 - rmse: 5.2376e-04\n",
      "Epoch 2889: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8135e-07 - rmse: 8.8394e-04 - val_loss: 6.4842e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2890/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1109e-07 - rmse: 8.4326e-04\n",
      "Epoch 2890: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2017e-07 - rmse: 8.4863e-04 - val_loss: 5.9800e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2891/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3408e-07 - rmse: 5.7800e-04\n",
      "Epoch 2891: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9886e-07 - rmse: 8.3598e-04 - val_loss: 6.1998e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2892/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3060e-08 - rmse: 2.8820e-04\n",
      "Epoch 2892: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3331e-07 - rmse: 7.3028e-04 - val_loss: 6.2919e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2893/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1548e-06 - rmse: 0.0011\n",
      "Epoch 2893: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1633e-07 - rmse: 7.8507e-04 - val_loss: 6.2163e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2894/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7206e-06 - rmse: 0.0013\n",
      "Epoch 2894: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4847e-07 - rmse: 8.6514e-04 - val_loss: 6.1646e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2895/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4914e-07 - rmse: 5.9088e-04\n",
      "Epoch 2895: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8079e-07 - rmse: 7.6210e-04 - val_loss: 6.2847e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2896/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8102e-07 - rmse: 4.2546e-04\n",
      "Epoch 2896: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7435e-07 - rmse: 8.2119e-04 - val_loss: 6.2793e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2897/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8405e-07 - rmse: 4.2901e-04\n",
      "Epoch 2897: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6529e-07 - rmse: 8.1565e-04 - val_loss: 5.9537e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2898/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5816e-07 - rmse: 5.0810e-04\n",
      "Epoch 2898: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5989e-07 - rmse: 8.1234e-04 - val_loss: 6.3773e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2899/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8014e-07 - rmse: 9.3816e-04\n",
      "Epoch 2899: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1146e-07 - rmse: 9.0081e-04 - val_loss: 6.5207e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2900/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5583e-07 - rmse: 5.9652e-04\n",
      "Epoch 2900: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2541e-07 - rmse: 7.9083e-04 - val_loss: 5.9822e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2901/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2414e-07 - rmse: 7.9003e-04\n",
      "Epoch 2901: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9994e-07 - rmse: 8.3663e-04 - val_loss: 6.2494e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2902/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3080e-06 - rmse: 0.0011\n",
      "Epoch 2902: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5912e-07 - rmse: 9.2689e-04 - val_loss: 6.3968e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2903/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1069e-07 - rmse: 6.4085e-04\n",
      "Epoch 2903: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1571e-07 - rmse: 7.8467e-04 - val_loss: 6.1045e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2904/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9927e-07 - rmse: 5.4706e-04\n",
      "Epoch 2904: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2842e-07 - rmse: 8.5348e-04 - val_loss: 6.0873e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2905/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2112e-07 - rmse: 9.5975e-04\n",
      "Epoch 2905: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0690e-07 - rmse: 8.9828e-04 - val_loss: 6.6423e-05 - val_rmse: 0.0082 - lr: 6.2500e-06\n",
      "Epoch 2906/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2601e-07 - rmse: 7.9121e-04\n",
      "Epoch 2906: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7395e-07 - rmse: 8.7974e-04 - val_loss: 6.0836e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2907/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3358e-07 - rmse: 7.3047e-04\n",
      "Epoch 2907: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7555e-07 - rmse: 8.2192e-04 - val_loss: 6.1752e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2908/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0902e-07 - rmse: 4.5718e-04\n",
      "Epoch 2908: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0214e-07 - rmse: 7.0862e-04 - val_loss: 6.2136e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2909/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9157e-07 - rmse: 4.3768e-04\n",
      "Epoch 2909: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7598e-07 - rmse: 7.5893e-04 - val_loss: 6.2276e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2910/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7736e-07 - rmse: 4.2114e-04\n",
      "Epoch 2910: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8798e-07 - rmse: 7.6680e-04 - val_loss: 6.2059e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2911/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5558e-07 - rmse: 5.9631e-04\n",
      "Epoch 2911: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3125e-07 - rmse: 7.9451e-04 - val_loss: 6.3942e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2912/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2496e-07 - rmse: 5.7005e-04\n",
      "Epoch 2912: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8632e-07 - rmse: 8.2845e-04 - val_loss: 6.0715e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2913/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5410e-06 - rmse: 0.0012\n",
      "Epoch 2913: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.3948e-07 - rmse: 9.1623e-04 - val_loss: 6.2565e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2914/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8640e-08 - rmse: 2.8043e-04\n",
      "Epoch 2914: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4785e-07 - rmse: 8.6478e-04 - val_loss: 6.4846e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2915/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5580e-06 - rmse: 0.0012\n",
      "Epoch 2915: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0126e-07 - rmse: 9.4935e-04 - val_loss: 5.8551e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2916/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0133e-06 - rmse: 0.0010\n",
      "Epoch 2916: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0305e-06 - rmse: 0.0010 - val_loss: 6.4188e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2917/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7639e-07 - rmse: 4.1999e-04\n",
      "Epoch 2917: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9418e-07 - rmse: 7.7083e-04 - val_loss: 6.2553e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2918/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2012e-07 - rmse: 5.6579e-04\n",
      "Epoch 2918: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0873e-07 - rmse: 8.4186e-04 - val_loss: 6.2374e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2919/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2648e-07 - rmse: 5.7138e-04\n",
      "Epoch 2919: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6573e-07 - rmse: 7.5215e-04 - val_loss: 6.3669e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2920/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8616e-07 - rmse: 8.8666e-04\n",
      "Epoch 2920: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3113e-07 - rmse: 7.9444e-04 - val_loss: 6.1704e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2921/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9593e-07 - rmse: 4.4264e-04\n",
      "Epoch 2921: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9765e-07 - rmse: 7.7308e-04 - val_loss: 6.2040e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2922/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6532e-07 - rmse: 4.0659e-04\n",
      "Epoch 2922: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5146e-07 - rmse: 8.0713e-04 - val_loss: 6.4616e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2923/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7126e-07 - rmse: 5.2082e-04\n",
      "Epoch 2923: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4752e-07 - rmse: 8.0469e-04 - val_loss: 6.1274e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2924/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3904e-07 - rmse: 4.8892e-04\n",
      "Epoch 2924: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7835e-07 - rmse: 7.6049e-04 - val_loss: 6.2998e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2925/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1315e-06 - rmse: 0.0011\n",
      "Epoch 2925: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1223e-07 - rmse: 7.8245e-04 - val_loss: 6.1744e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2926/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3368e-06 - rmse: 0.0012\n",
      "Epoch 2926: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3136e-07 - rmse: 7.9458e-04 - val_loss: 6.1824e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2927/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2340e-07 - rmse: 5.6868e-04\n",
      "Epoch 2927: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2946e-07 - rmse: 7.9339e-04 - val_loss: 6.4746e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2928/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3802e-06 - rmse: 0.0012\n",
      "Epoch 2928: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5233e-07 - rmse: 8.6737e-04 - val_loss: 5.9747e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2929/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7467e-06 - rmse: 0.0013\n",
      "Epoch 2929: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6024e-07 - rmse: 8.7192e-04 - val_loss: 6.4277e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2930/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9453e-07 - rmse: 6.2812e-04\n",
      "Epoch 2930: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0338e-07 - rmse: 7.7678e-04 - val_loss: 6.3334e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2931/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0304e-07 - rmse: 6.3485e-04\n",
      "Epoch 2931: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4091e-07 - rmse: 7.3547e-04 - val_loss: 6.0881e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2932/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5812e-07 - rmse: 9.2635e-04\n",
      "Epoch 2932: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5080e-07 - rmse: 7.4216e-04 - val_loss: 6.1942e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2933/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7796e-07 - rmse: 4.2185e-04\n",
      "Epoch 2933: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3336e-07 - rmse: 7.3032e-04 - val_loss: 6.2331e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2934/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2569e-08 - rmse: 2.8735e-04\n",
      "Epoch 2934: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8698e-07 - rmse: 7.6615e-04 - val_loss: 6.1635e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2935/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6144e-07 - rmse: 4.0179e-04\n",
      "Epoch 2935: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3042e-07 - rmse: 7.2830e-04 - val_loss: 6.3546e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2936/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6289e-07 - rmse: 4.0360e-04\n",
      "Epoch 2936: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8525e-07 - rmse: 7.6502e-04 - val_loss: 6.2467e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2937/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2343e-07 - rmse: 4.7268e-04\n",
      "Epoch 2937: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4062e-07 - rmse: 7.3527e-04 - val_loss: 6.0197e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2938/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5269e-07 - rmse: 5.9388e-04\n",
      "Epoch 2938: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3726e-07 - rmse: 7.3298e-04 - val_loss: 6.4016e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2939/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6378e-07 - rmse: 5.1359e-04\n",
      "Epoch 2939: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6721e-07 - rmse: 8.7591e-04 - val_loss: 6.2380e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2940/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1096e-07 - rmse: 4.5930e-04\n",
      "Epoch 2940: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2292e-07 - rmse: 7.2313e-04 - val_loss: 6.0441e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2941/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8752e-07 - rmse: 4.3303e-04\n",
      "Epoch 2941: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7166e-07 - rmse: 7.5609e-04 - val_loss: 6.1627e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2942/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7757e-07 - rmse: 5.2685e-04\n",
      "Epoch 2942: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1224e-07 - rmse: 7.8246e-04 - val_loss: 6.5396e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2943/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2234e-07 - rmse: 6.4988e-04\n",
      "Epoch 2943: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3226e-07 - rmse: 7.9515e-04 - val_loss: 6.0138e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2944/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0447e-07 - rmse: 4.5218e-04\n",
      "Epoch 2944: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4530e-07 - rmse: 7.3844e-04 - val_loss: 6.2050e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2945/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7940e-07 - rmse: 9.8965e-04\n",
      "Epoch 2945: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.5563e-07 - rmse: 7.4541e-04 - val_loss: 6.3029e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2946/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1680e-07 - rmse: 4.6562e-04\n",
      "Epoch 2946: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7256e-07 - rmse: 7.5668e-04 - val_loss: 6.3130e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2947/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5577e-07 - rmse: 8.0980e-04\n",
      "Epoch 2947: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6560e-07 - rmse: 7.5207e-04 - val_loss: 6.1460e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2948/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9170e-07 - rmse: 6.2586e-04\n",
      "Epoch 2948: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5702e-07 - rmse: 8.1057e-04 - val_loss: 6.0853e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2949/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6402e-07 - rmse: 6.8119e-04\n",
      "Epoch 2949: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0061e-07 - rmse: 8.3702e-04 - val_loss: 6.5380e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2950/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6062e-07 - rmse: 6.0051e-04\n",
      "Epoch 2950: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4934e-07 - rmse: 8.0581e-04 - val_loss: 5.8849e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2951/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8038e-07 - rmse: 9.3829e-04\n",
      "Epoch 2951: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9579e-07 - rmse: 8.3414e-04 - val_loss: 6.2066e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2952/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2944e-06 - rmse: 0.0011\n",
      "Epoch 2952: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8187e-07 - rmse: 7.6280e-04 - val_loss: 6.4341e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2953/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8876e-06 - rmse: 0.0014\n",
      "Epoch 2953: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1027e-07 - rmse: 8.4278e-04 - val_loss: 5.9623e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2954/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2967e-07 - rmse: 5.7417e-04\n",
      "Epoch 2954: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8885e-07 - rmse: 7.6737e-04 - val_loss: 6.3409e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2955/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2119e-06 - rmse: 0.0011\n",
      "Epoch 2955: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9311e-07 - rmse: 7.7014e-04 - val_loss: 6.3155e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2956/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0654e-08 - rmse: 3.0109e-04\n",
      "Epoch 2956: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7148e-07 - rmse: 7.5596e-04 - val_loss: 6.2426e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2957/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8047e-07 - rmse: 6.1682e-04\n",
      "Epoch 2957: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4826e-07 - rmse: 7.4045e-04 - val_loss: 6.3624e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2958/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0250e-07 - rmse: 4.5000e-04\n",
      "Epoch 2958: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8079e-07 - rmse: 7.6210e-04 - val_loss: 6.1434e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2959/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2473e-07 - rmse: 4.7405e-04\n",
      "Epoch 2959: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3012e-07 - rmse: 7.2809e-04 - val_loss: 6.1041e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2960/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6326e-07 - rmse: 8.7365e-04\n",
      "Epoch 2960: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9013e-07 - rmse: 8.3074e-04 - val_loss: 6.3873e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2961/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1755e-07 - rmse: 5.6351e-04\n",
      "Epoch 2961: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0764e-07 - rmse: 7.7951e-04 - val_loss: 5.9974e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2962/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7597e-07 - rmse: 6.1316e-04\n",
      "Epoch 2962: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9452e-07 - rmse: 7.7105e-04 - val_loss: 6.3810e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2963/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4922e-07 - rmse: 4.9922e-04\n",
      "Epoch 2963: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5545e-07 - rmse: 8.6917e-04 - val_loss: 6.1471e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2964/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2400e-07 - rmse: 6.5115e-04\n",
      "Epoch 2964: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0339e-07 - rmse: 8.3868e-04 - val_loss: 6.4244e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2965/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1293e-07 - rmse: 9.0163e-04\n",
      "Epoch 2965: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0152e-07 - rmse: 8.3757e-04 - val_loss: 6.0003e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2966/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3514e-07 - rmse: 3.6761e-04\n",
      "Epoch 2966: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2344e-07 - rmse: 7.8958e-04 - val_loss: 6.3299e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2967/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9364e-07 - rmse: 4.4004e-04\n",
      "Epoch 2967: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6433e-07 - rmse: 7.5122e-04 - val_loss: 6.0833e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2968/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8423e-07 - rmse: 9.9208e-04\n",
      "Epoch 2968: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4409e-07 - rmse: 8.0255e-04 - val_loss: 6.2361e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2969/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7754e-07 - rmse: 5.2682e-04\n",
      "Epoch 2969: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7700e-07 - rmse: 7.5961e-04 - val_loss: 6.1197e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2970/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4498e-07 - rmse: 4.9496e-04\n",
      "Epoch 2970: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.0349e-07 - rmse: 7.7685e-04 - val_loss: 6.3695e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2971/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8279e-07 - rmse: 8.2631e-04\n",
      "Epoch 2971: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3356e-07 - rmse: 7.9596e-04 - val_loss: 6.1829e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2972/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4532e-07 - rmse: 5.8764e-04\n",
      "Epoch 2972: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8339e-07 - rmse: 7.6380e-04 - val_loss: 6.2500e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2973/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2575e-07 - rmse: 6.5249e-04\n",
      "Epoch 2973: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8873e-07 - rmse: 8.2990e-04 - val_loss: 6.1573e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2974/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8295e-07 - rmse: 8.8485e-04\n",
      "Epoch 2974: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1838e-07 - rmse: 7.1999e-04 - val_loss: 6.2292e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2975/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6290e-06 - rmse: 0.0013\n",
      "Epoch 2975: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4127e-07 - rmse: 7.3571e-04 - val_loss: 6.1577e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2976/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9414e-07 - rmse: 8.9115e-04\n",
      "Epoch 2976: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9899e-07 - rmse: 8.3605e-04 - val_loss: 6.4156e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2977/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3267e-06 - rmse: 0.0012\n",
      "Epoch 2977: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0330e-07 - rmse: 8.3863e-04 - val_loss: 6.0533e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2978/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2923e-07 - rmse: 4.7878e-04\n",
      "Epoch 2978: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4982e-07 - rmse: 8.0612e-04 - val_loss: 6.4036e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2979/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2950e-07 - rmse: 4.7906e-04\n",
      "Epoch 2979: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5172e-07 - rmse: 7.4278e-04 - val_loss: 6.0293e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2980/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9273e-07 - rmse: 5.4105e-04\n",
      "Epoch 2980: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2030e-07 - rmse: 8.4870e-04 - val_loss: 6.1529e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 2981/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0822e-07 - rmse: 5.5517e-04\n",
      "Epoch 2981: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2313e-07 - rmse: 7.8938e-04 - val_loss: 6.4128e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2982/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3956e-07 - rmse: 5.8272e-04\n",
      "Epoch 2982: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8490e-07 - rmse: 8.8595e-04 - val_loss: 6.2399e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2983/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1418e-06 - rmse: 0.0011\n",
      "Epoch 2983: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8786e-07 - rmse: 8.8762e-04 - val_loss: 6.2762e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2984/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4655e-07 - rmse: 4.9654e-04\n",
      "Epoch 2984: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3928e-07 - rmse: 8.5982e-04 - val_loss: 6.3238e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2985/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2524e-07 - rmse: 7.9072e-04\n",
      "Epoch 2985: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9827e-07 - rmse: 8.3563e-04 - val_loss: 5.8740e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2986/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4150e-06 - rmse: 0.0012\n",
      "Epoch 2986: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.5225e-07 - rmse: 9.7583e-04 - val_loss: 6.2960e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2987/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2251e-06 - rmse: 0.0015\n",
      "Epoch 2987: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0050e-06 - rmse: 0.0010 - val_loss: 6.4069e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2988/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3171e-06 - rmse: 0.0015\n",
      "Epoch 2988: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0147e-06 - rmse: 0.0010 - val_loss: 5.9683e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2989/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0327e-07 - rmse: 8.9626e-04\n",
      "Epoch 2989: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.1340e-06 - rmse: 0.0011 - val_loss: 6.7797e-05 - val_rmse: 0.0082 - lr: 6.2500e-06\n",
      "Epoch 2990/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6744e-06 - rmse: 0.0013\n",
      "Epoch 2990: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.1793e-07 - rmse: 9.5809e-04 - val_loss: 5.8815e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2991/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2535e-07 - rmse: 7.9079e-04\n",
      "Epoch 2991: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.9329e-07 - rmse: 8.3264e-04 - val_loss: 6.3169e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2992/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0762e-07 - rmse: 8.4120e-04\n",
      "Epoch 2992: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5970e-07 - rmse: 9.2720e-04 - val_loss: 6.5855e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2993/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2834e-07 - rmse: 8.5343e-04\n",
      "Epoch 2993: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.0975e-07 - rmse: 9.5381e-04 - val_loss: 5.7140e-05 - val_rmse: 0.0076 - lr: 6.2500e-06\n",
      "Epoch 2994/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1474e-06 - rmse: 0.0011\n",
      "Epoch 2994: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.2497e-07 - rmse: 9.0828e-04 - val_loss: 6.6071e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 2995/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3225e-07 - rmse: 7.2956e-04\n",
      "Epoch 2995: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2974e-07 - rmse: 8.5425e-04 - val_loss: 5.9240e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2996/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4183e-07 - rmse: 9.7048e-04\n",
      "Epoch 2996: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9266e-07 - rmse: 7.6984e-04 - val_loss: 6.2543e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 2997/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6179e-07 - rmse: 4.0223e-04\n",
      "Epoch 2997: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0831e-07 - rmse: 8.4161e-04 - val_loss: 6.4189e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 2998/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2999e-07 - rmse: 5.7445e-04\n",
      "Epoch 2998: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0681e-07 - rmse: 8.4072e-04 - val_loss: 5.9330e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 2999/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1870e-06 - rmse: 0.0011\n",
      "Epoch 2999: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0612e-07 - rmse: 7.7854e-04 - val_loss: 6.6838e-05 - val_rmse: 0.0082 - lr: 6.2500e-06\n",
      "Epoch 3000/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3661e-07 - rmse: 3.6961e-04\n",
      "Epoch 3000: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2764e-07 - rmse: 9.0974e-04 - val_loss: 5.9126e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 3001/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3433e-07 - rmse: 7.3098e-04\n",
      "Epoch 3001: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0330e-07 - rmse: 8.9627e-04 - val_loss: 6.1870e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3002/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6081e-07 - rmse: 5.1070e-04\n",
      "Epoch 3002: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7256e-07 - rmse: 8.7895e-04 - val_loss: 6.4662e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 3003/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5346e-07 - rmse: 8.0837e-04\n",
      "Epoch 3003: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.8974e-07 - rmse: 8.8867e-04 - val_loss: 6.0352e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 3004/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8840e-07 - rmse: 4.3405e-04\n",
      "Epoch 3004: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2646e-07 - rmse: 7.2557e-04 - val_loss: 6.4515e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 3005/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8698e-07 - rmse: 4.3241e-04\n",
      "Epoch 3005: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7952e-07 - rmse: 8.2433e-04 - val_loss: 6.5189e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 3006/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5249e-06 - rmse: 0.0012\n",
      "Epoch 3006: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 9.9836e-07 - rmse: 9.9918e-04 - val_loss: 5.9845e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 3007/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1815e-06 - rmse: 0.0011\n",
      "Epoch 3007: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0265e-07 - rmse: 9.5008e-04 - val_loss: 6.5025e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 3008/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2931e-06 - rmse: 0.0015\n",
      "Epoch 3008: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9423e-07 - rmse: 9.4564e-04 - val_loss: 6.3808e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 3009/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3440e-07 - rmse: 4.8415e-04\n",
      "Epoch 3009: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2639e-07 - rmse: 7.9145e-04 - val_loss: 6.2033e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3010/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8164e-07 - rmse: 6.9400e-04\n",
      "Epoch 3010: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2492e-07 - rmse: 8.5142e-04 - val_loss: 6.1457e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 3011/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3312e-07 - rmse: 3.6485e-04\n",
      "Epoch 3011: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6457e-07 - rmse: 7.5138e-04 - val_loss: 6.1973e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3012/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8759e-07 - rmse: 6.2257e-04\n",
      "Epoch 3012: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3476e-07 - rmse: 7.3128e-04 - val_loss: 6.2824e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3013/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9759e-07 - rmse: 4.4451e-04\n",
      "Epoch 3013: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0688e-07 - rmse: 7.7903e-04 - val_loss: 6.1925e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3014/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7332e-07 - rmse: 6.8799e-04\n",
      "Epoch 3014: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7136e-07 - rmse: 8.7827e-04 - val_loss: 6.3037e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3015/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7855e-07 - rmse: 6.1527e-04\n",
      "Epoch 3015: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4476e-07 - rmse: 7.3808e-04 - val_loss: 6.0284e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n",
      "Epoch 3016/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0808e-07 - rmse: 6.3881e-04\n",
      "Epoch 3016: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8024e-07 - rmse: 7.6173e-04 - val_loss: 6.2417e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3017/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2110e-07 - rmse: 4.7021e-04\n",
      "Epoch 3017: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0936e-07 - rmse: 7.8062e-04 - val_loss: 6.4327e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 3018/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8301e-06 - rmse: 0.0014\n",
      "Epoch 3018: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0795e-06 - rmse: 0.0010 - val_loss: 5.8645e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 3019/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6188e-07 - rmse: 5.1174e-04\n",
      "Epoch 3019: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4720e-07 - rmse: 8.6441e-04 - val_loss: 6.3035e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3020/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9888e-07 - rmse: 7.7388e-04\n",
      "Epoch 3020: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4695e-07 - rmse: 8.6426e-04 - val_loss: 6.4694e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 3021/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9884e-07 - rmse: 8.3597e-04\n",
      "Epoch 3021: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.1229e-07 - rmse: 9.0127e-04 - val_loss: 5.9945e-05 - val_rmse: 0.0077 - lr: 6.2500e-06\n",
      "Epoch 3022/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5245e-07 - rmse: 8.0774e-04\n",
      "Epoch 3022: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0096e-06 - rmse: 0.0010 - val_loss: 6.5265e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 3023/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3524e-06 - rmse: 0.0015\n",
      "Epoch 3023: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0372e-06 - rmse: 0.0010 - val_loss: 6.3599e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 3024/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2928e-06 - rmse: 0.0015\n",
      "Epoch 3024: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.0058e-06 - rmse: 0.0010 - val_loss: 6.3483e-05 - val_rmse: 0.0080 - lr: 6.2500e-06\n",
      "Epoch 3025/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6479e-06 - rmse: 0.0013\n",
      "Epoch 3025: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7134e-07 - rmse: 8.1936e-04 - val_loss: 6.6114e-05 - val_rmse: 0.0081 - lr: 6.2500e-06\n",
      "Epoch 3026/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7090e-07 - rmse: 4.1340e-04\n",
      "Epoch 3026: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4796e-07 - rmse: 8.6485e-04 - val_loss: 6.2602e-05 - val_rmse: 0.0079 - lr: 6.2500e-06\n",
      "Epoch 3027/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2198e-06 - rmse: 0.0011\n",
      "Epoch 3027: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 3027: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6828e-07 - rmse: 8.7651e-04 - val_loss: 6.0127e-05 - val_rmse: 0.0078 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3028/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6690e-07 - rmse: 6.8330e-04\n",
      "Epoch 3028: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5941e-07 - rmse: 8.1204e-04 - val_loss: 6.4105e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3029/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8448e-07 - rmse: 9.9221e-04\n",
      "Epoch 3029: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6576e-07 - rmse: 7.5217e-04 - val_loss: 6.4377e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3030/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0390e-07 - rmse: 4.5155e-04\n",
      "Epoch 3030: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5082e-07 - rmse: 7.4217e-04 - val_loss: 6.2488e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3031/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2211e-07 - rmse: 4.7128e-04\n",
      "Epoch 3031: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2918e-07 - rmse: 7.2745e-04 - val_loss: 6.1837e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3032/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3335e-07 - rmse: 3.6517e-04\n",
      "Epoch 3032: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2334e-07 - rmse: 7.2343e-04 - val_loss: 6.2051e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3033/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7359e-07 - rmse: 5.2306e-04\n",
      "Epoch 3033: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9314e-07 - rmse: 7.0224e-04 - val_loss: 6.2360e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3034/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0305e-07 - rmse: 3.2101e-04\n",
      "Epoch 3034: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0262e-07 - rmse: 7.0895e-04 - val_loss: 6.3007e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3035/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0162e-07 - rmse: 3.1878e-04\n",
      "Epoch 3035: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0887e-07 - rmse: 7.1335e-04 - val_loss: 6.1720e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3036/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0912e-06 - rmse: 0.0010\n",
      "Epoch 3036: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9379e-07 - rmse: 7.0270e-04 - val_loss: 6.1568e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3037/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9263e-07 - rmse: 4.3890e-04\n",
      "Epoch 3037: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4171e-07 - rmse: 7.3601e-04 - val_loss: 6.2391e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3038/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3087e-07 - rmse: 9.1152e-04\n",
      "Epoch 3038: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9939e-07 - rmse: 7.0668e-04 - val_loss: 6.3079e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3039/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8097e-07 - rmse: 5.3007e-04\n",
      "Epoch 3039: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3389e-07 - rmse: 7.3068e-04 - val_loss: 6.4168e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3040/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6315e-07 - rmse: 6.0262e-04\n",
      "Epoch 3040: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2630e-07 - rmse: 7.2546e-04 - val_loss: 6.0646e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3041/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6674e-06 - rmse: 0.0013\n",
      "Epoch 3041: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3411e-07 - rmse: 7.3083e-04 - val_loss: 6.1153e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3042/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0424e-07 - rmse: 5.5158e-04\n",
      "Epoch 3042: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4040e-07 - rmse: 7.3512e-04 - val_loss: 6.3672e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3043/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6652e-06 - rmse: 0.0013\n",
      "Epoch 3043: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5744e-07 - rmse: 6.7634e-04 - val_loss: 6.1761e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3044/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9891e-07 - rmse: 4.4599e-04\n",
      "Epoch 3044: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8781e-07 - rmse: 6.9844e-04 - val_loss: 6.1904e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3045/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1141e-07 - rmse: 7.8193e-04\n",
      "Epoch 3045: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2792e-07 - rmse: 7.2658e-04 - val_loss: 6.3172e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3046/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3038e-08 - rmse: 2.0746e-04\n",
      "Epoch 3046: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2841e-07 - rmse: 7.2692e-04 - val_loss: 6.1293e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3047/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4916e-07 - rmse: 3.8621e-04\n",
      "Epoch 3047: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8890e-07 - rmse: 6.9921e-04 - val_loss: 6.2010e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3048/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5955e-07 - rmse: 8.1213e-04\n",
      "Epoch 3048: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0593e-07 - rmse: 7.1129e-04 - val_loss: 6.2455e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3049/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4418e-07 - rmse: 4.9415e-04\n",
      "Epoch 3049: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9365e-07 - rmse: 7.0260e-04 - val_loss: 6.1477e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3050/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0865e-06 - rmse: 0.0010\n",
      "Epoch 3050: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.0607e-07 - rmse: 7.1139e-04 - val_loss: 6.1971e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3051/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6231e-07 - rmse: 7.4987e-04\n",
      "Epoch 3051: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2586e-07 - rmse: 7.2516e-04 - val_loss: 6.3794e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3052/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4642e-07 - rmse: 8.0400e-04\n",
      "Epoch 3052: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0275e-07 - rmse: 7.0905e-04 - val_loss: 6.0881e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3053/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9685e-07 - rmse: 4.4368e-04\n",
      "Epoch 3053: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9350e-07 - rmse: 7.0249e-04 - val_loss: 6.1028e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3054/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3600e-07 - rmse: 5.7966e-04\n",
      "Epoch 3054: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1790e-07 - rmse: 7.1965e-04 - val_loss: 6.2428e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3055/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8505e-07 - rmse: 4.3017e-04\n",
      "Epoch 3055: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5662e-07 - rmse: 7.4607e-04 - val_loss: 6.2982e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3056/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1875e-06 - rmse: 0.0011\n",
      "Epoch 3056: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2076e-07 - rmse: 7.8788e-04 - val_loss: 6.0498e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3057/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4926e-07 - rmse: 8.6560e-04\n",
      "Epoch 3057: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2967e-07 - rmse: 7.9352e-04 - val_loss: 6.2799e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3058/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2669e-07 - rmse: 5.7157e-04\n",
      "Epoch 3058: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3825e-07 - rmse: 7.3365e-04 - val_loss: 6.2468e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3059/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4927e-07 - rmse: 4.9927e-04\n",
      "Epoch 3059: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2042e-07 - rmse: 7.2140e-04 - val_loss: 6.3028e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3060/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4306e-07 - rmse: 3.7823e-04\n",
      "Epoch 3060: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3183e-07 - rmse: 7.2927e-04 - val_loss: 6.0645e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3061/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6793e-06 - rmse: 0.0013\n",
      "Epoch 3061: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3428e-07 - rmse: 7.3095e-04 - val_loss: 6.2248e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3062/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5758e-07 - rmse: 3.9697e-04\n",
      "Epoch 3062: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9507e-07 - rmse: 7.0361e-04 - val_loss: 6.2170e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3063/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2777e-06 - rmse: 0.0011\n",
      "Epoch 3063: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0889e-07 - rmse: 7.8032e-04 - val_loss: 6.0418e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3064/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5120e-07 - rmse: 5.9262e-04\n",
      "Epoch 3064: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5844e-07 - rmse: 7.4729e-04 - val_loss: 6.3515e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3065/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2505e-06 - rmse: 0.0011\n",
      "Epoch 3065: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7576e-07 - rmse: 7.5879e-04 - val_loss: 6.1801e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3066/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6451e-06 - rmse: 0.0013\n",
      "Epoch 3066: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0435e-07 - rmse: 7.1018e-04 - val_loss: 6.2093e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3067/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1980e-07 - rmse: 4.6883e-04\n",
      "Epoch 3067: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8525e-07 - rmse: 6.9660e-04 - val_loss: 6.2147e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3068/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8955e-07 - rmse: 4.3537e-04\n",
      "Epoch 3068: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2813e-07 - rmse: 7.2672e-04 - val_loss: 6.1465e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3069/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6534e-07 - rmse: 5.1511e-04\n",
      "Epoch 3069: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5661e-07 - rmse: 8.1032e-04 - val_loss: 6.2818e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3070/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9497e-07 - rmse: 8.3365e-04\n",
      "Epoch 3070: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6862e-07 - rmse: 7.5407e-04 - val_loss: 6.1787e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3071/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9563e-08 - rmse: 2.8207e-04\n",
      "Epoch 3071: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3130e-07 - rmse: 7.2890e-04 - val_loss: 6.1374e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3072/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0059e-07 - rmse: 3.1716e-04\n",
      "Epoch 3072: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2453e-07 - rmse: 7.2425e-04 - val_loss: 6.1994e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3073/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6065e-07 - rmse: 4.0082e-04\n",
      "Epoch 3073: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0437e-07 - rmse: 7.1019e-04 - val_loss: 6.2299e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3074/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9066e-07 - rmse: 4.3664e-04\n",
      "Epoch 3074: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8322e-07 - rmse: 6.9514e-04 - val_loss: 6.1882e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3075/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1218e-06 - rmse: 0.0011\n",
      "Epoch 3075: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1761e-07 - rmse: 7.1945e-04 - val_loss: 6.2053e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3076/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4496e-07 - rmse: 3.8074e-04\n",
      "Epoch 3076: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5888e-07 - rmse: 7.4759e-04 - val_loss: 6.4031e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3077/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4319e-06 - rmse: 0.0012\n",
      "Epoch 3077: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5431e-07 - rmse: 8.0889e-04 - val_loss: 6.0785e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3078/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6760e-06 - rmse: 0.0013\n",
      "Epoch 3078: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4401e-07 - rmse: 7.3757e-04 - val_loss: 6.2171e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3079/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5409e-07 - rmse: 5.0408e-04\n",
      "Epoch 3079: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.3258e-07 - rmse: 7.2978e-04 - val_loss: 6.2463e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3080/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7462e-07 - rmse: 4.1788e-04\n",
      "Epoch 3080: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7684e-07 - rmse: 6.9054e-04 - val_loss: 6.2586e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3081/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4856e-07 - rmse: 4.9856e-04\n",
      "Epoch 3081: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6933e-07 - rmse: 7.5454e-04 - val_loss: 6.2141e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3082/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1387e-07 - rmse: 3.3745e-04\n",
      "Epoch 3082: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8200e-07 - rmse: 6.9426e-04 - val_loss: 6.2849e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3083/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4951e-07 - rmse: 3.8666e-04\n",
      "Epoch 3083: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1303e-07 - rmse: 7.1626e-04 - val_loss: 6.0613e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3084/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3285e-07 - rmse: 9.6584e-04\n",
      "Epoch 3084: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2520e-07 - rmse: 7.2471e-04 - val_loss: 6.2141e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3085/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8970e-07 - rmse: 8.3048e-04\n",
      "Epoch 3085: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4325e-07 - rmse: 7.3705e-04 - val_loss: 6.4151e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3086/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0737e-07 - rmse: 4.5538e-04\n",
      "Epoch 3086: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9679e-07 - rmse: 7.0483e-04 - val_loss: 6.1480e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3087/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2278e-07 - rmse: 4.7199e-04\n",
      "Epoch 3087: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4448e-07 - rmse: 6.6670e-04 - val_loss: 6.1137e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3088/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4560e-06 - rmse: 0.0012\n",
      "Epoch 3088: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7709e-07 - rmse: 6.9072e-04 - val_loss: 6.2256e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3089/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8209e-07 - rmse: 4.2672e-04\n",
      "Epoch 3089: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6131e-07 - rmse: 6.7920e-04 - val_loss: 6.2651e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3090/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2135e-07 - rmse: 3.4835e-04\n",
      "Epoch 3090: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8122e-07 - rmse: 6.9370e-04 - val_loss: 6.1329e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3091/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2238e-07 - rmse: 4.7157e-04\n",
      "Epoch 3091: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6454e-07 - rmse: 6.8157e-04 - val_loss: 6.2666e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3092/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9575e-07 - rmse: 5.4383e-04\n",
      "Epoch 3092: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9742e-07 - rmse: 7.0528e-04 - val_loss: 6.2152e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3093/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3448e-06 - rmse: 0.0012\n",
      "Epoch 3093: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3710e-07 - rmse: 7.3287e-04 - val_loss: 6.0488e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3094/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4751e-07 - rmse: 3.8407e-04\n",
      "Epoch 3094: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1732e-07 - rmse: 7.1925e-04 - val_loss: 6.3323e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3095/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6045e-07 - rmse: 5.1035e-04\n",
      "Epoch 3095: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0673e-07 - rmse: 7.1185e-04 - val_loss: 6.2634e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3096/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5515e-06 - rmse: 0.0012\n",
      "Epoch 3096: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5052e-07 - rmse: 6.7121e-04 - val_loss: 6.0463e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3097/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1000e-07 - rmse: 9.5394e-04\n",
      "Epoch 3097: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0871e-07 - rmse: 7.1324e-04 - val_loss: 6.1771e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3098/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2939e-07 - rmse: 4.7894e-04\n",
      "Epoch 3098: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.0894e-07 - rmse: 7.1340e-04 - val_loss: 6.1735e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3099/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2150e-07 - rmse: 4.7064e-04\n",
      "Epoch 3099: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3252e-07 - rmse: 7.2974e-04 - val_loss: 6.3050e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3100/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0157e-06 - rmse: 0.0010\n",
      "Epoch 3100: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4502e-07 - rmse: 7.3825e-04 - val_loss: 6.0925e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3101/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4164e-07 - rmse: 4.9157e-04\n",
      "Epoch 3101: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3205e-07 - rmse: 7.9501e-04 - val_loss: 6.2395e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3102/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5082e-06 - rmse: 0.0012\n",
      "Epoch 3102: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5394e-07 - rmse: 6.7375e-04 - val_loss: 6.3430e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3103/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9834e-07 - rmse: 4.4535e-04\n",
      "Epoch 3103: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1509e-07 - rmse: 8.4563e-04 - val_loss: 6.1606e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3104/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8718e-08 - rmse: 2.8057e-04\n",
      "Epoch 3104: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3394e-07 - rmse: 7.9620e-04 - val_loss: 6.1183e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3105/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9661e-07 - rmse: 4.4341e-04\n",
      "Epoch 3105: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4431e-07 - rmse: 7.3778e-04 - val_loss: 6.2610e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3106/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4311e-07 - rmse: 3.7830e-04\n",
      "Epoch 3106: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3860e-07 - rmse: 7.3389e-04 - val_loss: 6.2910e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3107/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9602e-07 - rmse: 4.4275e-04\n",
      "Epoch 3107: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8552e-07 - rmse: 6.9679e-04 - val_loss: 6.1201e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3108/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0984e-06 - rmse: 0.0010\n",
      "Epoch 3108: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0692e-07 - rmse: 7.1198e-04 - val_loss: 6.1353e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3109/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4667e-07 - rmse: 3.8298e-04\n",
      "Epoch 3109: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.9393e-07 - rmse: 7.0280e-04 - val_loss: 6.3858e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3110/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4111e-07 - rmse: 3.7565e-04\n",
      "Epoch 3110: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.0187e-07 - rmse: 7.0843e-04 - val_loss: 6.1649e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3111/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7355e-07 - rmse: 4.1659e-04\n",
      "Epoch 3111: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7710e-07 - rmse: 6.9073e-04 - val_loss: 6.1681e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3112/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8899e-07 - rmse: 4.3473e-04\n",
      "Epoch 3112: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0481e-07 - rmse: 7.1050e-04 - val_loss: 6.2882e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3113/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9863e-07 - rmse: 6.3137e-04\n",
      "Epoch 3113: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8503e-07 - rmse: 6.9644e-04 - val_loss: 6.2238e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3114/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3446e-07 - rmse: 4.8421e-04\n",
      "Epoch 3114: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0176e-07 - rmse: 7.0835e-04 - val_loss: 6.0523e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3115/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0041e-06 - rmse: 0.0010\n",
      "Epoch 3115: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9513e-07 - rmse: 7.7145e-04 - val_loss: 6.1105e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3116/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0131e-07 - rmse: 5.4892e-04\n",
      "Epoch 3116: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7830e-07 - rmse: 7.6046e-04 - val_loss: 6.4094e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3117/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2212e-06 - rmse: 0.0011\n",
      "Epoch 3117: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8772e-07 - rmse: 7.6663e-04 - val_loss: 6.2108e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3118/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8215e-07 - rmse: 6.9437e-04\n",
      "Epoch 3118: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0257e-07 - rmse: 7.0892e-04 - val_loss: 6.2741e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3119/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6575e-07 - rmse: 8.1593e-04\n",
      "Epoch 3119: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5265e-07 - rmse: 8.0787e-04 - val_loss: 6.1476e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3120/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5390e-07 - rmse: 5.9489e-04\n",
      "Epoch 3120: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3220e-07 - rmse: 7.9511e-04 - val_loss: 6.2832e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3121/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2201e-07 - rmse: 4.7118e-04\n",
      "Epoch 3121: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0931e-07 - rmse: 7.1366e-04 - val_loss: 6.2183e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3122/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9971e-08 - rmse: 2.4489e-04\n",
      "Epoch 3122: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1222e-07 - rmse: 7.1569e-04 - val_loss: 6.1146e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3123/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0602e-06 - rmse: 0.0010\n",
      "Epoch 3123: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7283e-07 - rmse: 6.8762e-04 - val_loss: 6.2800e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3124/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3237e-08 - rmse: 2.8851e-04\n",
      "Epoch 3124: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7312e-07 - rmse: 6.8783e-04 - val_loss: 6.3813e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3125/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8016e-07 - rmse: 4.2445e-04\n",
      "Epoch 3125: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6551e-07 - rmse: 6.8228e-04 - val_loss: 6.1699e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3126/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3250e-07 - rmse: 8.5586e-04\n",
      "Epoch 3126: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7684e-07 - rmse: 6.9053e-04 - val_loss: 6.1137e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3127/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7672e-07 - rmse: 4.2038e-04\n",
      "Epoch 3127: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0725e-07 - rmse: 7.1222e-04 - val_loss: 6.2266e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3128/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0825e-07 - rmse: 4.5635e-04\n",
      "Epoch 3128: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0769e-07 - rmse: 7.1252e-04 - val_loss: 6.2222e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3129/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6304e-07 - rmse: 4.0378e-04\n",
      "Epoch 3129: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1355e-07 - rmse: 7.1662e-04 - val_loss: 6.2459e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3130/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0357e-07 - rmse: 4.5119e-04\n",
      "Epoch 3130: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6646e-07 - rmse: 6.8298e-04 - val_loss: 6.2158e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3131/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2679e-07 - rmse: 4.7622e-04\n",
      "Epoch 3131: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9542e-07 - rmse: 7.0386e-04 - val_loss: 6.2011e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3132/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5910e-07 - rmse: 5.0902e-04\n",
      "Epoch 3132: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2412e-07 - rmse: 7.2396e-04 - val_loss: 6.1906e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3133/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4356e-07 - rmse: 3.7890e-04\n",
      "Epoch 3133: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.3941e-07 - rmse: 6.6288e-04 - val_loss: 6.2472e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3134/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0616e-06 - rmse: 0.0010\n",
      "Epoch 3134: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7203e-07 - rmse: 6.8704e-04 - val_loss: 6.1522e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3135/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8116e-07 - rmse: 7.6234e-04\n",
      "Epoch 3135: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8708e-07 - rmse: 6.9791e-04 - val_loss: 6.3556e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3136/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6359e-07 - rmse: 4.0447e-04\n",
      "Epoch 3136: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3832e-07 - rmse: 7.3370e-04 - val_loss: 6.0514e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3137/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8473e-07 - rmse: 5.3360e-04\n",
      "Epoch 3137: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8313e-07 - rmse: 7.6363e-04 - val_loss: 6.2515e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3138/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6808e-06 - rmse: 0.0013\n",
      "Epoch 3138: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5767e-07 - rmse: 7.4677e-04 - val_loss: 6.2226e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3139/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3210e-07 - rmse: 6.5734e-04\n",
      "Epoch 3139: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1543e-07 - rmse: 7.8449e-04 - val_loss: 6.3611e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3140/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4713e-07 - rmse: 3.8357e-04\n",
      "Epoch 3140: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4392e-07 - rmse: 7.3751e-04 - val_loss: 6.0329e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3141/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0540e-07 - rmse: 4.5321e-04\n",
      "Epoch 3141: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6938e-07 - rmse: 6.8511e-04 - val_loss: 6.2436e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3142/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6048e-07 - rmse: 4.0060e-04\n",
      "Epoch 3142: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.9840e-07 - rmse: 7.0597e-04 - val_loss: 6.2987e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3143/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2242e-07 - rmse: 4.7162e-04\n",
      "Epoch 3143: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.9697e-07 - rmse: 7.0496e-04 - val_loss: 6.2661e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3144/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8164e-07 - rmse: 6.1777e-04\n",
      "Epoch 3144: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.1659e-07 - rmse: 7.1875e-04 - val_loss: 6.1434e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3145/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8989e-08 - rmse: 2.9831e-04\n",
      "Epoch 3145: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1830e-07 - rmse: 7.1993e-04 - val_loss: 6.2403e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3146/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5005e-07 - rmse: 3.8737e-04\n",
      "Epoch 3146: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0995e-07 - rmse: 7.1411e-04 - val_loss: 6.1329e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3147/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4341e-07 - rmse: 3.7870e-04\n",
      "Epoch 3147: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0914e-07 - rmse: 7.1354e-04 - val_loss: 6.2812e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3148/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0915e-07 - rmse: 4.5733e-04\n",
      "Epoch 3148: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6172e-07 - rmse: 7.4948e-04 - val_loss: 6.2436e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3149/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3855e-07 - rmse: 6.6223e-04\n",
      "Epoch 3149: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2940e-07 - rmse: 7.2760e-04 - val_loss: 6.2014e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3150/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6922e-06 - rmse: 0.0013\n",
      "Epoch 3150: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3360e-07 - rmse: 7.3048e-04 - val_loss: 6.2402e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3151/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6027e-07 - rmse: 6.0022e-04\n",
      "Epoch 3151: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.3509e-07 - rmse: 7.3150e-04 - val_loss: 6.2431e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3152/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3519e-07 - rmse: 3.6769e-04\n",
      "Epoch 3152: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7082e-07 - rmse: 6.8616e-04 - val_loss: 6.0869e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3153/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1672e-07 - rmse: 9.5745e-04\n",
      "Epoch 3153: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0102e-07 - rmse: 8.3727e-04 - val_loss: 6.2001e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3154/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8092e-07 - rmse: 5.3002e-04\n",
      "Epoch 3154: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.0462e-07 - rmse: 7.7757e-04 - val_loss: 6.5364e-05 - val_rmse: 0.0081 - lr: 3.1250e-06\n",
      "Epoch 3155/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8025e-07 - rmse: 9.9008e-04\n",
      "Epoch 3155: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5248e-07 - rmse: 8.0776e-04 - val_loss: 6.0610e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3156/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8115e-07 - rmse: 4.2562e-04\n",
      "Epoch 3156: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5983e-07 - rmse: 7.4822e-04 - val_loss: 6.0270e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3157/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3252e-07 - rmse: 4.8220e-04\n",
      "Epoch 3157: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6532e-07 - rmse: 6.8214e-04 - val_loss: 6.2545e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3158/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6636e-08 - rmse: 2.5814e-04\n",
      "Epoch 3158: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4943e-07 - rmse: 7.4123e-04 - val_loss: 6.4496e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3159/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4685e-06 - rmse: 0.0012\n",
      "Epoch 3159: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7277e-07 - rmse: 8.2023e-04 - val_loss: 5.9543e-05 - val_rmse: 0.0077 - lr: 3.1250e-06\n",
      "Epoch 3160/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0015e-07 - rmse: 5.4786e-04\n",
      "Epoch 3160: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9301e-07 - rmse: 7.7007e-04 - val_loss: 6.3424e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3161/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3450e-07 - rmse: 3.6674e-04\n",
      "Epoch 3161: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6432e-07 - rmse: 7.5121e-04 - val_loss: 6.3310e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3162/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0811e-07 - rmse: 4.5619e-04\n",
      "Epoch 3162: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1212e-07 - rmse: 7.8238e-04 - val_loss: 6.1101e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3163/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0545e-07 - rmse: 8.9747e-04\n",
      "Epoch 3163: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6209e-07 - rmse: 8.1369e-04 - val_loss: 6.2564e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3164/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2109e-07 - rmse: 7.2186e-04\n",
      "Epoch 3164: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0599e-07 - rmse: 7.1133e-04 - val_loss: 6.1258e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3165/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0466e-07 - rmse: 7.7760e-04\n",
      "Epoch 3165: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2515e-07 - rmse: 7.2467e-04 - val_loss: 6.2663e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3166/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6879e-07 - rmse: 5.1845e-04\n",
      "Epoch 3166: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0820e-07 - rmse: 8.9900e-04 - val_loss: 6.1769e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3167/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8770e-07 - rmse: 6.9836e-04\n",
      "Epoch 3167: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9955e-07 - rmse: 7.7430e-04 - val_loss: 6.3290e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3168/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9554e-07 - rmse: 4.4219e-04\n",
      "Epoch 3168: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6191e-07 - rmse: 7.4961e-04 - val_loss: 6.0823e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3169/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4754e-06 - rmse: 0.0012\n",
      "Epoch 3169: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5178e-07 - rmse: 8.0733e-04 - val_loss: 5.9881e-05 - val_rmse: 0.0077 - lr: 3.1250e-06\n",
      "Epoch 3170/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7307e-07 - rmse: 6.8780e-04\n",
      "Epoch 3170: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1450e-07 - rmse: 7.8390e-04 - val_loss: 6.3938e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3171/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1091e-07 - rmse: 5.5759e-04\n",
      "Epoch 3171: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4217e-07 - rmse: 7.3632e-04 - val_loss: 6.0728e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3172/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9690e-07 - rmse: 5.4488e-04\n",
      "Epoch 3172: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4487e-07 - rmse: 7.3815e-04 - val_loss: 6.2040e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3173/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9165e-07 - rmse: 9.4427e-04\n",
      "Epoch 3173: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4671e-07 - rmse: 7.3940e-04 - val_loss: 6.1229e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3174/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9645e-07 - rmse: 8.9244e-04\n",
      "Epoch 3174: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6215e-07 - rmse: 8.1373e-04 - val_loss: 6.4687e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3175/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2180e-06 - rmse: 0.0011\n",
      "Epoch 3175: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2399e-07 - rmse: 7.8993e-04 - val_loss: 6.0582e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3176/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3633e-07 - rmse: 8.5810e-04\n",
      "Epoch 3176: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5643e-07 - rmse: 7.4594e-04 - val_loss: 6.2882e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3177/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4068e-06 - rmse: 0.0012\n",
      "Epoch 3177: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8188e-07 - rmse: 6.9418e-04 - val_loss: 6.2321e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3178/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3133e-06 - rmse: 0.0011\n",
      "Epoch 3178: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3817e-07 - rmse: 8.5917e-04 - val_loss: 6.1843e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3179/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3856e-07 - rmse: 4.8843e-04\n",
      "Epoch 3179: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2696e-07 - rmse: 7.9181e-04 - val_loss: 6.2998e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3180/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5072e-07 - rmse: 6.7136e-04\n",
      "Epoch 3180: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2650e-07 - rmse: 7.9152e-04 - val_loss: 6.2036e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3181/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4698e-06 - rmse: 0.0012\n",
      "Epoch 3181: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.1488e-07 - rmse: 7.8414e-04 - val_loss: 6.0532e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3182/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4850e-07 - rmse: 4.9849e-04\n",
      "Epoch 3182: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0641e-07 - rmse: 7.1162e-04 - val_loss: 6.2791e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3183/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2973e-08 - rmse: 2.8805e-04\n",
      "Epoch 3183: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5940e-07 - rmse: 6.7779e-04 - val_loss: 6.1705e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3184/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0968e-07 - rmse: 4.5790e-04\n",
      "Epoch 3184: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4562e-07 - rmse: 7.3866e-04 - val_loss: 6.2684e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3185/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8282e-07 - rmse: 6.1873e-04\n",
      "Epoch 3185: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8630e-07 - rmse: 8.2843e-04 - val_loss: 6.1989e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3186/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2891e-06 - rmse: 0.0011\n",
      "Epoch 3186: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2671e-07 - rmse: 9.0923e-04 - val_loss: 5.9947e-05 - val_rmse: 0.0077 - lr: 3.1250e-06\n",
      "Epoch 3187/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2201e-07 - rmse: 6.4962e-04\n",
      "Epoch 3187: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.5464e-07 - rmse: 9.2447e-04 - val_loss: 6.5066e-05 - val_rmse: 0.0081 - lr: 3.1250e-06\n",
      "Epoch 3188/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0402e-06 - rmse: 0.0010\n",
      "Epoch 3188: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6990e-07 - rmse: 8.1848e-04 - val_loss: 6.3821e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3189/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1695e-07 - rmse: 5.6299e-04\n",
      "Epoch 3189: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.2517e-07 - rmse: 8.5157e-04 - val_loss: 6.4808e-05 - val_rmse: 0.0081 - lr: 3.1250e-06\n",
      "Epoch 3190/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9656e-07 - rmse: 8.9250e-04\n",
      "Epoch 3190: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.4287e-07 - rmse: 8.0179e-04 - val_loss: 5.9676e-05 - val_rmse: 0.0077 - lr: 3.1250e-06\n",
      "Epoch 3191/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2896e-06 - rmse: 0.0011\n",
      "Epoch 3191: val_loss did not improve from 0.00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0038e-07 - rmse: 8.9464e-04 - val_loss: 6.1132e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3192/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5112e-07 - rmse: 9.2256e-04\n",
      "Epoch 3192: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.8504e-07 - rmse: 6.9645e-04 - val_loss: 6.5389e-05 - val_rmse: 0.0081 - lr: 3.1250e-06\n",
      "Epoch 3193/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1602e-07 - rmse: 3.4061e-04\n",
      "Epoch 3193: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5060e-07 - rmse: 8.0660e-04 - val_loss: 6.2032e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3194/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6246e-07 - rmse: 4.0306e-04\n",
      "Epoch 3194: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5753e-07 - rmse: 7.4668e-04 - val_loss: 6.1984e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3195/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0214e-07 - rmse: 5.4967e-04\n",
      "Epoch 3195: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5208e-07 - rmse: 8.0752e-04 - val_loss: 6.2817e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3196/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7833e-07 - rmse: 7.6048e-04\n",
      "Epoch 3196: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.7787e-07 - rmse: 6.9128e-04 - val_loss: 6.2640e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3197/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4340e-07 - rmse: 5.8601e-04\n",
      "Epoch 3197: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.1538e-07 - rmse: 7.8446e-04 - val_loss: 6.1545e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3198/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0532e-08 - rmse: 2.8378e-04\n",
      "Epoch 3198: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.2630e-07 - rmse: 7.2546e-04 - val_loss: 6.2571e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3199/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6744e-07 - rmse: 4.0919e-04\n",
      "Epoch 3199: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6244e-07 - rmse: 6.8003e-04 - val_loss: 6.1435e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3200/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3289e-07 - rmse: 3.6454e-04\n",
      "Epoch 3200: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.0113e-07 - rmse: 7.0791e-04 - val_loss: 6.2709e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3201/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0383e-07 - rmse: 9.5070e-04\n",
      "Epoch 3201: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1803e-07 - rmse: 7.1974e-04 - val_loss: 6.1076e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3202/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9909e-07 - rmse: 4.4619e-04\n",
      "Epoch 3202: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7483e-07 - rmse: 7.5818e-04 - val_loss: 6.3363e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3203/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6818e-07 - rmse: 9.3176e-04\n",
      "Epoch 3203: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7321e-07 - rmse: 8.7932e-04 - val_loss: 6.3696e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3204/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9047e-07 - rmse: 6.2487e-04\n",
      "Epoch 3204: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.4972e-07 - rmse: 6.7061e-04 - val_loss: 5.9561e-05 - val_rmse: 0.0077 - lr: 3.1250e-06\n",
      "Epoch 3205/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5606e-07 - rmse: 3.9504e-04\n",
      "Epoch 3205: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7929e-07 - rmse: 7.6111e-04 - val_loss: 6.1957e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3206/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3898e-07 - rmse: 9.6901e-04\n",
      "Epoch 3206: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.6731e-07 - rmse: 7.5320e-04 - val_loss: 6.3009e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3207/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8265e-07 - rmse: 5.3165e-04\n",
      "Epoch 3207: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2214e-07 - rmse: 7.8876e-04 - val_loss: 6.3120e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3208/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0903e-06 - rmse: 0.0010\n",
      "Epoch 3208: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0654e-07 - rmse: 8.4056e-04 - val_loss: 6.0733e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3209/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4535e-07 - rmse: 4.9533e-04\n",
      "Epoch 3209: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.1102e-07 - rmse: 8.4322e-04 - val_loss: 6.1839e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3210/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4427e-07 - rmse: 5.8674e-04\n",
      "Epoch 3210: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6377e-07 - rmse: 8.1472e-04 - val_loss: 6.3094e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3211/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0847e-06 - rmse: 0.0010\n",
      "Epoch 3211: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.4731e-07 - rmse: 7.3980e-04 - val_loss: 6.3121e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3212/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6719e-06 - rmse: 0.0013\n",
      "Epoch 3212: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.2494e-07 - rmse: 7.9053e-04 - val_loss: 6.2086e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3213/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7777e-07 - rmse: 5.2704e-04\n",
      "Epoch 3213: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.8611e-07 - rmse: 8.2832e-04 - val_loss: 6.3416e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3214/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6923e-07 - rmse: 5.1887e-04\n",
      "Epoch 3214: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3767e-07 - rmse: 7.9854e-04 - val_loss: 6.0788e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3215/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0759e-07 - rmse: 7.1245e-04\n",
      "Epoch 3215: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.9440e-07 - rmse: 7.7097e-04 - val_loss: 6.3755e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3216/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8480e-07 - rmse: 8.8589e-04\n",
      "Epoch 3216: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.5747e-07 - rmse: 8.1084e-04 - val_loss: 6.3477e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3217/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8755e-08 - rmse: 2.9792e-04\n",
      "Epoch 3217: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.3613e-07 - rmse: 7.9757e-04 - val_loss: 6.0097e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3218/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4821e-06 - rmse: 0.0012\n",
      "Epoch 3218: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.6689e-07 - rmse: 8.1663e-04 - val_loss: 6.2242e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3219/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6908e-07 - rmse: 5.1872e-04\n",
      "Epoch 3219: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1482e-07 - rmse: 7.1751e-04 - val_loss: 6.2909e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3220/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6408e-07 - rmse: 7.5105e-04\n",
      "Epoch 3220: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.6546e-07 - rmse: 6.8225e-04 - val_loss: 6.1874e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3221/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3004e-07 - rmse: 3.6061e-04\n",
      "Epoch 3221: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.8373e-07 - rmse: 7.6402e-04 - val_loss: 6.1403e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3222/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1428e-07 - rmse: 3.3806e-04\n",
      "Epoch 3222: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.5636e-07 - rmse: 7.4590e-04 - val_loss: 6.4143e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3223/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3414e-07 - rmse: 4.8388e-04\n",
      "Epoch 3223: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1282e-07 - rmse: 7.1611e-04 - val_loss: 6.2126e-05 - val_rmse: 0.0079 - lr: 3.1250e-06\n",
      "Epoch 3224/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6205e-08 - rmse: 2.3708e-04\n",
      "Epoch 3224: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7741e-07 - rmse: 6.9095e-04 - val_loss: 6.1458e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3225/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5945e-08 - rmse: 2.3653e-04\n",
      "Epoch 3225: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.2870e-07 - rmse: 7.2712e-04 - val_loss: 6.3454e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3226/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6399e-07 - rmse: 4.0496e-04\n",
      "Epoch 3226: val_loss did not improve from 0.00004\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.7394e-07 - rmse: 7.5759e-04 - val_loss: 6.0823e-05 - val_rmse: 0.0078 - lr: 3.1250e-06\n",
      "Epoch 3227/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0689e-07 - rmse: 5.5398e-04Restoring model weights from the end of the best epoch: 2227.\n",
      "\n",
      "Epoch 3227: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 3227: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.5794e-07 - rmse: 7.4695e-04 - val_loss: 6.4088e-05 - val_rmse: 0.0080 - lr: 3.1250e-06\n",
      "Epoch 3227: early stopping\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "history = model.fit([x_train, x_para_train], y_train, batch_size=batch_size,\n",
    "                    validation_data=([x_val, x_para_val], y_val),\n",
    "                    steps_per_epoch = STEP_SIZE_TRAIN, validation_steps=VALIDATION_STEPS,\n",
    "                    epochs=10000, shuffle=True, callbacks=[es, ckpt, rp])\n",
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79204ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:03:43.163144\n"
     ]
    }
   ],
   "source": [
    "time = end - start\n",
    "print(\"Training time:\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fbce220",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = \"D:\\\\VAWT_data\\\\flap_steady\\\\flap_steady\\\\result\\\\\"+\"20221205eroCNN1_optimalSettings\\\\test\"+str(test_rate)+'_'+str(n_kernels)+\"kernels_Clonly\"\n",
    "if not os.path.exists(storage_dir):\n",
    "    os.makedirs(storage_dir)\n",
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77cc60d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = history.history\n",
    "plt.plot(hist['loss'], lw=2, label='Training loss')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation loss')\n",
    "plt.title('Training loss (mean squared error)\\nAeroCNN-I, optimal settings, $C_l$ only', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "#plt.ylim([5e-5, 1e-1])\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"TrainingLoss_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e42b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2)\n",
    "plt.plot(hist['val_rmse'], lw=2)\n",
    "plt.title('Root Mean Squared Error, optimal settings, $C_l$ only', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "983b76cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2, label='Training RMSE')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation RMSE')\n",
    "plt.title('Root Mean Squared Error\\nAeroCNN-I, optimal settings, $C_l$ only', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"RMSE_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c76ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 2.2584e-05 - rmse: 0.0048\n"
     ]
    }
   ],
   "source": [
    "train_results = model.evaluate([x_train, x_para_train], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abc70d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step - loss: 3.8560e-05 - rmse: 0.0062\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate([x_test, x_para_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "745feda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "decoded_train_ = model.predict([x_train, x_para_train])\n",
    "decoded_val_ = model.predict([x_val, x_para_val])\n",
    "decoded_test_ = model.predict([x_test, x_para_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a50468a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = np.unique(np.where(np.isin(cl, y_train)))\n",
    "index_val = np.unique(np.where(np.isin(cl, y_val)))\n",
    "index_test = np.unique(np.where(np.isin(cl, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5d3a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "y_val = y_val*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "y_test = y_test*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "494df8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_train = decoded_train_*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "decoded_val = decoded_val_*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "decoded_test = decoded_test_*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55d01da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221205\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "os.chdir(model_directory)\n",
    "model_name = \"20221205steadyValidation_AeroCNN1_val_\"+str(val_rate)+\"_test\"+str(test_rate)+ \"_\" + str(n_kernels) +\"kernels_optimalSettings_Clonly.h5\"\n",
    "model.save(model_name, overwrite=True, include_optimizer=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c38a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train_abs = np.abs(decoded_train - y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3ff0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_val_abs = np.abs(decoded_val - y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c0dfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test_abs = np.abs(decoded_test - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e21d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31fb219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007261779318405373\n"
     ]
    }
   ],
   "source": [
    "l2_error_train = np.sqrt(np.sum((decoded_train - y_train)**2) / np.sum(y_train**2))\n",
    "print(l2_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69d47034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010093802883172253\n"
     ]
    }
   ],
   "source": [
    "l2_error_val = np.sqrt(np.sum((decoded_val - y_val)**2) / np.sum(y_val**2))\n",
    "print(l2_error_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3770434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009288958101235334\n"
     ]
    }
   ],
   "source": [
    "l2_error_test = np.sqrt(np.sum((decoded_test - y_test)**2) / np.sum(y_test**2))\n",
    "print(l2_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9785cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_train_list = []\n",
    "for i in range(0, len(x_train)):\n",
    "    l2_error_train_data = np.sqrt(np.sum((decoded_train[i] - y_train[i])**2) / np.sum(y_train[i]**2))\n",
    "    l2_error_train_list.append(l2_error_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab588246",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_error_val_list = []\n",
    "for i in range(0, len(x_val)):\n",
    "    l2_error_val_data = np.sqrt(np.sum((decoded_val[i] - y_val[i])**2) / np.sum((y_val[i]+1e-07)**2))\n",
    "    l2_error_val_list.append(l2_error_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72b6a696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_test_list = []\n",
    "for i in range(0, len(x_test)):\n",
    "    l2_error_test_data = np.sqrt(np.sum((decoded_test[i] - y_test[i])**2) / np.sum(y_test[i]**2))\n",
    "    l2_error_test_list.append(l2_error_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd795141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1,x_train.shape[0],x_train.shape[0]),\n",
    "         l2_error_train*np.ones(x_train.shape[0],), 'k', lw=2.5)\n",
    "plt.scatter(np.linspace(1, x_train.shape[0], x_train.shape[0]), l2_error_train_list, c='b')\n",
    "plt.xlabel('Index', fontsize=15)\n",
    "plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('$L_2$ error norm distribution - AeroCNN-I, training\\nValidation rate {0}, test rate {1}, optimal settings ($C_d$)'.format(\n",
    "    val_rate, test_rate), fontsize=15\n",
    "         )\n",
    "plt.grid()\n",
    "saveName = \"trainingErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8f39cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1,x_val.shape[0],x_val.shape[0]),\n",
    "         l2_error_val*np.ones(x_val.shape[0],), 'k', lw=2.5)\n",
    "plt.scatter(np.linspace(1, x_val.shape[0], x_val.shape[0]), l2_error_val_list, c='b')\n",
    "plt.xlabel('Index', fontsize=15)\n",
    "plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('$L_2$ error norm distribution - AeroCNN-I, validation\\nValidation rate {0}, test rate {1}, optimal settings ($C_d$)'.format(\n",
    "    val_rate, test_rate), fontsize=15\n",
    "         )\n",
    "plt.grid()\n",
    "saveName = \"validationErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "342bbd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1,x_test.shape[0],x_test.shape[0]),\n",
    "         l2_error_test*np.ones(x_test.shape[0],), 'k', lw=2.5)\n",
    "plt.scatter(np.linspace(1, x_test.shape[0], x_test.shape[0]), l2_error_test_list, c='b')\n",
    "plt.xlabel('Index', fontsize=15)\n",
    "plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('$L_2$ error norm distribution - AeroCNN-I, test\\nValidation rate {0}, test rate {1}, optimal settings ($C_d$)'.format(\n",
    "    val_rate, test_rate), fontsize=15\n",
    "         )\n",
    "plt.grid()\n",
    "saveName = \"testErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8978795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict([x, x_para])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9badb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CL_trainTestSplit_Plot(i, CL, cl, aTrain, aTest, iTrain, iTest):\n",
    "    \n",
    "    #title_0_Cd = 'Gurney flap not attached (NACA0018)\\n$C_D$ prediction, L2 error=%.4f' % l2_error_Cd\n",
    "    title_0_Cl = 'Gurney flap not attached (NACA0018)\\n$C_L$ prediction, L2 error=%.4f' % l2_error_Cl\n",
    "    \n",
    "    #title_n_Cd = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_D$ prediction, L2 error=%.4f'%(l2_error_Cd)\n",
    "    title_n_Cl = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_L$ prediction, L2 error=%.4f'%(l2_error_Cl)\n",
    "    \n",
    "    if i==0:\n",
    "#         title_Cd, title_Cl = title_0_Cd, title_0_Cl\n",
    "#         savename1,savename2 = \"CdComparison_NACA0018.jpg\", \"ClComparison_NACA0018.jpg\"\n",
    "        title_Cl = title_0_Cl\n",
    "        savename2 = \"ClComparison_NACA0018.jpg\"\n",
    "    else:\n",
    "#         title_Cd, title_Cl = title_n_Cd, title_n_Cl\n",
    "        title_Cl = title_n_Cl\n",
    "        savename2 = \"ClComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    \n",
    "    '''# CD graph plot\n",
    "    plt.plot(alpha, CD, 'k-', label='Ground truth')\n",
    "    plt.plot(alpha, cd, 'k--', label='Predicted value')\n",
    "    plt.scatter(aTrain, iTrain*(np.max(cd_orig)-np.max(cd_orig))+np.min(cd_orig), color='b', label='Training set')\n",
    "    plt.scatter(aVal, iVal*(np.max(cd_orig)-np.max(cd_orig))+np.min(cd_orig), color='g', label='Validation set')\n",
    "    plt.scatter(aTest, iTest*(np.max(cd_orig)-np.max(cd_orig))+np.min(cd_orig), color='r', label='Test set')\n",
    "    plt.xlabel('AoA (degree)')\n",
    "    plt.ylabel('$C_D$')\n",
    "    plt.title(title_Cd, fontsize=15)        \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylim([0, 0.12])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()'''\n",
    "\n",
    "   # CL graph plot\n",
    "    plt.plot(alpha, CL, 'k-', label='Ground truth')\n",
    "    plt.plot(alpha, cl, 'k--', label='Predicted value')\n",
    "    plt.scatter(aTrain, iTrain, color='b', label='Training set')\n",
    "    plt.scatter(aVal, iVal, color='g', label='Validation set')\n",
    "    plt.scatter(aTest, iTest, color='r', label='Test set')\n",
    "    plt.xlabel('AoA (degree)')\n",
    "    plt.ylabel('$C_L$')\n",
    "    plt.title(title_Cl, fontsize=15)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylim([0, 2])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename2, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36887acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "NACA0018 airfoil without Gurney flap\n",
      "L2 error of Cl: 0.0121\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "L2 error of Cl: 0.0064\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0056\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "L2 error of Cl: 0.0058\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0053\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0036\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0077\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEwCAYAAACkMUZEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABVDklEQVR4nO3dd3gUVffA8e8hBEgIhCqiKEFFeamhKiIlIogNAQXFqBSRIoKKDeVV4VWUn2BDEcQCKigWREURUSSiYqEISFMQItKkQwIESHJ+f8wkbpZNsmm7KefzPPskO3PvzNnZZM/eOzP3iqpijDHGZKdUsAMwxhhTNFjCMMYY4xdLGMYYY/xiCcMYY4xfLGEYY4zxiyUMY4wxfrGEYYwxxi+WMIwxxvjFEkaAiEg3EVkgIvtE5ISIbBeRWSLSJtix5ScRedR9bakiMt19LAt2XJ5EpJeI9PV3eT7ut8COhYg0FBEVkQ5BjKG+iCwUkaMiskNE/iciIXmtJyI9ReRT9+8qUUSWi0jvfIi3kYjMc/8n94nIHBE5LQ/bu15ElrjbShKR30XkvyJSxqtcro5TYWAJIwBE5DlgNrAdGABcBowEKgDfi8i5QQwv34hIC2AM8BLQBng8uBFlqhfQNwfLTTZEpDLwNaDAtcD/gHtx/h7yWm8EkAjcA3QFFgHviMiwPMR7prsdBWKBIUA7dx+5VdXd5gDgCuANYBTwrMd+c3WcCovSwQ6guBORa4G7gX6qOt1r9dsicg1wLI/7CAFCVPVEXraTD+q5Pyep6mEAEQliOCaABgNhQA/3vf9KRCoCo0Xk6bS/h1zWu0ZV93rU+UZEzsBJJC/mMt7hwGF3v8cBRKQ/zpe4XFHVV7wWLXJfy1ARGabOOEy5PU6FgrUwCt7dwFIfyQIAVZ2rqjsARCRORD70XC8iHdyuhoYey6aLyDK3m2stkARc6LG8k4isFpEjIvK9iDTw2uYlIvKt2yTeJyKvikgFj/VXuV1Kdbzq1XGXd/V+HSIyHXjbfXooq+4REWntdjHscGNcKSKx3tvzeI0b3Cb+9yJS39c2/d22G+d1QHs3RhWR0Zkt9zdet1w7EVnkdpscct/Ppj7K5en9ccvcISJ/u9uYC9TM6rjkNIZcuAL40usDbxbOh2P7vNTzShZpfgVy3X0EXAXM8UgWlYFLgKV52KYv+wDPLqncHqdCwRJGARKR0kBrYEEBbD4KeBp4CrgS2OIuPxsYD4wFeuP8U70v7ld9cc6ZLAR2AdfjJLQrgWke254P7AD6eO2zL7AHmOcjnseBJ9zfL8V53Ssyib028ANO0/0anO66aXJqv3RtnOb848BNQCTwpYiUy2S7/mz7cZxug1/dGFsDr2Wx3K943eS4EDiJc9xuAL4DzvSKL8/vj9tqnQR8BvQAfsPp/vBXdjGIiJTO7uG1zXrABs8FqroVOMq/LU9fclvvYmBdtq/UBxEpD/wHWCoiFUSkLc7f/DbgPbdMbo5B2vZDRCRcRC7BaclM1n9Hec3t6y0cVNUeBfQAauD0VQ7yWi443YFpD3GXxwEfepXt4G6jocey6e6yaK+y04FkoK7Hsm5u2Xru8++ARV71LvWxjydwkpB4xBwPTMji9fZ1txPhFdOyLOqkHYtXgG98vMaLPZbVdl/fYD+Pf2bb/hCI81He53I/t/kjsCzteGVSN1/eH+AX4AuvMq+6ZTpkE78/MaS9j1k+vLZ7Erjbx/62AU9mEU+O6wEdgVSgby7/L1u7r+ECYL/7exJwkY+/Zb+PgUfdJI8ybwKl8nqcCsvDzmEUrLQOfO8x5O/F+YaXZhjOieKc2K6qK30sj1fVjR7P076F1RKRrTj/LMO8vh19j/OH3BxY4y57A3gYJ2EtAmJwPrA9WyK54jb/x+Cc9DsTSLtCZLtX0d2quiTtiar+JSLLgVbAlDxuO9/idb+xXgjcpe5/fxby9P6IyHqgKc7fjKePcFpA/sg0Bpxvv3OBln5uy5Ov1y6ZLM9VPRGJAt4BPtFMunn9EI1zEn0zTiuuLk5L7nMRaaCqu8j9MQCn9ROO83f6KM7/9h0e63N7nILOEkbB2gscx/lH9PQ2TmsCct9n+k8myw96PU87EV4OqIzzYfey+/B2VtovqrpZROKAfjgJox/wi6quzWW8nqYDF+F0A63DOfk4BOcD2dNuH3V3k3V/vb/bzs94K+P8w+/0Y1sHvZ7n9P2pjvN/631sfB2r3MQAzrfuQznYHsABoJKP5ZE+9pereiJSBfgC2ArcnMP4PDUFVqnqSeAbnJPo3wB/4JxHeI/cHQMAVDWtK/Z7EdkLvCkiz6jqn+T+OBUKljAKkKomi8iPQGecbxppy//B/cCXjFcRJZHxBBlAlcw2n4uQDrr1RuP7PMQOr+evAa+KyEM4feX35mKfGbjnH64C7lTVKR7LfZ1P83VS8zTAZ9LK4bbzM94DOF0kOTrx7MNBsn9/9uB0KXkfm7ycAPbWB/9akp5/vBvw6oMXkbOA8nj12Xvxq56IhOOcsykDXKWqR/yILzPRwM9ey5Lcn2lfxHJzDHxJSx51gD/J/XEqFCxhFLzngY9F5BZVfTubsttwrgX31Cm/AlHVIyLyE3CBqv7Pjyof4ZxcnYVzgcSsfAijLM636ONpC9wrgLpyahI8TUQuTuuWEpGzgWZk/o/s77ZP8O+3abJZnu023eP6M3CriLzkR7eUT/6+PyKyEqd149kt1yM3+8xEbrpjvgDuF5EKqprgLrsB55Lxb/NSz+2e+wCn66iNquakNZWBOJegN8R5jZ5icVoV37vP89Il5Sntxty0i1Jye5wKBUsYBUxVPxGR54HpIhKD84e4F+cmn7RkkOj+nAPcJs6Nfp/jnDe4PJ9DegBYKCKpOCd5E3CumrkKGKWqf3jEniQiM4GhwLuqejCvO1fVQyKyFHhURA7jfDMfidP8r+hVfC/OvSqP4PxD/Q+n62V6Hre9AbhWRLrhJOkd6lza7HO5n9sciXND1hciMhU4gnM+YpmqfpaDQ+TP+/Mk8JGITMb5m2kPdMnBPrKkqvtwLgfNiSk4VwR9JCL/B5yD01J6Vv+9J+dWnHNj56rqX/7Ww+meuxK4C6giIhd57PdX/ffS2A6459tUNS6TOOvhXML6gIjsA9bjXE47Chiiqsm5PQYiMh/nb2AtkIKTLO4F3nO7o/x9vYVXsM+6l5QH0B34CudbzEmc7oXZwBVe5R4C/sb5oJjBv99kva+SOuXKI1/LcS6/VeBqj2UX4lxGeBjng20dzuWrkT62eZlb/zI/XmNf/LhKCjgPp+/4CE5/9AM4/zR7vevhfHP+A+cb/g+exyGTGPzZdjWcD9q0K2RGZ7M822265doDi3EukTyI8+EVXRDvD3AnTlI7itN91Rn/r5LKNoZc/o3Xd4/TMZzzOY/j3FDq/fcRlcN68WR+pVKUR7kr3WX1s4gxFqcl+ZZ7fA8BPwHX5cP/+OM4F40kuu//CpyLE0Jz8noL8yPtkkljfBKRp3GazHVUNTWA+52OkxxaBGqfpmgTkTFAO1WNyaLMeKCzqjYJXGTFh3VJGZ9E5AKcb0JDgDGBTBbG5NLFeIzblImmODdnmlywhGEy8wpO18inwMQgx2JMtlTVnwtEmuBcbWVywbqkjDHG+MXGkjLGGOMXSxjGGGP8YgnDGGOMXyxhGGOM8YslDGOMMX6xhGGMMcYvljCKKBEJFZF7ROQXcaYDPSYiy91l3iPeFkoi0lA8pnIVd1rWHG6jl4j09bE8x9vKT+JM++pralHPMj3Fmfp1uzjTui6XU2cdLLZEpL6ILBRnKtodIvI/d3DAfKnrZ5k4+XdKXu9Ha3/LlBR2414RJM6EPl8D5wIv8u/Q6VcA43Am9nk/ONHlyeM4A8PlRC+cMaCm58O2Am0Eziim9+AMtHgl8I6IVFPVF4MaWQHz+BtehzPy7rnAMzhfYv+b17o52P4dnDro5f9w7ghfmoMyJUOwB7OyR84eOOPvL8IZtKyej/UtcMZ9CkQsIUCZPNRviB8D5mWzjWynVQ3S+zQar8EJfZSp5mPZO8CWQL1X+fAe5qo+ziCbB4CKHssewBlMsWJe6+Z2+zjzbezHmYc712WK68O6pIqePjjTpg5W1VMmXFHVZaq65ZRaWUjrvhGRbiKyQUSSROR7EamfRbm1OJPOXOiuu0REvnWb//tE5FV33gjP+neIyN8ickRE5uI14VBm3Ugi0k5EFrndNofcLoKm7gCF1wHtPboIRme2Lbf76jcROe7GMVY8pkL1eH2dRGS1G+f3ItIgJ8fTX6rqq8vqV/yYDCm7453Ze5XNe5jl8clqu7l4+VcAX2rGIb1n4bQK2+dD3dxuvwvOzIfv5rFMsWQJo+gZAaxX1U/yebu1cQZuexy4CWfKyC/FmXHOUxTwNPAUThfKFhFpAywEduHMkXy3uy59oiMRuRZnMqbPcIYs/w1nboQsiXN+YyHOkPB9cEbO/Q5nbu3HcVpbv+LMPdEaZ5ZAX9vpjDP15gqcLooXgfs4dS71s3HmWx8L9Mb58H5fRLKbWS2/XMy/c2z75M/xdkXh9V5ltjwHxyez+iIipbN7eGyjHl4zzKnqVpwWQIYZ6Xzwp25ut38jTpfud3ksUzwFu4ljD/8fOB/qijORTn5ud7q73Yu99pWM05LxLhftVf87YJHXskvxmMcD+AX4wqvMq3h0SeF7roYfcebFkExi99kl5b0tnDkPvGN8AGeim1oedZKBuh5lurkxntL9l80xHU02XVI+6nTEmaCpbzbl/Dnemb1XmS3P9vhkU78vmc9Zkf7wKH8SuNvHa9sGPJnN68+2bm62D4TjzEPzTBb7zrZMcX5YC6NoaeT+XJNdQRG5XkS+yMG2d6s7FSqAOjOiLQdaeZXbrqorPfYTjvPN/n2vb5Lf4/zTNnevTGkKeLeKPsrmNZTH6e54U93/1txw998MZ5pPT+/htLI9r3SJV9WNHs/Tvu3Xyu3+/SEiUTjnLz5R1elZlMv2eHsUz/BeZbY8h8cns+2mTWma3cOTr/dUMlnuzZ+6Od3+NUAEWXc1+VOm2LKrpIqWSPfnP1mWckQDq3KwbV/zJO/G6zyDj31Xxjnx+bL78HYWUB3nb817H9nNzVwZ5x98ZzblslMNCOXU2NOeV/FYdtCrzAn3p685wPOFiFTBmet5K3BzNsX9Od5pMvs78V6ek+OT2Xb348xe568DQCUfyyM59T3ITd3cbP9GYJOqZnU5tj9lii1LGEVL2gfsGX6UbYLzjdVfvk60noYzP7En729nB91lo3GmCvW2A9iD09XjvY/sTu4ewOmi8U5aObUX59u39/5quD/353H7uea2GD7DufLmKlU9kk2Vg2R/vNNk9k3ae3lOj4+v7fbh1HMovqSdC9qA17kEETkLKI/XuQcf/Kmbo+2LSCTOifKnMw3cjzLFnXVJFS0/4sxD3M/XShG5xONpNDlrYZwmIhd7bOtsnG6KX7Kq5H7A/QRcoM4VWt6PHaqaAqzEOZnqqYcf2/4ZuDWLk84nyObbv7v/5UBPr1W9cBLSj1nVLyhuV9IHQF2cud2za3H5dbxzGkc+HZ+cdkl9AVzudSXdDTjzXH+bzb78qZvT7XcHypJ1V5M/ZYo1a2EUIaqaKCIPApNF5BPgbZxv7+fi/LNXBNq4XRzVgN9zsPm9wNsi8gjOP9X/cFo00/2o+wCwUERScU5CJ+BcbXQVzgn6P4AngY9EZDIwB+fSxi5+bHskzg1YX4jIVOAITp/6MlX9DOfb4rUi0g3nhOaOTD40H8O56msazuWVjXCusnpVVbf5EUc698qtRUCMqsZlUbSMiFzvY/m3qroHp0vpSuAuoIqIXORR5ldVPZ7Jdv053jmVp+OjqvuAfTnY3xRgOM7fxP8B5+C0mp5Vj0thReRWnKvpznXPq/lb16/te7gRWKWq67OI2Z8yxVuwz7rbI+cPnG/q3wGJ7mMdzj9IK3f9pXhdbZTN9qbjXInUA/gDOA78gHvFjXe5TLZxITAfpwV0xI3pWSDSo8ydOB/qR3G6UzqTzVVS7vL2wGK33kGcD+tod101nAS0393W6My2hfMN8zecVsk2nEtnS2f1+nAuIVXgao9lV7rL6mdxTEeT+dVCaa83PosyUdm8Z1ke7yyOZVbvYZbHJ7v6ufg7rg98g/MFZSdOggrxKtPX1/Hws262ZTz+hk4CI7OINdsyJeFhU7QWQyJyD86H/W1+lp/ulm9RoIEVEyIyBminqjHBjsWYQLJzGMVTE+A6EYn3eJyVbS3jr4txvs0bU6IELGGIyFniDO+wXkTWishdPsqIiEwUkU3u0AzNPNZ1EZHf3XUjAxV3UaSqfVW1kqpGeTz+DnZcxYWqdlLVucGOw5hAC1iXlIjUBGqq6gr3yoXlQDdVXedR5kpgGE4f8YXAC6p6oXtj0R9AJ5y+1aVAb8+6xhhjClbAWhiqulNVV7i/JwDrccYD8nQt8JY6fgIquYmmFc7NMptV9QTOVRzel2gaY4wpQEG5rNYdBqEpzjX2ns4EPLtOtrnLfC0/ZYRMERkIDAQICwtrftZZue+2T01NpVSpwneKx+LKGYsrZyyunCmOcf3xxx97VbW6z5WBviwLZxyW5UAPH+s+By7xeL4QZ2ycnsBrHstvAV7Maj/NmzfXvFi0aFGe6hcUiytnLK6csbhypjjGRRaXTQe0hSEiocBsYKaq+hp4bhsZx8KphTPUQZlMlhtjjAmQQF4lJcDrOHM5ZHZJ4qe4w0C4d70eUtWdOCe564pIHXHmq77RLWuMMSZAAtnCaIPTlfSbiKx0lz2MM6QBqjoF5+7fK4FNOHf19nPXJYvIncCXOCN1vqGq3oPiGWOMKUABSxiq+j3/jlSZWRkFhmaybh6+R+f028mTJ9m2bRtJSUnZlo2MjGT9+sI3ZExJj6tcuXLUqlWL0NDQAt+XMSajEjX44LZt26hQoQJRUVFkN+NmQkICFSpUyLJMMJTkuFSVffv2sW3bNurUqVOg+zLGnKrwXQ9WgJKSkqhatWq2ycIUTiJC1apV/WohGmPyX4lKGIAliyLO3j9jgqfEJQxjjDG5YwkjwP755x9uuukmzjnnHJo3b07r1q2ZM2dOQGOIj4+nYcOGPpe/805OZnX916RJkzh69Gj684iIiFzHZ4wpnCxhBJCq0q1bN9q1a8fmzZtZvnw5s2bNYtu2Uyc0S05ODnh8WSWM7OKZPHlyhoRhjCl+StRVUsH2zTffUKZMGQYPHpy+rHbt2gwbNgyA6dOn8/nnn5OUlMSRI0f48MMP6d+/P5s3byY8PJypU6dSp04dRo8eTUREBPfddx8ADRs25LPPPgPgiiuu4JJLLmHJkiWceeaZfPLJJ4SFhbF8+XL69+9PeHg4l1xyyanBASNHjmT9+vVER0fTp08fKleunCGeRx99lAkTJqTv684776RFixYcPnyYnTt3EhMTQ7Vq1Vi0aBEAo0aN4rPPPiMsLIxPPvmEGjVqFNixNcYUvBKbMO6++25WrlyZ6fqUlBRCQkJytM3o6Gief/75TNevXbuWZs2aZboe4Mcff2T16tVUqVKFYcOG0bRpUz7++GO++eYbbr31Vr777rss62/cuJF3332XV199lV69ejF79mxuvvlm+vXrx4svvkj79u25//77fdYdN25choQwffr0DPHExcX5rDd8+HCeeeYZFi1aRLVq1QA4cuQIF110EWPHjuWBBx7g1Vdf5b///W+WsRtjCjfrkgqioUOH0qRJE1q2bJm+rFOnTlSpUgWA77//nltuuQWASy+9lH379nHo0KEst1mnTh2io6MBaN68OfHx8Rw6dIiDBw/Svn17gPRt+sMznpwoU6YMV199dYY4jDFFW4ltYWTVEoCCuRGtQYMGzJ49O/35pEmT2Lt3Ly1a/DuVdvny5dN/Vx+TW4kIpUuXJjU1NX2Z530JZcuWTf89JCSEY8eOOZO35/JyVM94stqvt9DQ0PR9hoSEBOWcjDEmf1kLI4AuvfRSkpKSmDx5cvqyrE4Ut2vXjpkzZwIQFxdHtWrVqFixIlFRUaxYsQKAFStWsGXLliz3W6lSJSIjI/n+++8B0rfprUKFCiQkJGS6ndq1a7Nu3TqOHz/OoUOHWLhwYfq6iIiILOsaY4q+EtvCCAYR4eOPP+aee+7h6aefpnr16pQvX57/+7//81l+9OjR9OvXj8aNGxMeHs6bb74JwHXXXcdbb71FdHQ0LVu25Pzzz89239OmTUs/6X355Zf7LNO4cWNKly5NkyZN6Nu3L5UrV86w/qyzzqJXr140btyYunXr0rRp0/R1ffv25YorrqBmzZrpJ72NMcVMZhNlFPWHrwmU1q1b5/ckIocPH/a7bCBZXDl7H4vjBDcFyeLKmeIYF1lMoGRdUsYYY/xiCcMYY4xfLGEYY4zxiyUMY4wxfrGEYYwxxi8Bu6xWRN4ArgZ2q+opQ6WKyP1ArEdc/wGqq+p+EYkHEoAUIFlVW3jXN8YYU7AC2cKYDnTJbKWqjlfVaFWNBh4CvlXV/R5FYtz1RTpZhISEEB0dTcOGDenZs2eeRnjt27cvH374IQADBgxg3bp1mZaNi4tjyZIlOd5HVFQUe/fuzXWM+b0dY0zwBCxhqOpiYH+2BR29gXcLMJygCQsLY+XKlaxZs4YyZcowZcqUDOtTUlJytd3XXnuN+vXrZ7o+twnDGGPSFLpzGCISjtMSme2xWIEFIrJcRAYGJ7L817ZtWzZt2kRcXBwxMTHcdNNNNGrUiJSUFO6//35atmxJ48aNeeWVVwDnJst7772X+vXrc9VVV7F79+70bXXo0IFly5YBMH/+fJo1a0aTJk3o2LEj8fHxTJkyheeee47o6Gi+++479uzZw3XXXUfLli1p2bIlP/zwAwD79u2jc+fONG3alEGDBvkcz2ry5Mk88MAD6c+nT5+ePtR6t27daN68OQ0aNGDq1Kmn1PWevGnChAmMHj0agD///JMuXbrQvHlz2rZty4YNG/J4hI0x+akwDg1yDfCDV3dUG1XdISKnAV+JyAa3xZKBm0wGAtSoUeOU4bgjIyMzjHd05ZVXnrLz7t27c/vtt5OQkOBzfWxsLLGxsezbt++UUV/nzZvn1wtMSEggOTmZuXPnctlll3H06FF++eUXfvrpJ6Kiopg0aRLlypXjm2++4fjx43Tu3JmLL76Y1atXs3HjRpYsWcLu3btp1aoVvXv3JiEhgZSUFI4cOcKWLVsYMGAAX3zxBVFRUezfv58qVarQr18/IiIiGD58OAD9+/dn0KBBtG7dmr///pvu3buzbNkyRo0aRcuWLRk5ciTz589n6tSpJCYmZhjUsEuXLnTs2JFHHnkEcMamGjFiBAkJCbzwwgtUqVKFY8eO0aFDBzp37kzVqlVRVRITE0lMTCQ1NTX9fTh+/DjHjx8nISGB2267jeeee47zzjuPpUuXMmjQoPSh1j0lJSVlOtS6t8TERL/LBpLFlTMWV84UVFyFMWHciFd3lKrucH/uFpE5QCvglIShqlOBqQAtWrTQDh06ZFi/fv36DCPQ+prvoly5clSoUIGjR49muf748eOnrPdndNtjx47Rtm1bwGlhDB06lCVLltCqVSsaNWoEwOLFi1m9ejVz584F4NChQ+zcuZOlS5fSs2dPKlWqRKVKlbj00ksJCwujQoUKhISEUL58edasWUP79u3Tt5UWU9myZSlbtmz682+//ZaNGzemx5WYmAjATz/9xEcffUSFChXo2bMnlStXJiIiIsNrq1ChAueddx5r166lbt26/Pnnn7Rp04YKFSrwzDPPpE85u337dnbt2kVUVBQikj5ta6lSpTLEdfLkSUSEn3/+mX79+qXv5/jx4z6Pably5TKMY5WVuLg4vP8OCgOLK2csrpwpqLgKVcIQkUigPXCzx7LyQClVTXB/7wz8Lz/2l1UGDg8Pz3J9tWrVcpXB085hePMe1vzFF188ZZDAefPmZTtMufo5lHlqaio//vgjYWFhp6zzp/4NN9zA+++/T7169ejevTsiQlxcHF9//TU//vgj4eHhdOjQ4ZQh0DMbIj01NZVKlSplOamVMSa4AnYOQ0TeBX4ELhCRbSJym4gMFpHBHsW6AwtU9YjHshrA9yKyCvgF+FxV5wcq7mC4/PLLmTx5MidPngTgjz/+4MiRI7Rr144PP/yQlJQUdu7c6XNU2NatW/Ptt9+mD3m+f7/Ts+c9dHnnzp156aWX0p+nfVB7Dqn+xRdfcODAAZ8x9ujRg48//ph3332XG264AXBaQpUrVyY8PJwNGzbw008/nVKvRo0a7N69m3379nH8+PH0LqeKFStSp04dPvjgA8BJfKtWrfL/oBljClzAWhiq2tuPMtNxLr/1XLYZaFIwURVOAwYMID4+nmbNmqGqVK9enY8//pju3bszf/58GjVqxPnnn58+g56n6tWrM3XqVHr06EFqaiqnnXYaX331Fddccw3XX389n3zyCS+++CITJ05k6NChNG7cmOTkZNq1a8eUKVN47LHH6N27N82aNaN9+/acffbZPmOsXLky9evXZ926dbRq1YqEhAS6dOnClClTaNy4MRdccAEXXXTRKfVCQ0N59NFHufDCC6lTpw716tVLXzdz5kyGDBnCE088wcmTJ7nxxhtp0qREvfXGFG6ZDWNb1B82vHlg2fDmOWNx5YzFlTM2vLkxxpigsoRhjDHGL5YwjDEmh2bOhKgoWL7c+eleJ1LsFarLao0xprCbORMGDoSjR3exZ882/vrrH26/vRoQQmxsttWLNEsYxhjjpyVLlnD77U9x7NiPwD7+7/+c5ceO3codd9Tivfd+44cffqB8+fJEREQQERFBZGQk48aNo2bNmqxcuZL169cTGRlJxYoV0x8XXXQRpUqVIikpidKlS1O6dO4+mmfOhFGjYNgw6NsXxo4lX5OYJQxjjPGya9cufvzxR3788Ufi4uLo3Lkz27dvZ968eRw7ljaGWzgVK4Zx+HAK8AmHDycwd65zU2ra/U9pWrTIepDtmJgYIiMjWbt2LRs3biQ0NJSyZcsSFhZGpUqVGDduHBUqVGDhwoX89ddfVKlShWrVqlGtWjVOP/10evbsycyZMGDAnyQlOQOY/vWX0xKC/EsaljACaN++fXTs2BFw/iBDQkKoXr06AL/88gtlypTJtO6yZct46623GDt2bJb7uPjii4MyKu2ECRMYM2ZMwPdrTF6dPHmSY8eOUbFiRTZt2kRMTAzbtm3LUGbp0qVUrVqVdu3aERcXw4EDHYAGPProYu67rwMAZ5+trF9/jISEBA4fPkxCQsIpvx88eJB9+/axf/9+9u/fz6FDhzh8+DDHjx/nzz//5NChQ4SFhZGUlMTJkydJTExMHyg0MyLC8OHD2bu3AsnJu4AjzJ8fC3Tg6FGnxWEJowiqWrVq+h3Vo0ePJiIiIn2UV4Dk5ORMm6ItWrSgRYsWGe7W9iVYQ5g/88wzljBMkbBnz5701sOSJUv45ZdfiImJoUaNGnzzzTfpyaJixYrExMTQsWNHOnToQIMGDShVqpTHOYx/txkeDk8+KYSHhxMeHk6NGjXyFKOqpg/K6SsBHT58mAMHDrB3714OHToEwKuvHgb+Ag5y4sSx9G1t3ZqnUDKwhJGFtP7ArVvh7LPzvz8QnEmQqlSpwq+//kqzZs244YYbuPvuuzl27BhhYWFMmzaNCy64gLi4OCZMmMC7777L6NGj2bp1K5s3b2br1q3cfffd6aPQRkREpI9UOXr0aKpVq8aaNWto3rw5M2bMQESYN28eI0aMoFq1ajRr1ozNmzefMirs2rVr6devHydOnCA1NZXZs2dTt25dZsyYwcSJEzlx4gQXXnghL7/8MqNGjeLYsWNER0fToEGD9KFFjAm25ORk1qxZw4EDB4iJiUFVqVevHvv370dECA0N5cSJE3zxxRdUrVqV9u3bc99992VIEN7SPgNGjXJ+1q6d/58NIkK5cuUoV65cei9EdhYscLqhALp2jWOxOzxrJoM15IoljEy8/35phg//91tEQfQHpvnjjz/4+uuvCQkJ4fDhwyxevJjSpUvz9ddf8/DDDzN79uxT6mzYsIFFixaRkJDABRdcwJAhQwgNDc1Q5tdff2Xt2rWcccYZtGnThh9++IEWLVowaNAgFi9eTJ06dejd2/eILVOmTOGuu+4iNjaWEydOkJKSwvr163nvvff44YcfCA0N5Y477mDmzJmMGzeOl156yQYONIXC4sWLWbBgQXrr4ciRI5x++ul06dKFuLi49PMLlStXpkOHDumPzBKEL7GxziMuDuLjC+615MTYsb5bPtn0YueIJYxMjBlTFu/ZU/O7PzBNz54904dKP3ToEH369GHjxo2ISPoAhN6uuuqq9CHLTzvtNP755x9q1aqVoUyrVq3Sl0VHRxMfH09ERATnnHMOderUAaB3794+Jzpq3bo1Y8eOZdu2bfTo0YO6deuycOFCli9fTsuWLQFnqPbTTjst346DMd5m/jaTUQtHMazGMPo+35exHccS28j5B0xJSWHdunX8+OOPrFy5kkmTJiEiTJw4kTlz5lC5cuX0BLBr1y7mzp1L+/btGTFiRI4TRFEQiJaPJYxMbNvme4jv/OwPTOM5tPkjjzxCTEwMc+bMIT4+PtMx7T0nNAoJCSE5OdmvMupjBj1fbrrpJi688EI+//xzLr/8cl577TVUlT59+vDUU0/5+cqMyb2Zv81k4NyBHD15FGrAX4f+YuDcgaz+YTUrZq/g559/Tj+nV6FCBfbt28cvv/xCvMdX/k6dOuWqBVFUFXTLxxJGJmrVUv7++9SkkZ/9gb4cOnSIM888E3CmPs1v9erVY/PmzcTHxxMVFcV7773ns9zmzZs555xzGD58OJs3b2b16tV07tyZa6+9lnvuuYfTTjuN/fv3k5CQQO3atQkNDeXkyZOndIsZk1ujFo7i6O6j8BM8/erT0AWOnnWU1755jYg/IjjjjDPYt28fe/fuJSEhgYULFxbrFkRhYAkjE489dpzhw8MKtD/QlwceeIA+ffrw7LPPcumll+b79sPCwnj55Zfp0qUL1apVo1WrVj7Lvffee8yYMYPQ0FBOP/10Hn30UapUqcITTzxB586dSU1NJTQ0lEmTJlG7dm369u1L48aNadasmZ30Nnm2Zs0a/nrjL1gDCITUCnHm2NwD+w/uZz/7009Sl6QWRNBlNoxtUX/kx/DmM2ao1q6tKuL8nDHD7+oFJj+GEU9ISFBV1dTUVB0yZIg+++yzed6mDW+eMxZX5o4ePaqRkZEqpUWpgoL7CEP5D1q5e2VdvXq1pqSkBDvUQnG8fLHhzYMgNtbpB0xNdX4Wl3FiXn311fRLYA8dOsSgQYOCHZIpwVSV+fPnc/vtt3Po0CFef/11IiMj0WSFJOASGDFuBNwP4TeH8+KYF2nUqJG1JoLAuqRKoHvuuYd77rkn2GGYEi4lJYXZs2czbtw4fv31V8qXL88777zD0aNHadWqFY8//jgp9VMY8/0YzqhxBrX/qZ3hKikTeAFLGCLyBnA1sFtVG/pY3wH4BNjiLvpIVf/nrusCvACEAK+p6rhAxGyMKRgbN27kqquuYuPGjYSFhQHOEB29e/dm6NCh6ZduA/Rr0Y+4uDjie8cHKVqTJpAtjOnAS8BbWZT5TlWv9lwgIiHAJKATsA1YKiKfquq6ggrUGJP/EhIS+OOPPzjzzDOZMWMGf//9N+DMQz906FD69+9PtWrVghylyUrAEoaqLhaRqFxUbQVsUtXNACIyC7gWsIRhTBGwd+9eXnjhBV544QWSk5M5efIkycnJXH755QwdOpQrr7wy/cZVU7gVtnMYrUVkFbADuE9V1wJnAn97lNkGXBiM4Iwx/tu+fTtPPvkkr732GidOnACcm1TvvPNOhgwZwvnnnx/kCE1Oifp552++7MxpYXyWyTmMikCqqiaKyJXAC6paV0R6Aper6gC33C1AK1Ud5mMbA4GBADVq1Gg+a9asDOsjIyM577zz/Io1JSUl37/1XHnllYwYMYLLLrssfdmkSZPYtGkTzz33XKZ1nnjiCZo1a8Z1113Hq6++SpUqVTKUefLJJ4mIiEgfgNCXzz77jPPOO4969eoB8MQTT9CmTRtiYmLy4ZX5f7wmTJiQYYTe3Ni0aVP6CJ3ZSUxMJCIiIk/7KwjFOS5VZceOHUyZMoXvv/8egFq1atGrVy8uu+yy9HMWgY6rIBTHuGJiYparqu8JPDK73rYgHkAUsMbPsvFANaA18KXH8oeAh7Krnx/3YeS3KVOmaN++fTMsu/DCC3Xx4sWZ1mnfvr0uXbo0y7gee+wxHT9+fJb77tOnj37wwQc5jNh//h6v8uXL53lfdh9GwclLXD/99JNefPHFWqdOHQU0JCREu3btqt99952mpqYGLa6CVBzjoijchyEip4uIuL+3AkoB+4ClQF0RqSMiZYAbgU8DEdPM32YS9XwUpcaUIur5KGb+lrc7mK+//no+++wzjh8/DkB8fDw7duzgkksuYciQIbRo0YIGDRrw2GOP+awfFRXFvn37ABg7diwXXHABl112Gb///nt6mVdffZWWLVvSpEkTrrvuOo4ePcqSJUv49NNPuf/++4mOjubPP/+kb9++fPjhhwAsXLiQpk2b0qhRI/r3758eX1RUFI899hjNmjWjUaNGbNiw4ZSY1q5dS6tWrWjTpg2NGzdm48aNAMyYMYNWrVoRHR3NoEGDSElJYeTIkenDoMcWl5taSjhVZc6cOZx//vlcdNFFLFmyhL179zJmzBj+/vtvPvnkEy655BLcf21TxAUsYYjIu8CPwAUisk1EbhORwSIy2C1yPbDGPYcxEbjRTXjJwJ3Al8B64H11zm0UqPfXv8/AuQP569BfKJo+8FlekkbVqlVp1aoV8+fPB2DWrFnccMMNiAhjx45l2bJlrF69mm+//ZbVq1dnup3ly5cza9Ysfv31Vz766COWLl2avq5Hjx4sXbqUVatW8Z///IfXX3+diy++mK5duzJ+/HhWrlzJueeem14+KSmJvn378t577/Hbb7+RnJzM5MmT09dXq1aNFStWMGTIECZMmHBKLGnDoP/www8sW7aMWrVqZRgGfeXKlYSEhKQPgx4WFsbKlStt+JBiYPny5URHR9OjRw82btxInTp1mD59Ovv27ePRRx+lZs2awQ7R5LOAJQxV7a2qNVU1VFVrqerrqjpFVae4619S1Qaq2kRVL1LVJR5156nq+ap6rqoW8GhOjjHfj3FGyfRw9ORRRi0claft9u7dm7RzK7NmzUqfj+L999+nWbNmNG3alLVr17JuXeYXgX333Xd0796d8PBwKlasSNeuXdPXrVmzhrZt29KoUSNmzpzJ2rVZ59bff/+dOnXqpJ+A7NOnD4vTZl7BSUAAzZs3zzAKaJrWrVvz5JNP8txzz/HXX38RFhaWYRj06OhoFi5cyObNm/07QKZQS0xMZODAgTRp0oQWLVqwceNG2rVrx7Jly9i8eTN9+vSxASiLscJ2lVShsS1hm8/lWw/lbXzzbt26MWLECFasWMGxY8do1qwZW7ZsYcKECSxdupTKlSvTt29fkpKSstxOZk38vn378vHHH9OkSROmT59OXFxcltvRbC56SBsiPbMh1NOGQZ89e7YNg16M/f777wwbNoyFCxeSmppKlSpVmDhxIrfeeiuRkZHBDs8ESKE5h1HY1KpQy+fysyPzNr55REQEHTp0oH///umti8OHD1O+fHkiIyP5559/+OKLL7LcRrt27ZgzZw7HjjkTzs+dOzd9XUJCAjVr1uTkyZMZun0qVKjgcz7wevXqER8fz6ZNmwB4++23ad++vd+vJ20Y9CFDhtC1a1dWr15Nx44d+fDDD9m9ezcA+/fv5y937si0YdBN4aeqfP311zRq1Ih69erx1VdfUalSJZ544gn27NnDsGHDLFmUMJYwMvHYJY8RHhqeYVl4aDhjO+a9R6x3796sWrWKG2+8EYAmTZrQtGlTGjRoQP/+/WnTpk2W9dPm/o6Ojua6666jbdu26esef/xxLrzwQjp16pR+CS3AjTfeyPjx42natCl//vln+vJy5coxbdo0evbsmT6g2+DBg/HXe++9R8OGDWnTpg0bNmzg1ltvpX79+unDoDdu3JhOnTqxc+dOAAYOHEjjxo3tpHchM3MmREXB8uVw9tmH6NXrSerVq0enTp3YtGkT5557Lh988AH79u1j1KhRNvBfSZXZ5VNF/ZEvw5uvnqG1n6utMlq09nO1dcbq4I9vHshhxHPChjfPmcIU14wZquHhqrBWmzbtqFBaAa1Zs56+/fbbeuTIkWCHWKiOl6fiGBdZXFZr5zCyENso1kbGNMXeAw/8wdGj9wDz+PVXcDoeelCq1DPcfHNUUGMzhYu1K40pof766y9uu+02duz4D/AFEEqbNt1wRt+ZzY4dUcEMzxRCljCMKWF27txJv379OOecc5gxYwYVKtwFzAH+pnv3uwDn/omCnr/eFD3WJWVMCbF3715Gjx7NlClTSElJoVSpUrz77rscO9aDgQNx569fDwRm/npT9FjCMKaYO3ToEE8//TQTJkxIHzX22muv5Zlnnslw1/8o957U2rWdZGEXshlvljCMKaaOHDnCxIkTGT9+PAcOHCAyMpL27dvz3HPP0aBBgwxlY2OdR1ycM3+9Mb7YOYwA2rdvH9HR0URHR3P66adz5plnpj9P++aXlbi4OH7++ec8x3Hw4EFefvnlPG/HFE5JSUk888wznH766Tz88MO0bNmSFStWsH37dhYsWHBKsjDGX9bCCKCqVauycuVKAEaPHk1ERESO5oaIi4sjNDQ0w3wauZGWMO644448bccULidPnuS1115j1KhRHDhwAICmTZsyefJkzjnnnCBHZ4oDa2FkJe3211KlnJ8FMMLq8uXLad++Pc2bN+fyyy9PvyN64sSJ1K9fn8aNG3PjjTcSHx/PlClTmDRpEtHR0Xz33XcZtvPtt9+mt1aaNm2aPgzI+PHjadmyJY0bN04fNn3kyJH8+eefREdHc//99+f7azKBlZKSwowZMzj//PO54447OHDgQPpQHsuXL7dkYfKNtTAyUfr992H48LRLR+Cvv2DgQOf3fDobqKoMGzaMTz75hOrVq/Pee+8xatQo3njjDcaNG8eWLVsoW7YsBw8epFKlSgwePJjQ0FBGjTp1xNwJEyYwadIk2rRpQ2JiIuXKlWPBggVs3LiRX375BVWla9euLF68mHHjxrFmzZr01o4pmlSV2bNn8+CDD7J582aaNGlC165d6d+/P127drU5KEy+s4SRibJjxvybLNIcPepcSpJPCeP48eOsWbOGTp06Ac43xbQ5BNLGW+rWrRvdunXLdltt2rRhxIgRxMbG0qNHD2rVqsWCBQtYsGABTZs2BZyhqTdu3MjZdoF9kaaqfPHFF9x1113pg0a+9NJLDBkyxMZ4MgXKEkYmZJvv4c3ZmrfhzT2pKg0aNODHH388Zd3nn3/O4sWL+fTTT3n88ceznddi5MiRXHXVVcybN4+LLrqIr7/+GlXloYceYtCgQRnK+prXwhQNcXFxDBs2jDVr1gDOebFx48bRt29fSxamwNlfWCa0lu/hzfPz9teyZcuyZ8+e9IRx8uRJ1q5dS2pqKn///TcxMTE8/fTTHDx4kMTExEyHKAf4888/adSoEQ8++CAtWrRgw4YNXH755bzxxhskJiYCsH37dnbv3p3ldkzh9PPPP9OpUydiYmJYs2YNFStW5Pnnn2f79u0MGDCA0qXtu58peJYwMnH8scec21095fPtr6VKleLDDz/kwQcfpEmTJkRHR7NkyRJSUlK4+eabadSoEU2bNuWee+6hUqVKXHPNNXz22Wc+T3o///zzNGzYkCZNmhAWFsYVV1xB586duemmm2jdujWNGjXi+uuvJyEhgapVq9KmTRsaNmxoJ70LuVWrVtGhQwcuuugiVq1axbPPPsvcuXPZuXMnd911V/oEV8YERGbD2Bb1R34Mb64zZqjWrq0q4vycYcObZ8aGN8+Z7OLasGGDXnnllQoooGXLltWNGzcGPa5gsbhypqCGNw9YC0NE3hCR3SKyJpP1sSKy2n0sEZEmHuviReQ3EVkpIssCFTOxsc5tr6mpzk8bK8EUsPj4eHr37k29evWYN28eoaGh3H333ezYsYPzzjsv2OGZEi6QHZ/TgZeAtzJZvwVor6oHROQKYCpwocf6GFXdW7AhGhMcO3bs4PHHH+f1119HRAgPD+emm27i8ccf5/TTTw92eMYAAUwYqrpYRKKyWL/E4+lPQCZnnfMch12fXoQ5LebiY8+ePYwePZpXXnmFlJQUBg0axCOPPEKVKlUICwsLdnjGZCCB/Ad0E8Znqtowm3L3AfVUdYD7fAtwAKc/9xVVnZpJvYHAQIAaNWo0nzVrVob1ERER1KhRg8jIyGyTRkpKCiEhIX69rkAqyXGpKocOHeKff/5Jv/IrO4mJiURERBRoXDmxfz9s3w7ly+/inXc+5ttvZ5OcnAw499KMHDkyqPEWtuOVxuLKmbzEFRMTs1xVW/haV+gShojEAC8Dl6jqPnfZGaq6Q0ROA74Chqnq4qz21aJFC122LOPpjpMnT7Jt2zaSkpKyjTUpKYly5cplWy7QSnpc5cqVo1atWoSGhvpVPi4ujg4dOhRsUH6aORNuvz2JY8eep2zZ/3H8+DEAGjbsxLvvPkvDhll+jwqIwnS8PFlcOZOXuEQk04RRqC7eFpHGwGvAFWnJAkBVd7g/d4vIHKAVkGXC8CU0NJQ6der4VTYuLi79DunCxOIqmlSVu+/+lGPHhgF/U6dOKzZsOAP4LwkJzSkEucKYbBWa+zBE5GzgI+AWVf3DY3l5EamQ9jvQGfB5pZUxhdGGDRto27Yte/d2A3YAHzNgwP/hTIvaPD8HDzCmQAWshSEi7wIdgGoisg14DAgFUNUpwKNAVeBl9/xCstssqgHMcZeVBt5R1fmBituY3Dp8+DD//e9/eemll9yLLcJRHQ9cDfx746UN7WWKikBeJdU7m/UDgAE+lm8Gmpxaw5jCKTU1lbfffpv77ruPvXudK8FvueUWLr74Oe69t2qGMS1t7mxTlBSqcxjGFHXLli1jyJAhLFu2jFatWtGlSxeGDx9Oy5YtAahQwebONkWXJQxj8sHu3bu59957mTFjBiLC+PHjGTFixCkjyNrc2aYoKzQnvY0pik6ePMlzzz1HVFQUM2bMAKB///7079/fhhs3xY61MIzJpYULFzJ8+HDWrVsHQJMmTXjjjTdo1qxZkCMzpmDYVyBjcig+Pp7u3btz2WWXcezYMXr16sXrr7/OihUrLFmYYs1aGMb46dixY4wbN44nn3yS5ORkbr/9diZOnFgo77w3piBYwjAmG6rKRx99xJ133smuXbsAaN68OcOHD7dkYUoU65IyJgtr166lU6dOXH/99ezatYvKlSvz5ptvsnTp0kIx9pMxgWQtDGN8OHjwII899hgvvfQSkZGR9OjRgzPOOIPHH3+cSpUqBTs8Y4LCEoYxHlJTU5k2bRr33XcfBw8epGPHjsyaNYtq1aoFOzRjgs4ShjGun376icGDB7Nq1SoAqlWrxtChQy1ZGOOycximxNu1axd9+/aldevWrF69mlKlSnH33Xfz559/0r1792CHZ0yhYS0MU2KdOHGCiRMnMmbMGI4fP063bt3Yu3cvkydPthPaxvhgCcOUSF9++SVDhw7lzz//5IILLmDu3Lmcd955ADbnuzGZsC4pU6Js3ryZrl270qVLF7Zs2UJISAi9evWibt26iIglC2OyYC0MUyIcOXKEp556iqeffpqUlBQA2rVrx6RJk6hfv36QozOmaLAWhinWVJVZs2ZRr149xo4dS8eOHalZsybvv/8+33zzjSULY3LAWhimWJo5E+6/fzXlyt3Fli2/UaFCdb7//nvatGnDyZMnCQ0NDXaIxhQ5eW5hiEhbP8u9ISK7RWRNJutFRCaKyCYRWS0izTzWdRGR3911I/MasyneXn/9CH373sfOnU2Jj18LwJEjTdm0qRWAJQtjcik/uqR6+lluOtAli/VXAHXdx0BgMoCIhACT3PX1gd4iYv0Ixqd58+YxcGA9kpOfAVKpWLEKMJvU1Pk89pglCmPyIscJQ0Q+FZEXRKSPiDTEz24tVV0M7M+iyLXAW+r4CagkIjWBVsAmVd2sqieAWW5ZY9Lt2rWLG264gauuuorU1HJABPAADzzwFtADELZuDW6MxhR1oqpZFxB5BDiqqs94LKsNNAOaA01V9Sq/diYSBXymqqfcFSUinwHjVPV79/lC4EEgCuiiqgPc5bcAF6rqnT62MRCndUKNGjWaz5o1y5+wfEpMTCQiIiLX9QuKxZVRamoqn3/+OVOmTCEpKYlbb72VRo16c/ToCcLCIqhVK5Ft25y4ypSBRo0CHqJP9j7mjMWVM3mJKyYmZrmqtvC5UlWzfAB/AOE+lg8AHsquvledKGBNJus+By7xeL4QJyH1BF7zWH4L8GJ2+2revLnmxaJFi/JUv6BYXP9au3attm7dWgEVEY2IiNCNGzfqjBmq4eGqoDphwiIF5/mMGQEPMVP2PuaMxZUzeYkLWKaZfK760yV1TFWP+lj+FnCzXynLP9uAszye1wJ2ZLHclFBJSUk88sgjNG7cmF9++QWA6667jt9//53zzjuP2FiYOhVq13bK167tPI+NDWLQxhQD/px/OCYiNVV1p+dCVT0hIsn5GMunwJ0iMgu4EDikqjtFZA9QV0TqANuBG4Gb8nG/pgj55ptvGDx4MBs3biQ8PJwqVaowefJkrr766gzlYmOdR1wcxMcHJVRjih1/EsYzwCci0lNV/0pbKCKnAan+7khE3gU6ANVEZBvwGBAKoKpTgHnAlcAm4CjQz12XLCJ3Al8CIcAbqrrW3/2a4mHv3r3ce++9vPXWW5xzzjksWLCA6tWrc9555xXKPmRjiqNsE4aqfiAi4cByEfkJWIlzdVVPYLS/O1LV3tmsV2BoJuvm4SQUU8KoKjNmzOCuu+7i4MGDANxzzz106tQpuIEZUwL5dVmtqr4J1AHex2kVJAG9VXVmAcZmSrhNmzbRsWNHbr31Vg4ePEhYWBgTJ05kyJAhwQ7NmBLJ76FBVDUB50S3MQXqxIkTTJgwgccff5zkZOc0Wbdu3Zg4cSK1atUKcnTGlFw2lpQpVJYsWcJtt93Ghg0buP7667nttts4ceIEXbt2DXZoxpR4ljBMoXDw4EFGjhzJK6+8QqlSpejcuTMffPBBsMMyxniwhGGCSlX58MMPueOOO9i7dy8AjRs35sknnwxyZMYYbzYfhgmav/76i2uuuYZevXqxb98+wsPDef7551m6dCnNmzcPdnjGGC/WwjABl5yczMSJE3nkkUcA+O9//8vGjRsZP348Z511Vja1jTHBYgnDBNTy5cu57bbbWLVqFdWrV+eXX34hKioq2GEZY/xgXVImIBITE7nnnnto2bIlv/32GyLCjTfeyBlnnBHs0IwxfrIWhilwn332GYMHD2b79u0ANGrUiNdff50WLXyPoGyMKZyshWEKzM6dO+nZsyfXXHMNERERnH322Tz77LMsX77ckoUxRZC1MEy+S01N5ZVXXuH+++/n2LFjjB49moceeohSpUpRurT9yRlTVNl/r8lXa9asoX///ixduhSA0047jV69elGmTJkgR2aMySvrkjJ5MnMmREXBTz8dp2LFh2nSpAnLli1DRBg6dCgbN27kP//5T7DDNMbkA2thmFybORMGDoSjRxfyzDO3kZCwHahCrVo1mT37DVq1ahXsEI0x+cgShsm1hx46xNGjI4A3SE2tiTMN+wWUKlWDVq3sT8uY4sa6pEyuzJ8/n7//vgB4A4CLL74WuBQ4k7//tmRhTHFkCcPkyMGDB+nbty9XXHEFsBuoCMygffte6WXOPjtY0RljClJAE4aIdBGR30Vkk4iM9LH+fhFZ6T7WiEiKiFRx18WLyG/uumWBjNs4Pv/8cxo0aMBbbznzaEVHX0lY2AYgFhEBIDwcxo4NYpDGmAITsIQhIiHAJOAKoD7QW0Tqe5ZR1fGqGq2q0cBDwLequt+jSIy73u76CqADBw7Qp08frr76aqpUqcLcuXN5++23WbFiLq++WpPatZ1ytWvD1KkQGxvceI0xBSOQnc2tgE2quhlARGYB1wLrMinfG3g3QLGZTMydO5fbbruNPXv2EBUVxdKlSylXrlz6+thY5xEXB/HxQQvTGBMAoqqB2ZHI9UAXVR3gPr8FuFBV7/RRNhzYBpyX1sIQkS3AAUCBV1R1qo96A4GBADVq1Gg+a9asXMebmJhIRERErusXlEDFdfjwYV588UW+/vprRIRy5coxfPhwLr/88vTup2DElVMWV85YXDlTHOOKiYlZnmkvjqoG5AH0BF7zeH4L8GImZW8A5notO8P9eRqwCmiX1f6aN2+uebFo0aI81S8ogYhrzpw5Wq1aNcVJztqlSxfdtm1b0OPKDYsrZyyunCmOcQHLNJPP1UCe9N4GeM6OUwvYkUnZG/HqjlLVHe7P3cAcnC4uk4/27t3LTTfdRPfu3Tn99NOpW7cu06ZNY968eZx55pnBDs8YE2SBPIexFKgrInWA7ThJ4SbvQiISCbQHbvZYVh4opaoJ7u+dgf8FJOoSYvbs2QwaNIgDBw7wyCOP8MgjjxASEkKpUnbltTHGEbCEoarJInIn8CUQAryhqmtFZLC7fopbtDuwQFWPeFSvAcxx+85LA++o6vxAxV6c7dmzh6FDh/LBBx9QqlQpwsPD6dq1K6GhocEOzRhTyAT0llxVnQfM81o2xev5dGC617LNQJMCDq/E+eCDDxg0aBAHDx4EoGPHjrz++us2r7YxxifrbyiB/vnnH66//np69erFyZMnCQ8PZ+rUqXz55ZeWLIwxmbJBf0oQVeW9995jyJAhHDlyhKeeeoprrrmGChUqcLaN52GMyYYljBJi165dDBkyhI8//phSpUrRo0cPRo48ZXQWY4zJlHVJFXOqysyZM6lXrx6ffPIJAB06dGD8+PFBjswYU9RYwijGdu7cSbdu3bj55ptJTEwkLCyMyZMn8/XXXxMVFRXs8IwxRYx1SRVDqsrbb7/NXXfdRVJSEo888girVq3i+eefp06dOsEOzxhTRFnCKGa2b9/OwIEDmTdvHlWrVmXlypVccMEFwQ7LGFMMWJdUMaGqTJs2jf/85z/Mn+/c09igQQNq1KgR5MiMMcWFJYxi4O+//+aKK66gf//+HD16lLJly/Liiy+yaNEiKlWqFOzwjDHFhHVJFWGqyhtvvMGIESM4efIklStXpmHDhkybNo1zzz032OEZY4oZSxhF1NatWxkwYABfffUVbdu2Zdq0aZQuXZqzzjrLBgw0xhQISxhFjKoydepURowYQVJSEgA33HCDtSiMMQXOEkYRMXMmPPhgPGFh97Fp0wpKlSpNaGhpxo8fz+DBg4MdnjGmBLCEUQTMmKHcdttrnDgxgpCQJHdpK554Yhr33HN+UGMzxpQc1tldyO3YsYPbb7+KEycGAq3o1+8JYAKpqYt56SVLFsaYwLGEUUipKu+88w4NGjQgKWkBzoy0X1Gv3oXAvUAIW7cGN0ZjTMliCaMQ2rNnDz179iQ2NpZjx44BClx+SjkbkdwYE0iWMAqZTz/9lAYNGjBnzhxEhDPOOINHH/2O8PD/4fl2hYfD2LHBi9MYU/IENGGISBcR+V1ENonIKZMxiEgHETkkIivdx6P+1i3qDh06RN++fbn22mupXr065cuXp2/fvqxatYoxYy5m6lSoXdspW7s2TJ0KsbHBjdkYU7IE7CopEQkBJgGdgG3AUhH5VFXXeRX9TlWvzmXdIunrr7+mX79+bN++nYcffpjHHnuMvXv3csYZZ6SXiY11HnFxEB8ftFCNMSVYIFsYrYBNqrpZVU8As4BrA1C30Dpy5Ah33nknnTp14sCBA6gqTZs2pUyZMhmShTHGFAaiqoHZkcj1QBdVHeA+vwW4UFXv9CjTAZiN04rYAdynqmv9qesuHwgMBKhRo0bzWbNm5TrexMREIiIicl0/O2vWrGHcuHFs376dsmXLkpqaysCBA+nRo0eWQ3sUdFy5ZXHljMWVMxZXzuQlrpiYmOWq2sLnSlUNyAPoCbzm8fwW4EWvMhWBCPf3K4GN/tb1fjRv3lzzYtGiRXmqn5mkpCR98MEHtVSpUhoZGamANmrUSFevXh3UuPLK4soZiytnLK6cyUtcwDLN5HM1kHd6bwPO8nheC6cVkU5VD3v8Pk9EXhaRav7ULQp+/fVXbr31VtasWcOAAQO49tprWbRoEWPHjqVcuXLBDs8YY7IUyISxFKgrInWA7cCNwE2eBUTkdOAfVVURaYVzjmUfcDC7uoVZcnIyTz31FGPGjCE8PJybbrqJV199FYCrr746m9rGGFM4BCxhqGqyiNwJfAmEAG+oc35isLt+CnA9MEREkoFjwI1uE8ln3UDFnhfr16+nT58+LF26lOrVq7Nnzx5SUlJITU21YciNMUVKQAcfVNV5wDyvZVM8fn8JeMnfuoVZamoqL7zwAg899BChoaGEhYWRlJTEm2++yS233IKIBDtEY4zJEfuKWwC2bNlCTEwMI0aM4JJLLuHEiRM0a9aMVatWceutt1qyMMYUSTa8eT5SVV577TVGjBiBqjJt2jT69OnDzz//TIsWLShd2g63MaboshZGPtmxYwdXXXUVAwcOpEqVKhw9epRq1aohIlx00UWWLIwxRZ4ljDxSdxjyhg0b8s0333DmmWeydetWBg0aRExMTLDDM8aYfGMJIw/27NlDr169iI2NpVKlSgCcOHGCTz/9lMmTJ1O+fPngBmiMMfnI+kly6dNPP+X222/n4MGDjBs3jpo1a/L+++/z+uuvU6NGjWCHZ4wx+c4SRg4dOnSIu+++m+nTpxMVFcWwYcN48MEHUVW7XNYYU6xZl1QOLFy4kEaNGvHWW2/RpEkT4uPjWbBgAampqYiIJQtjTLFmCcMPacOQX3bZZemz4K1evZqHH36Yr7/+2u7YNsaUCNYllY0lS5bQp08fNm3aRL9+/Xj77bc588wz+fbbb2nbtm2wwzPGmICxhJGJEydOMHLkSMaPH0+tWrVYtGgRHTp04LLLLuOqq64iMjIy2CEaY0xAWcLw4ddff2Xw4MFs2bKFtm3bsnr1asqUKQPATTcVmUFyjTEmX1nnu5cxM8fQrEUz9uzbQ5naZfjuu+9o1qwZZ511VvaVjTGmGLOE4WHmbzP5vy3/Bw2d5ye2nSD0ilD6PdfPEoYxpsSzhOFh1MJRHEs5BtWhfMXycDucvPAkjyx6JNihGWNM0FnC8LD10FbnlzZw9xN3w+ley40xpgSzhOHh7MiznV9KQWiZ0FOXG2NMCWYJw8PYjmMJDw3PsCw8NJyxHccGKSJjjCk8ApowRKSLiPwuIptEZKSP9bEistp9LBGRJh7r4kXkNxFZKSLLCiK+2EaxTL1mKrUjawNQO7I2U6+ZSmyj2ILYnTHGFCkBuw9DREKASUAnYBuwVEQ+VdV1HsW2AO1V9YCIXAFMBS70WB+jqnsLMs7YRrHENoolLi6O+N7xBbkrY4wpUgLZwmgFbFLVzap6ApgFXOtZQFWXqOoB9+lPQK0AxmeMMSYLgUwYZwJ/ezzf5i7LzG3AFx7PFVggIstFZGABxGeMMSYLoqqB2ZFIT+ByVR3gPr8FaKWqw3yUjQFeBi5R1X3usjNUdYeInAZ8BQxT1cVe9QYCAwFq1KjRfNasWbmONzExkYiIiFzXLygWV85YXDljceVMcYwrJiZmuaq28LlSVQPyAFoDX3o8fwh4yEe5xsCfwPlZbGs0cF9W+2vevLnmxaJFi/JUv6BYXDljceWMxZUzxTEuYJlm8rkayC6ppUBdEakjImWAG4FPPQuIyNnAR8AtqvqHx/LyIlIh7XegM7AmYJEbY4wJ3FVSqposIncCXwIhwBuqulZEBrvrpwCPAlWBl93Z65LVaRrVAOa4y0oD76jq/EDFbowxJsDDm6vqPGCe17IpHr8PAAb4qLcZaOK93BhjTODYnd7GGGP8YgnDGGOMXyxhGGOM8YslDGOMMX6xhGGMMcYvljCMMcb4xRKGMcYYv1jCMMYY4xdLGMYYY/xiCcMYY4xfLGEYY4zxiyUMY4wxfrGEYYwxxi+WMIwxxvjFEoYxxhi/WMIwxhjjF0sYxhhj/GIJwxhjjF8CmjBEpIuI/C4im0RkpI/1IiIT3fWrRaSZv3WNMcYUrIAlDBEJASYBVwD1gd4iUt+r2BVAXfcxEJicg7rGGGMKUCBbGK2ATaq6WVVPALOAa73KXAu8pY6fgEoiUtPPusYYYwpQIBPGmcDfHs+3ucv8KeNPXWOMMQWodAD3JT6WqZ9l/KmLiAzE6coCSBSR33MUYUbVgL15qF9QLK6csbhyxuLKmeIYV+3MVgQyYWwDzvJ4XgvY4WeZMn7URVWnAlPzI1gRWaaqLfJjW/nJ4soZiytnLK6cKWlxBbJLailQV0TqiEgZ4EbgU68ynwK3uldLXQQcUtWdftY1xhhTgALWwlDVZBG5E/gSCAHeUNW1IjLYXT8FmAdcCWwCjgL9sqobqNiNMcYEtksKVZ2HkxQ8l03x+F2Bof7WLWD50rVVACyunLG4csbiypkSFZc4n9HGGGNM1mxoEGOMMX6xhOGlMA5BIiJnicgiEVkvImtF5K5gx+RJREJE5FcR+SzYsaQRkUoi8qGIbHCPW+tgxwQgIve47+EaEXlXRMoFMZY3RGS3iKzxWFZFRL4SkY3uz8qFJK7x7nu5WkTmiEilwhCXx7r7RERFpFphiUtEhrmfZWtF5On82JclDA+FeAiSZOBeVf0PcBEwtJDEleYuYH2wg/DyAjBfVesBTSgE8YnImcBwoIWqNsS5gOPGIIY0HejitWwksFBV6wIL3eeBNp1T4/oKaKiqjYE/gIcCHRS+40JEzgI6AVsDHZBrOl5xiUgMzmgYjVW1ATAhP3ZkCSOjQjkEiaruVNUV7u8JOB9+heJOdxGpBVwFvBbsWNKISEWgHfA6gKqeUNWDQQ3qX6WBMBEpDYTj436iQFHVxcB+r8XXAm+6v78JdAtkTOA7LlVdoKrJ7tOfcO7FCnpcrueAB/BxM3EgZBLXEGCcqh53y+zOj31Zwsio0A9BIiJRQFPg5yCHkuZ5nH+W1CDH4ekcYA8wze0qe01Eygc7KFXdjvNNbyuwE+c+owXBjeoUNdx7n3B/nhbkeHzpD3wR7CAARKQrsF1VVwU7Fi/nA21F5GcR+VZEWubHRi1hZOTXECTBIiIRwGzgblU9XAjiuRrYrarLgx2Ll9JAM2CyqjYFjhCcrpUM3PMB1wJ1gDOA8iJyc3CjKlpEZBROF+3MQhBLODAKeDTYsfhQGqiM04V9P/C+iPj6fMsRSxgZ+TN8SVCISChOspipqh8FOx5XG6CriMTjdN9dKiIzghsS4LyP21Q1rRX2IU4CCbbLgC2qukdVTwIfARcHOSZv/7gjROP+zJeujPwgIn2Aq4FYLRz3A5yLk/xXuf8DtYAVInJ6UKNybAM+ckf+/gWnByDPJ+QtYWRUKIcgcb8ZvA6sV9Vngx1PGlV9SFVrqWoUzrH6RlWD/o1ZVXcBf4vIBe6ijsC6IIaUZitwkYiEu+9pRwrByXgvnwJ93N/7AJ8EMZZ0ItIFeBDoqqpHgx0PgKr+pqqnqWqU+z+wDWjm/v0F28fApQAicj7OeHx5HiTREoYH96Ra2hAk64H3C8kQJG2AW3C+wa90H1cGO6hCbhgwU0RWA9HAk8ENB9wWz4fACuA3nP+/oN0pLCLvAj8CF4jINhG5DRgHdBKRjThX/owrJHG9BFQAvnL//qdkuZHAxRV0mcT1BnCOe6ntLKBPfrTK7E5vY4wxfrEWhjHGGL9YwjDGGOMXSxjGGGP8YgnDGGOMXyxhGGOM8YslDFNiiUh3d4TRejmo84KIbBeRTP93RKSpiPgcW0tE4oMxoqm776tFZEww9m2KB0sYpiTrDXyPnyPGukmiO854Y+2yKPow8GKeo8s6ltzMlvk5zp354fkdjykZLGGYEskdl6sNcBseCUNEyonINBH5zR24MMajWgywBpiMk2x8bbcCzpDSq9znVUVkgbutV/AYr0xEbhaRX9wb0V5xh9dHRG4TkT9EJE5EXhWRl9zl00XkWRFZBPyfiJwrIvNFZLmIfJfWUhKR6iIyW0SWuo82kD4FchzO8BrG5JglDFNSdcOZL+MPYL+IpI01NRRAVRvhJIU35d9JjnoD7wJzgKvd8b28tcBJKmkeA753B0H8FDgbQET+A9wAtFHVaCAFiBWRM4BHcAaN6wR4d5edD1ymqvfi3CU+TFWbA/cBL7tlXgCeU9WWwHVkHHp+GdA226NjjA+5adYaUxz0xhmaHZyhE3rjDNlxCW53kqpuEJG/gPNFZANwJXCPqiaIyM9AZ5xuHk81cYZWT9MO6OFu73MROeAu7wg0B5a6g4iG4Qz01wr4VlX3A4jIBzhJIs0HqpritpAuBj7wGIS0rPvzMqC+x/KKIlLBnUtlN85IucbkmCUMU+KISFWcgdkaiojizHynIvIAvoe4B2dGs0jgN/eDOBw4yqkJ4xjgPe2qr/F3BHhTVTPMHCci3bMJ/4j7sxRw0G2deCsFtFbVYz7WlXNjNCbHrEvKlETXA2+pam13pNGzgC04rYvFQCykj/J5NvA7TgtkgMfIpHWAzj5OIK8HzvN47rm9K3DmKABn+tPrReQ0d10VEakN/AK0F5HK7ont63y9AHc+lC0i0tOtLyLSxF29AGcQTdx10R5Vzydjl5kxfrOEYUqi3jjnITzNBm7COQ8QIiK/Ae8BfXFaIJfj0ZpQ1SM4V1hd47kRVd0ARLonvwHGAO1EZAVOF9ZWt9w64L/AAndE3a+Amu6sfE/izKj4Nc6w7IcyeR2xwG0isgpYy7/TCQ8HWojIahFZBwz2qBPDqa0iY/xio9Uak89E5B4gQVVzNc+5iESoaqLbwpgDvKGq3gkuN9utAbyjqh3zui1TMlkLw5j8Nxk4nof6o0VkJU7X0RacyXDyw9nAvfm0LVMCWQvDGGOMX6yFYYwxxi+WMIwxxvjFEoYxxhi/WMIwxhjjF0sYxhhj/GIJwxhjjF/+H/tRWrS7ueH7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0065\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cl: 0.0043\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0074\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cl: 0.0111\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0043\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cl: 0.0057\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cl: 0.0044\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cl: 0.0043\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cl: 0.0327\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = [0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03]\n",
    "beta = [0, 30, 45, 60, 75, 90, 30, 45, 60, 75, 90, 30, 45, 60, 75, 90]\n",
    "for i in range(0, 16):\n",
    "    # Index from each dataset\n",
    "    iTrain = []\n",
    "    iVal = []\n",
    "    iTest = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    alpha_train = []\n",
    "    alpha_val = []\n",
    "    alpha_test = []\n",
    "    \n",
    "    predictedValue = model.predict([x[n_alpha*i:n_alpha*(i+1),:], x_para[n_alpha*i:n_alpha*(i+1),:]])\n",
    "    y_corres = y[n_alpha*i:n_alpha*(i+1),:]\n",
    "    \n",
    "    l2_error_Cl = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    if i==0:\n",
    "        print('NACA0018 airfoil without Gurney flap\\nL2 error of Cl: {0:0.4f}'.format(l2_error_Cl))\n",
    "    else:\n",
    "        print('L2 error of Cl: {0:0.4f}'.format(l2_error_Cl))\n",
    "    \n",
    "    cl = predicted[n_alpha*i:n_alpha*(i+1)]*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "    CL = y[n_alpha*i:n_alpha*(i+1)]*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*n_alpha) & ((index_train>=i*n_alpha)))]):\n",
    "        iTrain.append(predicted[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*n_alpha) & ((index_val>=i*n_alpha)))]):\n",
    "        iVal.append(predicted[index])    \n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*n_alpha) & (index_test>=i*n_alpha))]):\n",
    "        iTest.append(predicted[index])\n",
    "        \n",
    "    iTrain = np.array(iTrain)*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "    iTest = np.array(iTest)*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "    iVal = np.array(iVal)*(np.max(cl_orig)-np.min(cl_orig))+np.min(cl_orig)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*n_alpha) & ((index_train>=i*n_alpha)))]):\n",
    "        alpha_train.append(aa[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*n_alpha) & ((index_val>=i*n_alpha)))]):\n",
    "        alpha_val.append(aa[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*n_alpha) & ((index_test>=i*n_alpha)))]):\n",
    "        alpha_test.append(aa[index])\n",
    "        \n",
    "    aTrain = np.array(alpha_train)*np.max(alpha)\n",
    "    aVal = np.array(alpha_val)*np.max(alpha)\n",
    "    aTest = np.array(alpha_test)*np.max(alpha)\n",
    "    \n",
    "    CL_trainTestSplit_Plot(i, CL, cl, aTrain, aTest, iTrain, iTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805a612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ac6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c3f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad9cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cb680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

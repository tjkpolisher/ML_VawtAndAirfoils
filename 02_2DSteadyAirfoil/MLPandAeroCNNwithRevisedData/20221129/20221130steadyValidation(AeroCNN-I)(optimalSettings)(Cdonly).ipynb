{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc91fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the steady-state simulation - Case 2: AeroCNN-I\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1130c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining parameters and hyperparameters of the model\n",
    "\n",
    "n_kernels=100 # Number of kernels in convolutional network\n",
    "n_units=128 # Number of units in the hidden layer of the MLP network\n",
    "input_size = 100 + 3 # Size of input for the network (100 coefficients and 3 other parameters, AoA, h, beta)\n",
    "lr = 1e-04 # Learning rate of the network\n",
    "test_rate=0.1 # Defines the ratio of training dataset and test dataset\n",
    "val_rate=0.2\n",
    "n_data = 16 # Number of txt files from which the aerodynamic coefficients are extracted\n",
    "batch_size = 20 # Mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d84b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing working directory\n",
    "\n",
    "main_directory = 'D:\\\\VAWT_data\\\\flap_steady\\\\flap_steady'\n",
    "os.chdir(main_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic parameters\n",
    "\n",
    "c = 1 # Chord length\n",
    "h = np.array([0.01, 0.02, 0.03]) * c # Height of the Gurney flaps\n",
    "t = 0.02 * h # Thickness of the Gurney flaps\n",
    "alpha = np.linspace(0, 16, 9).reshape((9,1)) # Angles of attack\n",
    "beta = np.linspace(30, 90, 5).reshape((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18aaa915",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h.reshape((-1,1))\n",
    "t = t.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9745480",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alpha = alpha.shape[0] # Number of the angles of attack\n",
    "n_beta = beta.shape[0] # Number of the Gurney flap inclination\n",
    "n_h = h.shape[0] # Number of the height of the Gurney flaps\n",
    "n_cases = n_data * n_alpha # Total number of cases(Number of geometries * Number of angles of attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d7f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Input dataset\n",
    "# Defining the angles of attack\n",
    "\n",
    "aa = np.zeros((n_cases,1))\n",
    "for i in range(0, n_data):\n",
    "    aa[n_alpha*i:n_alpha*(i+1),:] = alpha[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5014fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = aa / np.max(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa96208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937cc8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 1)\n"
     ]
    }
   ],
   "source": [
    "# Defining beta, the Gurney flap inclination\n",
    "# In case of mere NACA0018, the bb in those indexes are considered as zero.\n",
    "beta_0 = np.zeros((n_alpha,1)) # Values for sheer NACA0018\n",
    "b_ = np.ones((n_alpha,1)) # Template for the inclination for a single h and single beta\n",
    "bb_imp = np.zeros((n_alpha*n_beta,1))\n",
    "\n",
    "for j in range(n_beta):\n",
    "    b_imp = b_ * beta[j]\n",
    "    bb_imp[n_alpha*j:n_alpha*(j+1),:] = b_imp[:,:]\n",
    "    \n",
    "bb_imp = bb_imp.reshape((-1,1))\n",
    "\n",
    "bb = np.vstack((beta_0, bb_imp, bb_imp, bb_imp))\n",
    "bb = bb / np.max(beta)\n",
    "    \n",
    "print(bb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6302058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Gurney flap height\n",
    "# In case of mere NACA0018, the hh in those indexes are considered as zero.\n",
    "\n",
    "hh = np.concatenate((np.zeros(n_alpha), h[0]*np.ones(n_beta*n_alpha), h[1]*np.ones(n_beta*n_alpha), h[2]*np.ones(n_beta*n_alpha)))\n",
    "hh = hh.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "174336d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = hh / np.max(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5737c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the coordinates of NACA0018 (airfoil15)\n",
    "origin_coord = \"D:\\\\AirfoilClCdCoordinates_out\\\\AirfoilClCdCoordinates_out\\\\airfoil15\"\n",
    "\n",
    "csv_file_name = origin_coord + '\\\\airfoilOut15.txt'\n",
    "data = pd.read_csv(csv_file_name, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41daf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_coord = data.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd4c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_coord = baseline_coord.reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba48669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 100)\n"
     ]
    }
   ],
   "source": [
    "airfoil_coord = np.repeat(standard_coord, n_cases, axis=0)\n",
    "print(airfoil_coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492ab857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rows mean the number of points at the Gurney flap\n",
    "# and the columns mean the number of the cases\n",
    "flap_left = np.zeros((15,5))\n",
    "flap_right = np.zeros((15,5))\n",
    "\n",
    "for i in range(n_h):\n",
    "    # Defining coordinates of the flaps with respect to beta=90 degree.\n",
    "    yLeft = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "    yRight = np.linspace(-h[i]/5, -h[i], 5).reshape((-1,1))\n",
    "    xLeft = 0.5*np.ones((5,1)) - 0.02*h[i]\n",
    "    xRight = 0.5*np.ones((5,1))\n",
    "    \n",
    "    for j in range(n_beta):\n",
    "        betaValue = beta[j]\n",
    "        \n",
    "        # Rotating transformation\n",
    "        rotateTransf = np.array([[np.cos(90-betaValue), -np.sin(90-betaValue)],\n",
    "                                 [np.sin(90-betaValue), np.cos(90-betaValue)]])\n",
    "        rotateTransf = rotateTransf.reshape((2,2))\n",
    "        \n",
    "        LeftImp = np.hstack((xLeft-0.5, yLeft))\n",
    "        RightImp = np.hstack((xRight-0.5, yRight))\n",
    "        \n",
    "        rotatedFlapLeft = rotateTransf @ LeftImp.T # shape: 2*5 (x-coordinates on first row, y-coordinates on second row)\n",
    "        rotatedFlapRight = rotateTransf @ RightImp.T\n",
    "        \n",
    "        # All we need is the y-coordinates of the flaps\n",
    "        flap_left[5*i+j,:] = rotatedFlapLeft[1,:]\n",
    "        flap_right[5*i+j,:] = rotatedFlapRight[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "790fb777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10)\n"
     ]
    }
   ],
   "source": [
    "# Combining y-coordinates from the left and the right side of the flaps\n",
    "flap_coords = np.hstack((flap_left, np.flip(flap_right, axis=1)))\n",
    "print(flap_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66f6855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 10)\n"
     ]
    }
   ],
   "source": [
    "# Placing the flap_coords into total coordinate variable\n",
    "# Total coordinate = Airfoil coordinates + flap coordinates\n",
    "flap_coords2 = np.zeros((n_cases, 10))\n",
    "for i in range(n_alpha, n_cases):\n",
    "    flap_coords2[i,:] = flap_coords[i%15,:]\n",
    "    \n",
    "print(flap_coords2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8323888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 110)\n"
     ]
    }
   ],
   "source": [
    "total_coords = np.hstack((airfoil_coord, flap_coords2))\n",
    "print(total_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d72e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the coordinates, in order to apply convolutional operation\n",
    "x = total_coords.reshape((16*9, 2, 55, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c207ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input of parameters\n",
    "# These variables are put into the network after convolution and flattening\n",
    "x_para = np.hstack((aa, hh, bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b41130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating output dataset - Cl and Cd\n",
    "files_orig = os.listdir(main_directory)\n",
    "files_target = [file for file in files_orig if file.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4c62765",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame()\n",
    "for file in files_target:\n",
    "    data = pd.read_table(file, header=None)\n",
    "    target_df = pd.concat([target_df, data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac849035",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_orig = target_df.iloc[:,3].values # Cd values\n",
    "#target_c4 = target_df.iloc[:,4].values# Cl values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d97485dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = (cd_orig-np.min(cd_orig))/(np.max(cd_orig)-np.min(cd_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93bba2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cd.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7444ef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec562b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_para_all, x_para_test, x_all, x_test, y_all, y_test = train_test_split(x_para, x, y, test_size=test_rate, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de373720",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_para_train, x_para_val, x_train, x_val, y_train, y_val = train_test_split(x_para_all, x_all, y_all, test_size=val_rate/(1-test_rate), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82d60c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37290049",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tf.keras.Input(shape=(2,55,1))\n",
    "input_2 = tf.keras.Input(shape=(3))\n",
    "\n",
    "x_conv1 = tf.keras.layers.Conv2D(filters=n_kernels, kernel_size=(2,2), strides=1, padding='same',\n",
    "                                 activation='relu', name='convLayer')(input_1)\n",
    "x_pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))(x_conv1)\n",
    "\n",
    "x_flat = tf.keras.layers.Flatten()(x_pool)\n",
    "x_concat = tf.keras.layers.Concatenate()([x_flat, input_2])\n",
    "\n",
    "x_fc1 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc1')(x_concat)\n",
    "x_fc2 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc2')(x_fc1)\n",
    "x_fc3 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc3')(x_fc2)\n",
    "x_fc4 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc4')(x_fc3)\n",
    "x_fc5 = tf.keras.layers.Dense(units=n_units, activation='relu', name='fc5')(x_fc4)\n",
    "\n",
    "output_data = tf.keras.layers.Dense(units=1, activation='linear', name='outputLayer')(x_fc5)\n",
    "# AeroCNN-I\n",
    "model = tf.keras.Model([input_1, input_2], output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3c482f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2, 55, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " convLayer (Conv2D)             (None, 2, 55, 100)   500         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 27, 100)   0           ['convLayer[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2700)         0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2703)         0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " fc1 (Dense)                    (None, 128)          346112      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 128)          16512       ['fc1[0][0]']                    \n",
      "                                                                                                  \n",
      " fc3 (Dense)                    (None, 128)          16512       ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      " fc4 (Dense)                    (None, 128)          16512       ['fc3[0][0]']                    \n",
      "                                                                                                  \n",
      " fc5 (Dense)                    (None, 128)          16512       ['fc4[0][0]']                    \n",
      "                                                                                                  \n",
      " outputLayer (Dense)            (None, 1)            129         ['fc5[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 412,789\n",
      "Trainable params: 412,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30f3abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss = tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcb484c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221130\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf477620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = model_directory + \"20221130steadyValidation_MLP_val_\"+str(val_rate) + \"_test\"+str(test_rate)+ \"_\" + str(n_units) +\"units_OptimalSettings_checkpoint.h5\"\n",
    "\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(ckpt_name, monitor=\"val_loss\", mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1000, min_delta=6e-7,\n",
    "                                      restore_best_weights=True, verbose=1)\n",
    "rp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=200, factor=0.5,\n",
    "                                          min_delta = 1e-09, min_lr=1e-06, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4cc904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = len(x_train)//batch_size\n",
    "VALIDATION_STEPS = len(x_val)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17d6b26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/5 [=====>........................] - ETA: 10s - loss: 0.1161 - rmse: 0.3407\n",
      "Epoch 1: val_loss improved from inf to 0.09201, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 3s 59ms/step - loss: 0.1336 - rmse: 0.3655 - val_loss: 0.0920 - val_rmse: 0.3033 - lr: 1.0000e-04\n",
      "Epoch 2/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1601 - rmse: 0.4001\n",
      "Epoch 2: val_loss improved from 0.09201 to 0.08322, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1240 - rmse: 0.3521 - val_loss: 0.0832 - val_rmse: 0.2885 - lr: 1.0000e-04\n",
      "Epoch 3/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1556 - rmse: 0.3945\n",
      "Epoch 3: val_loss improved from 0.08322 to 0.07357, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1136 - rmse: 0.3370 - val_loss: 0.0736 - val_rmse: 0.2712 - lr: 1.0000e-04\n",
      "Epoch 4/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.1197 - rmse: 0.3460\n",
      "Epoch 4: val_loss improved from 0.07357 to 0.06236, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1021 - rmse: 0.3196 - val_loss: 0.0624 - val_rmse: 0.2497 - lr: 1.0000e-04\n",
      "Epoch 5/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0931 - rmse: 0.3052\n",
      "Epoch 5: val_loss improved from 0.06236 to 0.05014, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0901 - rmse: 0.3002 - val_loss: 0.0501 - val_rmse: 0.2239 - lr: 1.0000e-04\n",
      "Epoch 6/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0582 - rmse: 0.2412\n",
      "Epoch 6: val_loss improved from 0.05014 to 0.03968, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0761 - rmse: 0.2759 - val_loss: 0.0397 - val_rmse: 0.1992 - lr: 1.0000e-04\n",
      "Epoch 7/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0939 - rmse: 0.3065\n",
      "Epoch 7: val_loss improved from 0.03968 to 0.03332, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0649 - rmse: 0.2548 - val_loss: 0.0333 - val_rmse: 0.1825 - lr: 1.0000e-04\n",
      "Epoch 8/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0537 - rmse: 0.2318\n",
      "Epoch 8: val_loss improved from 0.03332 to 0.03269, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0575 - rmse: 0.2397 - val_loss: 0.0327 - val_rmse: 0.1808 - lr: 1.0000e-04\n",
      "Epoch 9/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0603 - rmse: 0.2455\n",
      "Epoch 9: val_loss did not improve from 0.03269\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0562 - rmse: 0.2371 - val_loss: 0.0339 - val_rmse: 0.1842 - lr: 1.0000e-04\n",
      "Epoch 10/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0490 - rmse: 0.2213\n",
      "Epoch 10: val_loss improved from 0.03269 to 0.03216, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0551 - rmse: 0.2348 - val_loss: 0.0322 - val_rmse: 0.1793 - lr: 1.0000e-04\n",
      "Epoch 11/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0756 - rmse: 0.2750\n",
      "Epoch 11: val_loss improved from 0.03216 to 0.03099, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0534 - rmse: 0.2312 - val_loss: 0.0310 - val_rmse: 0.1760 - lr: 1.0000e-04\n",
      "Epoch 12/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0625 - rmse: 0.2499\n",
      "Epoch 12: val_loss improved from 0.03099 to 0.02962, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0525 - rmse: 0.2291 - val_loss: 0.0296 - val_rmse: 0.1721 - lr: 1.0000e-04\n",
      "Epoch 13/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0557 - rmse: 0.2361\n",
      "Epoch 13: val_loss improved from 0.02962 to 0.02903, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0514 - rmse: 0.2267 - val_loss: 0.0290 - val_rmse: 0.1704 - lr: 1.0000e-04\n",
      "Epoch 14/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0436 - rmse: 0.2088\n",
      "Epoch 14: val_loss improved from 0.02903 to 0.02797, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0500 - rmse: 0.2236 - val_loss: 0.0280 - val_rmse: 0.1672 - lr: 1.0000e-04\n",
      "Epoch 15/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0601 - rmse: 0.2451\n",
      "Epoch 15: val_loss improved from 0.02797 to 0.02667, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0486 - rmse: 0.2204 - val_loss: 0.0267 - val_rmse: 0.1633 - lr: 1.0000e-04\n",
      "Epoch 16/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0552 - rmse: 0.2349\n",
      "Epoch 16: val_loss improved from 0.02667 to 0.02560, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0470 - rmse: 0.2167 - val_loss: 0.0256 - val_rmse: 0.1600 - lr: 1.0000e-04\n",
      "Epoch 17/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0265 - rmse: 0.1627\n",
      "Epoch 17: val_loss improved from 0.02560 to 0.02430, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0452 - rmse: 0.2126 - val_loss: 0.0243 - val_rmse: 0.1559 - lr: 1.0000e-04\n",
      "Epoch 18/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0649 - rmse: 0.2548\n",
      "Epoch 18: val_loss improved from 0.02430 to 0.02328, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0435 - rmse: 0.2086 - val_loss: 0.0233 - val_rmse: 0.1526 - lr: 1.0000e-04\n",
      "Epoch 19/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0270 - rmse: 0.1644\n",
      "Epoch 19: val_loss improved from 0.02328 to 0.02171, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0415 - rmse: 0.2037 - val_loss: 0.0217 - val_rmse: 0.1473 - lr: 1.0000e-04\n",
      "Epoch 20/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0281 - rmse: 0.1676\n",
      "Epoch 20: val_loss improved from 0.02171 to 0.02134, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0391 - rmse: 0.1978 - val_loss: 0.0213 - val_rmse: 0.1461 - lr: 1.0000e-04\n",
      "Epoch 21/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0623 - rmse: 0.2496\n",
      "Epoch 21: val_loss improved from 0.02134 to 0.02043, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0388 - rmse: 0.1970 - val_loss: 0.0204 - val_rmse: 0.1429 - lr: 1.0000e-04\n",
      "Epoch 22/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0237 - rmse: 0.1538\n",
      "Epoch 22: val_loss improved from 0.02043 to 0.01835, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0360 - rmse: 0.1896 - val_loss: 0.0184 - val_rmse: 0.1355 - lr: 1.0000e-04\n",
      "Epoch 23/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0312 - rmse: 0.1767\n",
      "Epoch 23: val_loss improved from 0.01835 to 0.01805, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0344 - rmse: 0.1856 - val_loss: 0.0181 - val_rmse: 0.1344 - lr: 1.0000e-04\n",
      "Epoch 24/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0336 - rmse: 0.1833\n",
      "Epoch 24: val_loss improved from 0.01805 to 0.01594, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0314 - rmse: 0.1771 - val_loss: 0.0159 - val_rmse: 0.1262 - lr: 1.0000e-04\n",
      "Epoch 25/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0159 - rmse: 0.1260\n",
      "Epoch 25: val_loss improved from 0.01594 to 0.01449, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0296 - rmse: 0.1721 - val_loss: 0.0145 - val_rmse: 0.1204 - lr: 1.0000e-04\n",
      "Epoch 26/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0348 - rmse: 0.1865\n",
      "Epoch 26: val_loss improved from 0.01449 to 0.01444, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0269 - rmse: 0.1641 - val_loss: 0.0144 - val_rmse: 0.1202 - lr: 1.0000e-04\n",
      "Epoch 27/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0251 - rmse: 0.1584\n",
      "Epoch 27: val_loss improved from 0.01444 to 0.01241, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0252 - rmse: 0.1589 - val_loss: 0.0124 - val_rmse: 0.1114 - lr: 1.0000e-04\n",
      "Epoch 28/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0443 - rmse: 0.2104\n",
      "Epoch 28: val_loss improved from 0.01241 to 0.01117, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0229 - rmse: 0.1514 - val_loss: 0.0112 - val_rmse: 0.1057 - lr: 1.0000e-04\n",
      "Epoch 29/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0111 - rmse: 0.1054\n",
      "Epoch 29: val_loss improved from 0.01117 to 0.01034, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0208 - rmse: 0.1442 - val_loss: 0.0103 - val_rmse: 0.1017 - lr: 1.0000e-04\n",
      "Epoch 30/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0203 - rmse: 0.1426\n",
      "Epoch 30: val_loss improved from 0.01034 to 0.00969, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0199 - rmse: 0.1411 - val_loss: 0.0097 - val_rmse: 0.0984 - lr: 1.0000e-04\n",
      "Epoch 31/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0056 - rmse: 0.0751\n",
      "Epoch 31: val_loss improved from 0.00969 to 0.00841, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0189 - rmse: 0.1373 - val_loss: 0.0084 - val_rmse: 0.0917 - lr: 1.0000e-04\n",
      "Epoch 32/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0199 - rmse: 0.1410\n",
      "Epoch 32: val_loss did not improve from 0.00841\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0160 - rmse: 0.1266 - val_loss: 0.0099 - val_rmse: 0.0994 - lr: 1.0000e-04\n",
      "Epoch 33/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0138 - rmse: 0.1176\n",
      "Epoch 33: val_loss improved from 0.00841 to 0.00763, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0143 - rmse: 0.1197 - val_loss: 0.0076 - val_rmse: 0.0874 - lr: 1.0000e-04\n",
      "Epoch 34/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0173 - rmse: 0.1317\n",
      "Epoch 34: val_loss did not improve from 0.00763\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0135 - rmse: 0.1162 - val_loss: 0.0095 - val_rmse: 0.0976 - lr: 1.0000e-04\n",
      "Epoch 35/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0159 - rmse: 0.1260\n",
      "Epoch 35: val_loss improved from 0.00763 to 0.00659, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0127 - rmse: 0.1125 - val_loss: 0.0066 - val_rmse: 0.0812 - lr: 1.0000e-04\n",
      "Epoch 36/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0108 - rmse: 0.1037\n",
      "Epoch 36: val_loss did not improve from 0.00659\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0118 - rmse: 0.1085 - val_loss: 0.0068 - val_rmse: 0.0825 - lr: 1.0000e-04\n",
      "Epoch 37/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0108 - rmse: 0.1040\n",
      "Epoch 37: val_loss did not improve from 0.00659\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0110 - rmse: 0.1048 - val_loss: 0.0072 - val_rmse: 0.0850 - lr: 1.0000e-04\n",
      "Epoch 38/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0062 - rmse: 0.0790\n",
      "Epoch 38: val_loss improved from 0.00659 to 0.00620, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0099 - rmse: 0.0993 - val_loss: 0.0062 - val_rmse: 0.0788 - lr: 1.0000e-04\n",
      "Epoch 39/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0057 - rmse: 0.0755\n",
      "Epoch 39: val_loss did not improve from 0.00620\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0101 - rmse: 0.1004 - val_loss: 0.0078 - val_rmse: 0.0882 - lr: 1.0000e-04\n",
      "Epoch 40/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0616\n",
      "Epoch 40: val_loss improved from 0.00620 to 0.00592, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0085 - rmse: 0.0925 - val_loss: 0.0059 - val_rmse: 0.0769 - lr: 1.0000e-04\n",
      "Epoch 41/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0497\n",
      "Epoch 41: val_loss did not improve from 0.00592\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0081 - rmse: 0.0900 - val_loss: 0.0069 - val_rmse: 0.0831 - lr: 1.0000e-04\n",
      "Epoch 42/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0068 - rmse: 0.0827\n",
      "Epoch 42: val_loss improved from 0.00592 to 0.00588, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0083 - rmse: 0.0909 - val_loss: 0.0059 - val_rmse: 0.0767 - lr: 1.0000e-04\n",
      "Epoch 43/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0067 - rmse: 0.0820\n",
      "Epoch 43: val_loss improved from 0.00588 to 0.00572, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0074 - rmse: 0.0862 - val_loss: 0.0057 - val_rmse: 0.0756 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0631\n",
      "Epoch 44: val_loss did not improve from 0.00572\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0071 - rmse: 0.0844 - val_loss: 0.0061 - val_rmse: 0.0783 - lr: 1.0000e-04\n",
      "Epoch 45/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0072 - rmse: 0.0847\n",
      "Epoch 45: val_loss improved from 0.00572 to 0.00566, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0070 - rmse: 0.0839 - val_loss: 0.0057 - val_rmse: 0.0752 - lr: 1.0000e-04\n",
      "Epoch 46/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0090 - rmse: 0.0951\n",
      "Epoch 46: val_loss did not improve from 0.00566\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - rmse: 0.0833 - val_loss: 0.0069 - val_rmse: 0.0833 - lr: 1.0000e-04\n",
      "Epoch 47/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0054 - rmse: 0.0735\n",
      "Epoch 47: val_loss improved from 0.00566 to 0.00507, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0062 - rmse: 0.0790 - val_loss: 0.0051 - val_rmse: 0.0712 - lr: 1.0000e-04\n",
      "Epoch 48/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0074 - rmse: 0.0859\n",
      "Epoch 48: val_loss did not improve from 0.00507\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0063 - rmse: 0.0794 - val_loss: 0.0058 - val_rmse: 0.0762 - lr: 1.0000e-04\n",
      "Epoch 49/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0065 - rmse: 0.0808\n",
      "Epoch 49: val_loss improved from 0.00507 to 0.00502, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0061 - rmse: 0.0783 - val_loss: 0.0050 - val_rmse: 0.0708 - lr: 1.0000e-04\n",
      "Epoch 50/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0121 - rmse: 0.1098\n",
      "Epoch 50: val_loss did not improve from 0.00502\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0058 - rmse: 0.0763 - val_loss: 0.0058 - val_rmse: 0.0763 - lr: 1.0000e-04\n",
      "Epoch 51/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0629\n",
      "Epoch 51: val_loss improved from 0.00502 to 0.00471, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0063 - rmse: 0.0797 - val_loss: 0.0047 - val_rmse: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 52/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0689\n",
      "Epoch 52: val_loss did not improve from 0.00471\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0060 - rmse: 0.0776 - val_loss: 0.0051 - val_rmse: 0.0713 - lr: 1.0000e-04\n",
      "Epoch 53/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0600\n",
      "Epoch 53: val_loss did not improve from 0.00471\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0056 - rmse: 0.0750 - val_loss: 0.0047 - val_rmse: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 54/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0471\n",
      "Epoch 54: val_loss did not improve from 0.00471\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - rmse: 0.0757 - val_loss: 0.0051 - val_rmse: 0.0711 - lr: 1.0000e-04\n",
      "Epoch 55/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0398\n",
      "Epoch 55: val_loss improved from 0.00471 to 0.00449, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0057 - rmse: 0.0755 - val_loss: 0.0045 - val_rmse: 0.0670 - lr: 1.0000e-04\n",
      "Epoch 56/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4232e-04 - rmse: 0.0272\n",
      "Epoch 56: val_loss did not improve from 0.00449\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0056 - rmse: 0.0751 - val_loss: 0.0046 - val_rmse: 0.0677 - lr: 1.0000e-04\n",
      "Epoch 57/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0492\n",
      "Epoch 57: val_loss did not improve from 0.00449\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0052 - rmse: 0.0721 - val_loss: 0.0051 - val_rmse: 0.0711 - lr: 1.0000e-04\n",
      "Epoch 58/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0042 - rmse: 0.0646\n",
      "Epoch 58: val_loss improved from 0.00449 to 0.00433, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0048 - rmse: 0.0692 - val_loss: 0.0043 - val_rmse: 0.0658 - lr: 1.0000e-04\n",
      "Epoch 59/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0081 - rmse: 0.0898\n",
      "Epoch 59: val_loss did not improve from 0.00433\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0048 - rmse: 0.0695 - val_loss: 0.0046 - val_rmse: 0.0680 - lr: 1.0000e-04\n",
      "Epoch 60/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0617\n",
      "Epoch 60: val_loss improved from 0.00433 to 0.00423, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0048 - rmse: 0.0690 - val_loss: 0.0042 - val_rmse: 0.0650 - lr: 1.0000e-04\n",
      "Epoch 61/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0053 - rmse: 0.0726\n",
      "Epoch 61: val_loss did not improve from 0.00423\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0047 - rmse: 0.0686 - val_loss: 0.0046 - val_rmse: 0.0675 - lr: 1.0000e-04\n",
      "Epoch 62/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0716\n",
      "Epoch 62: val_loss did not improve from 0.00423\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0044 - rmse: 0.0667 - val_loss: 0.0046 - val_rmse: 0.0676 - lr: 1.0000e-04\n",
      "Epoch 63/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0062 - rmse: 0.0786\n",
      "Epoch 63: val_loss improved from 0.00423 to 0.00411, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0048 - rmse: 0.0690 - val_loss: 0.0041 - val_rmse: 0.0641 - lr: 1.0000e-04\n",
      "Epoch 64/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0571\n",
      "Epoch 64: val_loss did not improve from 0.00411\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0046 - rmse: 0.0678 - val_loss: 0.0050 - val_rmse: 0.0705 - lr: 1.0000e-04\n",
      "Epoch 65/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0536\n",
      "Epoch 65: val_loss improved from 0.00411 to 0.00399, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0043 - rmse: 0.0655 - val_loss: 0.0040 - val_rmse: 0.0632 - lr: 1.0000e-04\n",
      "Epoch 66/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0479\n",
      "Epoch 66: val_loss did not improve from 0.00399\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0044 - rmse: 0.0664 - val_loss: 0.0047 - val_rmse: 0.0683 - lr: 1.0000e-04\n",
      "Epoch 67/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0073 - rmse: 0.0853\n",
      "Epoch 67: val_loss did not improve from 0.00399\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0044 - rmse: 0.0660 - val_loss: 0.0043 - val_rmse: 0.0657 - lr: 1.0000e-04\n",
      "Epoch 68/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0048 - rmse: 0.0696\n",
      "Epoch 68: val_loss did not improve from 0.00399\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0051 - rmse: 0.0714 - val_loss: 0.0044 - val_rmse: 0.0661 - lr: 1.0000e-04\n",
      "Epoch 69/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3211e-04 - rmse: 0.0305\n",
      "Epoch 69: val_loss improved from 0.00399 to 0.00390, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0058 - rmse: 0.0764 - val_loss: 0.0039 - val_rmse: 0.0625 - lr: 1.0000e-04\n",
      "Epoch 70/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0069 - rmse: 0.0828\n",
      "Epoch 70: val_loss did not improve from 0.00390\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0056 - rmse: 0.0751 - val_loss: 0.0055 - val_rmse: 0.0739 - lr: 1.0000e-04\n",
      "Epoch 71/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0041 - rmse: 0.0639\n",
      "Epoch 71: val_loss did not improve from 0.00390\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0051 - rmse: 0.0713 - val_loss: 0.0043 - val_rmse: 0.0654 - lr: 1.0000e-04\n",
      "Epoch 72/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0043 - rmse: 0.0657\n",
      "Epoch 72: val_loss did not improve from 0.00390\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0055 - rmse: 0.0740 - val_loss: 0.0055 - val_rmse: 0.0740 - lr: 1.0000e-04\n",
      "Epoch 73/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0663\n",
      "Epoch 73: val_loss improved from 0.00390 to 0.00374, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0056 - rmse: 0.0745 - val_loss: 0.0037 - val_rmse: 0.0611 - lr: 1.0000e-04\n",
      "Epoch 74/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0048 - rmse: 0.0696\n",
      "Epoch 74: val_loss did not improve from 0.00374\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0052 - rmse: 0.0722 - val_loss: 0.0040 - val_rmse: 0.0636 - lr: 1.0000e-04\n",
      "Epoch 75/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0625\n",
      "Epoch 75: val_loss did not improve from 0.00374\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0041 - rmse: 0.0643 - val_loss: 0.0042 - val_rmse: 0.0645 - lr: 1.0000e-04\n",
      "Epoch 76/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0043 - rmse: 0.0659\n",
      "Epoch 76: val_loss did not improve from 0.00374\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0040 - rmse: 0.0636 - val_loss: 0.0037 - val_rmse: 0.0612 - lr: 1.0000e-04\n",
      "Epoch 77/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0406\n",
      "Epoch 77: val_loss did not improve from 0.00374\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - rmse: 0.0696 - val_loss: 0.0047 - val_rmse: 0.0683 - lr: 1.0000e-04\n",
      "Epoch 78/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0068 - rmse: 0.0823\n",
      "Epoch 78: val_loss did not improve from 0.00374\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0039 - rmse: 0.0626 - val_loss: 0.0039 - val_rmse: 0.0625 - lr: 1.0000e-04\n",
      "Epoch 79/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0049 - rmse: 0.0703\n",
      "Epoch 79: val_loss did not improve from 0.00374\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0054 - rmse: 0.0736 - val_loss: 0.0042 - val_rmse: 0.0646 - lr: 1.0000e-04\n",
      "Epoch 80/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0367\n",
      "Epoch 80: val_loss improved from 0.00374 to 0.00362, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0047 - rmse: 0.0686 - val_loss: 0.0036 - val_rmse: 0.0601 - lr: 1.0000e-04\n",
      "Epoch 81/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0050 - rmse: 0.0706\n",
      "Epoch 81: val_loss did not improve from 0.00362\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0036 - rmse: 0.0599 - val_loss: 0.0043 - val_rmse: 0.0656 - lr: 1.0000e-04\n",
      "Epoch 82/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0715\n",
      "Epoch 82: val_loss improved from 0.00362 to 0.00358, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0037 - rmse: 0.0607 - val_loss: 0.0036 - val_rmse: 0.0598 - lr: 1.0000e-04\n",
      "Epoch 83/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0482\n",
      "Epoch 83: val_loss did not improve from 0.00358\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0041 - rmse: 0.0640 - val_loss: 0.0037 - val_rmse: 0.0606 - lr: 1.0000e-04\n",
      "Epoch 84/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5341e-04 - rmse: 0.0309\n",
      "Epoch 84: val_loss did not improve from 0.00358\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0038 - rmse: 0.0616 - val_loss: 0.0038 - val_rmse: 0.0617 - lr: 1.0000e-04\n",
      "Epoch 85/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0569\n",
      "Epoch 85: val_loss improved from 0.00358 to 0.00352, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0037 - rmse: 0.0610 - val_loss: 0.0035 - val_rmse: 0.0593 - lr: 1.0000e-04\n",
      "Epoch 86/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0042 - rmse: 0.0651\n",
      "Epoch 86: val_loss did not improve from 0.00352\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0036 - rmse: 0.0600 - val_loss: 0.0038 - val_rmse: 0.0618 - lr: 1.0000e-04\n",
      "Epoch 87/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0443\n",
      "Epoch 87: val_loss improved from 0.00352 to 0.00345, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0036 - rmse: 0.0600 - val_loss: 0.0034 - val_rmse: 0.0587 - lr: 1.0000e-04\n",
      "Epoch 88/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0558\n",
      "Epoch 88: val_loss did not improve from 0.00345\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0036 - rmse: 0.0598 - val_loss: 0.0039 - val_rmse: 0.0625 - lr: 1.0000e-04\n",
      "Epoch 89/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0546\n",
      "Epoch 89: val_loss did not improve from 0.00345\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0034 - rmse: 0.0580 - val_loss: 0.0035 - val_rmse: 0.0591 - lr: 1.0000e-04\n",
      "Epoch 90/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0441\n",
      "Epoch 90: val_loss did not improve from 0.00345\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0040 - rmse: 0.0630 - val_loss: 0.0041 - val_rmse: 0.0640 - lr: 1.0000e-04\n",
      "Epoch 91/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0081 - rmse: 0.0902\n",
      "Epoch 91: val_loss improved from 0.00345 to 0.00343, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0045 - rmse: 0.0668 - val_loss: 0.0034 - val_rmse: 0.0586 - lr: 1.0000e-04\n",
      "Epoch 92/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0592\n",
      "Epoch 92: val_loss did not improve from 0.00343\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0037 - rmse: 0.0607 - val_loss: 0.0037 - val_rmse: 0.0609 - lr: 1.0000e-04\n",
      "Epoch 93/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0549\n",
      "Epoch 93: val_loss did not improve from 0.00343\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0033 - rmse: 0.0574 - val_loss: 0.0035 - val_rmse: 0.0592 - lr: 1.0000e-04\n",
      "Epoch 94/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0717\n",
      "Epoch 94: val_loss did not improve from 0.00343\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0035 - rmse: 0.0588 - val_loss: 0.0038 - val_rmse: 0.0614 - lr: 1.0000e-04\n",
      "Epoch 95/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0079 - rmse: 0.0886\n",
      "Epoch 95: val_loss did not improve from 0.00343\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0035 - rmse: 0.0590 - val_loss: 0.0036 - val_rmse: 0.0599 - lr: 1.0000e-04\n",
      "Epoch 96/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0484\n",
      "Epoch 96: val_loss improved from 0.00343 to 0.00337, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0033 - rmse: 0.0575 - val_loss: 0.0034 - val_rmse: 0.0581 - lr: 1.0000e-04\n",
      "Epoch 97/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0670\n",
      "Epoch 97: val_loss did not improve from 0.00337\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0038 - rmse: 0.0617 - val_loss: 0.0037 - val_rmse: 0.0607 - lr: 1.0000e-04\n",
      "Epoch 98/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0504\n",
      "Epoch 98: val_loss improved from 0.00337 to 0.00330, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0037 - rmse: 0.0607 - val_loss: 0.0033 - val_rmse: 0.0575 - lr: 1.0000e-04\n",
      "Epoch 99/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6884e-04 - rmse: 0.0311\n",
      "Epoch 99: val_loss did not improve from 0.00330\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0035 - rmse: 0.0589 - val_loss: 0.0034 - val_rmse: 0.0580 - lr: 1.0000e-04\n",
      "Epoch 100/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0603\n",
      "Epoch 100: val_loss improved from 0.00330 to 0.00327, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0033 - rmse: 0.0571 - val_loss: 0.0033 - val_rmse: 0.0572 - lr: 1.0000e-04\n",
      "Epoch 101/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0711\n",
      "Epoch 101: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0037 - val_rmse: 0.0612 - lr: 1.0000e-04\n",
      "Epoch 102/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0517\n",
      "Epoch 102: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0037 - rmse: 0.0608 - val_loss: 0.0033 - val_rmse: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 103/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0352\n",
      "Epoch 103: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0032 - rmse: 0.0569 - val_loss: 0.0035 - val_rmse: 0.0595 - lr: 1.0000e-04\n",
      "Epoch 104/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8427e-04 - rmse: 0.0314\n",
      "Epoch 104: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0032 - rmse: 0.0566 - val_loss: 0.0033 - val_rmse: 0.0574 - lr: 1.0000e-04\n",
      "Epoch 105/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0507\n",
      "Epoch 105: val_loss did not improve from 0.00327\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0032 - rmse: 0.0567 - val_loss: 0.0034 - val_rmse: 0.0581 - lr: 1.0000e-04\n",
      "Epoch 106/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0460\n",
      "Epoch 106: val_loss improved from 0.00327 to 0.00323, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0033 - rmse: 0.0576 - val_loss: 0.0032 - val_rmse: 0.0568 - lr: 1.0000e-04\n",
      "Epoch 107/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0592\n",
      "Epoch 107: val_loss did not improve from 0.00323\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0572 - val_loss: 0.0034 - val_rmse: 0.0579 - lr: 1.0000e-04\n",
      "Epoch 108/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0486\n",
      "Epoch 108: val_loss did not improve from 0.00323\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0033 - rmse: 0.0577 - val_loss: 0.0033 - val_rmse: 0.0572 - lr: 1.0000e-04\n",
      "Epoch 109/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0380\n",
      "Epoch 109: val_loss did not improve from 0.00323\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0035 - rmse: 0.0595 - val_loss: 0.0033 - val_rmse: 0.0572 - lr: 1.0000e-04\n",
      "Epoch 110/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0568\n",
      "Epoch 110: val_loss improved from 0.00323 to 0.00314, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0031 - val_rmse: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 111/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9164e-04 - rmse: 0.0315\n",
      "Epoch 111: val_loss improved from 0.00314 to 0.00313, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0031 - rmse: 0.0553 - val_loss: 0.0031 - val_rmse: 0.0559 - lr: 1.0000e-04\n",
      "Epoch 112/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0546\n",
      "Epoch 112: val_loss did not improve from 0.00313\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0031 - rmse: 0.0553 - val_loss: 0.0035 - val_rmse: 0.0590 - lr: 1.0000e-04\n",
      "Epoch 113/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0088 - rmse: 0.0937\n",
      "Epoch 113: val_loss did not improve from 0.00313\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0035 - rmse: 0.0591 - val_loss: 0.0033 - val_rmse: 0.0576 - lr: 1.0000e-04\n",
      "Epoch 114/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0495\n",
      "Epoch 114: val_loss improved from 0.00313 to 0.00310, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0031 - val_rmse: 0.0556 - lr: 1.0000e-04\n",
      "Epoch 115/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0036 - rmse: 0.0599\n",
      "Epoch 115: val_loss did not improve from 0.00310\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0033 - rmse: 0.0572 - val_loss: 0.0032 - val_rmse: 0.0563 - lr: 1.0000e-04\n",
      "Epoch 116/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0372\n",
      "Epoch 116: val_loss did not improve from 0.00310\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0030 - rmse: 0.0549 - val_loss: 0.0031 - val_rmse: 0.0558 - lr: 1.0000e-04\n",
      "Epoch 117/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0460\n",
      "Epoch 117: val_loss improved from 0.00310 to 0.00307, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0029 - rmse: 0.0540 - val_loss: 0.0031 - val_rmse: 0.0554 - lr: 1.0000e-04\n",
      "Epoch 118/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0667\n",
      "Epoch 118: val_loss did not improve from 0.00307\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0030 - rmse: 0.0547 - val_loss: 0.0031 - val_rmse: 0.0556 - lr: 1.0000e-04\n",
      "Epoch 119/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0454\n",
      "Epoch 119: val_loss did not improve from 0.00307\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0031 - rmse: 0.0556 - val_loss: 0.0031 - val_rmse: 0.0555 - lr: 1.0000e-04\n",
      "Epoch 120/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0056 - rmse: 0.0750\n",
      "Epoch 120: val_loss did not improve from 0.00307\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0535 - val_loss: 0.0032 - val_rmse: 0.0561 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0537\n",
      "Epoch 121: val_loss did not improve from 0.00307\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0032 - rmse: 0.0562 - val_loss: 0.0034 - val_rmse: 0.0581 - lr: 1.0000e-04\n",
      "Epoch 122/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0056 - rmse: 0.0750\n",
      "Epoch 122: val_loss improved from 0.00307 to 0.00304, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0033 - rmse: 0.0574 - val_loss: 0.0030 - val_rmse: 0.0552 - lr: 1.0000e-04\n",
      "Epoch 123/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0505\n",
      "Epoch 123: val_loss did not improve from 0.00304\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - rmse: 0.0542 - val_loss: 0.0032 - val_rmse: 0.0565 - lr: 1.0000e-04\n",
      "Epoch 124/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0581\n",
      "Epoch 124: val_loss improved from 0.00304 to 0.00297, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0030 - rmse: 0.0550 - val_loss: 0.0030 - val_rmse: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 125/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0556\n",
      "Epoch 125: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0031 - rmse: 0.0560 - val_loss: 0.0031 - val_rmse: 0.0558 - lr: 1.0000e-04\n",
      "Epoch 126/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0635\n",
      "Epoch 126: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0031 - rmse: 0.0554 - val_loss: 0.0030 - val_rmse: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 127/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0524\n",
      "Epoch 127: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0030 - rmse: 0.0550 - val_loss: 0.0030 - val_rmse: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 128/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0451\n",
      "Epoch 128: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0030 - rmse: 0.0543 - val_loss: 0.0032 - val_rmse: 0.0569 - lr: 1.0000e-04\n",
      "Epoch 129/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0435\n",
      "Epoch 129: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0575 - val_loss: 0.0032 - val_rmse: 0.0566 - lr: 1.0000e-04\n",
      "Epoch 130/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5065e-04 - rmse: 0.0292\n",
      "Epoch 130: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0035 - rmse: 0.0592 - val_loss: 0.0039 - val_rmse: 0.0627 - lr: 1.0000e-04\n",
      "Epoch 131/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0444\n",
      "Epoch 131: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0031 - rmse: 0.0557 - val_loss: 0.0033 - val_rmse: 0.0575 - lr: 1.0000e-04\n",
      "Epoch 132/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0489\n",
      "Epoch 132: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - rmse: 0.0572 - val_loss: 0.0033 - val_rmse: 0.0575 - lr: 1.0000e-04\n",
      "Epoch 133/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0396\n",
      "Epoch 133: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - rmse: 0.0539 - val_loss: 0.0034 - val_rmse: 0.0586 - lr: 1.0000e-04\n",
      "Epoch 134/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0331\n",
      "Epoch 134: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0035 - rmse: 0.0594 - val_loss: 0.0035 - val_rmse: 0.0594 - lr: 1.0000e-04\n",
      "Epoch 135/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0555\n",
      "Epoch 135: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0042 - rmse: 0.0645 - val_loss: 0.0039 - val_rmse: 0.0625 - lr: 1.0000e-04\n",
      "Epoch 136/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0062 - rmse: 0.0786\n",
      "Epoch 136: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0032 - rmse: 0.0563 - val_loss: 0.0036 - val_rmse: 0.0598 - lr: 1.0000e-04\n",
      "Epoch 137/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0661\n",
      "Epoch 137: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0030 - rmse: 0.0547 - val_loss: 0.0031 - val_rmse: 0.0555 - lr: 1.0000e-04\n",
      "Epoch 138/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0545\n",
      "Epoch 138: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0031 - rmse: 0.0560 - val_loss: 0.0032 - val_rmse: 0.0565 - lr: 1.0000e-04\n",
      "Epoch 139/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7079e-04 - rmse: 0.0312\n",
      "Epoch 139: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0035 - rmse: 0.0590 - val_loss: 0.0035 - val_rmse: 0.0591 - lr: 1.0000e-04\n",
      "Epoch 140/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0480\n",
      "Epoch 140: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0038 - rmse: 0.0618 - val_loss: 0.0032 - val_rmse: 0.0564 - lr: 1.0000e-04\n",
      "Epoch 141/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0320\n",
      "Epoch 141: val_loss did not improve from 0.00297\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0039 - rmse: 0.0627 - val_loss: 0.0034 - val_rmse: 0.0584 - lr: 1.0000e-04\n",
      "Epoch 142/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0041 - rmse: 0.0643\n",
      "Epoch 142: val_loss improved from 0.00297 to 0.00293, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0035 - rmse: 0.0594 - val_loss: 0.0029 - val_rmse: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 143/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0504\n",
      "Epoch 143: val_loss did not improve from 0.00293\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0029 - rmse: 0.0539 - val_loss: 0.0029 - val_rmse: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 144/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0409\n",
      "Epoch 144: val_loss did not improve from 0.00293\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0032 - rmse: 0.0563 - val_loss: 0.0031 - val_rmse: 0.0554 - lr: 1.0000e-04\n",
      "Epoch 145/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0456\n",
      "Epoch 145: val_loss did not improve from 0.00293\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - rmse: 0.0526 - val_loss: 0.0031 - val_rmse: 0.0557 - lr: 1.0000e-04\n",
      "Epoch 146/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0358\n",
      "Epoch 146: val_loss improved from 0.00293 to 0.00286, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0028 - rmse: 0.0531 - val_loss: 0.0029 - val_rmse: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 147/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0423\n",
      "Epoch 147: val_loss did not improve from 0.00286\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - rmse: 0.0523 - val_loss: 0.0029 - val_rmse: 0.0540 - lr: 1.0000e-04\n",
      "Epoch 148/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0344\n",
      "Epoch 148: val_loss improved from 0.00286 to 0.00285, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0028 - rmse: 0.0525 - val_loss: 0.0029 - val_rmse: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 149/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0509\n",
      "Epoch 149: val_loss did not improve from 0.00285\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0034 - rmse: 0.0582 - val_loss: 0.0031 - val_rmse: 0.0553 - lr: 1.0000e-04\n",
      "Epoch 150/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0535\n",
      "Epoch 150: val_loss improved from 0.00285 to 0.00285, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0036 - rmse: 0.0596 - val_loss: 0.0029 - val_rmse: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 151/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0454\n",
      "Epoch 151: val_loss did not improve from 0.00285\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0041 - rmse: 0.0641 - val_loss: 0.0032 - val_rmse: 0.0570 - lr: 1.0000e-04\n",
      "Epoch 152/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0557\n",
      "Epoch 152: val_loss did not improve from 0.00285\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0041 - rmse: 0.0638 - val_loss: 0.0030 - val_rmse: 0.0547 - lr: 1.0000e-04\n",
      "Epoch 153/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0400\n",
      "Epoch 153: val_loss did not improve from 0.00285\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0030 - rmse: 0.0545 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 154/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0571\n",
      "Epoch 154: val_loss did not improve from 0.00285\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - rmse: 0.0538 - val_loss: 0.0029 - val_rmse: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 155/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0506\n",
      "Epoch 155: val_loss did not improve from 0.00285\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0030 - rmse: 0.0552 - val_loss: 0.0030 - val_rmse: 0.0544 - lr: 1.0000e-04\n",
      "Epoch 156/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0058 - rmse: 0.0761\n",
      "Epoch 156: val_loss improved from 0.00285 to 0.00283, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0031 - rmse: 0.0553 - val_loss: 0.0028 - val_rmse: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 157/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0404\n",
      "Epoch 157: val_loss did not improve from 0.00283\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - rmse: 0.0542 - val_loss: 0.0029 - val_rmse: 0.0543 - lr: 1.0000e-04\n",
      "Epoch 158/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0340\n",
      "Epoch 158: val_loss did not improve from 0.00283\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - rmse: 0.0514 - val_loss: 0.0029 - val_rmse: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 159/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0351\n",
      "Epoch 159: val_loss did not improve from 0.00283\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - rmse: 0.0507 - val_loss: 0.0029 - val_rmse: 0.0536 - lr: 1.0000e-04\n",
      "Epoch 160/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0344\n",
      "Epoch 160: val_loss did not improve from 0.00283\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - rmse: 0.0512 - val_loss: 0.0029 - val_rmse: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 161/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0386\n",
      "Epoch 161: val_loss did not improve from 0.00283\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0514 - val_loss: 0.0032 - val_rmse: 0.0566 - lr: 1.0000e-04\n",
      "Epoch 162/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0576\n",
      "Epoch 162: val_loss improved from 0.00283 to 0.00281, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0030 - rmse: 0.0548 - val_loss: 0.0028 - val_rmse: 0.0530 - lr: 1.0000e-04\n",
      "Epoch 163/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0606\n",
      "Epoch 163: val_loss did not improve from 0.00281\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0029 - rmse: 0.0539 - val_loss: 0.0029 - val_rmse: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 164/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0414\n",
      "Epoch 164: val_loss did not improve from 0.00281\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - rmse: 0.0509 - val_loss: 0.0028 - val_rmse: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 165/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0523\n",
      "Epoch 165: val_loss improved from 0.00281 to 0.00276, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0026 - rmse: 0.0506 - val_loss: 0.0028 - val_rmse: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 166/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0628\n",
      "Epoch 166: val_loss did not improve from 0.00276\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0025 - rmse: 0.0502 - val_loss: 0.0028 - val_rmse: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 167/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0525\n",
      "Epoch 167: val_loss improved from 0.00276 to 0.00276, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0025 - rmse: 0.0504 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 168/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0454\n",
      "Epoch 168: val_loss improved from 0.00276 to 0.00275, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0025 - rmse: 0.0505 - val_loss: 0.0027 - val_rmse: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 169/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0713\n",
      "Epoch 169: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0025 - rmse: 0.0503 - val_loss: 0.0027 - val_rmse: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 170/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0590\n",
      "Epoch 170: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0025 - rmse: 0.0499 - val_loss: 0.0027 - val_rmse: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 171/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0551\n",
      "Epoch 171: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - rmse: 0.0519 - val_loss: 0.0029 - val_rmse: 0.0537 - lr: 1.0000e-04\n",
      "Epoch 172/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0509\n",
      "Epoch 172: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0491 - val_loss: 0.0028 - val_rmse: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 173/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0039 - rmse: 0.0623\n",
      "Epoch 173: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0030 - rmse: 0.0549 - val_loss: 0.0028 - val_rmse: 0.0529 - lr: 1.0000e-04\n",
      "Epoch 174/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0331\n",
      "Epoch 174: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0028 - rmse: 0.0530 - val_loss: 0.0029 - val_rmse: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 175/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0329\n",
      "Epoch 175: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - rmse: 0.0506 - val_loss: 0.0029 - val_rmse: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 176/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0581\n",
      "Epoch 176: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0514 - val_loss: 0.0029 - val_rmse: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 177/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0552\n",
      "Epoch 177: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0512 - val_loss: 0.0029 - val_rmse: 0.0539 - lr: 1.0000e-04\n",
      "Epoch 178/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0568\n",
      "Epoch 178: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - rmse: 0.0517 - val_loss: 0.0028 - val_rmse: 0.0529 - lr: 1.0000e-04\n",
      "Epoch 179/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3986e-04 - rmse: 0.0307\n",
      "Epoch 179: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0025 - rmse: 0.0505 - val_loss: 0.0028 - val_rmse: 0.0527 - lr: 1.0000e-04\n",
      "Epoch 180/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0387\n",
      "Epoch 180: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0484 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 181/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0497\n",
      "Epoch 181: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0494 - val_loss: 0.0029 - val_rmse: 0.0536 - lr: 1.0000e-04\n",
      "Epoch 182/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0530\n",
      "Epoch 182: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0027 - rmse: 0.0515 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 183/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0508\n",
      "Epoch 183: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0024 - rmse: 0.0495 - val_loss: 0.0028 - val_rmse: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 184/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2710e-04 - rmse: 0.0270\n",
      "Epoch 184: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0025 - rmse: 0.0496 - val_loss: 0.0027 - val_rmse: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 185/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0418\n",
      "Epoch 185: val_loss did not improve from 0.00275\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - rmse: 0.0515 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 186/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0434\n",
      "Epoch 186: val_loss improved from 0.00275 to 0.00271, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0026 - rmse: 0.0507 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 187/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0460\n",
      "Epoch 187: val_loss did not improve from 0.00271\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - rmse: 0.0534 - val_loss: 0.0034 - val_rmse: 0.0581 - lr: 1.0000e-04\n",
      "Epoch 188/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0041 - rmse: 0.0639\n",
      "Epoch 188: val_loss did not improve from 0.00271\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0509 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 189/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0050 - rmse: 0.0710\n",
      "Epoch 189: val_loss did not improve from 0.00271\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0494 - val_loss: 0.0028 - val_rmse: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 190/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5061e-04 - rmse: 0.0235\n",
      "Epoch 190: val_loss did not improve from 0.00271\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0488 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 191/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0362\n",
      "Epoch 191: val_loss improved from 0.00271 to 0.00270, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0024 - rmse: 0.0495 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 192/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0507\n",
      "Epoch 192: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0490 - val_loss: 0.0027 - val_rmse: 0.0521 - lr: 1.0000e-04\n",
      "Epoch 193/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0549\n",
      "Epoch 193: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0025 - rmse: 0.0499 - val_loss: 0.0028 - val_rmse: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 194/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0378\n",
      "Epoch 194: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0495 - val_loss: 0.0027 - val_rmse: 0.0523 - lr: 1.0000e-04\n",
      "Epoch 195/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0354\n",
      "Epoch 195: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0487 - val_loss: 0.0028 - val_rmse: 0.0530 - lr: 1.0000e-04\n",
      "Epoch 196/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0505\n",
      "Epoch 196: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0023 - rmse: 0.0480 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 197/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0461\n",
      "Epoch 197: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0505 - val_loss: 0.0028 - val_rmse: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 198/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0370\n",
      "Epoch 198: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - rmse: 0.0524 - val_loss: 0.0031 - val_rmse: 0.0558 - lr: 1.0000e-04\n",
      "Epoch 199/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0459\n",
      "Epoch 199: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0031 - rmse: 0.0553 - val_loss: 0.0028 - val_rmse: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 200/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0635\n",
      "Epoch 200: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0030 - rmse: 0.0544 - val_loss: 0.0034 - val_rmse: 0.0583 - lr: 1.0000e-04\n",
      "Epoch 201/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0587\n",
      "Epoch 201: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0025 - rmse: 0.0503 - val_loss: 0.0028 - val_rmse: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 202/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0056 - rmse: 0.0747\n",
      "Epoch 202: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0493 - val_loss: 0.0029 - val_rmse: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 203/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0415\n",
      "Epoch 203: val_loss did not improve from 0.00270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0486 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 204/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0552\n",
      "Epoch 204: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0487 - val_loss: 0.0028 - val_rmse: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 205/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0346\n",
      "Epoch 205: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0024 - rmse: 0.0490 - val_loss: 0.0028 - val_rmse: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 206/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4564e-04 - rmse: 0.0291\n",
      "Epoch 206: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0479 - val_loss: 0.0029 - val_rmse: 0.0536 - lr: 1.0000e-04\n",
      "Epoch 207/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0468\n",
      "Epoch 207: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0481 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 208/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0618\n",
      "Epoch 208: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0478 - val_loss: 0.0030 - val_rmse: 0.0549 - lr: 1.0000e-04\n",
      "Epoch 209/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0496\n",
      "Epoch 209: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0024 - rmse: 0.0485 - val_loss: 0.0028 - val_rmse: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 210/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0517\n",
      "Epoch 210: val_loss improved from 0.00270 to 0.00270, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0025 - rmse: 0.0497 - val_loss: 0.0027 - val_rmse: 0.0519 - lr: 1.0000e-04\n",
      "Epoch 211/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0561\n",
      "Epoch 211: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - rmse: 0.0519 - val_loss: 0.0028 - val_rmse: 0.0529 - lr: 1.0000e-04\n",
      "Epoch 212/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9051e-04 - rmse: 0.0263\n",
      "Epoch 212: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0484 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 213/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0969e-04 - rmse: 0.0285\n",
      "Epoch 213: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0476 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 214/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0407\n",
      "Epoch 214: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0473 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 215/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8389e-04 - rmse: 0.0262\n",
      "Epoch 215: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0465 - val_loss: 0.0027 - val_rmse: 0.0521 - lr: 1.0000e-04\n",
      "Epoch 216/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0453\n",
      "Epoch 216: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0467 - val_loss: 0.0029 - val_rmse: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 217/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0391\n",
      "Epoch 217: val_loss did not improve from 0.00270\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0023 - rmse: 0.0478 - val_loss: 0.0029 - val_rmse: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 218/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0038 - rmse: 0.0614\n",
      "Epoch 218: val_loss improved from 0.00270 to 0.00265, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0027 - val_rmse: 0.0515 - lr: 1.0000e-04\n",
      "Epoch 219/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0373\n",
      "Epoch 219: val_loss did not improve from 0.00265\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0472 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 220/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0354\n",
      "Epoch 220: val_loss did not improve from 0.00265\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0473 - val_loss: 0.0028 - val_rmse: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 221/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0589\n",
      "Epoch 221: val_loss improved from 0.00265 to 0.00262, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0022 - rmse: 0.0472 - val_loss: 0.0026 - val_rmse: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 222/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0420\n",
      "Epoch 222: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0509 - val_loss: 0.0037 - val_rmse: 0.0608 - lr: 1.0000e-04\n",
      "Epoch 223/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0560\n",
      "Epoch 223: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0031 - rmse: 0.0558 - val_loss: 0.0030 - val_rmse: 0.0548 - lr: 1.0000e-04\n",
      "Epoch 224/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0255e-04 - rmse: 0.0224\n",
      "Epoch 224: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0032 - rmse: 0.0566 - val_loss: 0.0043 - val_rmse: 0.0654 - lr: 1.0000e-04\n",
      "Epoch 225/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0576\n",
      "Epoch 225: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0506 - val_loss: 0.0034 - val_rmse: 0.0583 - lr: 1.0000e-04\n",
      "Epoch 226/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0075 - rmse: 0.0864\n",
      "Epoch 226: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0031 - rmse: 0.0553 - val_loss: 0.0032 - val_rmse: 0.0566 - lr: 1.0000e-04\n",
      "Epoch 227/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0048 - rmse: 0.0692\n",
      "Epoch 227: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0478 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 228/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0687\n",
      "Epoch 228: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0489 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 229/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0331\n",
      "Epoch 229: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0476 - val_loss: 0.0028 - val_rmse: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 230/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0548\n",
      "Epoch 230: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0477 - val_loss: 0.0027 - val_rmse: 0.0519 - lr: 1.0000e-04\n",
      "Epoch 231/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0352\n",
      "Epoch 231: val_loss did not improve from 0.00262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - rmse: 0.0460 - val_loss: 0.0031 - val_rmse: 0.0559 - lr: 1.0000e-04\n",
      "Epoch 232/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0480\n",
      "Epoch 232: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0024 - rmse: 0.0486 - val_loss: 0.0026 - val_rmse: 0.0513 - lr: 1.0000e-04\n",
      "Epoch 233/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0389\n",
      "Epoch 233: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0466 - val_loss: 0.0030 - val_rmse: 0.0549 - lr: 1.0000e-04\n",
      "Epoch 234/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6867e-04 - rmse: 0.0277\n",
      "Epoch 234: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0466 - val_loss: 0.0028 - val_rmse: 0.0527 - lr: 1.0000e-04\n",
      "Epoch 235/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0559\n",
      "Epoch 235: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0473 - val_loss: 0.0026 - val_rmse: 0.0514 - lr: 1.0000e-04\n",
      "Epoch 236/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0488\n",
      "Epoch 236: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0027 - val_rmse: 0.0517 - lr: 1.0000e-04\n",
      "Epoch 237/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0475\n",
      "Epoch 237: val_loss did not improve from 0.00262\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - rmse: 0.0462 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 238/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3700e-04 - rmse: 0.0289\n",
      "Epoch 238: val_loss improved from 0.00262 to 0.00261, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0023 - rmse: 0.0480 - val_loss: 0.0026 - val_rmse: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 239/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0040 - rmse: 0.0636\n",
      "Epoch 239: val_loss did not improve from 0.00261\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0466 - val_loss: 0.0026 - val_rmse: 0.0515 - lr: 1.0000e-04\n",
      "Epoch 240/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0529\n",
      "Epoch 240: val_loss did not improve from 0.00261\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - rmse: 0.0456 - val_loss: 0.0028 - val_rmse: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 241/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0528\n",
      "Epoch 241: val_loss improved from 0.00261 to 0.00259, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0022 - rmse: 0.0467 - val_loss: 0.0026 - val_rmse: 0.0509 - lr: 1.0000e-04\n",
      "Epoch 242/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0057 - rmse: 0.0752\n",
      "Epoch 242: val_loss did not improve from 0.00259\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0467 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 243/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8960e-04 - rmse: 0.0281\n",
      "Epoch 243: val_loss did not improve from 0.00259\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - rmse: 0.0455 - val_loss: 0.0027 - val_rmse: 0.0518 - lr: 1.0000e-04\n",
      "Epoch 244/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5360e-04 - rmse: 0.0309\n",
      "Epoch 244: val_loss did not improve from 0.00259\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0020 - rmse: 0.0450 - val_loss: 0.0026 - val_rmse: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 245/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0385\n",
      "Epoch 245: val_loss did not improve from 0.00259\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0020 - rmse: 0.0450 - val_loss: 0.0032 - val_rmse: 0.0567 - lr: 1.0000e-04\n",
      "Epoch 246/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0503\n",
      "Epoch 246: val_loss did not improve from 0.00259\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0467 - val_loss: 0.0026 - val_rmse: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 247/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0323\n",
      "Epoch 247: val_loss did not improve from 0.00259\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - rmse: 0.0454 - val_loss: 0.0028 - val_rmse: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 248/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0365\n",
      "Epoch 248: val_loss improved from 0.00259 to 0.00255, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0022 - rmse: 0.0473 - val_loss: 0.0026 - val_rmse: 0.0505 - lr: 1.0000e-04\n",
      "Epoch 249/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0478\n",
      "Epoch 249: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0466 - val_loss: 0.0028 - val_rmse: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 250/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0333\n",
      "Epoch 250: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0024 - rmse: 0.0493 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 251/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0323\n",
      "Epoch 251: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0028 - rmse: 0.0532 - val_loss: 0.0026 - val_rmse: 0.0506 - lr: 1.0000e-04\n",
      "Epoch 252/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0031 - rmse: 0.0559\n",
      "Epoch 252: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0033 - rmse: 0.0570 - val_loss: 0.0036 - val_rmse: 0.0600 - lr: 1.0000e-04\n",
      "Epoch 253/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0387\n",
      "Epoch 253: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - rmse: 0.0511 - val_loss: 0.0029 - val_rmse: 0.0537 - lr: 1.0000e-04\n",
      "Epoch 254/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0329\n",
      "Epoch 254: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - rmse: 0.0537 - val_loss: 0.0034 - val_rmse: 0.0583 - lr: 1.0000e-04\n",
      "Epoch 255/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0605\n",
      "Epoch 255: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0033 - rmse: 0.0574 - val_loss: 0.0026 - val_rmse: 0.0514 - lr: 1.0000e-04\n",
      "Epoch 256/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0047 - rmse: 0.0683\n",
      "Epoch 256: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - rmse: 0.0530 - val_loss: 0.0028 - val_rmse: 0.0529 - lr: 1.0000e-04\n",
      "Epoch 257/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0469\n",
      "Epoch 257: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0034 - rmse: 0.0580 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 258/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0045 - rmse: 0.0674\n",
      "Epoch 258: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0031 - rmse: 0.0559 - val_loss: 0.0027 - val_rmse: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 259/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0407\n",
      "Epoch 259: val_loss did not improve from 0.00255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0025 - rmse: 0.0496 - val_loss: 0.0028 - val_rmse: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 260/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0583\n",
      "Epoch 260: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0471 - val_loss: 0.0026 - val_rmse: 0.0514 - lr: 1.0000e-04\n",
      "Epoch 261/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5253e-04 - rmse: 0.0255\n",
      "Epoch 261: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0023 - rmse: 0.0478 - val_loss: 0.0026 - val_rmse: 0.0513 - lr: 1.0000e-04\n",
      "Epoch 262/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0500\n",
      "Epoch 262: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - rmse: 0.0462 - val_loss: 0.0026 - val_rmse: 0.0509 - lr: 1.0000e-04\n",
      "Epoch 263/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0447\n",
      "Epoch 263: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - rmse: 0.0460 - val_loss: 0.0027 - val_rmse: 0.0517 - lr: 1.0000e-04\n",
      "Epoch 264/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0400\n",
      "Epoch 264: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - rmse: 0.0454 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 265/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0409\n",
      "Epoch 265: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0441 - val_loss: 0.0026 - val_rmse: 0.0515 - lr: 1.0000e-04\n",
      "Epoch 266/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0520\n",
      "Epoch 266: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0441 - val_loss: 0.0026 - val_rmse: 0.0507 - lr: 1.0000e-04\n",
      "Epoch 267/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0335\n",
      "Epoch 267: val_loss did not improve from 0.00255\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0449 - val_loss: 0.0028 - val_rmse: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 268/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0578\n",
      "Epoch 268: val_loss improved from 0.00255 to 0.00252, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0024 - rmse: 0.0486 - val_loss: 0.0025 - val_rmse: 0.0502 - lr: 1.0000e-04\n",
      "Epoch 269/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0531\n",
      "Epoch 269: val_loss did not improve from 0.00252\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0464 - val_loss: 0.0032 - val_rmse: 0.0569 - lr: 1.0000e-04\n",
      "Epoch 270/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0433\n",
      "Epoch 270: val_loss did not improve from 0.00252\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0023 - rmse: 0.0485 - val_loss: 0.0027 - val_rmse: 0.0516 - lr: 1.0000e-04\n",
      "Epoch 271/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0333\n",
      "Epoch 271: val_loss did not improve from 0.00252\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - rmse: 0.0541 - val_loss: 0.0033 - val_rmse: 0.0572 - lr: 1.0000e-04\n",
      "Epoch 272/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0336\n",
      "Epoch 272: val_loss improved from 0.00252 to 0.00250, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0020 - rmse: 0.0452 - val_loss: 0.0025 - val_rmse: 0.0500 - lr: 1.0000e-04\n",
      "Epoch 273/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9409e-04 - rmse: 0.0299\n",
      "Epoch 273: val_loss did not improve from 0.00250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0443 - val_loss: 0.0027 - val_rmse: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 274/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7273e-04 - rmse: 0.0217\n",
      "Epoch 274: val_loss improved from 0.00250 to 0.00246, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0019 - rmse: 0.0437 - val_loss: 0.0025 - val_rmse: 0.0496 - lr: 1.0000e-04\n",
      "Epoch 275/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4158e-04 - rmse: 0.0290\n",
      "Epoch 275: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0437 - val_loss: 0.0030 - val_rmse: 0.0543 - lr: 1.0000e-04\n",
      "Epoch 276/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9561e-04 - rmse: 0.0299\n",
      "Epoch 276: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0467 - val_loss: 0.0025 - val_rmse: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 277/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0343\n",
      "Epoch 277: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0473 - val_loss: 0.0031 - val_rmse: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 278/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0513e-04 - rmse: 0.0246\n",
      "Epoch 278: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0466 - val_loss: 0.0026 - val_rmse: 0.0507 - lr: 1.0000e-04\n",
      "Epoch 279/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0712\n",
      "Epoch 279: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0025 - val_rmse: 0.0502 - lr: 1.0000e-04\n",
      "Epoch 280/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7562e-04 - rmse: 0.0279\n",
      "Epoch 280: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0437 - val_loss: 0.0027 - val_rmse: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 281/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0421\n",
      "Epoch 281: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0020 - rmse: 0.0445 - val_loss: 0.0025 - val_rmse: 0.0500 - lr: 1.0000e-04\n",
      "Epoch 282/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0444\n",
      "Epoch 282: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0446 - val_loss: 0.0029 - val_rmse: 0.0543 - lr: 1.0000e-04\n",
      "Epoch 283/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0499\n",
      "Epoch 283: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - rmse: 0.0510 - val_loss: 0.0026 - val_rmse: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 284/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0048 - rmse: 0.0696\n",
      "Epoch 284: val_loss did not improve from 0.00246\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0486 - val_loss: 0.0027 - val_rmse: 0.0521 - lr: 1.0000e-04\n",
      "Epoch 285/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0580\n",
      "Epoch 285: val_loss improved from 0.00246 to 0.00244, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0024 - rmse: 0.0495 - val_loss: 0.0024 - val_rmse: 0.0494 - lr: 1.0000e-04\n",
      "Epoch 286/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0034 - rmse: 0.0580\n",
      "Epoch 286: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0483 - val_loss: 0.0030 - val_rmse: 0.0550 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0419\n",
      "Epoch 287: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - rmse: 0.0464 - val_loss: 0.0025 - val_rmse: 0.0503 - lr: 1.0000e-04\n",
      "Epoch 288/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0386\n",
      "Epoch 288: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0483 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 289/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3772e-04 - rmse: 0.0306\n",
      "Epoch 289: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0031 - rmse: 0.0556 - val_loss: 0.0025 - val_rmse: 0.0496 - lr: 1.0000e-04\n",
      "Epoch 290/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0327\n",
      "Epoch 290: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0036 - rmse: 0.0600 - val_loss: 0.0029 - val_rmse: 0.0541 - lr: 1.0000e-04\n",
      "Epoch 291/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0037 - rmse: 0.0605\n",
      "Epoch 291: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0051 - rmse: 0.0715 - val_loss: 0.0025 - val_rmse: 0.0504 - lr: 1.0000e-04\n",
      "Epoch 292/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1943e-04 - rmse: 0.0303\n",
      "Epoch 292: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0034 - rmse: 0.0586 - val_loss: 0.0027 - val_rmse: 0.0519 - lr: 1.0000e-04\n",
      "Epoch 293/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0564\n",
      "Epoch 293: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - rmse: 0.0526 - val_loss: 0.0028 - val_rmse: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 294/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0517\n",
      "Epoch 294: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0025 - rmse: 0.0499 - val_loss: 0.0026 - val_rmse: 0.0508 - lr: 1.0000e-04\n",
      "Epoch 295/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0435\n",
      "Epoch 295: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0446 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 296/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0407\n",
      "Epoch 296: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0451 - val_loss: 0.0025 - val_rmse: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 297/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0394\n",
      "Epoch 297: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0442 - val_loss: 0.0026 - val_rmse: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 298/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5170e-04 - rmse: 0.0274\n",
      "Epoch 298: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0446 - val_loss: 0.0027 - val_rmse: 0.0517 - lr: 1.0000e-04\n",
      "Epoch 299/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0372\n",
      "Epoch 299: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0430 - val_loss: 0.0025 - val_rmse: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 300/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0376\n",
      "Epoch 300: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - rmse: 0.0454 - val_loss: 0.0026 - val_rmse: 0.0508 - lr: 1.0000e-04\n",
      "Epoch 301/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0336e-04 - rmse: 0.0283\n",
      "Epoch 301: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - rmse: 0.0455 - val_loss: 0.0027 - val_rmse: 0.0521 - lr: 1.0000e-04\n",
      "Epoch 302/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0521\n",
      "Epoch 302: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0433 - val_loss: 0.0025 - val_rmse: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 303/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0354\n",
      "Epoch 303: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0472 - val_loss: 0.0028 - val_rmse: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 304/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0453\n",
      "Epoch 304: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - rmse: 0.0456 - val_loss: 0.0025 - val_rmse: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 305/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4265e-04 - rmse: 0.0273\n",
      "Epoch 305: val_loss did not improve from 0.00244\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0018 - rmse: 0.0422 - val_loss: 0.0029 - val_rmse: 0.0537 - lr: 1.0000e-04\n",
      "Epoch 306/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0032 - rmse: 0.0563\n",
      "Epoch 306: val_loss improved from 0.00244 to 0.00239, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0020 - rmse: 0.0444 - val_loss: 0.0024 - val_rmse: 0.0489 - lr: 1.0000e-04\n",
      "Epoch 307/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0374\n",
      "Epoch 307: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0018 - rmse: 0.0428 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 308/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4690e-04 - rmse: 0.0308\n",
      "Epoch 308: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0019 - rmse: 0.0434 - val_loss: 0.0024 - val_rmse: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 309/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0481\n",
      "Epoch 309: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0423 - val_loss: 0.0027 - val_rmse: 0.0517 - lr: 1.0000e-04\n",
      "Epoch 310/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0404\n",
      "Epoch 310: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0421 - val_loss: 0.0024 - val_rmse: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 311/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0370\n",
      "Epoch 311: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0419 - val_loss: 0.0025 - val_rmse: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 312/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1367e-04 - rmse: 0.0248\n",
      "Epoch 312: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0018 - rmse: 0.0421 - val_loss: 0.0026 - val_rmse: 0.0506 - lr: 1.0000e-04\n",
      "Epoch 313/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0355\n",
      "Epoch 313: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0420 - val_loss: 0.0025 - val_rmse: 0.0498 - lr: 1.0000e-04\n",
      "Epoch 314/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0028 - rmse: 0.0527\n",
      "Epoch 314: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0426 - val_loss: 0.0024 - val_rmse: 0.0490 - lr: 1.0000e-04\n",
      "Epoch 315/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9571e-04 - rmse: 0.0199\n",
      "Epoch 315: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0444 - val_loss: 0.0027 - val_rmse: 0.0524 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0425\n",
      "Epoch 316: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - rmse: 0.0463 - val_loss: 0.0024 - val_rmse: 0.0491 - lr: 1.0000e-04\n",
      "Epoch 317/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0348\n",
      "Epoch 317: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - rmse: 0.0529 - val_loss: 0.0038 - val_rmse: 0.0616 - lr: 1.0000e-04\n",
      "Epoch 318/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0043 - rmse: 0.0655\n",
      "Epoch 318: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - rmse: 0.0532 - val_loss: 0.0030 - val_rmse: 0.0547 - lr: 1.0000e-04\n",
      "Epoch 319/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0051 - rmse: 0.0711\n",
      "Epoch 319: val_loss did not improve from 0.00239\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0032 - rmse: 0.0568 - val_loss: 0.0030 - val_rmse: 0.0547 - lr: 1.0000e-04\n",
      "Epoch 320/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1567e-04 - rmse: 0.0286\n",
      "Epoch 320: val_loss improved from 0.00239 to 0.00237, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0031 - rmse: 0.0559 - val_loss: 0.0024 - val_rmse: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 321/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8364e-04 - rmse: 0.0280\n",
      "Epoch 321: val_loss did not improve from 0.00237\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - rmse: 0.0524 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 322/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0488\n",
      "Epoch 322: val_loss did not improve from 0.00237\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0480 - val_loss: 0.0024 - val_rmse: 0.0492 - lr: 1.0000e-04\n",
      "Epoch 323/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1286e-04 - rmse: 0.0302\n",
      "Epoch 323: val_loss did not improve from 0.00237\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - rmse: 0.0473 - val_loss: 0.0026 - val_rmse: 0.0508 - lr: 1.0000e-04\n",
      "Epoch 324/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0449\n",
      "Epoch 324: val_loss did not improve from 0.00237\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0476 - val_loss: 0.0024 - val_rmse: 0.0490 - lr: 1.0000e-04\n",
      "Epoch 325/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0035 - rmse: 0.0594\n",
      "Epoch 325: val_loss improved from 0.00237 to 0.00233, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0019 - rmse: 0.0438 - val_loss: 0.0023 - val_rmse: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 326/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0460\n",
      "Epoch 326: val_loss did not improve from 0.00233\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0023 - rmse: 0.0478 - val_loss: 0.0024 - val_rmse: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 327/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0456\n",
      "Epoch 327: val_loss did not improve from 0.00233\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0020 - rmse: 0.0445 - val_loss: 0.0024 - val_rmse: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 328/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0542\n",
      "Epoch 328: val_loss did not improve from 0.00233\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0020 - rmse: 0.0445 - val_loss: 0.0026 - val_rmse: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 329/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0328\n",
      "Epoch 329: val_loss did not improve from 0.00233\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0017 - rmse: 0.0418 - val_loss: 0.0024 - val_rmse: 0.0492 - lr: 1.0000e-04\n",
      "Epoch 330/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4479e-04 - rmse: 0.0307\n",
      "Epoch 330: val_loss did not improve from 0.00233\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0017 - rmse: 0.0412 - val_loss: 0.0028 - val_rmse: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 331/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0456\n",
      "Epoch 331: val_loss improved from 0.00233 to 0.00230, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0018 - rmse: 0.0427 - val_loss: 0.0023 - val_rmse: 0.0480 - lr: 1.0000e-04\n",
      "Epoch 332/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0430\n",
      "Epoch 332: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0017 - rmse: 0.0412 - val_loss: 0.0026 - val_rmse: 0.0508 - lr: 1.0000e-04\n",
      "Epoch 333/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0025 - rmse: 0.0504\n",
      "Epoch 333: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0426 - val_loss: 0.0023 - val_rmse: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 334/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0334\n",
      "Epoch 334: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0427 - val_loss: 0.0027 - val_rmse: 0.0519 - lr: 1.0000e-04\n",
      "Epoch 335/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0548\n",
      "Epoch 335: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0020 - rmse: 0.0443 - val_loss: 0.0024 - val_rmse: 0.0489 - lr: 1.0000e-04\n",
      "Epoch 336/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0577\n",
      "Epoch 336: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0431 - val_loss: 0.0027 - val_rmse: 0.0518 - lr: 1.0000e-04\n",
      "Epoch 337/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0391\n",
      "Epoch 337: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - rmse: 0.0453 - val_loss: 0.0023 - val_rmse: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 338/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0473\n",
      "Epoch 338: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0410 - val_loss: 0.0025 - val_rmse: 0.0501 - lr: 1.0000e-04\n",
      "Epoch 339/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0358\n",
      "Epoch 339: val_loss improved from 0.00230 to 0.00230, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0016 - rmse: 0.0403 - val_loss: 0.0023 - val_rmse: 0.0479 - lr: 1.0000e-04\n",
      "Epoch 340/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0341\n",
      "Epoch 340: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0409 - val_loss: 0.0024 - val_rmse: 0.0492 - lr: 1.0000e-04\n",
      "Epoch 341/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0462\n",
      "Epoch 341: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0414 - val_loss: 0.0024 - val_rmse: 0.0485 - lr: 1.0000e-04\n",
      "Epoch 342/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0510\n",
      "Epoch 342: val_loss did not improve from 0.00230\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0400 - val_loss: 0.0024 - val_rmse: 0.0494 - lr: 1.0000e-04\n",
      "Epoch 343/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0523\n",
      "Epoch 343: val_loss improved from 0.00230 to 0.00228, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0016 - rmse: 0.0398 - val_loss: 0.0023 - val_rmse: 0.0478 - lr: 1.0000e-04\n",
      "Epoch 344/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0552\n",
      "Epoch 344: val_loss did not improve from 0.00228\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0016 - rmse: 0.0397 - val_loss: 0.0024 - val_rmse: 0.0491 - lr: 1.0000e-04\n",
      "Epoch 345/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0356\n",
      "Epoch 345: val_loss improved from 0.00228 to 0.00228, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0016 - rmse: 0.0404 - val_loss: 0.0023 - val_rmse: 0.0478 - lr: 1.0000e-04\n",
      "Epoch 346/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0394\n",
      "Epoch 346: val_loss did not improve from 0.00228\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0404 - val_loss: 0.0026 - val_rmse: 0.0513 - lr: 1.0000e-04\n",
      "Epoch 347/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0509\n",
      "Epoch 347: val_loss did not improve from 0.00228\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0017 - rmse: 0.0409 - val_loss: 0.0023 - val_rmse: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 348/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0535\n",
      "Epoch 348: val_loss did not improve from 0.00228\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0015 - rmse: 0.0394 - val_loss: 0.0024 - val_rmse: 0.0485 - lr: 1.0000e-04\n",
      "Epoch 349/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0435\n",
      "Epoch 349: val_loss improved from 0.00228 to 0.00226, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0017 - rmse: 0.0413 - val_loss: 0.0023 - val_rmse: 0.0476 - lr: 1.0000e-04\n",
      "Epoch 350/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0450\n",
      "Epoch 350: val_loss did not improve from 0.00226\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0425 - val_loss: 0.0024 - val_rmse: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 351/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6675e-04 - rmse: 0.0163\n",
      "Epoch 351: val_loss improved from 0.00226 to 0.00223, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0018 - rmse: 0.0430 - val_loss: 0.0022 - val_rmse: 0.0472 - lr: 1.0000e-04\n",
      "Epoch 352/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0335\n",
      "Epoch 352: val_loss did not improve from 0.00223\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0437 - val_loss: 0.0027 - val_rmse: 0.0517 - lr: 1.0000e-04\n",
      "Epoch 353/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0471\n",
      "Epoch 353: val_loss did not improve from 0.00223\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0410 - val_loss: 0.0022 - val_rmse: 0.0473 - lr: 1.0000e-04\n",
      "Epoch 354/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0376\n",
      "Epoch 354: val_loss did not improve from 0.00223\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0396 - val_loss: 0.0024 - val_rmse: 0.0488 - lr: 1.0000e-04\n",
      "Epoch 355/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6132e-04 - rmse: 0.0276\n",
      "Epoch 355: val_loss improved from 0.00223 to 0.00222, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0016 - rmse: 0.0396 - val_loss: 0.0022 - val_rmse: 0.0471 - lr: 1.0000e-04\n",
      "Epoch 356/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3090e-04 - rmse: 0.0182\n",
      "Epoch 356: val_loss did not improve from 0.00222\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0397 - val_loss: 0.0026 - val_rmse: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 357/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8527e-04 - rmse: 0.0298\n",
      "Epoch 357: val_loss improved from 0.00222 to 0.00220, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0017 - rmse: 0.0414 - val_loss: 0.0022 - val_rmse: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 358/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0428\n",
      "Epoch 358: val_loss did not improve from 0.00220\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0400 - val_loss: 0.0024 - val_rmse: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 359/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0374\n",
      "Epoch 359: val_loss did not improve from 0.00220\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0015 - rmse: 0.0390 - val_loss: 0.0023 - val_rmse: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 360/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0357\n",
      "Epoch 360: val_loss did not improve from 0.00220\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0392 - val_loss: 0.0023 - val_rmse: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 361/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0353\n",
      "Epoch 361: val_loss did not improve from 0.00220\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0395 - val_loss: 0.0023 - val_rmse: 0.0484 - lr: 1.0000e-04\n",
      "Epoch 362/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0370\n",
      "Epoch 362: val_loss did not improve from 0.00220\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0016 - rmse: 0.0396 - val_loss: 0.0022 - val_rmse: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 363/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0342\n",
      "Epoch 363: val_loss did not improve from 0.00220\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0015 - rmse: 0.0390 - val_loss: 0.0024 - val_rmse: 0.0486 - lr: 1.0000e-04\n",
      "Epoch 364/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0406\n",
      "Epoch 364: val_loss did not improve from 0.00220\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0016 - rmse: 0.0402 - val_loss: 0.0024 - val_rmse: 0.0485 - lr: 1.0000e-04\n",
      "Epoch 365/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8253e-04 - rmse: 0.0280\n",
      "Epoch 365: val_loss improved from 0.00220 to 0.00217, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0016 - rmse: 0.0402 - val_loss: 0.0022 - val_rmse: 0.0465 - lr: 1.0000e-04\n",
      "Epoch 366/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1111e-04 - rmse: 0.0302\n",
      "Epoch 366: val_loss did not improve from 0.00217\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0401 - val_loss: 0.0028 - val_rmse: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 367/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0506\n",
      "Epoch 367: val_loss did not improve from 0.00217\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0016 - rmse: 0.0406 - val_loss: 0.0022 - val_rmse: 0.0468 - lr: 1.0000e-04\n",
      "Epoch 368/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6182e-04 - rmse: 0.0276\n",
      "Epoch 368: val_loss did not improve from 0.00217\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0388 - val_loss: 0.0025 - val_rmse: 0.0503 - lr: 1.0000e-04\n",
      "Epoch 369/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3059e-04 - rmse: 0.0208\n",
      "Epoch 369: val_loss improved from 0.00217 to 0.00215, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0017 - rmse: 0.0409 - val_loss: 0.0022 - val_rmse: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 370/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0541\n",
      "Epoch 370: val_loss did not improve from 0.00215\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0398 - val_loss: 0.0023 - val_rmse: 0.0479 - lr: 1.0000e-04\n",
      "Epoch 371/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0026 - rmse: 0.0511\n",
      "Epoch 371: val_loss improved from 0.00215 to 0.00212, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0016 - rmse: 0.0399 - val_loss: 0.0021 - val_rmse: 0.0460 - lr: 1.0000e-04\n",
      "Epoch 372/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7012e-04 - rmse: 0.0311\n",
      "Epoch 372: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0019 - rmse: 0.0436 - val_loss: 0.0030 - val_rmse: 0.0548 - lr: 1.0000e-04\n",
      "Epoch 373/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0441\n",
      "Epoch 373: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0023 - rmse: 0.0480 - val_loss: 0.0021 - val_rmse: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 374/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0370\n",
      "Epoch 374: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0417 - val_loss: 0.0023 - val_rmse: 0.0483 - lr: 1.0000e-04\n",
      "Epoch 375/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0425\n",
      "Epoch 375: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0395 - val_loss: 0.0022 - val_rmse: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 376/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - rmse: 0.0549\n",
      "Epoch 376: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0394 - val_loss: 0.0023 - val_rmse: 0.0480 - lr: 1.0000e-04\n",
      "Epoch 377/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0475\n",
      "Epoch 377: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0017 - rmse: 0.0417 - val_loss: 0.0027 - val_rmse: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 378/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0457\n",
      "Epoch 378: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0017 - rmse: 0.0412 - val_loss: 0.0022 - val_rmse: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 379/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0338\n",
      "Epoch 379: val_loss did not improve from 0.00212\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0413 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 380/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0479\n",
      "Epoch 380: val_loss improved from 0.00212 to 0.00210, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0017 - rmse: 0.0418 - val_loss: 0.0021 - val_rmse: 0.0458 - lr: 1.0000e-04\n",
      "Epoch 381/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0474\n",
      "Epoch 381: val_loss did not improve from 0.00210\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0411 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 382/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0033 - rmse: 0.0575\n",
      "Epoch 382: val_loss improved from 0.00210 to 0.00206, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0021 - val_rmse: 0.0453 - lr: 1.0000e-04\n",
      "Epoch 383/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2966e-04 - rmse: 0.0288\n",
      "Epoch 383: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0381 - val_loss: 0.0025 - val_rmse: 0.0500 - lr: 1.0000e-04\n",
      "Epoch 384/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0437\n",
      "Epoch 384: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0374 - val_loss: 0.0021 - val_rmse: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 385/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8000e-04 - rmse: 0.0261\n",
      "Epoch 385: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0393 - val_loss: 0.0026 - val_rmse: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 386/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0337\n",
      "Epoch 386: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0015 - rmse: 0.0382 - val_loss: 0.0021 - val_rmse: 0.0454 - lr: 1.0000e-04\n",
      "Epoch 387/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0537\n",
      "Epoch 387: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0023 - val_rmse: 0.0480 - lr: 1.0000e-04\n",
      "Epoch 388/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0027 - rmse: 0.0519\n",
      "Epoch 388: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0016 - rmse: 0.0400 - val_loss: 0.0021 - val_rmse: 0.0457 - lr: 1.0000e-04\n",
      "Epoch 389/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4612e-04 - rmse: 0.0308\n",
      "Epoch 389: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0021 - val_rmse: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 390/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9485e-04 - rmse: 0.0222\n",
      "Epoch 390: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0014 - rmse: 0.0370 - val_loss: 0.0024 - val_rmse: 0.0485 - lr: 1.0000e-04\n",
      "Epoch 391/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2212e-04 - rmse: 0.0249\n",
      "Epoch 391: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0405 - val_loss: 0.0021 - val_rmse: 0.0456 - lr: 1.0000e-04\n",
      "Epoch 392/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0436\n",
      "Epoch 392: val_loss did not improve from 0.00206\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - rmse: 0.0416 - val_loss: 0.0029 - val_rmse: 0.0537 - lr: 1.0000e-04\n",
      "Epoch 393/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0432\n",
      "Epoch 393: val_loss improved from 0.00206 to 0.00200, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0019 - rmse: 0.0433 - val_loss: 0.0020 - val_rmse: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 394/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0020 - rmse: 0.0447\n",
      "Epoch 394: val_loss did not improve from 0.00200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0023 - val_rmse: 0.0483 - lr: 1.0000e-04\n",
      "Epoch 395/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0430\n",
      "Epoch 395: val_loss did not improve from 0.00200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0374 - val_loss: 0.0020 - val_rmse: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 396/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0329\n",
      "Epoch 396: val_loss did not improve from 0.00200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0370 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0483\n",
      "Epoch 397: val_loss did not improve from 0.00200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0363 - val_loss: 0.0020 - val_rmse: 0.0450 - lr: 1.0000e-04\n",
      "Epoch 398/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0466\n",
      "Epoch 398: val_loss did not improve from 0.00200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0013 - rmse: 0.0365 - val_loss: 0.0021 - val_rmse: 0.0455 - lr: 1.0000e-04\n",
      "Epoch 399/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0457\n",
      "Epoch 399: val_loss did not improve from 0.00200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0366 - val_loss: 0.0022 - val_rmse: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 400/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4241e-04 - rmse: 0.0210\n",
      "Epoch 400: val_loss improved from 0.00200 to 0.00199, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0014 - rmse: 0.0370 - val_loss: 0.0020 - val_rmse: 0.0446 - lr: 1.0000e-04\n",
      "Epoch 401/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0394\n",
      "Epoch 401: val_loss did not improve from 0.00199\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - rmse: 0.0353 - val_loss: 0.0022 - val_rmse: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 402/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8305e-04 - rmse: 0.0241\n",
      "Epoch 402: val_loss did not improve from 0.00199\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0361 - val_loss: 0.0020 - val_rmse: 0.0452 - lr: 1.0000e-04\n",
      "Epoch 403/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0479\n",
      "Epoch 403: val_loss did not improve from 0.00199\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0359 - val_loss: 0.0022 - val_rmse: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 404/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0356\n",
      "Epoch 404: val_loss improved from 0.00199 to 0.00196, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0013 - rmse: 0.0367 - val_loss: 0.0020 - val_rmse: 0.0442 - lr: 1.0000e-04\n",
      "Epoch 405/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0911e-04 - rmse: 0.0226\n",
      "Epoch 405: val_loss did not improve from 0.00196\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0428 - val_loss: 0.0029 - val_rmse: 0.0538 - lr: 1.0000e-04\n",
      "Epoch 406/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1052e-04 - rmse: 0.0267\n",
      "Epoch 406: val_loss did not improve from 0.00196\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0024 - rmse: 0.0490 - val_loss: 0.0025 - val_rmse: 0.0501 - lr: 1.0000e-04\n",
      "Epoch 407/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0023 - rmse: 0.0481\n",
      "Epoch 407: val_loss did not improve from 0.00196\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0020 - rmse: 0.0452 - val_loss: 0.0034 - val_rmse: 0.0584 - lr: 1.0000e-04\n",
      "Epoch 408/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0044 - rmse: 0.0660\n",
      "Epoch 408: val_loss improved from 0.00196 to 0.00193, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0018 - rmse: 0.0422 - val_loss: 0.0019 - val_rmse: 0.0439 - lr: 1.0000e-04\n",
      "Epoch 409/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0486\n",
      "Epoch 409: val_loss did not improve from 0.00193\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0421 - val_loss: 0.0023 - val_rmse: 0.0476 - lr: 1.0000e-04\n",
      "Epoch 410/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0429\n",
      "Epoch 410: val_loss improved from 0.00193 to 0.00189, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0014 - rmse: 0.0379 - val_loss: 0.0019 - val_rmse: 0.0435 - lr: 1.0000e-04\n",
      "Epoch 411/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7918e-04 - rmse: 0.0313\n",
      "Epoch 411: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0363 - val_loss: 0.0020 - val_rmse: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 412/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0440\n",
      "Epoch 412: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0354 - val_loss: 0.0020 - val_rmse: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 413/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2781e-04 - rmse: 0.0251\n",
      "Epoch 413: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - rmse: 0.0352 - val_loss: 0.0020 - val_rmse: 0.0445 - lr: 1.0000e-04\n",
      "Epoch 414/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7887e-04 - rmse: 0.0296\n",
      "Epoch 414: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0353 - val_loss: 0.0020 - val_rmse: 0.0444 - lr: 1.0000e-04\n",
      "Epoch 415/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2155e-04 - rmse: 0.0249\n",
      "Epoch 415: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0347 - val_loss: 0.0019 - val_rmse: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 416/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0368\n",
      "Epoch 416: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0368 - val_loss: 0.0022 - val_rmse: 0.0468 - lr: 1.0000e-04\n",
      "Epoch 417/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0363\n",
      "Epoch 417: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0395 - val_loss: 0.0019 - val_rmse: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 418/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0400\n",
      "Epoch 418: val_loss did not improve from 0.00189\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0392 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 419/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0431\n",
      "Epoch 419: val_loss improved from 0.00189 to 0.00184, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0018 - val_rmse: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 420/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4201e-04 - rmse: 0.0253\n",
      "Epoch 420: val_loss did not improve from 0.00184\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0369 - val_loss: 0.0024 - val_rmse: 0.0492 - lr: 1.0000e-04\n",
      "Epoch 421/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0372\n",
      "Epoch 421: val_loss improved from 0.00184 to 0.00181, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0014 - rmse: 0.0369 - val_loss: 0.0018 - val_rmse: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 422/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2125e-04 - rmse: 0.0304\n",
      "Epoch 422: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - rmse: 0.0345 - val_loss: 0.0023 - val_rmse: 0.0480 - lr: 1.0000e-04\n",
      "Epoch 423/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0367\n",
      "Epoch 423: val_loss did not improve from 0.00181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - rmse: 0.0348 - val_loss: 0.0018 - val_rmse: 0.0428 - lr: 1.0000e-04\n",
      "Epoch 424/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0021 - rmse: 0.0456\n",
      "Epoch 424: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0019 - val_rmse: 0.0441 - lr: 1.0000e-04\n",
      "Epoch 425/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0409\n",
      "Epoch 425: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0019 - val_rmse: 0.0431 - lr: 1.0000e-04\n",
      "Epoch 426/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0369\n",
      "Epoch 426: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0342 - val_loss: 0.0020 - val_rmse: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 427/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3445e-04 - rmse: 0.0289\n",
      "Epoch 427: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0019 - val_rmse: 0.0436 - lr: 1.0000e-04\n",
      "Epoch 428/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0345\n",
      "Epoch 428: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0019 - val_rmse: 0.0434 - lr: 1.0000e-04\n",
      "Epoch 429/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0417\n",
      "Epoch 429: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0347 - val_loss: 0.0019 - val_rmse: 0.0442 - lr: 1.0000e-04\n",
      "Epoch 430/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0345\n",
      "Epoch 430: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0352 - val_loss: 0.0018 - val_rmse: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 431/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5036e-04 - rmse: 0.0274\n",
      "Epoch 431: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0393 - val_loss: 0.0026 - val_rmse: 0.0509 - lr: 1.0000e-04\n",
      "Epoch 432/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0492\n",
      "Epoch 432: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0016 - rmse: 0.0403 - val_loss: 0.0019 - val_rmse: 0.0432 - lr: 1.0000e-04\n",
      "Epoch 433/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6102e-04 - rmse: 0.0276\n",
      "Epoch 433: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0386 - val_loss: 0.0023 - val_rmse: 0.0478 - lr: 1.0000e-04\n",
      "Epoch 434/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0333\n",
      "Epoch 434: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0373 - val_loss: 0.0018 - val_rmse: 0.0428 - lr: 1.0000e-04\n",
      "Epoch 435/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0321\n",
      "Epoch 435: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - rmse: 0.0347 - val_loss: 0.0021 - val_rmse: 0.0456 - lr: 1.0000e-04\n",
      "Epoch 436/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0378\n",
      "Epoch 436: val_loss did not improve from 0.00181\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0357 - val_loss: 0.0019 - val_rmse: 0.0434 - lr: 1.0000e-04\n",
      "Epoch 437/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0029 - rmse: 0.0534\n",
      "Epoch 437: val_loss improved from 0.00181 to 0.00179, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0014 - rmse: 0.0375 - val_loss: 0.0018 - val_rmse: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 438/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0389\n",
      "Epoch 438: val_loss did not improve from 0.00179\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0019 - val_rmse: 0.0431 - lr: 1.0000e-04\n",
      "Epoch 439/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0353\n",
      "Epoch 439: val_loss did not improve from 0.00179\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0325 - val_loss: 0.0018 - val_rmse: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 440/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0357\n",
      "Epoch 440: val_loss improved from 0.00179 to 0.00177, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0011 - rmse: 0.0330 - val_loss: 0.0018 - val_rmse: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 441/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3518e-04 - rmse: 0.0271\n",
      "Epoch 441: val_loss did not improve from 0.00177\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0352 - val_loss: 0.0022 - val_rmse: 0.0468 - lr: 1.0000e-04\n",
      "Epoch 442/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0415\n",
      "Epoch 442: val_loss did not improve from 0.00177\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0357 - val_loss: 0.0018 - val_rmse: 0.0422 - lr: 1.0000e-04\n",
      "Epoch 443/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8504e-04 - rmse: 0.0262\n",
      "Epoch 443: val_loss did not improve from 0.00177\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0013 - rmse: 0.0366 - val_loss: 0.0023 - val_rmse: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 444/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0344\n",
      "Epoch 444: val_loss improved from 0.00177 to 0.00174, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0017 - val_rmse: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 445/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0238e-04 - rmse: 0.0201\n",
      "Epoch 445: val_loss did not improve from 0.00174\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0344 - val_loss: 0.0022 - val_rmse: 0.0471 - lr: 1.0000e-04\n",
      "Epoch 446/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0338\n",
      "Epoch 446: val_loss improved from 0.00174 to 0.00174, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0012 - rmse: 0.0349 - val_loss: 0.0017 - val_rmse: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 447/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7926e-04 - rmse: 0.0261\n",
      "Epoch 447: val_loss did not improve from 0.00174\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0331 - val_loss: 0.0020 - val_rmse: 0.0446 - lr: 1.0000e-04\n",
      "Epoch 448/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3566e-04 - rmse: 0.0306\n",
      "Epoch 448: val_loss did not improve from 0.00174\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0339 - val_loss: 0.0018 - val_rmse: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 449/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0018 - rmse: 0.0423\n",
      "Epoch 449: val_loss improved from 0.00174 to 0.00172, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0017 - val_rmse: 0.0415 - lr: 1.0000e-04\n",
      "Epoch 450/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9472e-04 - rmse: 0.0172\n",
      "Epoch 450: val_loss did not improve from 0.00172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0349 - val_loss: 0.0023 - val_rmse: 0.0484 - lr: 1.0000e-04\n",
      "Epoch 451/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0328\n",
      "Epoch 451: val_loss did not improve from 0.00172\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0017 - val_rmse: 0.0415 - lr: 1.0000e-04\n",
      "Epoch 452/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0321\n",
      "Epoch 452: val_loss did not improve from 0.00172\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0338 - val_loss: 0.0019 - val_rmse: 0.0441 - lr: 1.0000e-04\n",
      "Epoch 453/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0388\n",
      "Epoch 453: val_loss did not improve from 0.00172\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0321 - val_loss: 0.0017 - val_rmse: 0.0418 - lr: 1.0000e-04\n",
      "Epoch 454/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7055e-04 - rmse: 0.0295\n",
      "Epoch 454: val_loss did not improve from 0.00172\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0020 - val_rmse: 0.0451 - lr: 1.0000e-04\n",
      "Epoch 455/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7064e-04 - rmse: 0.0217\n",
      "Epoch 455: val_loss improved from 0.00172 to 0.00171, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0010 - rmse: 0.0324 - val_loss: 0.0017 - val_rmse: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 456/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0024 - rmse: 0.0490\n",
      "Epoch 456: val_loss did not improve from 0.00171\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0343 - val_loss: 0.0018 - val_rmse: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 457/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0401\n",
      "Epoch 457: val_loss improved from 0.00171 to 0.00163, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0010 - rmse: 0.0317 - val_loss: 0.0016 - val_rmse: 0.0404 - lr: 1.0000e-04\n",
      "Epoch 458/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6309e-04 - rmse: 0.0191\n",
      "Epoch 458: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0327 - val_loss: 0.0020 - val_rmse: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 459/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0367\n",
      "Epoch 459: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0323 - val_loss: 0.0017 - val_rmse: 0.0408 - lr: 1.0000e-04\n",
      "Epoch 460/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0370\n",
      "Epoch 460: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0017 - val_rmse: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 461/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4111e-04 - rmse: 0.0233\n",
      "Epoch 461: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0317 - val_loss: 0.0018 - val_rmse: 0.0422 - lr: 1.0000e-04\n",
      "Epoch 462/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0376\n",
      "Epoch 462: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6581e-04 - rmse: 0.0311 - val_loss: 0.0018 - val_rmse: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 463/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0396\n",
      "Epoch 463: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7217e-04 - rmse: 0.0312 - val_loss: 0.0018 - val_rmse: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 464/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0320\n",
      "Epoch 464: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5664e-04 - rmse: 0.0309 - val_loss: 0.0017 - val_rmse: 0.0412 - lr: 1.0000e-04\n",
      "Epoch 465/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3056e-04 - rmse: 0.0182\n",
      "Epoch 465: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1389e-04 - rmse: 0.0302 - val_loss: 0.0018 - val_rmse: 0.0422 - lr: 1.0000e-04\n",
      "Epoch 466/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3218e-04 - rmse: 0.0182\n",
      "Epoch 466: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0945e-04 - rmse: 0.0302 - val_loss: 0.0017 - val_rmse: 0.0411 - lr: 1.0000e-04\n",
      "Epoch 467/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2544e-04 - rmse: 0.0250\n",
      "Epoch 467: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0320 - val_loss: 0.0021 - val_rmse: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 468/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0354\n",
      "Epoch 468: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0336 - val_loss: 0.0017 - val_rmse: 0.0407 - lr: 1.0000e-04\n",
      "Epoch 469/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2034e-04 - rmse: 0.0303\n",
      "Epoch 469: val_loss did not improve from 0.00163\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0010 - rmse: 0.0323 - val_loss: 0.0019 - val_rmse: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 470/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2638e-04 - rmse: 0.0150\n",
      "Epoch 470: val_loss improved from 0.00163 to 0.00160, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 9.3229e-04 - rmse: 0.0305 - val_loss: 0.0016 - val_rmse: 0.0400 - lr: 1.0000e-04\n",
      "Epoch 471/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0377\n",
      "Epoch 471: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0332 - val_loss: 0.0016 - val_rmse: 0.0405 - lr: 1.0000e-04\n",
      "Epoch 472/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6741e-04 - rmse: 0.0295\n",
      "Epoch 472: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0010 - rmse: 0.0324 - val_loss: 0.0020 - val_rmse: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 473/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6698e-04 - rmse: 0.0258\n",
      "Epoch 473: val_loss improved from 0.00160 to 0.00160, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0012 - rmse: 0.0342 - val_loss: 0.0016 - val_rmse: 0.0399 - lr: 1.0000e-04\n",
      "Epoch 474/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0383\n",
      "Epoch 474: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0336 - val_loss: 0.0017 - val_rmse: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 475/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0332\n",
      "Epoch 475: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.9435e-04 - rmse: 0.0315 - val_loss: 0.0019 - val_rmse: 0.0432 - lr: 1.0000e-04\n",
      "Epoch 476/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1259e-04 - rmse: 0.0177\n",
      "Epoch 476: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0013 - rmse: 0.0357 - val_loss: 0.0018 - val_rmse: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 477/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0396\n",
      "Epoch 477: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0018 - rmse: 0.0419 - val_loss: 0.0028 - val_rmse: 0.0525 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 478/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0361\n",
      "Epoch 478: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0015 - rmse: 0.0389 - val_loss: 0.0022 - val_rmse: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 479/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0405\n",
      "Epoch 479: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0019 - rmse: 0.0433 - val_loss: 0.0024 - val_rmse: 0.0495 - lr: 1.0000e-04\n",
      "Epoch 480/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0019 - rmse: 0.0434\n",
      "Epoch 480: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0363 - val_loss: 0.0016 - val_rmse: 0.0403 - lr: 1.0000e-04\n",
      "Epoch 481/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0319e-04 - rmse: 0.0174\n",
      "Epoch 481: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0327 - val_loss: 0.0018 - val_rmse: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 482/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0352\n",
      "Epoch 482: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0016 - val_rmse: 0.0406 - lr: 1.0000e-04\n",
      "Epoch 483/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0357\n",
      "Epoch 483: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1149e-04 - rmse: 0.0302 - val_loss: 0.0016 - val_rmse: 0.0403 - lr: 1.0000e-04\n",
      "Epoch 484/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3450e-04 - rmse: 0.0306\n",
      "Epoch 484: val_loss did not improve from 0.00160\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1355e-04 - rmse: 0.0302 - val_loss: 0.0016 - val_rmse: 0.0403 - lr: 1.0000e-04\n",
      "Epoch 485/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0377\n",
      "Epoch 485: val_loss improved from 0.00160 to 0.00153, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0011 - rmse: 0.0334 - val_loss: 0.0015 - val_rmse: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 486/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1990e-04 - rmse: 0.0249\n",
      "Epoch 486: val_loss did not improve from 0.00153\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7707e-04 - rmse: 0.0313 - val_loss: 0.0016 - val_rmse: 0.0394 - lr: 1.0000e-04\n",
      "Epoch 487/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9298e-04 - rmse: 0.0244\n",
      "Epoch 487: val_loss did not improve from 0.00153\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0317 - val_loss: 0.0020 - val_rmse: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 488/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4170e-04 - rmse: 0.0272\n",
      "Epoch 488: val_loss did not improve from 0.00153\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0332 - val_loss: 0.0016 - val_rmse: 0.0401 - lr: 1.0000e-04\n",
      "Epoch 489/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8322e-04 - rmse: 0.0241\n",
      "Epoch 489: val_loss did not improve from 0.00153\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0012 - rmse: 0.0339 - val_loss: 0.0020 - val_rmse: 0.0452 - lr: 1.0000e-04\n",
      "Epoch 490/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0412\n",
      "Epoch 490: val_loss improved from 0.00153 to 0.00152, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0011 - rmse: 0.0336 - val_loss: 0.0015 - val_rmse: 0.0390 - lr: 1.0000e-04\n",
      "Epoch 491/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2637e-04 - rmse: 0.0304\n",
      "Epoch 491: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1409e-04 - rmse: 0.0285 - val_loss: 0.0017 - val_rmse: 0.0414 - lr: 1.0000e-04\n",
      "Epoch 492/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8206e-04 - rmse: 0.0241\n",
      "Epoch 492: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6709e-04 - rmse: 0.0294 - val_loss: 0.0016 - val_rmse: 0.0402 - lr: 1.0000e-04\n",
      "Epoch 493/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7366e-04 - rmse: 0.0240\n",
      "Epoch 493: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4609e-04 - rmse: 0.0291 - val_loss: 0.0016 - val_rmse: 0.0397 - lr: 1.0000e-04\n",
      "Epoch 494/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9224e-04 - rmse: 0.0299\n",
      "Epoch 494: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1339e-04 - rmse: 0.0285 - val_loss: 0.0016 - val_rmse: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 495/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0449e-04 - rmse: 0.0265\n",
      "Epoch 495: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.0122e-04 - rmse: 0.0283 - val_loss: 0.0016 - val_rmse: 0.0397 - lr: 1.0000e-04\n",
      "Epoch 496/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0099e-04 - rmse: 0.0283\n",
      "Epoch 496: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5722e-04 - rmse: 0.0293 - val_loss: 0.0018 - val_rmse: 0.0422 - lr: 1.0000e-04\n",
      "Epoch 497/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9109e-04 - rmse: 0.0281\n",
      "Epoch 497: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4223e-04 - rmse: 0.0290 - val_loss: 0.0015 - val_rmse: 0.0393 - lr: 1.0000e-04\n",
      "Epoch 498/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0047e-04 - rmse: 0.0300\n",
      "Epoch 498: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.0019e-04 - rmse: 0.0283 - val_loss: 0.0016 - val_rmse: 0.0397 - lr: 1.0000e-04\n",
      "Epoch 499/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2415e-04 - rmse: 0.0180\n",
      "Epoch 499: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4439e-04 - rmse: 0.0291 - val_loss: 0.0017 - val_rmse: 0.0411 - lr: 1.0000e-04\n",
      "Epoch 500/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4958e-04 - rmse: 0.0122\n",
      "Epoch 500: val_loss improved from 0.00152 to 0.00152, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.7796e-04 - rmse: 0.0279 - val_loss: 0.0015 - val_rmse: 0.0390 - lr: 1.0000e-04\n",
      "Epoch 501/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0351\n",
      "Epoch 501: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.8107e-04 - rmse: 0.0279 - val_loss: 0.0016 - val_rmse: 0.0399 - lr: 1.0000e-04\n",
      "Epoch 502/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3033e-04 - rmse: 0.0207\n",
      "Epoch 502: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6559e-04 - rmse: 0.0277 - val_loss: 0.0016 - val_rmse: 0.0399 - lr: 1.0000e-04\n",
      "Epoch 503/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4143e-04 - rmse: 0.0185\n",
      "Epoch 503: val_loss did not improve from 0.00152\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.0616e-04 - rmse: 0.0284 - val_loss: 0.0016 - val_rmse: 0.0400 - lr: 1.0000e-04\n",
      "Epoch 504/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0341\n",
      "Epoch 504: val_loss improved from 0.00152 to 0.00151, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 7.7432e-04 - rmse: 0.0278 - val_loss: 0.0015 - val_rmse: 0.0388 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0343\n",
      "Epoch 505: val_loss did not improve from 0.00151\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.2746e-04 - rmse: 0.0288 - val_loss: 0.0015 - val_rmse: 0.0391 - lr: 1.0000e-04\n",
      "Epoch 506/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1023e-04 - rmse: 0.0285\n",
      "Epoch 506: val_loss did not improve from 0.00151\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8772e-04 - rmse: 0.0281 - val_loss: 0.0015 - val_rmse: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 507/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2558e-04 - rmse: 0.0229\n",
      "Epoch 507: val_loss did not improve from 0.00151\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5168e-04 - rmse: 0.0274 - val_loss: 0.0016 - val_rmse: 0.0403 - lr: 1.0000e-04\n",
      "Epoch 508/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0317\n",
      "Epoch 508: val_loss improved from 0.00151 to 0.00148, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.9349e-04 - rmse: 0.0282 - val_loss: 0.0015 - val_rmse: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 509/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6892e-04 - rmse: 0.0277\n",
      "Epoch 509: val_loss did not improve from 0.00148\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1446e-04 - rmse: 0.0285 - val_loss: 0.0018 - val_rmse: 0.0424 - lr: 1.0000e-04\n",
      "Epoch 510/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2534e-04 - rmse: 0.0229\n",
      "Epoch 510: val_loss did not improve from 0.00148\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7013e-04 - rmse: 0.0295 - val_loss: 0.0015 - val_rmse: 0.0386 - lr: 1.0000e-04\n",
      "Epoch 511/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0342\n",
      "Epoch 511: val_loss did not improve from 0.00148\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9734e-04 - rmse: 0.0300 - val_loss: 0.0015 - val_rmse: 0.0393 - lr: 1.0000e-04\n",
      "Epoch 512/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8523e-04 - rmse: 0.0298\n",
      "Epoch 512: val_loss did not improve from 0.00148\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2335e-04 - rmse: 0.0304 - val_loss: 0.0018 - val_rmse: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 513/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2395e-04 - rmse: 0.0269\n",
      "Epoch 513: val_loss improved from 0.00148 to 0.00146, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.4349e-04 - rmse: 0.0290 - val_loss: 0.0015 - val_rmse: 0.0382 - lr: 1.0000e-04\n",
      "Epoch 514/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1512e-04 - rmse: 0.0227\n",
      "Epoch 514: val_loss did not improve from 0.00146\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6778e-04 - rmse: 0.0277 - val_loss: 0.0017 - val_rmse: 0.0418 - lr: 1.0000e-04\n",
      "Epoch 515/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0161e-04 - rmse: 0.0101\n",
      "Epoch 515: val_loss did not improve from 0.00146\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.1869e-04 - rmse: 0.0268 - val_loss: 0.0015 - val_rmse: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 516/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1138e-04 - rmse: 0.0285\n",
      "Epoch 516: val_loss did not improve from 0.00146\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6156e-04 - rmse: 0.0310 - val_loss: 0.0021 - val_rmse: 0.0455 - lr: 1.0000e-04\n",
      "Epoch 517/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0373\n",
      "Epoch 517: val_loss did not improve from 0.00146\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0357 - val_loss: 0.0015 - val_rmse: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 518/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1505e-04 - rmse: 0.0267\n",
      "Epoch 518: val_loss did not improve from 0.00146\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1678e-04 - rmse: 0.0286 - val_loss: 0.0016 - val_rmse: 0.0404 - lr: 1.0000e-04\n",
      "Epoch 519/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6801e-04 - rmse: 0.0192\n",
      "Epoch 519: val_loss did not improve from 0.00146\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7121e-04 - rmse: 0.0278 - val_loss: 0.0015 - val_rmse: 0.0390 - lr: 1.0000e-04\n",
      "Epoch 520/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0344\n",
      "Epoch 520: val_loss did not improve from 0.00146\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.0668e-04 - rmse: 0.0284 - val_loss: 0.0017 - val_rmse: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 521/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6945e-04 - rmse: 0.0217\n",
      "Epoch 521: val_loss improved from 0.00146 to 0.00144, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8.6313e-04 - rmse: 0.0294 - val_loss: 0.0014 - val_rmse: 0.0380 - lr: 1.0000e-04\n",
      "Epoch 522/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2809e-04 - rmse: 0.0270\n",
      "Epoch 522: val_loss did not improve from 0.00144\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8140e-04 - rmse: 0.0297 - val_loss: 0.0017 - val_rmse: 0.0411 - lr: 1.0000e-04\n",
      "Epoch 523/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3820e-04 - rmse: 0.0209\n",
      "Epoch 523: val_loss did not improve from 0.00144\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0010 - rmse: 0.0320 - val_loss: 0.0016 - val_rmse: 0.0406 - lr: 1.0000e-04\n",
      "Epoch 524/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7409e-04 - rmse: 0.0218\n",
      "Epoch 524: val_loss improved from 0.00144 to 0.00142, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 9.5416e-04 - rmse: 0.0309 - val_loss: 0.0014 - val_rmse: 0.0376 - lr: 1.0000e-04\n",
      "Epoch 525/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1353e-04 - rmse: 0.0285\n",
      "Epoch 525: val_loss did not improve from 0.00142\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1960e-04 - rmse: 0.0286 - val_loss: 0.0017 - val_rmse: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 526/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6095e-04 - rmse: 0.0293\n",
      "Epoch 526: val_loss improved from 0.00142 to 0.00141, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 8.3511e-04 - rmse: 0.0289 - val_loss: 0.0014 - val_rmse: 0.0375 - lr: 1.0000e-04\n",
      "Epoch 527/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7586e-04 - rmse: 0.0240\n",
      "Epoch 527: val_loss did not improve from 0.00141\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.9598e-04 - rmse: 0.0264 - val_loss: 0.0017 - val_rmse: 0.0418 - lr: 1.0000e-04\n",
      "Epoch 528/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3019e-04 - rmse: 0.0152\n",
      "Epoch 528: val_loss did not improve from 0.00141\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.8875e-04 - rmse: 0.0262 - val_loss: 0.0014 - val_rmse: 0.0380 - lr: 1.0000e-04\n",
      "Epoch 529/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3912e-04 - rmse: 0.0290\n",
      "Epoch 529: val_loss did not improve from 0.00141\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5507e-04 - rmse: 0.0275 - val_loss: 0.0019 - val_rmse: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 530/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0356\n",
      "Epoch 530: val_loss improved from 0.00141 to 0.00137, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.9386e-04 - rmse: 0.0282 - val_loss: 0.0014 - val_rmse: 0.0370 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3797e-04 - rmse: 0.0253\n",
      "Epoch 531: val_loss did not improve from 0.00137\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.8775e-04 - rmse: 0.0281 - val_loss: 0.0014 - val_rmse: 0.0378 - lr: 1.0000e-04\n",
      "Epoch 532/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6068e-04 - rmse: 0.0237\n",
      "Epoch 532: val_loss did not improve from 0.00137\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.7118e-04 - rmse: 0.0278 - val_loss: 0.0016 - val_rmse: 0.0398 - lr: 1.0000e-04\n",
      "Epoch 533/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0317\n",
      "Epoch 533: val_loss improved from 0.00137 to 0.00136, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 7.7976e-04 - rmse: 0.0279 - val_loss: 0.0014 - val_rmse: 0.0369 - lr: 1.0000e-04\n",
      "Epoch 534/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0381\n",
      "Epoch 534: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.9698e-04 - rmse: 0.0282 - val_loss: 0.0017 - val_rmse: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 535/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3455e-04 - rmse: 0.0289\n",
      "Epoch 535: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5407e-04 - rmse: 0.0275 - val_loss: 0.0014 - val_rmse: 0.0374 - lr: 1.0000e-04\n",
      "Epoch 536/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7539e-04 - rmse: 0.0278\n",
      "Epoch 536: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5333e-04 - rmse: 0.0274 - val_loss: 0.0019 - val_rmse: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 537/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7117e-04 - rmse: 0.0165\n",
      "Epoch 537: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3638e-04 - rmse: 0.0289 - val_loss: 0.0014 - val_rmse: 0.0378 - lr: 1.0000e-04\n",
      "Epoch 538/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0349\n",
      "Epoch 538: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7403e-04 - rmse: 0.0312 - val_loss: 0.0018 - val_rmse: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 539/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3395e-04 - rmse: 0.0289\n",
      "Epoch 539: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8873e-04 - rmse: 0.0298 - val_loss: 0.0014 - val_rmse: 0.0369 - lr: 1.0000e-04\n",
      "Epoch 540/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5085e-04 - rmse: 0.0255\n",
      "Epoch 540: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5787e-04 - rmse: 0.0256 - val_loss: 0.0014 - val_rmse: 0.0376 - lr: 1.0000e-04\n",
      "Epoch 541/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0053e-04 - rmse: 0.0245\n",
      "Epoch 541: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.6158e-04 - rmse: 0.0257 - val_loss: 0.0016 - val_rmse: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 542/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0325\n",
      "Epoch 542: val_loss did not improve from 0.00136\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3105e-04 - rmse: 0.0270 - val_loss: 0.0016 - val_rmse: 0.0397 - lr: 1.0000e-04\n",
      "Epoch 543/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5132e-04 - rmse: 0.0292\n",
      "Epoch 543: val_loss improved from 0.00136 to 0.00135, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.4188e-04 - rmse: 0.0272 - val_loss: 0.0014 - val_rmse: 0.0368 - lr: 1.0000e-04\n",
      "Epoch 544/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0394\n",
      "Epoch 544: val_loss did not improve from 0.00135\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.4077e-04 - rmse: 0.0253 - val_loss: 0.0014 - val_rmse: 0.0378 - lr: 1.0000e-04\n",
      "Epoch 545/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9291e-04 - rmse: 0.0243\n",
      "Epoch 545: val_loss improved from 0.00135 to 0.00132, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.1425e-04 - rmse: 0.0248 - val_loss: 0.0013 - val_rmse: 0.0364 - lr: 1.0000e-04\n",
      "Epoch 546/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0367\n",
      "Epoch 546: val_loss did not improve from 0.00132\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.2620e-04 - rmse: 0.0250 - val_loss: 0.0015 - val_rmse: 0.0391 - lr: 1.0000e-04\n",
      "Epoch 547/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4302e-04 - rmse: 0.0254\n",
      "Epoch 547: val_loss did not improve from 0.00132\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.4974e-04 - rmse: 0.0255 - val_loss: 0.0014 - val_rmse: 0.0368 - lr: 1.0000e-04\n",
      "Epoch 548/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6201e-04 - rmse: 0.0237\n",
      "Epoch 548: val_loss improved from 0.00132 to 0.00131, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.0347e-04 - rmse: 0.0265 - val_loss: 0.0013 - val_rmse: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 549/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3973e-04 - rmse: 0.0253\n",
      "Epoch 549: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.4526e-04 - rmse: 0.0273 - val_loss: 0.0019 - val_rmse: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 550/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0334\n",
      "Epoch 550: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0328 - val_loss: 0.0013 - val_rmse: 0.0365 - lr: 1.0000e-04\n",
      "Epoch 551/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4756e-04 - rmse: 0.0234\n",
      "Epoch 551: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8282e-04 - rmse: 0.0280 - val_loss: 0.0015 - val_rmse: 0.0386 - lr: 1.0000e-04\n",
      "Epoch 552/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6705e-04 - rmse: 0.0129\n",
      "Epoch 552: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.8255e-04 - rmse: 0.0261 - val_loss: 0.0014 - val_rmse: 0.0379 - lr: 1.0000e-04\n",
      "Epoch 553/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3759e-04 - rmse: 0.0306\n",
      "Epoch 553: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.8182e-04 - rmse: 0.0261 - val_loss: 0.0013 - val_rmse: 0.0365 - lr: 1.0000e-04\n",
      "Epoch 554/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6659e-04 - rmse: 0.0294\n",
      "Epoch 554: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.0261e-04 - rmse: 0.0265 - val_loss: 0.0015 - val_rmse: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 555/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5021e-04 - rmse: 0.0274\n",
      "Epoch 555: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.2527e-04 - rmse: 0.0250 - val_loss: 0.0013 - val_rmse: 0.0365 - lr: 1.0000e-04\n",
      "Epoch 556/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9275e-04 - rmse: 0.0171\n",
      "Epoch 556: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.2476e-04 - rmse: 0.0250 - val_loss: 0.0014 - val_rmse: 0.0378 - lr: 1.0000e-04\n",
      "Epoch 557/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3321e-04 - rmse: 0.0231\n",
      "Epoch 557: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.9925e-04 - rmse: 0.0245 - val_loss: 0.0013 - val_rmse: 0.0364 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 558/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6313e-04 - rmse: 0.0191\n",
      "Epoch 558: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.2937e-04 - rmse: 0.0251 - val_loss: 0.0014 - val_rmse: 0.0373 - lr: 1.0000e-04\n",
      "Epoch 559/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5582e-04 - rmse: 0.0189\n",
      "Epoch 559: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.5711e-04 - rmse: 0.0236 - val_loss: 0.0014 - val_rmse: 0.0369 - lr: 1.0000e-04\n",
      "Epoch 560/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8057e-04 - rmse: 0.0261\n",
      "Epoch 560: val_loss did not improve from 0.00131\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.1836e-04 - rmse: 0.0249 - val_loss: 0.0014 - val_rmse: 0.0376 - lr: 1.0000e-04\n",
      "Epoch 561/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9050e-04 - rmse: 0.0198\n",
      "Epoch 561: val_loss improved from 0.00131 to 0.00129, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 5.5168e-04 - rmse: 0.0235 - val_loss: 0.0013 - val_rmse: 0.0359 - lr: 1.0000e-04\n",
      "Epoch 562/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6605e-04 - rmse: 0.0258\n",
      "Epoch 562: val_loss did not improve from 0.00129\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.7284e-04 - rmse: 0.0259 - val_loss: 0.0016 - val_rmse: 0.0401 - lr: 1.0000e-04\n",
      "Epoch 563/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2943e-04 - rmse: 0.0305\n",
      "Epoch 563: val_loss improved from 0.00129 to 0.00125, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8.8026e-04 - rmse: 0.0297 - val_loss: 0.0012 - val_rmse: 0.0353 - lr: 1.0000e-04\n",
      "Epoch 564/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1308e-04 - rmse: 0.0248\n",
      "Epoch 564: val_loss did not improve from 0.00125\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.0642e-04 - rmse: 0.0266 - val_loss: 0.0013 - val_rmse: 0.0358 - lr: 1.0000e-04\n",
      "Epoch 565/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6639e-04 - rmse: 0.0311\n",
      "Epoch 565: val_loss did not improve from 0.00125\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.8355e-04 - rmse: 0.0261 - val_loss: 0.0014 - val_rmse: 0.0375 - lr: 1.0000e-04\n",
      "Epoch 566/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2502e-04 - rmse: 0.0180\n",
      "Epoch 566: val_loss improved from 0.00125 to 0.00117, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.4675e-04 - rmse: 0.0254 - val_loss: 0.0012 - val_rmse: 0.0343 - lr: 1.0000e-04\n",
      "Epoch 567/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3181e-04 - rmse: 0.0251\n",
      "Epoch 567: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3571e-04 - rmse: 0.0271 - val_loss: 0.0022 - val_rmse: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 568/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0418\n",
      "Epoch 568: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8515e-04 - rmse: 0.0298 - val_loss: 0.0013 - val_rmse: 0.0362 - lr: 1.0000e-04\n",
      "Epoch 569/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0321\n",
      "Epoch 569: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.9913e-04 - rmse: 0.0316 - val_loss: 0.0015 - val_rmse: 0.0383 - lr: 1.0000e-04\n",
      "Epoch 570/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6078e-04 - rmse: 0.0276\n",
      "Epoch 570: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5500e-04 - rmse: 0.0256 - val_loss: 0.0012 - val_rmse: 0.0346 - lr: 1.0000e-04\n",
      "Epoch 571/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5330e-04 - rmse: 0.0188\n",
      "Epoch 571: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8433e-04 - rmse: 0.0242 - val_loss: 0.0012 - val_rmse: 0.0345 - lr: 1.0000e-04\n",
      "Epoch 572/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7800e-04 - rmse: 0.0219\n",
      "Epoch 572: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.0664e-04 - rmse: 0.0266 - val_loss: 0.0015 - val_rmse: 0.0388 - lr: 1.0000e-04\n",
      "Epoch 573/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0013 - rmse: 0.0360\n",
      "Epoch 573: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.4437e-04 - rmse: 0.0254 - val_loss: 0.0012 - val_rmse: 0.0352 - lr: 1.0000e-04\n",
      "Epoch 574/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0317\n",
      "Epoch 574: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5411e-04 - rmse: 0.0235 - val_loss: 0.0013 - val_rmse: 0.0361 - lr: 1.0000e-04\n",
      "Epoch 575/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7922e-04 - rmse: 0.0297\n",
      "Epoch 575: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2289e-04 - rmse: 0.0229 - val_loss: 0.0012 - val_rmse: 0.0351 - lr: 1.0000e-04\n",
      "Epoch 576/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6051e-04 - rmse: 0.0276\n",
      "Epoch 576: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5427e-04 - rmse: 0.0235 - val_loss: 0.0012 - val_rmse: 0.0349 - lr: 1.0000e-04\n",
      "Epoch 577/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4930e-04 - rmse: 0.0158\n",
      "Epoch 577: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1873e-04 - rmse: 0.0228 - val_loss: 0.0013 - val_rmse: 0.0359 - lr: 1.0000e-04\n",
      "Epoch 578/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4314e-04 - rmse: 0.0273\n",
      "Epoch 578: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2511e-04 - rmse: 0.0229 - val_loss: 0.0014 - val_rmse: 0.0370 - lr: 1.0000e-04\n",
      "Epoch 579/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5397e-04 - rmse: 0.0235\n",
      "Epoch 579: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9290e-04 - rmse: 0.0222 - val_loss: 0.0012 - val_rmse: 0.0345 - lr: 1.0000e-04\n",
      "Epoch 580/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7642e-04 - rmse: 0.0312\n",
      "Epoch 580: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1131e-04 - rmse: 0.0226 - val_loss: 0.0013 - val_rmse: 0.0364 - lr: 1.0000e-04\n",
      "Epoch 581/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2102e-04 - rmse: 0.0205\n",
      "Epoch 581: val_loss improved from 0.00117 to 0.00117, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 5.0931e-04 - rmse: 0.0226 - val_loss: 0.0012 - val_rmse: 0.0342 - lr: 1.0000e-04\n",
      "Epoch 582/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7525e-04 - rmse: 0.0132\n",
      "Epoch 582: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9617e-04 - rmse: 0.0223 - val_loss: 0.0012 - val_rmse: 0.0347 - lr: 1.0000e-04\n",
      "Epoch 583/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4919e-04 - rmse: 0.0158\n",
      "Epoch 583: val_loss did not improve from 0.00117\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5167e-04 - rmse: 0.0235 - val_loss: 0.0014 - val_rmse: 0.0370 - lr: 1.0000e-04\n",
      "Epoch 584/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7751e-04 - rmse: 0.0133\n",
      "Epoch 584: val_loss improved from 0.00117 to 0.00116, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 27ms/step - loss: 5.8793e-04 - rmse: 0.0242 - val_loss: 0.0012 - val_rmse: 0.0341 - lr: 1.0000e-04\n",
      "Epoch 585/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0082e-04 - rmse: 0.0200\n",
      "Epoch 585: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8125e-04 - rmse: 0.0241 - val_loss: 0.0017 - val_rmse: 0.0412 - lr: 1.0000e-04\n",
      "Epoch 586/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1867e-04 - rmse: 0.0179\n",
      "Epoch 586: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6474e-04 - rmse: 0.0238 - val_loss: 0.0012 - val_rmse: 0.0348 - lr: 1.0000e-04\n",
      "Epoch 587/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5143e-04 - rmse: 0.0212\n",
      "Epoch 587: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.0818e-04 - rmse: 0.0247 - val_loss: 0.0013 - val_rmse: 0.0359 - lr: 1.0000e-04\n",
      "Epoch 588/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3555e-04 - rmse: 0.0252\n",
      "Epoch 588: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8373e-04 - rmse: 0.0242 - val_loss: 0.0014 - val_rmse: 0.0373 - lr: 1.0000e-04\n",
      "Epoch 589/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5120e-04 - rmse: 0.0274\n",
      "Epoch 589: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6756e-04 - rmse: 0.0238 - val_loss: 0.0012 - val_rmse: 0.0342 - lr: 1.0000e-04\n",
      "Epoch 590/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8053e-04 - rmse: 0.0241\n",
      "Epoch 590: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.7486e-04 - rmse: 0.0260 - val_loss: 0.0021 - val_rmse: 0.0462 - lr: 1.0000e-04\n",
      "Epoch 591/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7002e-04 - rmse: 0.0311\n",
      "Epoch 591: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0013 - rmse: 0.0360 - val_loss: 0.0014 - val_rmse: 0.0370 - lr: 1.0000e-04\n",
      "Epoch 592/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5611e-04 - rmse: 0.0189\n",
      "Epoch 592: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0012 - val_rmse: 0.0343 - lr: 1.0000e-04\n",
      "Epoch 593/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3777e-04 - rmse: 0.0154\n",
      "Epoch 593: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.0395e-04 - rmse: 0.0284 - val_loss: 0.0015 - val_rmse: 0.0388 - lr: 1.0000e-04\n",
      "Epoch 594/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3422e-04 - rmse: 0.0252\n",
      "Epoch 594: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2790e-04 - rmse: 0.0305 - val_loss: 0.0017 - val_rmse: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 595/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0418\n",
      "Epoch 595: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0335 - val_loss: 0.0022 - val_rmse: 0.0473 - lr: 1.0000e-04\n",
      "Epoch 596/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0015 - rmse: 0.0387\n",
      "Epoch 596: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5663e-04 - rmse: 0.0293 - val_loss: 0.0014 - val_rmse: 0.0369 - lr: 1.0000e-04\n",
      "Epoch 597/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2489e-04 - rmse: 0.0287\n",
      "Epoch 597: val_loss did not improve from 0.00116\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4945e-04 - rmse: 0.0291 - val_loss: 0.0013 - val_rmse: 0.0366 - lr: 1.0000e-04\n",
      "Epoch 598/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1154e-04 - rmse: 0.0247\n",
      "Epoch 598: val_loss improved from 0.00116 to 0.00112, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.2930e-04 - rmse: 0.0230 - val_loss: 0.0011 - val_rmse: 0.0335 - lr: 1.0000e-04\n",
      "Epoch 599/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5558e-04 - rmse: 0.0213\n",
      "Epoch 599: val_loss did not improve from 0.00112\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2708e-04 - rmse: 0.0230 - val_loss: 0.0012 - val_rmse: 0.0346 - lr: 1.0000e-04\n",
      "Epoch 600/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8557e-04 - rmse: 0.0196\n",
      "Epoch 600: val_loss did not improve from 0.00112\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.3532e-04 - rmse: 0.0231 - val_loss: 0.0013 - val_rmse: 0.0363 - lr: 1.0000e-04\n",
      "Epoch 601/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0034e-04 - rmse: 0.0224\n",
      "Epoch 601: val_loss improved from 0.00112 to 0.00111, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.7158e-04 - rmse: 0.0239 - val_loss: 0.0011 - val_rmse: 0.0333 - lr: 1.0000e-04\n",
      "Epoch 602/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8262e-04 - rmse: 0.0241\n",
      "Epoch 602: val_loss did not improve from 0.00111\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7967e-04 - rmse: 0.0219 - val_loss: 0.0012 - val_rmse: 0.0344 - lr: 1.0000e-04\n",
      "Epoch 603/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7088e-04 - rmse: 0.0239\n",
      "Epoch 603: val_loss improved from 0.00111 to 0.00105, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.7035e-04 - rmse: 0.0217 - val_loss: 0.0011 - val_rmse: 0.0325 - lr: 1.0000e-04\n",
      "Epoch 604/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6845e-04 - rmse: 0.0277\n",
      "Epoch 604: val_loss improved from 0.00105 to 0.00105, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 4.6335e-04 - rmse: 0.0215 - val_loss: 0.0011 - val_rmse: 0.0324 - lr: 1.0000e-04\n",
      "Epoch 605/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4205e-04 - rmse: 0.0233\n",
      "Epoch 605: val_loss did not improve from 0.00105\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6187e-04 - rmse: 0.0237 - val_loss: 0.0012 - val_rmse: 0.0346 - lr: 1.0000e-04\n",
      "Epoch 606/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9493e-04 - rmse: 0.0244\n",
      "Epoch 606: val_loss did not improve from 0.00105\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.3859e-04 - rmse: 0.0232 - val_loss: 0.0011 - val_rmse: 0.0339 - lr: 1.0000e-04\n",
      "Epoch 607/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0911e-04 - rmse: 0.0247\n",
      "Epoch 607: val_loss did not improve from 0.00105\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.4913e-04 - rmse: 0.0212 - val_loss: 0.0011 - val_rmse: 0.0334 - lr: 1.0000e-04\n",
      "Epoch 608/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5059e-04 - rmse: 0.0212\n",
      "Epoch 608: val_loss improved from 0.00105 to 0.00104, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 4.3052e-04 - rmse: 0.0207 - val_loss: 0.0010 - val_rmse: 0.0322 - lr: 1.0000e-04\n",
      "Epoch 609/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1661e-04 - rmse: 0.0227\n",
      "Epoch 609: val_loss did not improve from 0.00104\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.9377e-04 - rmse: 0.0222 - val_loss: 0.0012 - val_rmse: 0.0350 - lr: 1.0000e-04\n",
      "Epoch 610/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5301e-04 - rmse: 0.0235\n",
      "Epoch 610: val_loss did not improve from 0.00104\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2033e-04 - rmse: 0.0205 - val_loss: 0.0011 - val_rmse: 0.0333 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 611/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5861e-05 - rmse: 0.0060\n",
      "Epoch 611: val_loss did not improve from 0.00104\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3227e-04 - rmse: 0.0208 - val_loss: 0.0011 - val_rmse: 0.0327 - lr: 1.0000e-04\n",
      "Epoch 612/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8014e-04 - rmse: 0.0167\n",
      "Epoch 612: val_loss did not improve from 0.00104\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9715e-04 - rmse: 0.0199 - val_loss: 0.0011 - val_rmse: 0.0331 - lr: 1.0000e-04\n",
      "Epoch 613/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2717e-04 - rmse: 0.0181\n",
      "Epoch 613: val_loss did not improve from 0.00104\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8349e-04 - rmse: 0.0196 - val_loss: 0.0011 - val_rmse: 0.0329 - lr: 1.0000e-04\n",
      "Epoch 614/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1245e-04 - rmse: 0.0177\n",
      "Epoch 614: val_loss did not improve from 0.00104\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9038e-04 - rmse: 0.0198 - val_loss: 0.0011 - val_rmse: 0.0336 - lr: 1.0000e-04\n",
      "Epoch 615/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1263e-04 - rmse: 0.0285\n",
      "Epoch 615: val_loss did not improve from 0.00104\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9911e-04 - rmse: 0.0200 - val_loss: 0.0011 - val_rmse: 0.0331 - lr: 1.0000e-04\n",
      "Epoch 616/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0682e-05 - rmse: 0.0090\n",
      "Epoch 616: val_loss improved from 0.00104 to 0.00099, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 3.8730e-04 - rmse: 0.0197 - val_loss: 9.9344e-04 - val_rmse: 0.0315 - lr: 1.0000e-04\n",
      "Epoch 617/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9176e-04 - rmse: 0.0171\n",
      "Epoch 617: val_loss did not improve from 0.00099\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0455e-04 - rmse: 0.0201 - val_loss: 0.0012 - val_rmse: 0.0343 - lr: 1.0000e-04\n",
      "Epoch 618/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9004e-04 - rmse: 0.0221\n",
      "Epoch 618: val_loss did not improve from 0.00099\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.7968e-04 - rmse: 0.0219 - val_loss: 0.0010 - val_rmse: 0.0320 - lr: 1.0000e-04\n",
      "Epoch 619/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1682e-04 - rmse: 0.0178\n",
      "Epoch 619: val_loss did not improve from 0.00099\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8283e-04 - rmse: 0.0220 - val_loss: 0.0010 - val_rmse: 0.0321 - lr: 1.0000e-04\n",
      "Epoch 620/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7525e-04 - rmse: 0.0260\n",
      "Epoch 620: val_loss did not improve from 0.00099\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8480e-04 - rmse: 0.0242 - val_loss: 0.0016 - val_rmse: 0.0394 - lr: 1.0000e-04\n",
      "Epoch 621/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8529e-04 - rmse: 0.0298\n",
      "Epoch 621: val_loss did not improve from 0.00099\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5433e-04 - rmse: 0.0256 - val_loss: 0.0011 - val_rmse: 0.0333 - lr: 1.0000e-04\n",
      "Epoch 622/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9311e-04 - rmse: 0.0299\n",
      "Epoch 622: val_loss did not improve from 0.00099\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.8437e-04 - rmse: 0.0262 - val_loss: 0.0015 - val_rmse: 0.0385 - lr: 1.0000e-04\n",
      "Epoch 623/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1046e-04 - rmse: 0.0302\n",
      "Epoch 623: val_loss improved from 0.00099 to 0.00094, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 6.3575e-04 - rmse: 0.0252 - val_loss: 9.3703e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 624/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8258e-04 - rmse: 0.0220\n",
      "Epoch 624: val_loss did not improve from 0.00094\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7827e-04 - rmse: 0.0219 - val_loss: 0.0010 - val_rmse: 0.0317 - lr: 1.0000e-04\n",
      "Epoch 625/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6603e-04 - rmse: 0.0216\n",
      "Epoch 625: val_loss did not improve from 0.00094\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.5123e-04 - rmse: 0.0235 - val_loss: 0.0012 - val_rmse: 0.0345 - lr: 1.0000e-04\n",
      "Epoch 626/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3408e-04 - rmse: 0.0183\n",
      "Epoch 626: val_loss improved from 0.00094 to 0.00093, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.6076e-04 - rmse: 0.0215 - val_loss: 9.3398e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 627/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0010 - rmse: 0.0323\n",
      "Epoch 627: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.3852e-04 - rmse: 0.0232 - val_loss: 0.0010 - val_rmse: 0.0317 - lr: 1.0000e-04\n",
      "Epoch 628/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1488e-04 - rmse: 0.0204\n",
      "Epoch 628: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1475e-04 - rmse: 0.0204 - val_loss: 0.0011 - val_rmse: 0.0331 - lr: 1.0000e-04\n",
      "Epoch 629/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8253e-04 - rmse: 0.0220\n",
      "Epoch 629: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7427e-04 - rmse: 0.0193 - val_loss: 9.4791e-04 - val_rmse: 0.0308 - lr: 1.0000e-04\n",
      "Epoch 630/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9645e-04 - rmse: 0.0223\n",
      "Epoch 630: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.0967e-04 - rmse: 0.0202 - val_loss: 0.0012 - val_rmse: 0.0349 - lr: 1.0000e-04\n",
      "Epoch 631/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0012 - rmse: 0.0348\n",
      "Epoch 631: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1495e-04 - rmse: 0.0204 - val_loss: 9.8332e-04 - val_rmse: 0.0314 - lr: 1.0000e-04\n",
      "Epoch 632/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8531e-04 - rmse: 0.0242\n",
      "Epoch 632: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7389e-04 - rmse: 0.0218 - val_loss: 9.9046e-04 - val_rmse: 0.0315 - lr: 1.0000e-04\n",
      "Epoch 633/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4467e-04 - rmse: 0.0254\n",
      "Epoch 633: val_loss did not improve from 0.00093\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.6098e-04 - rmse: 0.0215 - val_loss: 0.0012 - val_rmse: 0.0343 - lr: 1.0000e-04\n",
      "Epoch 634/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4523e-04 - rmse: 0.0186\n",
      "Epoch 634: val_loss improved from 0.00093 to 0.00090, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5.2935e-04 - rmse: 0.0230 - val_loss: 9.0243e-04 - val_rmse: 0.0300 - lr: 1.0000e-04\n",
      "Epoch 635/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0338\n",
      "Epoch 635: val_loss did not improve from 0.00090\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1213e-04 - rmse: 0.0226 - val_loss: 0.0010 - val_rmse: 0.0318 - lr: 1.0000e-04\n",
      "Epoch 636/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0833e-04 - rmse: 0.0144\n",
      "Epoch 636: val_loss did not improve from 0.00090\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8707e-04 - rmse: 0.0197 - val_loss: 9.7085e-04 - val_rmse: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 637/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8928e-04 - rmse: 0.0197\n",
      "Epoch 637: val_loss improved from 0.00090 to 0.00088, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 27ms/step - loss: 3.3152e-04 - rmse: 0.0182 - val_loss: 8.7595e-04 - val_rmse: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 638/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9897e-04 - rmse: 0.0141\n",
      "Epoch 638: val_loss did not improve from 0.00088\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.8048e-04 - rmse: 0.0195 - val_loss: 0.0010 - val_rmse: 0.0323 - lr: 1.0000e-04\n",
      "Epoch 639/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8954e-04 - rmse: 0.0221\n",
      "Epoch 639: val_loss did not improve from 0.00088\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9211e-04 - rmse: 0.0198 - val_loss: 9.4927e-04 - val_rmse: 0.0308 - lr: 1.0000e-04\n",
      "Epoch 640/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0681e-04 - rmse: 0.0284\n",
      "Epoch 640: val_loss did not improve from 0.00088\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.2137e-04 - rmse: 0.0205 - val_loss: 0.0010 - val_rmse: 0.0320 - lr: 1.0000e-04\n",
      "Epoch 641/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1208e-04 - rmse: 0.0106\n",
      "Epoch 641: val_loss did not improve from 0.00088\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.1753e-04 - rmse: 0.0178 - val_loss: 9.3526e-04 - val_rmse: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 642/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5348e-04 - rmse: 0.0124\n",
      "Epoch 642: val_loss did not improve from 0.00088\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7456e-04 - rmse: 0.0194 - val_loss: 8.9950e-04 - val_rmse: 0.0300 - lr: 1.0000e-04\n",
      "Epoch 643/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7445e-04 - rmse: 0.0240\n",
      "Epoch 643: val_loss did not improve from 0.00088\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4250e-04 - rmse: 0.0185 - val_loss: 9.6115e-04 - val_rmse: 0.0310 - lr: 1.0000e-04\n",
      "Epoch 644/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2977e-04 - rmse: 0.0270\n",
      "Epoch 644: val_loss did not improve from 0.00088\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9238e-04 - rmse: 0.0198 - val_loss: 9.6638e-04 - val_rmse: 0.0311 - lr: 1.0000e-04\n",
      "Epoch 645/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8116e-04 - rmse: 0.0135\n",
      "Epoch 645: val_loss improved from 0.00088 to 0.00087, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 3.0737e-04 - rmse: 0.0175 - val_loss: 8.6710e-04 - val_rmse: 0.0294 - lr: 1.0000e-04\n",
      "Epoch 646/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6657e-04 - rmse: 0.0238\n",
      "Epoch 646: val_loss did not improve from 0.00087\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3186e-04 - rmse: 0.0182 - val_loss: 9.3004e-04 - val_rmse: 0.0305 - lr: 1.0000e-04\n",
      "Epoch 647/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3516e-04 - rmse: 0.0183\n",
      "Epoch 647: val_loss improved from 0.00087 to 0.00086, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.9731e-04 - rmse: 0.0172 - val_loss: 8.5921e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 648/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0870e-04 - rmse: 0.0247\n",
      "Epoch 648: val_loss improved from 0.00086 to 0.00083, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.4860e-04 - rmse: 0.0187 - val_loss: 8.2633e-04 - val_rmse: 0.0287 - lr: 1.0000e-04\n",
      "Epoch 649/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8425e-04 - rmse: 0.0196\n",
      "Epoch 649: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0232e-04 - rmse: 0.0201 - val_loss: 0.0012 - val_rmse: 0.0348 - lr: 1.0000e-04\n",
      "Epoch 650/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7160e-04 - rmse: 0.0217\n",
      "Epoch 650: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2164e-04 - rmse: 0.0205 - val_loss: 8.6355e-04 - val_rmse: 0.0294 - lr: 1.0000e-04\n",
      "Epoch 651/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6846e-04 - rmse: 0.0130\n",
      "Epoch 651: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3376e-04 - rmse: 0.0208 - val_loss: 8.9669e-04 - val_rmse: 0.0299 - lr: 1.0000e-04\n",
      "Epoch 652/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2735e-04 - rmse: 0.0230\n",
      "Epoch 652: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5390e-04 - rmse: 0.0188 - val_loss: 8.9066e-04 - val_rmse: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 653/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7110e-04 - rmse: 0.0131\n",
      "Epoch 653: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0318e-04 - rmse: 0.0174 - val_loss: 8.9427e-04 - val_rmse: 0.0299 - lr: 1.0000e-04\n",
      "Epoch 654/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5405e-04 - rmse: 0.0124\n",
      "Epoch 654: val_loss did not improve from 0.00083\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0660e-04 - rmse: 0.0175 - val_loss: 9.2320e-04 - val_rmse: 0.0304 - lr: 1.0000e-04\n",
      "Epoch 655/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3235e-04 - rmse: 0.0208\n",
      "Epoch 655: val_loss improved from 0.00083 to 0.00081, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.3352e-04 - rmse: 0.0183 - val_loss: 8.0881e-04 - val_rmse: 0.0284 - lr: 1.0000e-04\n",
      "Epoch 656/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4311e-04 - rmse: 0.0120\n",
      "Epoch 656: val_loss did not improve from 0.00081\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0890e-04 - rmse: 0.0176 - val_loss: 8.7239e-04 - val_rmse: 0.0295 - lr: 1.0000e-04\n",
      "Epoch 657/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0254e-04 - rmse: 0.0101\n",
      "Epoch 657: val_loss did not improve from 0.00081\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5961e-04 - rmse: 0.0161 - val_loss: 8.8117e-04 - val_rmse: 0.0297 - lr: 1.0000e-04\n",
      "Epoch 658/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2371e-04 - rmse: 0.0150\n",
      "Epoch 658: val_loss did not improve from 0.00081\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1609e-04 - rmse: 0.0178 - val_loss: 9.5652e-04 - val_rmse: 0.0309 - lr: 1.0000e-04\n",
      "Epoch 659/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4831e-05 - rmse: 0.0081\n",
      "Epoch 659: val_loss did not improve from 0.00081\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.8980e-04 - rmse: 0.0197 - val_loss: 8.0887e-04 - val_rmse: 0.0284 - lr: 1.0000e-04\n",
      "Epoch 660/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3898e-05 - rmse: 0.0097\n",
      "Epoch 660: val_loss did not improve from 0.00081\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6384e-04 - rmse: 0.0191 - val_loss: 0.0010 - val_rmse: 0.0317 - lr: 1.0000e-04\n",
      "Epoch 661/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8560e-04 - rmse: 0.0220\n",
      "Epoch 661: val_loss did not improve from 0.00081\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2935e-04 - rmse: 0.0151 - val_loss: 8.4755e-04 - val_rmse: 0.0291 - lr: 1.0000e-04\n",
      "Epoch 662/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2075e-04 - rmse: 0.0179\n",
      "Epoch 662: val_loss did not improve from 0.00081\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.5589e-04 - rmse: 0.0189 - val_loss: 8.7090e-04 - val_rmse: 0.0295 - lr: 1.0000e-04\n",
      "Epoch 663/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5951e-04 - rmse: 0.0126\n",
      "Epoch 663: val_loss improved from 0.00081 to 0.00075, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 28ms/step - loss: 2.6713e-04 - rmse: 0.0163 - val_loss: 7.5329e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 664/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0670e-04 - rmse: 0.0175\n",
      "Epoch 664: val_loss did not improve from 0.00075\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6855e-04 - rmse: 0.0164 - val_loss: 0.0011 - val_rmse: 0.0325 - lr: 1.0000e-04\n",
      "Epoch 665/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0431e-04 - rmse: 0.0174\n",
      "Epoch 665: val_loss improved from 0.00075 to 0.00075, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.2773e-04 - rmse: 0.0181 - val_loss: 7.5086e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 666/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2740e-04 - rmse: 0.0207\n",
      "Epoch 666: val_loss improved from 0.00075 to 0.00074, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 3.8379e-04 - rmse: 0.0196 - val_loss: 7.4251e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n",
      "Epoch 667/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1972e-04 - rmse: 0.0179\n",
      "Epoch 667: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.8058e-04 - rmse: 0.0219 - val_loss: 0.0011 - val_rmse: 0.0329 - lr: 1.0000e-04\n",
      "Epoch 668/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3338e-04 - rmse: 0.0115\n",
      "Epoch 668: val_loss improved from 0.00074 to 0.00074, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 3.9252e-04 - rmse: 0.0198 - val_loss: 7.3956e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n",
      "Epoch 669/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9660e-04 - rmse: 0.0140\n",
      "Epoch 669: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1788e-04 - rmse: 0.0178 - val_loss: 8.2794e-04 - val_rmse: 0.0288 - lr: 1.0000e-04\n",
      "Epoch 670/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0778e-04 - rmse: 0.0104\n",
      "Epoch 670: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6823e-04 - rmse: 0.0164 - val_loss: 8.5846e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 671/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2427e-04 - rmse: 0.0180\n",
      "Epoch 671: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5751e-04 - rmse: 0.0160 - val_loss: 8.2958e-04 - val_rmse: 0.0288 - lr: 1.0000e-04\n",
      "Epoch 672/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5966e-04 - rmse: 0.0161\n",
      "Epoch 672: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6129e-04 - rmse: 0.0162 - val_loss: 8.5979e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 673/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5136e-04 - rmse: 0.0187\n",
      "Epoch 673: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5937e-04 - rmse: 0.0161 - val_loss: 7.7562e-04 - val_rmse: 0.0278 - lr: 1.0000e-04\n",
      "Epoch 674/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5282e-04 - rmse: 0.0159\n",
      "Epoch 674: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7283e-04 - rmse: 0.0165 - val_loss: 8.3776e-04 - val_rmse: 0.0289 - lr: 1.0000e-04\n",
      "Epoch 675/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2943e-04 - rmse: 0.0151\n",
      "Epoch 675: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6780e-04 - rmse: 0.0164 - val_loss: 8.4771e-04 - val_rmse: 0.0291 - lr: 1.0000e-04\n",
      "Epoch 676/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8035e-04 - rmse: 0.0219\n",
      "Epoch 676: val_loss did not improve from 0.00074\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5278e-04 - rmse: 0.0159 - val_loss: 7.9387e-04 - val_rmse: 0.0282 - lr: 1.0000e-04\n",
      "Epoch 677/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0315e-04 - rmse: 0.0224\n",
      "Epoch 677: val_loss improved from 0.00074 to 0.00073, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.8400e-04 - rmse: 0.0169 - val_loss: 7.2807e-04 - val_rmse: 0.0270 - lr: 1.0000e-04\n",
      "Epoch 678/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8011e-05 - rmse: 0.0099\n",
      "Epoch 678: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3871e-04 - rmse: 0.0155 - val_loss: 8.6251e-04 - val_rmse: 0.0294 - lr: 1.0000e-04\n",
      "Epoch 679/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8998e-04 - rmse: 0.0243\n",
      "Epoch 679: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0099e-04 - rmse: 0.0173 - val_loss: 7.5792e-04 - val_rmse: 0.0275 - lr: 1.0000e-04\n",
      "Epoch 680/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3511e-04 - rmse: 0.0183\n",
      "Epoch 680: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0588e-04 - rmse: 0.0175 - val_loss: 7.4088e-04 - val_rmse: 0.0272 - lr: 1.0000e-04\n",
      "Epoch 681/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7674e-04 - rmse: 0.0194\n",
      "Epoch 681: val_loss did not improve from 0.00073\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2168e-04 - rmse: 0.0149 - val_loss: 8.2639e-04 - val_rmse: 0.0287 - lr: 1.0000e-04\n",
      "Epoch 682/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8208e-04 - rmse: 0.0135\n",
      "Epoch 682: val_loss improved from 0.00073 to 0.00071, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.9556e-04 - rmse: 0.0172 - val_loss: 7.0864e-04 - val_rmse: 0.0266 - lr: 1.0000e-04\n",
      "Epoch 683/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6018e-04 - rmse: 0.0215\n",
      "Epoch 683: val_loss did not improve from 0.00071\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9013e-04 - rmse: 0.0170 - val_loss: 9.8086e-04 - val_rmse: 0.0313 - lr: 1.0000e-04\n",
      "Epoch 684/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7550e-04 - rmse: 0.0132\n",
      "Epoch 684: val_loss did not improve from 0.00071\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.5445e-04 - rmse: 0.0188 - val_loss: 7.3527e-04 - val_rmse: 0.0271 - lr: 1.0000e-04\n",
      "Epoch 685/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9567e-04 - rmse: 0.0140\n",
      "Epoch 685: val_loss improved from 0.00071 to 0.00066, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.7845e-04 - rmse: 0.0167 - val_loss: 6.6044e-04 - val_rmse: 0.0257 - lr: 1.0000e-04\n",
      "Epoch 686/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9397e-04 - rmse: 0.0139\n",
      "Epoch 686: val_loss did not improve from 0.00066\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4013e-04 - rmse: 0.0155 - val_loss: 7.9741e-04 - val_rmse: 0.0282 - lr: 1.0000e-04\n",
      "Epoch 687/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0132e-04 - rmse: 0.0174\n",
      "Epoch 687: val_loss did not improve from 0.00066\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2102e-04 - rmse: 0.0149 - val_loss: 7.1306e-04 - val_rmse: 0.0267 - lr: 1.0000e-04\n",
      "Epoch 688/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0523e-04 - rmse: 0.0225\n",
      "Epoch 688: val_loss improved from 0.00066 to 0.00066, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.6476e-04 - rmse: 0.0163 - val_loss: 6.5669e-04 - val_rmse: 0.0256 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 689/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4536e-04 - rmse: 0.0121\n",
      "Epoch 689: val_loss did not improve from 0.00066\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8418e-04 - rmse: 0.0169 - val_loss: 9.9101e-04 - val_rmse: 0.0315 - lr: 1.0000e-04\n",
      "Epoch 690/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1784e-04 - rmse: 0.0228\n",
      "Epoch 690: val_loss improved from 0.00066 to 0.00063, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.3501e-04 - rmse: 0.0183 - val_loss: 6.3317e-04 - val_rmse: 0.0252 - lr: 1.0000e-04\n",
      "Epoch 691/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3719e-04 - rmse: 0.0117\n",
      "Epoch 691: val_loss did not improve from 0.00063\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7963e-04 - rmse: 0.0167 - val_loss: 7.1923e-04 - val_rmse: 0.0268 - lr: 1.0000e-04\n",
      "Epoch 692/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1539e-04 - rmse: 0.0107\n",
      "Epoch 692: val_loss did not improve from 0.00063\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2907e-04 - rmse: 0.0181 - val_loss: 8.9825e-04 - val_rmse: 0.0300 - lr: 1.0000e-04\n",
      "Epoch 693/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6305e-04 - rmse: 0.0237\n",
      "Epoch 693: val_loss improved from 0.00063 to 0.00060, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.7738e-04 - rmse: 0.0167 - val_loss: 6.0200e-04 - val_rmse: 0.0245 - lr: 1.0000e-04\n",
      "Epoch 694/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2971e-04 - rmse: 0.0152\n",
      "Epoch 694: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4446e-04 - rmse: 0.0156 - val_loss: 7.0200e-04 - val_rmse: 0.0265 - lr: 1.0000e-04\n",
      "Epoch 695/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8210e-04 - rmse: 0.0168\n",
      "Epoch 695: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2297e-04 - rmse: 0.0149 - val_loss: 7.8566e-04 - val_rmse: 0.0280 - lr: 1.0000e-04\n",
      "Epoch 696/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2087e-04 - rmse: 0.0205\n",
      "Epoch 696: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0925e-04 - rmse: 0.0145 - val_loss: 6.3077e-04 - val_rmse: 0.0251 - lr: 1.0000e-04\n",
      "Epoch 697/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8864e-04 - rmse: 0.0197\n",
      "Epoch 697: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3906e-04 - rmse: 0.0155 - val_loss: 7.6571e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 698/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6780e-04 - rmse: 0.0130\n",
      "Epoch 698: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1084e-04 - rmse: 0.0145 - val_loss: 6.4920e-04 - val_rmse: 0.0255 - lr: 1.0000e-04\n",
      "Epoch 699/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7324e-04 - rmse: 0.0165\n",
      "Epoch 699: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3682e-04 - rmse: 0.0154 - val_loss: 6.8536e-04 - val_rmse: 0.0262 - lr: 1.0000e-04\n",
      "Epoch 700/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5499e-05 - rmse: 0.0092\n",
      "Epoch 700: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4199e-04 - rmse: 0.0156 - val_loss: 6.5793e-04 - val_rmse: 0.0257 - lr: 1.0000e-04\n",
      "Epoch 701/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8100e-04 - rmse: 0.0168\n",
      "Epoch 701: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3841e-04 - rmse: 0.0154 - val_loss: 7.1926e-04 - val_rmse: 0.0268 - lr: 1.0000e-04\n",
      "Epoch 702/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1922e-04 - rmse: 0.0148\n",
      "Epoch 702: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5890e-04 - rmse: 0.0161 - val_loss: 7.6644e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 703/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4488e-04 - rmse: 0.0156\n",
      "Epoch 703: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1501e-04 - rmse: 0.0147 - val_loss: 7.6364e-04 - val_rmse: 0.0276 - lr: 1.0000e-04\n",
      "Epoch 704/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0069e-05 - rmse: 0.0063\n",
      "Epoch 704: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3820e-04 - rmse: 0.0154 - val_loss: 6.5062e-04 - val_rmse: 0.0255 - lr: 1.0000e-04\n",
      "Epoch 705/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6381e-04 - rmse: 0.0215\n",
      "Epoch 705: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8640e-04 - rmse: 0.0197 - val_loss: 7.9081e-04 - val_rmse: 0.0281 - lr: 1.0000e-04\n",
      "Epoch 706/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6442e-05 - rmse: 0.0098\n",
      "Epoch 706: val_loss did not improve from 0.00060\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5560e-04 - rmse: 0.0160 - val_loss: 8.5316e-04 - val_rmse: 0.0292 - lr: 1.0000e-04\n",
      "Epoch 707/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8250e-04 - rmse: 0.0241\n",
      "Epoch 707: val_loss improved from 0.00060 to 0.00053, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2.8402e-04 - rmse: 0.0169 - val_loss: 5.2683e-04 - val_rmse: 0.0230 - lr: 1.0000e-04\n",
      "Epoch 708/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0854e-04 - rmse: 0.0144\n",
      "Epoch 708: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4074e-04 - rmse: 0.0155 - val_loss: 6.8609e-04 - val_rmse: 0.0262 - lr: 1.0000e-04\n",
      "Epoch 709/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4313e-04 - rmse: 0.0120\n",
      "Epoch 709: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7690e-04 - rmse: 0.0166 - val_loss: 8.4996e-04 - val_rmse: 0.0292 - lr: 1.0000e-04\n",
      "Epoch 710/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6818e-04 - rmse: 0.0164\n",
      "Epoch 710: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7080e-04 - rmse: 0.0165 - val_loss: 6.3875e-04 - val_rmse: 0.0253 - lr: 1.0000e-04\n",
      "Epoch 711/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8427e-04 - rmse: 0.0169\n",
      "Epoch 711: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3056e-04 - rmse: 0.0152 - val_loss: 6.4823e-04 - val_rmse: 0.0255 - lr: 1.0000e-04\n",
      "Epoch 712/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3132e-05 - rmse: 0.0097\n",
      "Epoch 712: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9298e-04 - rmse: 0.0139 - val_loss: 6.7526e-04 - val_rmse: 0.0260 - lr: 1.0000e-04\n",
      "Epoch 713/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4424e-04 - rmse: 0.0211\n",
      "Epoch 713: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0855e-04 - rmse: 0.0144 - val_loss: 7.7616e-04 - val_rmse: 0.0279 - lr: 1.0000e-04\n",
      "Epoch 714/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9732e-05 - rmse: 0.0071\n",
      "Epoch 714: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9991e-04 - rmse: 0.0141 - val_loss: 7.0235e-04 - val_rmse: 0.0265 - lr: 1.0000e-04\n",
      "Epoch 715/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3300e-04 - rmse: 0.0115\n",
      "Epoch 715: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2267e-04 - rmse: 0.0149 - val_loss: 6.0589e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 716/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9266e-04 - rmse: 0.0171\n",
      "Epoch 716: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1358e-04 - rmse: 0.0146 - val_loss: 5.4436e-04 - val_rmse: 0.0233 - lr: 1.0000e-04\n",
      "Epoch 717/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1265e-04 - rmse: 0.0146\n",
      "Epoch 717: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6425e-04 - rmse: 0.0163 - val_loss: 9.4814e-04 - val_rmse: 0.0308 - lr: 1.0000e-04\n",
      "Epoch 718/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2012e-04 - rmse: 0.0179\n",
      "Epoch 718: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5283e-04 - rmse: 0.0159 - val_loss: 6.4411e-04 - val_rmse: 0.0254 - lr: 1.0000e-04\n",
      "Epoch 719/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8496e-05 - rmse: 0.0094\n",
      "Epoch 719: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9272e-04 - rmse: 0.0139 - val_loss: 5.6478e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 720/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6144e-04 - rmse: 0.0127\n",
      "Epoch 720: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1986e-04 - rmse: 0.0148 - val_loss: 7.6913e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 721/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2338e-04 - rmse: 0.0111\n",
      "Epoch 721: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8730e-04 - rmse: 0.0137 - val_loss: 5.6462e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 722/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1859e-04 - rmse: 0.0178\n",
      "Epoch 722: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9996e-04 - rmse: 0.0141 - val_loss: 5.6027e-04 - val_rmse: 0.0237 - lr: 1.0000e-04\n",
      "Epoch 723/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4345e-04 - rmse: 0.0211\n",
      "Epoch 723: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1184e-04 - rmse: 0.0146 - val_loss: 5.5159e-04 - val_rmse: 0.0235 - lr: 1.0000e-04\n",
      "Epoch 724/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2637e-05 - rmse: 0.0096\n",
      "Epoch 724: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7571e-04 - rmse: 0.0133 - val_loss: 7.5620e-04 - val_rmse: 0.0275 - lr: 1.0000e-04\n",
      "Epoch 725/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1433e-04 - rmse: 0.0107\n",
      "Epoch 725: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9677e-04 - rmse: 0.0172 - val_loss: 6.4403e-04 - val_rmse: 0.0254 - lr: 1.0000e-04\n",
      "Epoch 726/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0050e-04 - rmse: 0.0224\n",
      "Epoch 726: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4448e-04 - rmse: 0.0156 - val_loss: 6.1373e-04 - val_rmse: 0.0248 - lr: 1.0000e-04\n",
      "Epoch 727/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7941e-05 - rmse: 0.0099\n",
      "Epoch 727: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7653e-04 - rmse: 0.0133 - val_loss: 5.7473e-04 - val_rmse: 0.0240 - lr: 1.0000e-04\n",
      "Epoch 728/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3034e-04 - rmse: 0.0207\n",
      "Epoch 728: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5614e-04 - rmse: 0.0160 - val_loss: 6.6936e-04 - val_rmse: 0.0259 - lr: 1.0000e-04\n",
      "Epoch 729/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0610e-04 - rmse: 0.0144\n",
      "Epoch 729: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8395e-04 - rmse: 0.0169 - val_loss: 8.3503e-04 - val_rmse: 0.0289 - lr: 1.0000e-04\n",
      "Epoch 730/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0161e-04 - rmse: 0.0174\n",
      "Epoch 730: val_loss did not improve from 0.00053\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4641e-04 - rmse: 0.0157 - val_loss: 5.6555e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 731/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2728e-04 - rmse: 0.0151\n",
      "Epoch 731: val_loss improved from 0.00053 to 0.00049, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.5956e-04 - rmse: 0.0161 - val_loss: 4.8882e-04 - val_rmse: 0.0221 - lr: 1.0000e-04\n",
      "Epoch 732/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0714e-04 - rmse: 0.0175\n",
      "Epoch 732: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6805e-04 - rmse: 0.0192 - val_loss: 8.0253e-04 - val_rmse: 0.0283 - lr: 1.0000e-04\n",
      "Epoch 733/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2952e-04 - rmse: 0.0182\n",
      "Epoch 733: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0406e-04 - rmse: 0.0143 - val_loss: 6.6341e-04 - val_rmse: 0.0258 - lr: 1.0000e-04\n",
      "Epoch 734/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4393e-05 - rmse: 0.0086\n",
      "Epoch 734: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5606e-04 - rmse: 0.0160 - val_loss: 5.6376e-04 - val_rmse: 0.0237 - lr: 1.0000e-04\n",
      "Epoch 735/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8804e-04 - rmse: 0.0197\n",
      "Epoch 735: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5003e-04 - rmse: 0.0187 - val_loss: 6.3247e-04 - val_rmse: 0.0251 - lr: 1.0000e-04\n",
      "Epoch 736/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2849e-05 - rmse: 0.0085\n",
      "Epoch 736: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5619e-04 - rmse: 0.0160 - val_loss: 7.0798e-04 - val_rmse: 0.0266 - lr: 1.0000e-04\n",
      "Epoch 737/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1347e-05 - rmse: 0.0090\n",
      "Epoch 737: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1398e-04 - rmse: 0.0146 - val_loss: 5.1551e-04 - val_rmse: 0.0227 - lr: 1.0000e-04\n",
      "Epoch 738/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7039e-04 - rmse: 0.0131\n",
      "Epoch 738: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1934e-04 - rmse: 0.0148 - val_loss: 5.2079e-04 - val_rmse: 0.0228 - lr: 1.0000e-04\n",
      "Epoch 739/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9012e-04 - rmse: 0.0138\n",
      "Epoch 739: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2405e-04 - rmse: 0.0150 - val_loss: 7.3506e-04 - val_rmse: 0.0271 - lr: 1.0000e-04\n",
      "Epoch 740/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3648e-04 - rmse: 0.0117\n",
      "Epoch 740: val_loss did not improve from 0.00049\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2481e-04 - rmse: 0.0150 - val_loss: 6.2091e-04 - val_rmse: 0.0249 - lr: 1.0000e-04\n",
      "Epoch 741/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0674e-04 - rmse: 0.0103\n",
      "Epoch 741: val_loss improved from 0.00049 to 0.00048, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.4714e-04 - rmse: 0.0121 - val_loss: 4.7526e-04 - val_rmse: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 742/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5293e-04 - rmse: 0.0188\n",
      "Epoch 742: val_loss did not improve from 0.00048\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7381e-04 - rmse: 0.0132 - val_loss: 5.4659e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 743/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0729e-04 - rmse: 0.0104\n",
      "Epoch 743: val_loss did not improve from 0.00048\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6196e-04 - rmse: 0.0127 - val_loss: 5.7359e-04 - val_rmse: 0.0239 - lr: 1.0000e-04\n",
      "Epoch 744/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7361e-05 - rmse: 0.0088\n",
      "Epoch 744: val_loss did not improve from 0.00048\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2425e-04 - rmse: 0.0111 - val_loss: 5.7840e-04 - val_rmse: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 745/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1850e-04 - rmse: 0.0148\n",
      "Epoch 745: val_loss improved from 0.00048 to 0.00047, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.3456e-04 - rmse: 0.0116 - val_loss: 4.7153e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 746/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4406e-05 - rmse: 0.0080\n",
      "Epoch 746: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6385e-04 - rmse: 0.0128 - val_loss: 5.9064e-04 - val_rmse: 0.0243 - lr: 1.0000e-04\n",
      "Epoch 747/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8509e-04 - rmse: 0.0169\n",
      "Epoch 747: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3342e-04 - rmse: 0.0116 - val_loss: 6.3863e-04 - val_rmse: 0.0253 - lr: 1.0000e-04\n",
      "Epoch 748/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0311e-04 - rmse: 0.0143\n",
      "Epoch 748: val_loss improved from 0.00047 to 0.00047, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 1.4754e-04 - rmse: 0.0121 - val_loss: 4.6742e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 749/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0300e-04 - rmse: 0.0101\n",
      "Epoch 749: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4730e-04 - rmse: 0.0121 - val_loss: 5.2887e-04 - val_rmse: 0.0230 - lr: 1.0000e-04\n",
      "Epoch 750/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1488e-04 - rmse: 0.0147\n",
      "Epoch 750: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2316e-04 - rmse: 0.0111 - val_loss: 5.8577e-04 - val_rmse: 0.0242 - lr: 1.0000e-04\n",
      "Epoch 751/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2445e-04 - rmse: 0.0112\n",
      "Epoch 751: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1970e-04 - rmse: 0.0109 - val_loss: 4.7305e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 752/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3476e-05 - rmse: 0.0086\n",
      "Epoch 752: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1376e-04 - rmse: 0.0107 - val_loss: 5.0618e-04 - val_rmse: 0.0225 - lr: 1.0000e-04\n",
      "Epoch 753/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0619e-04 - rmse: 0.0144\n",
      "Epoch 753: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3061e-04 - rmse: 0.0114 - val_loss: 5.8195e-04 - val_rmse: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 754/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3730e-05 - rmse: 0.0080\n",
      "Epoch 754: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2872e-04 - rmse: 0.0113 - val_loss: 5.6510e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 755/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8248e-04 - rmse: 0.0135\n",
      "Epoch 755: val_loss did not improve from 0.00047\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0903e-04 - rmse: 0.0104 - val_loss: 4.7865e-04 - val_rmse: 0.0219 - lr: 1.0000e-04\n",
      "Epoch 756/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5707e-04 - rmse: 0.0160\n",
      "Epoch 756: val_loss improved from 0.00047 to 0.00046, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 1.1617e-04 - rmse: 0.0108 - val_loss: 4.5512e-04 - val_rmse: 0.0213 - lr: 1.0000e-04\n",
      "Epoch 757/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6702e-04 - rmse: 0.0192\n",
      "Epoch 757: val_loss improved from 0.00046 to 0.00045, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 1.5957e-04 - rmse: 0.0126 - val_loss: 4.5417e-04 - val_rmse: 0.0213 - lr: 1.0000e-04\n",
      "Epoch 758/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9983e-05 - rmse: 0.0089\n",
      "Epoch 758: val_loss did not improve from 0.00045\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3332e-04 - rmse: 0.0115 - val_loss: 6.9558e-04 - val_rmse: 0.0264 - lr: 1.0000e-04\n",
      "Epoch 759/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6151e-04 - rmse: 0.0127\n",
      "Epoch 759: val_loss improved from 0.00045 to 0.00043, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 1.5324e-04 - rmse: 0.0124 - val_loss: 4.3230e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 760/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1781e-04 - rmse: 0.0148\n",
      "Epoch 760: val_loss did not improve from 0.00043\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9324e-04 - rmse: 0.0139 - val_loss: 5.5737e-04 - val_rmse: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 761/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1635e-04 - rmse: 0.0108\n",
      "Epoch 761: val_loss did not improve from 0.00043\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2990e-04 - rmse: 0.0114 - val_loss: 4.6478e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 762/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2112e-05 - rmse: 0.0072\n",
      "Epoch 762: val_loss did not improve from 0.00043\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1153e-04 - rmse: 0.0106 - val_loss: 5.3963e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 763/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1912e-04 - rmse: 0.0109\n",
      "Epoch 763: val_loss did not improve from 0.00043\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8899e-04 - rmse: 0.0137 - val_loss: 5.9303e-04 - val_rmse: 0.0244 - lr: 1.0000e-04\n",
      "Epoch 764/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3162e-04 - rmse: 0.0115\n",
      "Epoch 764: val_loss did not improve from 0.00043\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2335e-04 - rmse: 0.0149 - val_loss: 6.7660e-04 - val_rmse: 0.0260 - lr: 1.0000e-04\n",
      "Epoch 765/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1231e-04 - rmse: 0.0106\n",
      "Epoch 765: val_loss improved from 0.00043 to 0.00040, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.1824e-04 - rmse: 0.0148 - val_loss: 3.9761e-04 - val_rmse: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 766/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5867e-04 - rmse: 0.0161\n",
      "Epoch 766: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9354e-04 - rmse: 0.0139 - val_loss: 4.4660e-04 - val_rmse: 0.0211 - lr: 1.0000e-04\n",
      "Epoch 767/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8879e-05 - rmse: 0.0054\n",
      "Epoch 767: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1689e-04 - rmse: 0.0108 - val_loss: 5.9248e-04 - val_rmse: 0.0243 - lr: 1.0000e-04\n",
      "Epoch 768/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2105e-04 - rmse: 0.0110\n",
      "Epoch 768: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2432e-04 - rmse: 0.0111 - val_loss: 4.9496e-04 - val_rmse: 0.0222 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 769/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0563e-04 - rmse: 0.0103\n",
      "Epoch 769: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1333e-04 - rmse: 0.0106 - val_loss: 5.1337e-04 - val_rmse: 0.0227 - lr: 1.0000e-04\n",
      "Epoch 770/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9990e-05 - rmse: 0.0100\n",
      "Epoch 770: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1592e-04 - rmse: 0.0108 - val_loss: 4.9747e-04 - val_rmse: 0.0223 - lr: 1.0000e-04\n",
      "Epoch 771/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8447e-05 - rmse: 0.0062\n",
      "Epoch 771: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0090e-04 - rmse: 0.0100 - val_loss: 4.0614e-04 - val_rmse: 0.0202 - lr: 1.0000e-04\n",
      "Epoch 772/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2644e-04 - rmse: 0.0112\n",
      "Epoch 772: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9083e-05 - rmse: 0.0100 - val_loss: 4.7564e-04 - val_rmse: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 773/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5263e-05 - rmse: 0.0081\n",
      "Epoch 773: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1070e-04 - rmse: 0.0105 - val_loss: 5.9604e-04 - val_rmse: 0.0244 - lr: 1.0000e-04\n",
      "Epoch 774/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9712e-04 - rmse: 0.0140\n",
      "Epoch 774: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4816e-04 - rmse: 0.0122 - val_loss: 4.3654e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 775/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6402e-04 - rmse: 0.0128\n",
      "Epoch 775: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2448e-04 - rmse: 0.0112 - val_loss: 4.1080e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 776/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2485e-04 - rmse: 0.0112\n",
      "Epoch 776: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3084e-04 - rmse: 0.0114 - val_loss: 4.4413e-04 - val_rmse: 0.0211 - lr: 1.0000e-04\n",
      "Epoch 777/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0553e-05 - rmse: 0.0084\n",
      "Epoch 777: val_loss did not improve from 0.00040\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1387e-04 - rmse: 0.0107 - val_loss: 6.0320e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 778/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3456e-04 - rmse: 0.0116\n",
      "Epoch 778: val_loss improved from 0.00040 to 0.00037, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.1470e-04 - rmse: 0.0107 - val_loss: 3.7425e-04 - val_rmse: 0.0193 - lr: 1.0000e-04\n",
      "Epoch 779/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3559e-05 - rmse: 0.0086\n",
      "Epoch 779: val_loss did not improve from 0.00037\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2278e-04 - rmse: 0.0111 - val_loss: 4.5814e-04 - val_rmse: 0.0214 - lr: 1.0000e-04\n",
      "Epoch 780/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0017e-05 - rmse: 0.0063\n",
      "Epoch 780: val_loss did not improve from 0.00037\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3165e-05 - rmse: 0.0097 - val_loss: 4.7570e-04 - val_rmse: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 781/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5108e-05 - rmse: 0.0074\n",
      "Epoch 781: val_loss did not improve from 0.00037\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3151e-04 - rmse: 0.0115 - val_loss: 6.0415e-04 - val_rmse: 0.0246 - lr: 1.0000e-04\n",
      "Epoch 782/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4729e-04 - rmse: 0.0157\n",
      "Epoch 782: val_loss improved from 0.00037 to 0.00035, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 1.3478e-04 - rmse: 0.0116 - val_loss: 3.5410e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 783/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1400e-04 - rmse: 0.0107\n",
      "Epoch 783: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4627e-04 - rmse: 0.0121 - val_loss: 5.5901e-04 - val_rmse: 0.0236 - lr: 1.0000e-04\n",
      "Epoch 784/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4014e-05 - rmse: 0.0092\n",
      "Epoch 784: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6384e-04 - rmse: 0.0128 - val_loss: 4.3460e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 785/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2098e-05 - rmse: 0.0072\n",
      "Epoch 785: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1342e-04 - rmse: 0.0107 - val_loss: 4.2690e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 786/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0580e-04 - rmse: 0.0103\n",
      "Epoch 786: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4615e-04 - rmse: 0.0121 - val_loss: 4.2414e-04 - val_rmse: 0.0206 - lr: 1.0000e-04\n",
      "Epoch 787/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9836e-05 - rmse: 0.0063\n",
      "Epoch 787: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5677e-05 - rmse: 0.0098 - val_loss: 4.0562e-04 - val_rmse: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 788/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4176e-05 - rmse: 0.0038\n",
      "Epoch 788: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5718e-05 - rmse: 0.0093 - val_loss: 4.2378e-04 - val_rmse: 0.0206 - lr: 1.0000e-04\n",
      "Epoch 789/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5496e-05 - rmse: 0.0074\n",
      "Epoch 789: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1349e-04 - rmse: 0.0107 - val_loss: 4.1215e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 790/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0551e-05 - rmse: 0.0084\n",
      "Epoch 790: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.2321e-05 - rmse: 0.0091 - val_loss: 4.4699e-04 - val_rmse: 0.0211 - lr: 1.0000e-04\n",
      "Epoch 791/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3197e-05 - rmse: 0.0097\n",
      "Epoch 791: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7480e-05 - rmse: 0.0094 - val_loss: 3.6104e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 792/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6669e-05 - rmse: 0.0082\n",
      "Epoch 792: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3496e-05 - rmse: 0.0091 - val_loss: 4.1539e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 793/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7654e-05 - rmse: 0.0099\n",
      "Epoch 793: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1364e-05 - rmse: 0.0096 - val_loss: 5.0335e-04 - val_rmse: 0.0224 - lr: 1.0000e-04\n",
      "Epoch 794/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1077e-05 - rmse: 0.0095\n",
      "Epoch 794: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2852e-04 - rmse: 0.0113 - val_loss: 5.3702e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 795/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4625e-05 - rmse: 0.0080\n",
      "Epoch 795: val_loss improved from 0.00035 to 0.00035, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 1.3289e-04 - rmse: 0.0115 - val_loss: 3.4913e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 796/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9863e-05 - rmse: 0.0089\n",
      "Epoch 796: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0905e-04 - rmse: 0.0104 - val_loss: 3.4981e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 797/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1936e-05 - rmse: 0.0091\n",
      "Epoch 797: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.6569e-05 - rmse: 0.0098 - val_loss: 4.3000e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 798/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6484e-05 - rmse: 0.0093\n",
      "Epoch 798: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5616e-05 - rmse: 0.0093 - val_loss: 4.1630e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 799/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8534e-05 - rmse: 0.0070\n",
      "Epoch 799: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.9035e-05 - rmse: 0.0083 - val_loss: 4.0146e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 800/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6992e-05 - rmse: 0.0052\n",
      "Epoch 800: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.8933e-05 - rmse: 0.0089 - val_loss: 4.4191e-04 - val_rmse: 0.0210 - lr: 1.0000e-04\n",
      "Epoch 801/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4107e-05 - rmse: 0.0086\n",
      "Epoch 801: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.7703e-05 - rmse: 0.0088 - val_loss: 3.9113e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 802/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3077e-05 - rmse: 0.0079\n",
      "Epoch 802: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1564e-05 - rmse: 0.0096 - val_loss: 3.5621e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 803/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2866e-04 - rmse: 0.0113\n",
      "Epoch 803: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8289e-05 - rmse: 0.0099 - val_loss: 4.7820e-04 - val_rmse: 0.0219 - lr: 1.0000e-04\n",
      "Epoch 804/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0968e-04 - rmse: 0.0145\n",
      "Epoch 804: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3264e-04 - rmse: 0.0115 - val_loss: 4.8726e-04 - val_rmse: 0.0221 - lr: 1.0000e-04\n",
      "Epoch 805/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5318e-05 - rmse: 0.0074\n",
      "Epoch 805: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5850e-04 - rmse: 0.0126 - val_loss: 4.1402e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 806/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0014e-04 - rmse: 0.0224\n",
      "Epoch 806: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1499e-04 - rmse: 0.0147 - val_loss: 4.6897e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 807/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1105e-04 - rmse: 0.0105\n",
      "Epoch 807: val_loss did not improve from 0.00035\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.9960e-05 - rmse: 0.0095 - val_loss: 4.5948e-04 - val_rmse: 0.0214 - lr: 1.0000e-04\n",
      "Epoch 808/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7521e-05 - rmse: 0.0076\n",
      "Epoch 808: val_loss improved from 0.00035 to 0.00031, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 9.1381e-05 - rmse: 0.0096 - val_loss: 3.1304e-04 - val_rmse: 0.0177 - lr: 1.0000e-04\n",
      "Epoch 809/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3592e-05 - rmse: 0.0086\n",
      "Epoch 809: val_loss did not improve from 0.00031\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0059e-04 - rmse: 0.0100 - val_loss: 3.4935e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 810/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3178e-05 - rmse: 0.0058\n",
      "Epoch 810: val_loss did not improve from 0.00031\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0446e-04 - rmse: 0.0102 - val_loss: 3.5204e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 811/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9816e-05 - rmse: 0.0084\n",
      "Epoch 811: val_loss did not improve from 0.00031\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1944e-05 - rmse: 0.0091 - val_loss: 4.8290e-04 - val_rmse: 0.0220 - lr: 1.0000e-04\n",
      "Epoch 812/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1730e-05 - rmse: 0.0090\n",
      "Epoch 812: val_loss did not improve from 0.00031\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8079e-05 - rmse: 0.0094 - val_loss: 3.6231e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 813/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2632e-05 - rmse: 0.0079\n",
      "Epoch 813: val_loss did not improve from 0.00031\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4248e-04 - rmse: 0.0119 - val_loss: 5.0340e-04 - val_rmse: 0.0224 - lr: 1.0000e-04\n",
      "Epoch 814/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6393e-05 - rmse: 0.0093\n",
      "Epoch 814: val_loss improved from 0.00031 to 0.00027, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.1628e-04 - rmse: 0.0108 - val_loss: 2.6813e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 815/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6965e-04 - rmse: 0.0130\n",
      "Epoch 815: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.9990e-05 - rmse: 0.0100 - val_loss: 4.6687e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 816/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4607e-04 - rmse: 0.0157\n",
      "Epoch 816: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.7969e-05 - rmse: 0.0099 - val_loss: 3.2268e-04 - val_rmse: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 817/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3781e-05 - rmse: 0.0080\n",
      "Epoch 817: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.2933e-05 - rmse: 0.0085 - val_loss: 3.9120e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 818/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4371e-05 - rmse: 0.0067\n",
      "Epoch 818: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1674e-04 - rmse: 0.0108 - val_loss: 3.0243e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 819/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6021e-05 - rmse: 0.0068\n",
      "Epoch 819: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3583e-04 - rmse: 0.0154 - val_loss: 4.3016e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 820/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0723e-04 - rmse: 0.0104\n",
      "Epoch 820: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3213e-04 - rmse: 0.0115 - val_loss: 4.7665e-04 - val_rmse: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 821/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3441e-05 - rmse: 0.0091\n",
      "Epoch 821: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8518e-05 - rmse: 0.0099 - val_loss: 3.5763e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 822/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5988e-05 - rmse: 0.0098\n",
      "Epoch 822: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4105e-04 - rmse: 0.0119 - val_loss: 4.1091e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 823/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7307e-04 - rmse: 0.0132\n",
      "Epoch 823: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6698e-05 - rmse: 0.0098 - val_loss: 3.0691e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 824/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9407e-05 - rmse: 0.0083\n",
      "Epoch 824: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6132e-05 - rmse: 0.0093 - val_loss: 3.4836e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 825/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3275e-05 - rmse: 0.0086\n",
      "Epoch 825: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2757e-04 - rmse: 0.0113 - val_loss: 4.9978e-04 - val_rmse: 0.0224 - lr: 1.0000e-04\n",
      "Epoch 826/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8923e-04 - rmse: 0.0170\n",
      "Epoch 826: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1814e-04 - rmse: 0.0109 - val_loss: 3.4199e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 827/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8608e-05 - rmse: 0.0053\n",
      "Epoch 827: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2869e-04 - rmse: 0.0113 - val_loss: 2.8561e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 828/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4761e-04 - rmse: 0.0121\n",
      "Epoch 828: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8455e-04 - rmse: 0.0136 - val_loss: 2.7899e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 829/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9049e-05 - rmse: 0.0089\n",
      "Epoch 829: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4671e-04 - rmse: 0.0121 - val_loss: 5.6423e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n",
      "Epoch 830/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1794e-04 - rmse: 0.0148\n",
      "Epoch 830: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7643e-04 - rmse: 0.0133 - val_loss: 4.0009e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 831/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1882e-04 - rmse: 0.0109\n",
      "Epoch 831: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2904e-04 - rmse: 0.0114 - val_loss: 6.9682e-04 - val_rmse: 0.0264 - lr: 1.0000e-04\n",
      "Epoch 832/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7575e-04 - rmse: 0.0312\n",
      "Epoch 832: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1103e-04 - rmse: 0.0176 - val_loss: 3.1147e-04 - val_rmse: 0.0176 - lr: 1.0000e-04\n",
      "Epoch 833/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1222e-04 - rmse: 0.0146\n",
      "Epoch 833: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0936e-04 - rmse: 0.0145 - val_loss: 3.9021e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 834/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6786e-05 - rmse: 0.0082\n",
      "Epoch 834: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9058e-04 - rmse: 0.0138 - val_loss: 5.0486e-04 - val_rmse: 0.0225 - lr: 1.0000e-04\n",
      "Epoch 835/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7392e-04 - rmse: 0.0132\n",
      "Epoch 835: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2618e-04 - rmse: 0.0112 - val_loss: 5.8806e-04 - val_rmse: 0.0242 - lr: 1.0000e-04\n",
      "Epoch 836/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6299e-04 - rmse: 0.0128\n",
      "Epoch 836: val_loss did not improve from 0.00027\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6946e-04 - rmse: 0.0130 - val_loss: 4.6841e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 837/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3665e-04 - rmse: 0.0154\n",
      "Epoch 837: val_loss improved from 0.00027 to 0.00022, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.5918e-04 - rmse: 0.0126 - val_loss: 2.1703e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 838/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7312e-04 - rmse: 0.0132\n",
      "Epoch 838: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2825e-04 - rmse: 0.0113 - val_loss: 2.4853e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 839/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9583e-05 - rmse: 0.0095\n",
      "Epoch 839: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3999e-04 - rmse: 0.0118 - val_loss: 3.8291e-04 - val_rmse: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 840/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3583e-05 - rmse: 0.0073\n",
      "Epoch 840: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3933e-04 - rmse: 0.0118 - val_loss: 4.1090e-04 - val_rmse: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 841/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5850e-05 - rmse: 0.0068\n",
      "Epoch 841: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6520e-05 - rmse: 0.0087 - val_loss: 4.3821e-04 - val_rmse: 0.0209 - lr: 1.0000e-04\n",
      "Epoch 842/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2812e-05 - rmse: 0.0096\n",
      "Epoch 842: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1122e-04 - rmse: 0.0105 - val_loss: 3.0437e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 843/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8095e-05 - rmse: 0.0094\n",
      "Epoch 843: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3596e-04 - rmse: 0.0117 - val_loss: 5.6943e-04 - val_rmse: 0.0239 - lr: 1.0000e-04\n",
      "Epoch 844/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3680e-04 - rmse: 0.0154\n",
      "Epoch 844: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2708e-04 - rmse: 0.0151 - val_loss: 7.5313e-04 - val_rmse: 0.0274 - lr: 1.0000e-04\n",
      "Epoch 845/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6270e-04 - rmse: 0.0128\n",
      "Epoch 845: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.4414e-04 - rmse: 0.0273 - val_loss: 3.5776e-04 - val_rmse: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 846/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4324e-05 - rmse: 0.0097\n",
      "Epoch 846: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.1945e-04 - rmse: 0.0249 - val_loss: 0.0012 - val_rmse: 0.0342 - lr: 1.0000e-04\n",
      "Epoch 847/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4530e-04 - rmse: 0.0307\n",
      "Epoch 847: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8133e-04 - rmse: 0.0280 - val_loss: 6.3438e-04 - val_rmse: 0.0252 - lr: 1.0000e-04\n",
      "Epoch 848/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5722e-04 - rmse: 0.0189\n",
      "Epoch 848: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5135e-04 - rmse: 0.0212 - val_loss: 6.8191e-04 - val_rmse: 0.0261 - lr: 1.0000e-04\n",
      "Epoch 849/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2819e-04 - rmse: 0.0151\n",
      "Epoch 849: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1540e-04 - rmse: 0.0204 - val_loss: 0.0013 - val_rmse: 0.0366 - lr: 1.0000e-04\n",
      "Epoch 850/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0014 - rmse: 0.0373\n",
      "Epoch 850: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2256e-04 - rmse: 0.0304 - val_loss: 5.6821e-04 - val_rmse: 0.0238 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 851/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9021e-04 - rmse: 0.0138\n",
      "Epoch 851: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.0382e-04 - rmse: 0.0246 - val_loss: 8.7036e-04 - val_rmse: 0.0295 - lr: 1.0000e-04\n",
      "Epoch 852/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6402e-04 - rmse: 0.0191\n",
      "Epoch 852: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5873e-04 - rmse: 0.0293 - val_loss: 7.2600e-04 - val_rmse: 0.0269 - lr: 1.0000e-04\n",
      "Epoch 853/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0397\n",
      "Epoch 853: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6665e-04 - rmse: 0.0311 - val_loss: 9.8077e-04 - val_rmse: 0.0313 - lr: 1.0000e-04\n",
      "Epoch 854/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1237e-04 - rmse: 0.0226\n",
      "Epoch 854: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0011 - rmse: 0.0329 - val_loss: 0.0014 - val_rmse: 0.0371 - lr: 1.0000e-04\n",
      "Epoch 855/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0016 - rmse: 0.0402\n",
      "Epoch 855: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0010 - rmse: 0.0322 - val_loss: 8.5968e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 856/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0016e-04 - rmse: 0.0300\n",
      "Epoch 856: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.1108e-04 - rmse: 0.0267 - val_loss: 5.8749e-04 - val_rmse: 0.0242 - lr: 1.0000e-04\n",
      "Epoch 857/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0465e-04 - rmse: 0.0175\n",
      "Epoch 857: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.6478e-04 - rmse: 0.0238 - val_loss: 4.1940e-04 - val_rmse: 0.0205 - lr: 1.0000e-04\n",
      "Epoch 858/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7716e-05 - rmse: 0.0099\n",
      "Epoch 858: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3410e-04 - rmse: 0.0183 - val_loss: 5.3083e-04 - val_rmse: 0.0230 - lr: 1.0000e-04\n",
      "Epoch 859/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1776e-04 - rmse: 0.0109\n",
      "Epoch 859: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8337e-04 - rmse: 0.0196 - val_loss: 9.4459e-04 - val_rmse: 0.0307 - lr: 1.0000e-04\n",
      "Epoch 860/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3866e-04 - rmse: 0.0209\n",
      "Epoch 860: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3128e-04 - rmse: 0.0208 - val_loss: 3.0668e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 861/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9754e-04 - rmse: 0.0141\n",
      "Epoch 861: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6246e-04 - rmse: 0.0162 - val_loss: 3.5072e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 862/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0153e-04 - rmse: 0.0101\n",
      "Epoch 862: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3793e-04 - rmse: 0.0154 - val_loss: 8.5746e-04 - val_rmse: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 863/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4499e-04 - rmse: 0.0233\n",
      "Epoch 863: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9619e-04 - rmse: 0.0172 - val_loss: 3.8227e-04 - val_rmse: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 864/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2491e-04 - rmse: 0.0112\n",
      "Epoch 864: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6260e-04 - rmse: 0.0128 - val_loss: 4.1776e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 865/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9660e-05 - rmse: 0.0089\n",
      "Epoch 865: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3339e-04 - rmse: 0.0115 - val_loss: 5.7506e-04 - val_rmse: 0.0240 - lr: 1.0000e-04\n",
      "Epoch 866/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1555e-04 - rmse: 0.0178\n",
      "Epoch 866: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3218e-04 - rmse: 0.0115 - val_loss: 4.5444e-04 - val_rmse: 0.0213 - lr: 1.0000e-04\n",
      "Epoch 867/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5280e-05 - rmse: 0.0074\n",
      "Epoch 867: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7760e-05 - rmse: 0.0094 - val_loss: 3.2783e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 868/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0386e-05 - rmse: 0.0084\n",
      "Epoch 868: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6779e-05 - rmse: 0.0088 - val_loss: 2.9417e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 869/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9948e-05 - rmse: 0.0071\n",
      "Epoch 869: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7641e-05 - rmse: 0.0076 - val_loss: 3.8345e-04 - val_rmse: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 870/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6881e-05 - rmse: 0.0082\n",
      "Epoch 870: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.4311e-05 - rmse: 0.0074 - val_loss: 4.8330e-04 - val_rmse: 0.0220 - lr: 1.0000e-04\n",
      "Epoch 871/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4447e-04 - rmse: 0.0120\n",
      "Epoch 871: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9256e-05 - rmse: 0.0100 - val_loss: 3.2767e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 872/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6475e-05 - rmse: 0.0087\n",
      "Epoch 872: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.5111e-05 - rmse: 0.0098 - val_loss: 2.8365e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 873/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7297e-05 - rmse: 0.0069\n",
      "Epoch 873: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.8911e-05 - rmse: 0.0077 - val_loss: 5.4775e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 874/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5366e-04 - rmse: 0.0124\n",
      "Epoch 874: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3308e-04 - rmse: 0.0115 - val_loss: 4.7368e-04 - val_rmse: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 875/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8363e-05 - rmse: 0.0070\n",
      "Epoch 875: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6081e-04 - rmse: 0.0127 - val_loss: 2.4850e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 876/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0799e-05 - rmse: 0.0078\n",
      "Epoch 876: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3934e-04 - rmse: 0.0118 - val_loss: 4.7284e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 877/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1294e-05 - rmse: 0.0084\n",
      "Epoch 877: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1121e-04 - rmse: 0.0105 - val_loss: 3.6322e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 878/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9995e-05 - rmse: 0.0045\n",
      "Epoch 878: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7467e-05 - rmse: 0.0094 - val_loss: 2.3140e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 879/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5867e-05 - rmse: 0.0075\n",
      "Epoch 879: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.7727e-05 - rmse: 0.0082 - val_loss: 4.7260e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 880/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8381e-05 - rmse: 0.0070\n",
      "Epoch 880: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5802e-05 - rmse: 0.0093 - val_loss: 3.2096e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 881/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2569e-05 - rmse: 0.0065\n",
      "Epoch 881: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.0200e-05 - rmse: 0.0071 - val_loss: 3.5149e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 882/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0259e-05 - rmse: 0.0055\n",
      "Epoch 882: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.9224e-05 - rmse: 0.0083 - val_loss: 3.3265e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 883/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9112e-05 - rmse: 0.0083\n",
      "Epoch 883: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.1966e-05 - rmse: 0.0085 - val_loss: 3.0061e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 884/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9372e-05 - rmse: 0.0054\n",
      "Epoch 884: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1473e-05 - rmse: 0.0064 - val_loss: 3.8896e-04 - val_rmse: 0.0197 - lr: 1.0000e-04\n",
      "Epoch 885/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4909e-05 - rmse: 0.0039\n",
      "Epoch 885: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9243e-05 - rmse: 0.0070 - val_loss: 2.7237e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 886/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2152e-05 - rmse: 0.0057\n",
      "Epoch 886: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7736e-05 - rmse: 0.0076 - val_loss: 3.3654e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 887/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7483e-05 - rmse: 0.0052\n",
      "Epoch 887: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1466e-05 - rmse: 0.0072 - val_loss: 3.9821e-04 - val_rmse: 0.0200 - lr: 1.0000e-04\n",
      "Epoch 888/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9847e-05 - rmse: 0.0077\n",
      "Epoch 888: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.4113e-05 - rmse: 0.0080 - val_loss: 2.5843e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 889/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3987e-04 - rmse: 0.0118\n",
      "Epoch 889: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1540e-05 - rmse: 0.0090 - val_loss: 2.9269e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 890/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5541e-05 - rmse: 0.0060\n",
      "Epoch 890: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.9446e-05 - rmse: 0.0083 - val_loss: 3.5522e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 891/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6434e-05 - rmse: 0.0051\n",
      "Epoch 891: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8445e-05 - rmse: 0.0062 - val_loss: 2.6758e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 892/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1517e-05 - rmse: 0.0064\n",
      "Epoch 892: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7887e-05 - rmse: 0.0062 - val_loss: 2.7908e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 893/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1632e-05 - rmse: 0.0047\n",
      "Epoch 893: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.7754e-05 - rmse: 0.0069 - val_loss: 3.2118e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 894/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1550e-05 - rmse: 0.0056\n",
      "Epoch 894: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.0681e-05 - rmse: 0.0071 - val_loss: 3.7017e-04 - val_rmse: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 895/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3755e-05 - rmse: 0.0049\n",
      "Epoch 895: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7220e-05 - rmse: 0.0069 - val_loss: 2.6378e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 896/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9374e-05 - rmse: 0.0063\n",
      "Epoch 896: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.1790e-05 - rmse: 0.0065 - val_loss: 3.6138e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 897/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4801e-05 - rmse: 0.0038\n",
      "Epoch 897: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.4168e-05 - rmse: 0.0058 - val_loss: 3.0591e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 898/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1919e-05 - rmse: 0.0056\n",
      "Epoch 898: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5034e-05 - rmse: 0.0059 - val_loss: 2.9528e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 899/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1958e-05 - rmse: 0.0047\n",
      "Epoch 899: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3046e-05 - rmse: 0.0066 - val_loss: 2.6857e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 900/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6542e-05 - rmse: 0.0052\n",
      "Epoch 900: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7602e-05 - rmse: 0.0069 - val_loss: 3.7456e-04 - val_rmse: 0.0194 - lr: 1.0000e-04\n",
      "Epoch 901/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5278e-05 - rmse: 0.0087\n",
      "Epoch 901: val_loss did not improve from 0.00022\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3190e-05 - rmse: 0.0086 - val_loss: 2.6578e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 902/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5892e-05 - rmse: 0.0068\n",
      "Epoch 902: val_loss improved from 0.00022 to 0.00021, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 4.4237e-05 - rmse: 0.0067 - val_loss: 2.1478e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 903/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0262e-05 - rmse: 0.0063\n",
      "Epoch 903: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.1388e-05 - rmse: 0.0056 - val_loss: 3.4263e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 904/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5751e-05 - rmse: 0.0051\n",
      "Epoch 904: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1508e-05 - rmse: 0.0064 - val_loss: 2.8646e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 905/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0974e-05 - rmse: 0.0046\n",
      "Epoch 905: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6974e-05 - rmse: 0.0061 - val_loss: 2.5000e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 906/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7573e-05 - rmse: 0.0053\n",
      "Epoch 906: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1839e-05 - rmse: 0.0072 - val_loss: 2.5151e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 907/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0166e-05 - rmse: 0.0045\n",
      "Epoch 907: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.3063e-05 - rmse: 0.0079 - val_loss: 3.6720e-04 - val_rmse: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 908/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3935e-05 - rmse: 0.0066\n",
      "Epoch 908: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.0899e-05 - rmse: 0.0084 - val_loss: 3.6165e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 909/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6126e-05 - rmse: 0.0051\n",
      "Epoch 909: val_loss improved from 0.00021 to 0.00021, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.8005e-05 - rmse: 0.0062 - val_loss: 2.0612e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 910/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5385e-05 - rmse: 0.0087\n",
      "Epoch 910: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.7616e-05 - rmse: 0.0069 - val_loss: 3.4261e-04 - val_rmse: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 911/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4700e-05 - rmse: 0.0050\n",
      "Epoch 911: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3175e-05 - rmse: 0.0079 - val_loss: 3.6700e-04 - val_rmse: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 912/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8898e-05 - rmse: 0.0077\n",
      "Epoch 912: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0238e-04 - rmse: 0.0101 - val_loss: 3.9695e-04 - val_rmse: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 913/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0078e-04 - rmse: 0.0100\n",
      "Epoch 913: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2478e-04 - rmse: 0.0112 - val_loss: 2.6175e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 914/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6801e-04 - rmse: 0.0130\n",
      "Epoch 914: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0887e-05 - rmse: 0.0095 - val_loss: 4.9448e-04 - val_rmse: 0.0222 - lr: 1.0000e-04\n",
      "Epoch 915/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4972e-04 - rmse: 0.0158\n",
      "Epoch 915: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1448e-04 - rmse: 0.0107 - val_loss: 4.5868e-04 - val_rmse: 0.0214 - lr: 1.0000e-04\n",
      "Epoch 916/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8114e-05 - rmse: 0.0076\n",
      "Epoch 916: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4268e-05 - rmse: 0.0097 - val_loss: 2.6008e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 917/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2393e-05 - rmse: 0.0096\n",
      "Epoch 917: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0992e-04 - rmse: 0.0105 - val_loss: 2.6685e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 918/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0430e-05 - rmse: 0.0095\n",
      "Epoch 918: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.9762e-05 - rmse: 0.0095 - val_loss: 3.8133e-04 - val_rmse: 0.0195 - lr: 1.0000e-04\n",
      "Epoch 919/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4998e-05 - rmse: 0.0074\n",
      "Epoch 919: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.6474e-05 - rmse: 0.0082 - val_loss: 4.3242e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 920/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6070e-04 - rmse: 0.0127\n",
      "Epoch 920: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3625e-04 - rmse: 0.0117 - val_loss: 5.3727e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 921/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4833e-04 - rmse: 0.0122\n",
      "Epoch 921: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9160e-04 - rmse: 0.0138 - val_loss: 2.2109e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 922/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0214e-05 - rmse: 0.0063\n",
      "Epoch 922: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3099e-04 - rmse: 0.0114 - val_loss: 4.6484e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 923/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0524e-04 - rmse: 0.0103\n",
      "Epoch 923: val_loss did not improve from 0.00021\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0930e-04 - rmse: 0.0105 - val_loss: 3.0103e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 924/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4833e-05 - rmse: 0.0081\n",
      "Epoch 924: val_loss improved from 0.00021 to 0.00020, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 5.1215e-05 - rmse: 0.0072 - val_loss: 2.0453e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 925/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6449e-05 - rmse: 0.0075\n",
      "Epoch 925: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.6451e-05 - rmse: 0.0082 - val_loss: 4.5488e-04 - val_rmse: 0.0213 - lr: 1.0000e-04\n",
      "Epoch 926/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2093e-04 - rmse: 0.0110\n",
      "Epoch 926: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8719e-05 - rmse: 0.0099 - val_loss: 2.8641e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 927/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3997e-05 - rmse: 0.0058\n",
      "Epoch 927: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.4615e-05 - rmse: 0.0097 - val_loss: 2.4785e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 928/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7011e-04 - rmse: 0.0130\n",
      "Epoch 928: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2680e-04 - rmse: 0.0113 - val_loss: 3.3348e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 929/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1271e-05 - rmse: 0.0064\n",
      "Epoch 929: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1806e-05 - rmse: 0.0090 - val_loss: 3.5156e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 930/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7111e-05 - rmse: 0.0088\n",
      "Epoch 930: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.1901e-05 - rmse: 0.0085 - val_loss: 2.4346e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 931/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4235e-05 - rmse: 0.0080\n",
      "Epoch 931: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.0714e-05 - rmse: 0.0078 - val_loss: 2.7646e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 932/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9236e-05 - rmse: 0.0054\n",
      "Epoch 932: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2739e-05 - rmse: 0.0057 - val_loss: 2.7798e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 933/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0939e-05 - rmse: 0.0046\n",
      "Epoch 933: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4776e-05 - rmse: 0.0050 - val_loss: 2.6929e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 934/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5260e-05 - rmse: 0.0067\n",
      "Epoch 934: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6228e-05 - rmse: 0.0051 - val_loss: 2.3167e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 935/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8005e-05 - rmse: 0.0053\n",
      "Epoch 935: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7618e-05 - rmse: 0.0053 - val_loss: 2.8225e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 936/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7240e-06 - rmse: 0.0031\n",
      "Epoch 936: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8261e-05 - rmse: 0.0053 - val_loss: 2.7998e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 937/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9932e-05 - rmse: 0.0045\n",
      "Epoch 937: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6467e-05 - rmse: 0.0051 - val_loss: 2.1660e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 938/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9093e-05 - rmse: 0.0063\n",
      "Epoch 938: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9413e-05 - rmse: 0.0063 - val_loss: 2.7256e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 939/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9861e-05 - rmse: 0.0045\n",
      "Epoch 939: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9307e-05 - rmse: 0.0054 - val_loss: 2.8648e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 940/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5319e-05 - rmse: 0.0050\n",
      "Epoch 940: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3358e-05 - rmse: 0.0058 - val_loss: 2.6636e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 941/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2943e-05 - rmse: 0.0091\n",
      "Epoch 941: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.4389e-05 - rmse: 0.0059 - val_loss: 2.1556e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 942/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9673e-05 - rmse: 0.0054\n",
      "Epoch 942: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9567e-05 - rmse: 0.0054 - val_loss: 2.7479e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 943/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4394e-05 - rmse: 0.0049\n",
      "Epoch 943: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9240e-05 - rmse: 0.0054 - val_loss: 2.1025e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 944/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9784e-05 - rmse: 0.0063\n",
      "Epoch 944: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9161e-05 - rmse: 0.0054 - val_loss: 2.5292e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 945/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5803e-05 - rmse: 0.0040\n",
      "Epoch 945: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0573e-05 - rmse: 0.0055 - val_loss: 2.9721e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 946/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5609e-05 - rmse: 0.0040\n",
      "Epoch 946: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4742e-05 - rmse: 0.0050 - val_loss: 2.1093e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 947/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8093e-05 - rmse: 0.0062\n",
      "Epoch 947: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7795e-05 - rmse: 0.0053 - val_loss: 2.4884e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 948/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5619e-05 - rmse: 0.0051\n",
      "Epoch 948: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5510e-05 - rmse: 0.0051 - val_loss: 2.5376e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 949/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9676e-05 - rmse: 0.0054\n",
      "Epoch 949: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0468e-05 - rmse: 0.0045 - val_loss: 2.4976e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 950/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5166e-06 - rmse: 0.0031\n",
      "Epoch 950: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0681e-05 - rmse: 0.0045 - val_loss: 2.3592e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 951/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8049e-05 - rmse: 0.0042\n",
      "Epoch 951: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0777e-05 - rmse: 0.0046 - val_loss: 2.5352e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 952/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2390e-05 - rmse: 0.0057\n",
      "Epoch 952: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4257e-05 - rmse: 0.0049 - val_loss: 2.4586e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 953/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1156e-05 - rmse: 0.0046\n",
      "Epoch 953: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9256e-05 - rmse: 0.0054 - val_loss: 2.0673e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 954/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9606e-05 - rmse: 0.0054\n",
      "Epoch 954: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3277e-05 - rmse: 0.0058 - val_loss: 2.4796e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 955/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2878e-05 - rmse: 0.0048\n",
      "Epoch 955: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9276e-05 - rmse: 0.0054 - val_loss: 3.0698e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 956/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6272e-05 - rmse: 0.0060\n",
      "Epoch 956: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.6499e-05 - rmse: 0.0068 - val_loss: 2.3394e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 957/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2087e-05 - rmse: 0.0079\n",
      "Epoch 957: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.2849e-05 - rmse: 0.0073 - val_loss: 2.1140e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 958/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2604e-05 - rmse: 0.0057\n",
      "Epoch 958: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5288e-05 - rmse: 0.0067 - val_loss: 2.7232e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 959/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6880e-05 - rmse: 0.0052\n",
      "Epoch 959: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.0988e-05 - rmse: 0.0071 - val_loss: 3.0213e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 960/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5585e-05 - rmse: 0.0060\n",
      "Epoch 960: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9761e-05 - rmse: 0.0063 - val_loss: 2.2930e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 961/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1367e-05 - rmse: 0.0078\n",
      "Epoch 961: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9704e-05 - rmse: 0.0063 - val_loss: 2.6976e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 962/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3595e-05 - rmse: 0.0058\n",
      "Epoch 962: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8338e-05 - rmse: 0.0053 - val_loss: 2.9355e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 963/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0144e-05 - rmse: 0.0063\n",
      "Epoch 963: val_loss did not improve from 0.00020\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2084e-05 - rmse: 0.0047 - val_loss: 2.2439e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 964/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7741e-05 - rmse: 0.0061\n",
      "Epoch 964: val_loss improved from 0.00020 to 0.00017, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.8849e-05 - rmse: 0.0062 - val_loss: 1.7488e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 965/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8623e-05 - rmse: 0.0043\n",
      "Epoch 965: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.4901e-05 - rmse: 0.0074 - val_loss: 2.5893e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 966/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2540e-05 - rmse: 0.0047\n",
      "Epoch 966: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0758e-05 - rmse: 0.0046 - val_loss: 2.4625e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 967/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7452e-05 - rmse: 0.0042\n",
      "Epoch 967: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4851e-05 - rmse: 0.0050 - val_loss: 3.2516e-04 - val_rmse: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 968/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0278e-05 - rmse: 0.0071\n",
      "Epoch 968: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3255e-05 - rmse: 0.0048 - val_loss: 2.2535e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 969/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0911e-05 - rmse: 0.0046\n",
      "Epoch 969: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2085e-05 - rmse: 0.0047 - val_loss: 2.3058e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 970/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1147e-05 - rmse: 0.0033\n",
      "Epoch 970: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0531e-05 - rmse: 0.0055 - val_loss: 3.3167e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 971/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0380e-05 - rmse: 0.0064\n",
      "Epoch 971: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3173e-05 - rmse: 0.0058 - val_loss: 2.5125e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 972/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2725e-05 - rmse: 0.0036\n",
      "Epoch 972: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5234e-05 - rmse: 0.0059 - val_loss: 1.9589e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 973/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1893e-05 - rmse: 0.0047\n",
      "Epoch 973: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9198e-05 - rmse: 0.0044 - val_loss: 2.1941e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 974/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6250e-05 - rmse: 0.0040\n",
      "Epoch 974: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2162e-05 - rmse: 0.0047 - val_loss: 2.3980e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 975/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5780e-05 - rmse: 0.0040\n",
      "Epoch 975: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1688e-05 - rmse: 0.0047 - val_loss: 2.6434e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 976/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0059e-05 - rmse: 0.0032\n",
      "Epoch 976: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7641e-05 - rmse: 0.0053 - val_loss: 2.8769e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 977/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1606e-05 - rmse: 0.0034\n",
      "Epoch 977: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.4575e-05 - rmse: 0.0067 - val_loss: 2.3093e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 978/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6921e-05 - rmse: 0.0061\n",
      "Epoch 978: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0729e-05 - rmse: 0.0055 - val_loss: 1.7686e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 979/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8232e-05 - rmse: 0.0053\n",
      "Epoch 979: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3542e-05 - rmse: 0.0049 - val_loss: 2.6398e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 980/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7591e-06 - rmse: 0.0031\n",
      "Epoch 980: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2394e-05 - rmse: 0.0047 - val_loss: 2.4573e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 981/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0688e-05 - rmse: 0.0064\n",
      "Epoch 981: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5959e-05 - rmse: 0.0051 - val_loss: 2.6274e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 982/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9473e-05 - rmse: 0.0044\n",
      "Epoch 982: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0671e-05 - rmse: 0.0045 - val_loss: 2.3509e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 983/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1541e-06 - rmse: 0.0030\n",
      "Epoch 983: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7156e-05 - rmse: 0.0041 - val_loss: 2.5413e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 984/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8934e-05 - rmse: 0.0044\n",
      "Epoch 984: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8730e-05 - rmse: 0.0043 - val_loss: 2.2696e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 985/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7199e-05 - rmse: 0.0069\n",
      "Epoch 985: val_loss did not improve from 0.00017\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9379e-05 - rmse: 0.0054 - val_loss: 2.9279e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 986/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4231e-05 - rmse: 0.0086\n",
      "Epoch 986: val_loss improved from 0.00017 to 0.00016, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.9662e-05 - rmse: 0.0054 - val_loss: 1.5949e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 987/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3161e-05 - rmse: 0.0066\n",
      "Epoch 987: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5790e-05 - rmse: 0.0051 - val_loss: 2.5690e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 988/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1307e-05 - rmse: 0.0034\n",
      "Epoch 988: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1353e-05 - rmse: 0.0064 - val_loss: 2.5552e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 989/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5430e-05 - rmse: 0.0060\n",
      "Epoch 989: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5060e-05 - rmse: 0.0067 - val_loss: 3.7080e-04 - val_rmse: 0.0193 - lr: 1.0000e-04\n",
      "Epoch 990/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9756e-05 - rmse: 0.0095\n",
      "Epoch 990: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.3166e-05 - rmse: 0.0079 - val_loss: 2.0911e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 991/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9771e-05 - rmse: 0.0044\n",
      "Epoch 991: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2646e-05 - rmse: 0.0065 - val_loss: 2.0790e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 992/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9292e-05 - rmse: 0.0063\n",
      "Epoch 992: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2561e-05 - rmse: 0.0057 - val_loss: 2.5107e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 993/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3451e-05 - rmse: 0.0048\n",
      "Epoch 993: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1459e-05 - rmse: 0.0064 - val_loss: 2.9162e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 994/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5911e-05 - rmse: 0.0051\n",
      "Epoch 994: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.5948e-05 - rmse: 0.0060 - val_loss: 2.1434e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 995/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2775e-05 - rmse: 0.0073\n",
      "Epoch 995: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9593e-05 - rmse: 0.0063 - val_loss: 2.7244e-04 - val_rmse: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 996/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9913e-05 - rmse: 0.0045\n",
      "Epoch 996: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.0530e-05 - rmse: 0.0090 - val_loss: 3.6408e-04 - val_rmse: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 997/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2747e-04 - rmse: 0.0151\n",
      "Epoch 997: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8920e-04 - rmse: 0.0138 - val_loss: 4.1985e-04 - val_rmse: 0.0205 - lr: 1.0000e-04\n",
      "Epoch 998/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0800e-04 - rmse: 0.0104\n",
      "Epoch 998: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.0743e-04 - rmse: 0.0202 - val_loss: 2.8858e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 999/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0931e-04 - rmse: 0.0226\n",
      "Epoch 999: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0900e-04 - rmse: 0.0202 - val_loss: 3.6209e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 1000/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8187e-04 - rmse: 0.0195\n",
      "Epoch 1000: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4334e-04 - rmse: 0.0185 - val_loss: 6.2814e-04 - val_rmse: 0.0251 - lr: 1.0000e-04\n",
      "Epoch 1001/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3424e-04 - rmse: 0.0153\n",
      "Epoch 1001: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0383e-04 - rmse: 0.0143 - val_loss: 2.5499e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 1002/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9018e-04 - rmse: 0.0138\n",
      "Epoch 1002: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5745e-04 - rmse: 0.0125 - val_loss: 2.6077e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 1003/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8125e-05 - rmse: 0.0076\n",
      "Epoch 1003: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0957e-04 - rmse: 0.0105 - val_loss: 2.3173e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1004/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2391e-05 - rmse: 0.0079\n",
      "Epoch 1004: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.0410e-05 - rmse: 0.0078 - val_loss: 3.5181e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 1005/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5127e-05 - rmse: 0.0081\n",
      "Epoch 1005: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.4383e-05 - rmse: 0.0074 - val_loss: 2.8309e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 1006/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1477e-05 - rmse: 0.0056\n",
      "Epoch 1006: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3833e-05 - rmse: 0.0092 - val_loss: 2.6877e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1007/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4836e-05 - rmse: 0.0050\n",
      "Epoch 1007: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3809e-05 - rmse: 0.0058 - val_loss: 2.4634e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1008/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6080e-05 - rmse: 0.0060\n",
      "Epoch 1008: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5668e-05 - rmse: 0.0051 - val_loss: 2.7447e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 1009/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7417e-05 - rmse: 0.0052\n",
      "Epoch 1009: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1082e-05 - rmse: 0.0064 - val_loss: 2.7505e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 1010/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6374e-05 - rmse: 0.0051\n",
      "Epoch 1010: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5668e-05 - rmse: 0.0068 - val_loss: 3.4432e-04 - val_rmse: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 1011/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3943e-05 - rmse: 0.0058\n",
      "Epoch 1011: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2988e-05 - rmse: 0.0066 - val_loss: 1.8444e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1012/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8117e-05 - rmse: 0.0069\n",
      "Epoch 1012: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1310e-05 - rmse: 0.0064 - val_loss: 2.7726e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 1013/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5415e-05 - rmse: 0.0067\n",
      "Epoch 1013: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.9814e-05 - rmse: 0.0071 - val_loss: 2.0590e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1014/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2956e-05 - rmse: 0.0073\n",
      "Epoch 1014: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.7351e-05 - rmse: 0.0076 - val_loss: 2.6500e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1015/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9413e-05 - rmse: 0.0083\n",
      "Epoch 1015: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6934e-05 - rmse: 0.0075 - val_loss: 3.4806e-04 - val_rmse: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 1016/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4188e-05 - rmse: 0.0074\n",
      "Epoch 1016: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.3567e-05 - rmse: 0.0066 - val_loss: 2.4588e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1017/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8069e-05 - rmse: 0.0043\n",
      "Epoch 1017: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2926e-05 - rmse: 0.0057 - val_loss: 1.8124e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1018/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4384e-05 - rmse: 0.0067\n",
      "Epoch 1018: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2325e-05 - rmse: 0.0057 - val_loss: 2.5135e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1019/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8587e-05 - rmse: 0.0062\n",
      "Epoch 1019: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6103e-05 - rmse: 0.0060 - val_loss: 3.1740e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 1020/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5793e-05 - rmse: 0.0051\n",
      "Epoch 1020: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8903e-05 - rmse: 0.0054 - val_loss: 2.3378e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 1021/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5954e-05 - rmse: 0.0060\n",
      "Epoch 1021: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2781e-05 - rmse: 0.0048 - val_loss: 2.5220e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1022/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0062e-05 - rmse: 0.0032\n",
      "Epoch 1022: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5863e-05 - rmse: 0.0051 - val_loss: 2.2083e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1023/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5026e-05 - rmse: 0.0039\n",
      "Epoch 1023: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8572e-05 - rmse: 0.0053 - val_loss: 2.9484e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 1024/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3423e-05 - rmse: 0.0058\n",
      "Epoch 1024: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9737e-05 - rmse: 0.0044 - val_loss: 2.5277e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1025/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4040e-06 - rmse: 0.0027\n",
      "Epoch 1025: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6397e-05 - rmse: 0.0040 - val_loss: 2.1077e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1026/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1002e-05 - rmse: 0.0046\n",
      "Epoch 1026: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9295e-05 - rmse: 0.0044 - val_loss: 2.4509e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1027/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4619e-05 - rmse: 0.0038\n",
      "Epoch 1027: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0604e-05 - rmse: 0.0055 - val_loss: 3.2190e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 1028/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7040e-05 - rmse: 0.0061\n",
      "Epoch 1028: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8608e-05 - rmse: 0.0053 - val_loss: 2.9797e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 1029/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3891e-05 - rmse: 0.0066\n",
      "Epoch 1029: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2748e-05 - rmse: 0.0048 - val_loss: 2.7518e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 1030/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0286e-05 - rmse: 0.0045\n",
      "Epoch 1030: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7771e-05 - rmse: 0.0042 - val_loss: 1.8338e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1031/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9676e-05 - rmse: 0.0070\n",
      "Epoch 1031: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4034e-05 - rmse: 0.0049 - val_loss: 2.9791e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 1032/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0490e-05 - rmse: 0.0032\n",
      "Epoch 1032: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7530e-05 - rmse: 0.0042 - val_loss: 2.3650e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 1033/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1499e-05 - rmse: 0.0034\n",
      "Epoch 1033: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5968e-05 - rmse: 0.0040 - val_loss: 2.5017e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1034/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1229e-06 - rmse: 0.0029\n",
      "Epoch 1034: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3851e-05 - rmse: 0.0037 - val_loss: 2.1225e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1035/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0464e-05 - rmse: 0.0032\n",
      "Epoch 1035: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7860e-05 - rmse: 0.0042 - val_loss: 2.0845e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1036/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3111e-05 - rmse: 0.0036\n",
      "Epoch 1036: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2139e-05 - rmse: 0.0057 - val_loss: 2.3747e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 1037/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1942e-05 - rmse: 0.0035\n",
      "Epoch 1037: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5182e-05 - rmse: 0.0050 - val_loss: 2.5683e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 1038/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8195e-05 - rmse: 0.0043\n",
      "Epoch 1038: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3781e-05 - rmse: 0.0049 - val_loss: 2.7055e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1039/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4303e-05 - rmse: 0.0038\n",
      "Epoch 1039: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.5345e-05 - rmse: 0.0067 - val_loss: 1.7036e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1040/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1197e-05 - rmse: 0.0090\n",
      "Epoch 1040: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.8152e-05 - rmse: 0.0076 - val_loss: 2.3941e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1041/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1959e-05 - rmse: 0.0072\n",
      "Epoch 1041: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2143e-04 - rmse: 0.0110 - val_loss: 2.4967e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1042/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4486e-05 - rmse: 0.0049\n",
      "Epoch 1042: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.2323e-05 - rmse: 0.0085 - val_loss: 5.3976e-04 - val_rmse: 0.0232 - lr: 1.0000e-04\n",
      "Epoch 1043/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8759e-04 - rmse: 0.0137\n",
      "Epoch 1043: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1911e-04 - rmse: 0.0109 - val_loss: 1.9581e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1044/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5396e-05 - rmse: 0.0039\n",
      "Epoch 1044: val_loss did not improve from 0.00016\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2997e-04 - rmse: 0.0114 - val_loss: 2.4091e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1045/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2389e-04 - rmse: 0.0150\n",
      "Epoch 1045: val_loss improved from 0.00016 to 0.00015, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 1.4793e-04 - rmse: 0.0122 - val_loss: 1.5097e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1046/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1458e-05 - rmse: 0.0072\n",
      "Epoch 1046: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0830e-04 - rmse: 0.0104 - val_loss: 2.6485e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1047/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0517e-05 - rmse: 0.0064\n",
      "Epoch 1047: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.8162e-05 - rmse: 0.0069 - val_loss: 3.0010e-04 - val_rmse: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 1048/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2934e-05 - rmse: 0.0066\n",
      "Epoch 1048: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5038e-05 - rmse: 0.0081 - val_loss: 2.1957e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1049/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2359e-05 - rmse: 0.0047\n",
      "Epoch 1049: val_loss improved from 0.00015 to 0.00015, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.1119e-05 - rmse: 0.0056 - val_loss: 1.4771e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1050/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4534e-05 - rmse: 0.0067\n",
      "Epoch 1050: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6717e-05 - rmse: 0.0052 - val_loss: 2.3311e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 1051/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3424e-05 - rmse: 0.0048\n",
      "Epoch 1051: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8029e-05 - rmse: 0.0042 - val_loss: 2.1907e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1052/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1767e-05 - rmse: 0.0034\n",
      "Epoch 1052: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7752e-05 - rmse: 0.0042 - val_loss: 2.0921e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1053/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0815e-05 - rmse: 0.0033\n",
      "Epoch 1053: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8608e-05 - rmse: 0.0043 - val_loss: 1.9487e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1054/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9737e-06 - rmse: 0.0028\n",
      "Epoch 1054: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2711e-05 - rmse: 0.0036 - val_loss: 2.0761e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1055/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5020e-06 - rmse: 0.0029\n",
      "Epoch 1055: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1345e-05 - rmse: 0.0034 - val_loss: 2.2167e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1056/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2872e-05 - rmse: 0.0036\n",
      "Epoch 1056: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3738e-05 - rmse: 0.0037 - val_loss: 2.5303e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1057/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1758e-05 - rmse: 0.0034\n",
      "Epoch 1057: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2047e-05 - rmse: 0.0035 - val_loss: 1.7653e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1058/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2884e-05 - rmse: 0.0048\n",
      "Epoch 1058: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6068e-05 - rmse: 0.0040 - val_loss: 1.8944e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1059/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5119e-06 - rmse: 0.0027\n",
      "Epoch 1059: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4648e-05 - rmse: 0.0038 - val_loss: 2.2107e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1060/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0807e-06 - rmse: 0.0027\n",
      "Epoch 1060: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5966e-05 - rmse: 0.0040 - val_loss: 2.4538e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1061/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0173e-05 - rmse: 0.0032\n",
      "Epoch 1061: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0198e-05 - rmse: 0.0032 - val_loss: 1.9364e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1062/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2956e-05 - rmse: 0.0036\n",
      "Epoch 1062: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4037e-05 - rmse: 0.0037 - val_loss: 2.1367e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1063/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6060e-06 - rmse: 0.0031\n",
      "Epoch 1063: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1725e-05 - rmse: 0.0034 - val_loss: 2.3007e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1064/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1027e-05 - rmse: 0.0033\n",
      "Epoch 1064: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4173e-05 - rmse: 0.0038 - val_loss: 2.2050e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1065/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5533e-06 - rmse: 0.0029\n",
      "Epoch 1065: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9954e-06 - rmse: 0.0032 - val_loss: 2.0756e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1066/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1588e-05 - rmse: 0.0034\n",
      "Epoch 1066: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1105e-06 - rmse: 0.0030 - val_loss: 1.9453e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1067/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7906e-06 - rmse: 0.0031\n",
      "Epoch 1067: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5101e-06 - rmse: 0.0031 - val_loss: 2.2049e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1068/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0455e-06 - rmse: 0.0028\n",
      "Epoch 1068: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0494e-05 - rmse: 0.0032 - val_loss: 1.9359e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1069/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0457e-05 - rmse: 0.0032\n",
      "Epoch 1069: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2180e-05 - rmse: 0.0035 - val_loss: 2.0151e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1070/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9107e-06 - rmse: 0.0031\n",
      "Epoch 1070: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3086e-05 - rmse: 0.0036 - val_loss: 1.9583e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1071/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4909e-06 - rmse: 0.0021\n",
      "Epoch 1071: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2895e-05 - rmse: 0.0036 - val_loss: 2.0651e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1072/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8684e-06 - rmse: 0.0026\n",
      "Epoch 1072: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1601e-05 - rmse: 0.0034 - val_loss: 2.0173e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1073/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6320e-06 - rmse: 0.0029\n",
      "Epoch 1073: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3527e-05 - rmse: 0.0037 - val_loss: 2.3662e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 1074/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7111e-05 - rmse: 0.0041\n",
      "Epoch 1074: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3284e-05 - rmse: 0.0036 - val_loss: 2.1489e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1075/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1787e-05 - rmse: 0.0034\n",
      "Epoch 1075: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3096e-05 - rmse: 0.0036 - val_loss: 2.1755e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1076/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6926e-06 - rmse: 0.0028\n",
      "Epoch 1076: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7461e-06 - rmse: 0.0031 - val_loss: 1.8445e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1077/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2156e-05 - rmse: 0.0035\n",
      "Epoch 1077: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6069e-05 - rmse: 0.0051 - val_loss: 2.3927e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1078/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7617e-06 - rmse: 0.0028\n",
      "Epoch 1078: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3305e-05 - rmse: 0.0048 - val_loss: 2.5587e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 1079/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6331e-05 - rmse: 0.0051\n",
      "Epoch 1079: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3483e-05 - rmse: 0.0048 - val_loss: 3.0463e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 1080/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8967e-05 - rmse: 0.0062\n",
      "Epoch 1080: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9909e-05 - rmse: 0.0063 - val_loss: 2.0812e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1081/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7874e-05 - rmse: 0.0042\n",
      "Epoch 1081: val_loss did not improve from 0.00015\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6958e-05 - rmse: 0.0061 - val_loss: 2.1717e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1082/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7074e-05 - rmse: 0.0088\n",
      "Epoch 1082: val_loss improved from 0.00015 to 0.00011, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.8983e-05 - rmse: 0.0070 - val_loss: 1.1095e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1083/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1028e-04 - rmse: 0.0105\n",
      "Epoch 1083: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.8282e-05 - rmse: 0.0083 - val_loss: 1.5090e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1084/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7856e-05 - rmse: 0.0076\n",
      "Epoch 1084: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7837e-05 - rmse: 0.0088 - val_loss: 1.8142e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1085/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4793e-05 - rmse: 0.0038\n",
      "Epoch 1085: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7163e-05 - rmse: 0.0099 - val_loss: 3.2895e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 1086/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9049e-04 - rmse: 0.0138\n",
      "Epoch 1086: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9603e-05 - rmse: 0.0095 - val_loss: 2.8495e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 1087/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9461e-05 - rmse: 0.0100\n",
      "Epoch 1087: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7570e-05 - rmse: 0.0094 - val_loss: 2.2962e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1088/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4232e-05 - rmse: 0.0086\n",
      "Epoch 1088: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7354e-05 - rmse: 0.0088 - val_loss: 2.8114e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 1089/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0972e-05 - rmse: 0.0071\n",
      "Epoch 1089: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9022e-04 - rmse: 0.0138 - val_loss: 1.7052e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1090/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2163e-05 - rmse: 0.0035\n",
      "Epoch 1090: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6498e-04 - rmse: 0.0163 - val_loss: 5.2195e-04 - val_rmse: 0.0228 - lr: 1.0000e-04\n",
      "Epoch 1091/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1479e-04 - rmse: 0.0204\n",
      "Epoch 1091: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.4352e-04 - rmse: 0.0211 - val_loss: 1.6403e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1092/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3858e-04 - rmse: 0.0118\n",
      "Epoch 1092: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1651e-04 - rmse: 0.0178 - val_loss: 1.8221e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1093/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0842e-04 - rmse: 0.0104\n",
      "Epoch 1093: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0831e-04 - rmse: 0.0176 - val_loss: 4.3190e-04 - val_rmse: 0.0208 - lr: 1.0000e-04\n",
      "Epoch 1094/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6914e-04 - rmse: 0.0130\n",
      "Epoch 1094: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0548e-04 - rmse: 0.0143 - val_loss: 4.4603e-04 - val_rmse: 0.0211 - lr: 1.0000e-04\n",
      "Epoch 1095/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5957e-04 - rmse: 0.0190\n",
      "Epoch 1095: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7045e-04 - rmse: 0.0131 - val_loss: 7.9238e-04 - val_rmse: 0.0281 - lr: 1.0000e-04\n",
      "Epoch 1096/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0485e-04 - rmse: 0.0225\n",
      "Epoch 1096: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7077e-04 - rmse: 0.0193 - val_loss: 5.8977e-04 - val_rmse: 0.0243 - lr: 1.0000e-04\n",
      "Epoch 1097/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1953e-04 - rmse: 0.0205\n",
      "Epoch 1097: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8370e-04 - rmse: 0.0196 - val_loss: 7.6465e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 1098/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1680e-04 - rmse: 0.0204\n",
      "Epoch 1098: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2140e-04 - rmse: 0.0228 - val_loss: 2.5177e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1099/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2131e-04 - rmse: 0.0110\n",
      "Epoch 1099: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.4121e-04 - rmse: 0.0253 - val_loss: 4.5068e-04 - val_rmse: 0.0212 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3339e-04 - rmse: 0.0115\n",
      "Epoch 1100: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.6510e-04 - rmse: 0.0258 - val_loss: 3.9313e-04 - val_rmse: 0.0198 - lr: 1.0000e-04\n",
      "Epoch 1101/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7243e-04 - rmse: 0.0193\n",
      "Epoch 1101: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8413e-04 - rmse: 0.0242 - val_loss: 3.6963e-04 - val_rmse: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 1102/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4941e-04 - rmse: 0.0291\n",
      "Epoch 1102: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.0691e-04 - rmse: 0.0266 - val_loss: 0.0014 - val_rmse: 0.0377 - lr: 1.0000e-04\n",
      "Epoch 1103/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0022 - rmse: 0.0471\n",
      "Epoch 1103: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8027e-04 - rmse: 0.0313 - val_loss: 0.0011 - val_rmse: 0.0330 - lr: 1.0000e-04\n",
      "Epoch 1104/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0011 - rmse: 0.0329\n",
      "Epoch 1104: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.8146e-04 - rmse: 0.0261 - val_loss: 0.0019 - val_rmse: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 1105/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.0017 - rmse: 0.0413\n",
      "Epoch 1105: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6672e-04 - rmse: 0.0311 - val_loss: 2.8791e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 1106/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0925e-04 - rmse: 0.0176\n",
      "Epoch 1106: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2317e-04 - rmse: 0.0229 - val_loss: 5.1385e-04 - val_rmse: 0.0227 - lr: 1.0000e-04\n",
      "Epoch 1107/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4009e-04 - rmse: 0.0184\n",
      "Epoch 1107: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4522e-04 - rmse: 0.0186 - val_loss: 0.0010 - val_rmse: 0.0318 - lr: 1.0000e-04\n",
      "Epoch 1108/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4978e-04 - rmse: 0.0212\n",
      "Epoch 1108: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.4890e-04 - rmse: 0.0255 - val_loss: 1.9288e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1109/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7229e-05 - rmse: 0.0093\n",
      "Epoch 1109: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6820e-04 - rmse: 0.0130 - val_loss: 3.7722e-04 - val_rmse: 0.0194 - lr: 1.0000e-04\n",
      "Epoch 1110/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3069e-04 - rmse: 0.0182\n",
      "Epoch 1110: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9992e-04 - rmse: 0.0141 - val_loss: 2.0335e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1111/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8435e-05 - rmse: 0.0083\n",
      "Epoch 1111: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7360e-05 - rmse: 0.0093 - val_loss: 2.0635e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1112/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1250e-04 - rmse: 0.0106\n",
      "Epoch 1112: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4950e-05 - rmse: 0.0097 - val_loss: 1.8462e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1113/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5076e-05 - rmse: 0.0081\n",
      "Epoch 1113: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.3041e-05 - rmse: 0.0073 - val_loss: 3.5313e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 1114/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9720e-05 - rmse: 0.0095\n",
      "Epoch 1114: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3468e-05 - rmse: 0.0086 - val_loss: 2.6484e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1115/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8157e-05 - rmse: 0.0043\n",
      "Epoch 1115: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5840e-05 - rmse: 0.0075 - val_loss: 1.7939e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1116/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8468e-05 - rmse: 0.0089\n",
      "Epoch 1116: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5993e-05 - rmse: 0.0075 - val_loss: 2.6108e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1117/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1642e-05 - rmse: 0.0065\n",
      "Epoch 1117: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8551e-05 - rmse: 0.0062 - val_loss: 2.6856e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1118/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0664e-05 - rmse: 0.0055\n",
      "Epoch 1118: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9700e-05 - rmse: 0.0054 - val_loss: 2.1570e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1119/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5212e-05 - rmse: 0.0050\n",
      "Epoch 1119: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9010e-05 - rmse: 0.0054 - val_loss: 1.6737e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1120/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6593e-05 - rmse: 0.0052\n",
      "Epoch 1120: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6561e-05 - rmse: 0.0052 - val_loss: 2.0861e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1121/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8343e-05 - rmse: 0.0053\n",
      "Epoch 1121: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6703e-05 - rmse: 0.0052 - val_loss: 2.0390e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1122/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2944e-05 - rmse: 0.0057\n",
      "Epoch 1122: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8772e-05 - rmse: 0.0043 - val_loss: 2.7606e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 1123/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8685e-05 - rmse: 0.0054\n",
      "Epoch 1123: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3129e-05 - rmse: 0.0048 - val_loss: 2.0585e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1124/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2213e-05 - rmse: 0.0035\n",
      "Epoch 1124: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8813e-05 - rmse: 0.0043 - val_loss: 1.7883e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1125/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4543e-05 - rmse: 0.0050\n",
      "Epoch 1125: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0520e-05 - rmse: 0.0045 - val_loss: 2.1547e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1126/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5118e-05 - rmse: 0.0039\n",
      "Epoch 1126: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5974e-05 - rmse: 0.0040 - val_loss: 2.6581e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1127/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5531e-05 - rmse: 0.0051\n",
      "Epoch 1127: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8368e-05 - rmse: 0.0053 - val_loss: 2.0429e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1128/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3780e-05 - rmse: 0.0037\n",
      "Epoch 1128: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9426e-05 - rmse: 0.0054 - val_loss: 1.7931e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1129/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1162e-05 - rmse: 0.0090\n",
      "Epoch 1129: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2129e-05 - rmse: 0.0065 - val_loss: 1.9392e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1130/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7005e-05 - rmse: 0.0041\n",
      "Epoch 1130: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2129e-05 - rmse: 0.0072 - val_loss: 2.6480e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1131/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3281e-05 - rmse: 0.0048\n",
      "Epoch 1131: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6205e-05 - rmse: 0.0051 - val_loss: 2.8151e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 1132/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0873e-05 - rmse: 0.0046\n",
      "Epoch 1132: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0257e-05 - rmse: 0.0045 - val_loss: 2.4175e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1133/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7742e-06 - rmse: 0.0030\n",
      "Epoch 1133: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3833e-05 - rmse: 0.0037 - val_loss: 1.8482e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1134/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6046e-05 - rmse: 0.0040\n",
      "Epoch 1134: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5941e-05 - rmse: 0.0040 - val_loss: 2.8353e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 1135/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5204e-05 - rmse: 0.0050\n",
      "Epoch 1135: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0110e-05 - rmse: 0.0045 - val_loss: 2.1340e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1136/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9209e-05 - rmse: 0.0044\n",
      "Epoch 1136: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7414e-05 - rmse: 0.0042 - val_loss: 2.1366e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1137/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1010e-05 - rmse: 0.0056\n",
      "Epoch 1137: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1015e-05 - rmse: 0.0056 - val_loss: 2.3202e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1138/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6516e-05 - rmse: 0.0041\n",
      "Epoch 1138: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4476e-05 - rmse: 0.0049 - val_loss: 2.1965e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1139/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5611e-05 - rmse: 0.0040\n",
      "Epoch 1139: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1654e-05 - rmse: 0.0047 - val_loss: 1.8751e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1140/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1408e-05 - rmse: 0.0034\n",
      "Epoch 1140: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5503e-05 - rmse: 0.0039 - val_loss: 2.2897e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1141/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5907e-05 - rmse: 0.0051\n",
      "Epoch 1141: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7583e-05 - rmse: 0.0042 - val_loss: 2.5226e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1142/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4843e-05 - rmse: 0.0039\n",
      "Epoch 1142: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8709e-05 - rmse: 0.0043 - val_loss: 2.0492e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1143/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3731e-05 - rmse: 0.0037\n",
      "Epoch 1143: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9613e-05 - rmse: 0.0044 - val_loss: 1.6608e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1144/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4493e-05 - rmse: 0.0038\n",
      "Epoch 1144: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5230e-05 - rmse: 0.0050 - val_loss: 2.0073e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1145/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5669e-06 - rmse: 0.0026\n",
      "Epoch 1145: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5770e-05 - rmse: 0.0060 - val_loss: 2.6180e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1146/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5278e-05 - rmse: 0.0039\n",
      "Epoch 1146: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2993e-05 - rmse: 0.0057 - val_loss: 2.4335e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 1147/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8452e-06 - rmse: 0.0030\n",
      "Epoch 1147: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5719e-05 - rmse: 0.0051 - val_loss: 2.8888e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 1148/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5520e-05 - rmse: 0.0039\n",
      "Epoch 1148: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4979e-05 - rmse: 0.0039 - val_loss: 2.0030e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1149/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6127e-05 - rmse: 0.0051\n",
      "Epoch 1149: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9308e-05 - rmse: 0.0044 - val_loss: 2.1124e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1150/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6873e-06 - rmse: 0.0029\n",
      "Epoch 1150: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7330e-05 - rmse: 0.0042 - val_loss: 2.3789e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 1151/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5377e-05 - rmse: 0.0039\n",
      "Epoch 1151: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4534e-05 - rmse: 0.0038 - val_loss: 2.4056e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1152/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0155e-05 - rmse: 0.0032\n",
      "Epoch 1152: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3551e-05 - rmse: 0.0037 - val_loss: 1.8740e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1153/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4156e-06 - rmse: 0.0025\n",
      "Epoch 1153: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3551e-05 - rmse: 0.0037 - val_loss: 1.6196e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1154/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6711e-05 - rmse: 0.0041\n",
      "Epoch 1154: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2279e-05 - rmse: 0.0035 - val_loss: 2.5728e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 1155/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1512e-05 - rmse: 0.0046\n",
      "Epoch 1155: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8709e-05 - rmse: 0.0043 - val_loss: 2.3953e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1156/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9075e-05 - rmse: 0.0044\n",
      "Epoch 1156: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0137e-05 - rmse: 0.0045 - val_loss: 1.7971e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1157/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2555e-05 - rmse: 0.0035\n",
      "Epoch 1157: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1228e-05 - rmse: 0.0034 - val_loss: 1.8457e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1158/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6409e-05 - rmse: 0.0041\n",
      "Epoch 1158: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6423e-05 - rmse: 0.0041 - val_loss: 1.8475e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1159/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0307e-05 - rmse: 0.0045\n",
      "Epoch 1159: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0818e-05 - rmse: 0.0056 - val_loss: 2.6472e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1160/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5192e-05 - rmse: 0.0050\n",
      "Epoch 1160: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5124e-05 - rmse: 0.0050 - val_loss: 2.1553e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1161/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2155e-05 - rmse: 0.0072\n",
      "Epoch 1161: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3376e-05 - rmse: 0.0048 - val_loss: 2.5659e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 1162/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3126e-05 - rmse: 0.0066\n",
      "Epoch 1162: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8917e-05 - rmse: 0.0070 - val_loss: 1.3364e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1163/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4948e-05 - rmse: 0.0081\n",
      "Epoch 1163: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3744e-05 - rmse: 0.0058 - val_loss: 2.2748e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1164/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4013e-05 - rmse: 0.0058\n",
      "Epoch 1164: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2754e-05 - rmse: 0.0048 - val_loss: 2.0675e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1165/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2201e-06 - rmse: 0.0025\n",
      "Epoch 1165: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3407e-05 - rmse: 0.0037 - val_loss: 2.1548e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1166/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0194e-05 - rmse: 0.0032\n",
      "Epoch 1166: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4947e-06 - rmse: 0.0031 - val_loss: 2.2613e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1167/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1654e-05 - rmse: 0.0034\n",
      "Epoch 1167: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7543e-06 - rmse: 0.0028 - val_loss: 1.9322e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1168/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6776e-06 - rmse: 0.0029\n",
      "Epoch 1168: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2673e-06 - rmse: 0.0030 - val_loss: 2.1736e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1169/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2670e-06 - rmse: 0.0029\n",
      "Epoch 1169: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1385e-05 - rmse: 0.0034 - val_loss: 1.8344e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1170/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4778e-05 - rmse: 0.0038\n",
      "Epoch 1170: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2164e-05 - rmse: 0.0035 - val_loss: 1.8779e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1171/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4892e-06 - rmse: 0.0025\n",
      "Epoch 1171: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4542e-05 - rmse: 0.0038 - val_loss: 2.2631e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1172/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2392e-06 - rmse: 0.0023\n",
      "Epoch 1172: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2201e-05 - rmse: 0.0035 - val_loss: 2.2149e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1173/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1431e-05 - rmse: 0.0034\n",
      "Epoch 1173: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1475e-05 - rmse: 0.0034 - val_loss: 1.9606e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1174/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5822e-06 - rmse: 0.0031\n",
      "Epoch 1174: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3042e-05 - rmse: 0.0036 - val_loss: 1.7497e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1175/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0736e-05 - rmse: 0.0046\n",
      "Epoch 1175: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2716e-05 - rmse: 0.0036 - val_loss: 1.6109e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1176/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7710e-05 - rmse: 0.0042\n",
      "Epoch 1176: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0283e-05 - rmse: 0.0032 - val_loss: 2.0493e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1177/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0690e-06 - rmse: 0.0025\n",
      "Epoch 1177: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.2378e-06 - rmse: 0.0029 - val_loss: 2.3276e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 1178/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7258e-06 - rmse: 0.0030\n",
      "Epoch 1178: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7012e-06 - rmse: 0.0028 - val_loss: 2.0486e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1179/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1332e-06 - rmse: 0.0020\n",
      "Epoch 1179: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5414e-06 - rmse: 0.0029 - val_loss: 2.0280e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1180/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5078e-06 - rmse: 0.0027\n",
      "Epoch 1180: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7346e-06 - rmse: 0.0028 - val_loss: 2.3251e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1181/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9342e-05 - rmse: 0.0044\n",
      "Epoch 1181: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2579e-05 - rmse: 0.0035 - val_loss: 2.6107e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1182/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2596e-05 - rmse: 0.0048\n",
      "Epoch 1182: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0512e-05 - rmse: 0.0045 - val_loss: 2.5114e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1183/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3545e-05 - rmse: 0.0037\n",
      "Epoch 1183: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3829e-05 - rmse: 0.0037 - val_loss: 1.8989e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1184/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4032e-06 - rmse: 0.0027\n",
      "Epoch 1184: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0785e-05 - rmse: 0.0033 - val_loss: 1.9206e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1185/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6875e-05 - rmse: 0.0041\n",
      "Epoch 1185: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4830e-05 - rmse: 0.0039 - val_loss: 2.0091e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1186/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3347e-05 - rmse: 0.0037\n",
      "Epoch 1186: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1347e-05 - rmse: 0.0034 - val_loss: 2.3115e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1187/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0715e-05 - rmse: 0.0033\n",
      "Epoch 1187: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2543e-05 - rmse: 0.0035 - val_loss: 2.1562e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1188/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3483e-05 - rmse: 0.0037\n",
      "Epoch 1188: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0524e-05 - rmse: 0.0032 - val_loss: 2.0580e-04 - val_rmse: 0.0143 - lr: 1.0000e-04\n",
      "Epoch 1189/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6218e-05 - rmse: 0.0040\n",
      "Epoch 1189: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6831e-06 - rmse: 0.0029 - val_loss: 1.8658e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1190/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9497e-06 - rmse: 0.0024\n",
      "Epoch 1190: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.9133e-06 - rmse: 0.0028 - val_loss: 1.6902e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1191/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2188e-06 - rmse: 0.0029\n",
      "Epoch 1191: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2504e-05 - rmse: 0.0035 - val_loss: 2.0640e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1192/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3381e-06 - rmse: 0.0015\n",
      "Epoch 1192: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1297e-06 - rmse: 0.0029 - val_loss: 1.9417e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1193/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9767e-06 - rmse: 0.0030\n",
      "Epoch 1193: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3745e-06 - rmse: 0.0027 - val_loss: 2.1149e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1194/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0365e-05 - rmse: 0.0032\n",
      "Epoch 1194: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5359e-06 - rmse: 0.0029 - val_loss: 2.1828e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1195/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5324e-06 - rmse: 0.0029\n",
      "Epoch 1195: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3408e-06 - rmse: 0.0029 - val_loss: 1.9260e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1196/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7430e-06 - rmse: 0.0022\n",
      "Epoch 1196: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8363e-06 - rmse: 0.0030 - val_loss: 1.7129e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1197/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9634e-06 - rmse: 0.0032\n",
      "Epoch 1197: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1334e-05 - rmse: 0.0034 - val_loss: 1.6226e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1198/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5269e-06 - rmse: 0.0029\n",
      "Epoch 1198: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1680e-06 - rmse: 0.0029 - val_loss: 1.9228e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1199/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9393e-06 - rmse: 0.0022\n",
      "Epoch 1199: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5479e-06 - rmse: 0.0027 - val_loss: 2.4974e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1200/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2642e-05 - rmse: 0.0036\n",
      "Epoch 1200: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9170e-05 - rmse: 0.0044 - val_loss: 2.3909e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1201/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2543e-05 - rmse: 0.0047\n",
      "Epoch 1201: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3172e-05 - rmse: 0.0036 - val_loss: 2.1798e-04 - val_rmse: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 1202/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7729e-06 - rmse: 0.0028\n",
      "Epoch 1202: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1908e-05 - rmse: 0.0035 - val_loss: 2.3014e-04 - val_rmse: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 1203/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0320e-06 - rmse: 0.0027\n",
      "Epoch 1203: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3100e-05 - rmse: 0.0048 - val_loss: 2.0910e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1204/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7416e-05 - rmse: 0.0052\n",
      "Epoch 1204: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4204e-05 - rmse: 0.0049 - val_loss: 2.6011e-04 - val_rmse: 0.0161 - lr: 1.0000e-04\n",
      "Epoch 1205/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9519e-05 - rmse: 0.0044\n",
      "Epoch 1205: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9532e-05 - rmse: 0.0070 - val_loss: 1.5809e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1206/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7608e-05 - rmse: 0.0082\n",
      "Epoch 1206: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.5149e-05 - rmse: 0.0074 - val_loss: 2.9104e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 1207/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3247e-05 - rmse: 0.0066\n",
      "Epoch 1207: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.8323e-05 - rmse: 0.0076 - val_loss: 2.6465e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1208/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8967e-05 - rmse: 0.0089\n",
      "Epoch 1208: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5779e-05 - rmse: 0.0075 - val_loss: 3.1517e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 1209/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3619e-05 - rmse: 0.0073\n",
      "Epoch 1209: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.6545e-05 - rmse: 0.0082 - val_loss: 1.7078e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1210/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4935e-05 - rmse: 0.0039\n",
      "Epoch 1210: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.9720e-05 - rmse: 0.0089 - val_loss: 2.4337e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 1211/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7880e-04 - rmse: 0.0167\n",
      "Epoch 1211: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5733e-04 - rmse: 0.0125 - val_loss: 2.4569e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1212/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9471e-04 - rmse: 0.0140\n",
      "Epoch 1212: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4977e-04 - rmse: 0.0122 - val_loss: 2.1481e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1213/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7607e-05 - rmse: 0.0069\n",
      "Epoch 1213: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5757e-04 - rmse: 0.0126 - val_loss: 3.3283e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 1214/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2758e-05 - rmse: 0.0079\n",
      "Epoch 1214: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0496e-04 - rmse: 0.0102 - val_loss: 5.2828e-04 - val_rmse: 0.0230 - lr: 1.0000e-04\n",
      "Epoch 1215/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5992e-04 - rmse: 0.0161\n",
      "Epoch 1215: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1686e-04 - rmse: 0.0108 - val_loss: 3.8506e-04 - val_rmse: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 1216/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9691e-05 - rmse: 0.0089\n",
      "Epoch 1216: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2565e-04 - rmse: 0.0112 - val_loss: 1.3517e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1217/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0761e-05 - rmse: 0.0046\n",
      "Epoch 1217: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7263e-04 - rmse: 0.0131 - val_loss: 3.3062e-04 - val_rmse: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 1218/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0525e-04 - rmse: 0.0201\n",
      "Epoch 1218: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2818e-04 - rmse: 0.0151 - val_loss: 3.5163e-04 - val_rmse: 0.0188 - lr: 1.0000e-04\n",
      "Epoch 1219/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9057e-04 - rmse: 0.0198\n",
      "Epoch 1219: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.7155e-04 - rmse: 0.0193 - val_loss: 2.2716e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1220/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7893e-05 - rmse: 0.0069\n",
      "Epoch 1220: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3275e-04 - rmse: 0.0208 - val_loss: 7.6872e-04 - val_rmse: 0.0277 - lr: 1.0000e-04\n",
      "Epoch 1221/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9311e-04 - rmse: 0.0315\n",
      "Epoch 1221: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.3850e-04 - rmse: 0.0232 - val_loss: 0.0012 - val_rmse: 0.0341 - lr: 1.0000e-04\n",
      "Epoch 1222/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3698e-04 - rmse: 0.0252\n",
      "Epoch 1222: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.2871e-04 - rmse: 0.0251 - val_loss: 2.6909e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1223/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5596e-04 - rmse: 0.0125\n",
      "Epoch 1223: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8824e-04 - rmse: 0.0137 - val_loss: 2.9263e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 1224/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8022e-04 - rmse: 0.0195\n",
      "Epoch 1224: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1194e-04 - rmse: 0.0146 - val_loss: 3.0330e-04 - val_rmse: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 1225/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2654e-04 - rmse: 0.0112\n",
      "Epoch 1225: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.9066e-05 - rmse: 0.0089 - val_loss: 2.7027e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1226/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4162e-05 - rmse: 0.0080\n",
      "Epoch 1226: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0414e-04 - rmse: 0.0102 - val_loss: 1.5839e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1227/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3880e-04 - rmse: 0.0118\n",
      "Epoch 1227: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6538e-04 - rmse: 0.0129 - val_loss: 1.8698e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1228/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0842e-04 - rmse: 0.0104\n",
      "Epoch 1228: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3619e-04 - rmse: 0.0117 - val_loss: 1.4289e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1229/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3203e-05 - rmse: 0.0066\n",
      "Epoch 1229: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2780e-05 - rmse: 0.0096 - val_loss: 4.1750e-04 - val_rmse: 0.0204 - lr: 1.0000e-04\n",
      "Epoch 1230/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0292e-04 - rmse: 0.0142\n",
      "Epoch 1230: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1202e-04 - rmse: 0.0106 - val_loss: 4.2723e-04 - val_rmse: 0.0207 - lr: 1.0000e-04\n",
      "Epoch 1231/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5381e-05 - rmse: 0.0087\n",
      "Epoch 1231: val_loss did not improve from 0.00011\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1487e-04 - rmse: 0.0107 - val_loss: 2.3566e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 1232/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1474e-04 - rmse: 0.0107\n",
      "Epoch 1232: val_loss improved from 0.00011 to 0.00009, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 9.8400e-05 - rmse: 0.0099 - val_loss: 9.2098e-05 - val_rmse: 0.0096 - lr: 1.0000e-04\n",
      "Epoch 1233/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1196e-04 - rmse: 0.0106\n",
      "Epoch 1233: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9531e-05 - rmse: 0.0070 - val_loss: 2.9196e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 1234/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7449e-05 - rmse: 0.0061\n",
      "Epoch 1234: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5520e-05 - rmse: 0.0067 - val_loss: 1.8709e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1235/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0320e-05 - rmse: 0.0055\n",
      "Epoch 1235: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.1571e-05 - rmse: 0.0056 - val_loss: 1.5392e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1236/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7614e-05 - rmse: 0.0053\n",
      "Epoch 1236: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2062e-05 - rmse: 0.0047 - val_loss: 1.1464e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1237/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1279e-05 - rmse: 0.0046\n",
      "Epoch 1237: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7313e-05 - rmse: 0.0042 - val_loss: 1.3021e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1238/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4359e-06 - rmse: 0.0016\n",
      "Epoch 1238: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2502e-05 - rmse: 0.0035 - val_loss: 1.9717e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1239/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0955e-05 - rmse: 0.0033\n",
      "Epoch 1239: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9304e-05 - rmse: 0.0044 - val_loss: 1.4654e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1240/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3802e-05 - rmse: 0.0037\n",
      "Epoch 1240: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5686e-05 - rmse: 0.0040 - val_loss: 1.3410e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1241/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9062e-05 - rmse: 0.0044\n",
      "Epoch 1241: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8035e-05 - rmse: 0.0042 - val_loss: 1.5351e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1242/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6484e-06 - rmse: 0.0028\n",
      "Epoch 1242: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0187e-05 - rmse: 0.0045 - val_loss: 2.2629e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1243/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2498e-05 - rmse: 0.0057\n",
      "Epoch 1243: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3237e-05 - rmse: 0.0048 - val_loss: 1.7708e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1244/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8875e-06 - rmse: 0.0024\n",
      "Epoch 1244: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3167e-05 - rmse: 0.0036 - val_loss: 1.4532e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1245/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8595e-06 - rmse: 0.0026\n",
      "Epoch 1245: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1228e-06 - rmse: 0.0030 - val_loss: 1.6939e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1246/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4459e-06 - rmse: 0.0025\n",
      "Epoch 1246: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0552e-05 - rmse: 0.0032 - val_loss: 1.4158e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1247/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4236e-05 - rmse: 0.0038\n",
      "Epoch 1247: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1831e-05 - rmse: 0.0034 - val_loss: 1.8988e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1248/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1934e-05 - rmse: 0.0035\n",
      "Epoch 1248: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3974e-05 - rmse: 0.0037 - val_loss: 1.9117e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1249/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0465e-05 - rmse: 0.0045\n",
      "Epoch 1249: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1427e-05 - rmse: 0.0056 - val_loss: 1.5944e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1250/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4229e-05 - rmse: 0.0038\n",
      "Epoch 1250: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4600e-05 - rmse: 0.0038 - val_loss: 1.4246e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1251/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4562e-05 - rmse: 0.0050\n",
      "Epoch 1251: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3686e-05 - rmse: 0.0037 - val_loss: 1.9139e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1252/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1297e-05 - rmse: 0.0034\n",
      "Epoch 1252: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0239e-05 - rmse: 0.0045 - val_loss: 1.8622e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1253/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6443e-05 - rmse: 0.0041\n",
      "Epoch 1253: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0418e-05 - rmse: 0.0055 - val_loss: 1.2116e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1254/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8664e-05 - rmse: 0.0043\n",
      "Epoch 1254: val_loss did not improve from 0.00009\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.7030e-05 - rmse: 0.0061 - val_loss: 1.8563e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1255/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2520e-05 - rmse: 0.0091\n",
      "Epoch 1255: val_loss improved from 0.00009 to 0.00008, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.7630e-05 - rmse: 0.0088 - val_loss: 7.5573e-05 - val_rmse: 0.0087 - lr: 1.0000e-04\n",
      "Epoch 1256/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2773e-05 - rmse: 0.0091\n",
      "Epoch 1256: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7921e-05 - rmse: 0.0088 - val_loss: 2.4849e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1257/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6907e-05 - rmse: 0.0052\n",
      "Epoch 1257: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.3851e-05 - rmse: 0.0066 - val_loss: 1.5160e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1258/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2410e-05 - rmse: 0.0047\n",
      "Epoch 1258: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.7332e-05 - rmse: 0.0088 - val_loss: 1.7201e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1259/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4221e-05 - rmse: 0.0074\n",
      "Epoch 1259: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.7424e-05 - rmse: 0.0088 - val_loss: 1.3122e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1260/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0867e-04 - rmse: 0.0104\n",
      "Epoch 1260: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4850e-05 - rmse: 0.0097 - val_loss: 1.9468e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1261/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9030e-05 - rmse: 0.0044\n",
      "Epoch 1261: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.1396e-05 - rmse: 0.0064 - val_loss: 2.6915e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1262/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2703e-05 - rmse: 0.0073\n",
      "Epoch 1262: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7397e-05 - rmse: 0.0069 - val_loss: 1.9218e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1263/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0343e-05 - rmse: 0.0032\n",
      "Epoch 1263: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9053e-05 - rmse: 0.0044 - val_loss: 1.7209e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1264/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4251e-05 - rmse: 0.0038\n",
      "Epoch 1264: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8206e-05 - rmse: 0.0043 - val_loss: 1.5254e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1265/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2514e-06 - rmse: 0.0025\n",
      "Epoch 1265: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1562e-05 - rmse: 0.0034 - val_loss: 1.8507e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1266/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0651e-05 - rmse: 0.0033\n",
      "Epoch 1266: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1136e-05 - rmse: 0.0033 - val_loss: 1.7711e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1267/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0369e-05 - rmse: 0.0045\n",
      "Epoch 1267: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6723e-05 - rmse: 0.0041 - val_loss: 2.2513e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1268/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7781e-05 - rmse: 0.0053\n",
      "Epoch 1268: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2820e-05 - rmse: 0.0036 - val_loss: 1.8638e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1269/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1920e-05 - rmse: 0.0035\n",
      "Epoch 1269: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.2087e-06 - rmse: 0.0027 - val_loss: 1.5930e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1270/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6142e-05 - rmse: 0.0040\n",
      "Epoch 1270: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4185e-06 - rmse: 0.0029 - val_loss: 1.7657e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1271/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8016e-06 - rmse: 0.0019\n",
      "Epoch 1271: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.9801e-06 - rmse: 0.0024 - val_loss: 1.6259e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1272/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6927e-06 - rmse: 0.0016\n",
      "Epoch 1272: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3302e-06 - rmse: 0.0027 - val_loss: 1.5812e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1273/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2668e-06 - rmse: 0.0027\n",
      "Epoch 1273: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5857e-06 - rmse: 0.0029 - val_loss: 1.3120e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1274/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9070e-05 - rmse: 0.0044\n",
      "Epoch 1274: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2034e-05 - rmse: 0.0035 - val_loss: 1.4386e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1275/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7196e-06 - rmse: 0.0030\n",
      "Epoch 1275: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.6519e-06 - rmse: 0.0026 - val_loss: 1.8807e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1276/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3381e-06 - rmse: 0.0023\n",
      "Epoch 1276: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6263e-06 - rmse: 0.0024 - val_loss: 1.7106e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1277/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9681e-06 - rmse: 0.0026\n",
      "Epoch 1277: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.3770e-06 - rmse: 0.0023 - val_loss: 1.6312e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1278/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6759e-06 - rmse: 0.0019\n",
      "Epoch 1278: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.6106e-06 - rmse: 0.0021 - val_loss: 1.6995e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1279/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9687e-06 - rmse: 0.0022\n",
      "Epoch 1279: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.4391e-06 - rmse: 0.0023 - val_loss: 1.7169e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1280/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9417e-06 - rmse: 0.0024\n",
      "Epoch 1280: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.9085e-06 - rmse: 0.0022 - val_loss: 1.6982e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1281/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2646e-06 - rmse: 0.0021\n",
      "Epoch 1281: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2650e-06 - rmse: 0.0023 - val_loss: 1.5727e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1282/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0810e-06 - rmse: 0.0025\n",
      "Epoch 1282: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.7024e-06 - rmse: 0.0028 - val_loss: 1.4841e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1283/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0717e-05 - rmse: 0.0033\n",
      "Epoch 1283: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.0178e-06 - rmse: 0.0028 - val_loss: 1.4697e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1284/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5483e-06 - rmse: 0.0027\n",
      "Epoch 1284: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0736e-05 - rmse: 0.0033 - val_loss: 1.5883e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1285/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4009e-06 - rmse: 0.0023\n",
      "Epoch 1285: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.9579e-06 - rmse: 0.0026 - val_loss: 1.9463e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1286/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0403e-05 - rmse: 0.0032\n",
      "Epoch 1286: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6132e-06 - rmse: 0.0029 - val_loss: 1.7635e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1287/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5707e-06 - rmse: 0.0021\n",
      "Epoch 1287: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5859e-05 - rmse: 0.0040 - val_loss: 1.4624e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1288/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9525e-06 - rmse: 0.0026\n",
      "Epoch 1288: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7857e-05 - rmse: 0.0042 - val_loss: 1.2684e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1289/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2765e-05 - rmse: 0.0073\n",
      "Epoch 1289: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6320e-05 - rmse: 0.0051 - val_loss: 1.3329e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1290/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1763e-05 - rmse: 0.0034\n",
      "Epoch 1290: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2282e-05 - rmse: 0.0057 - val_loss: 1.5443e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1291/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4822e-06 - rmse: 0.0021\n",
      "Epoch 1291: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4604e-05 - rmse: 0.0059 - val_loss: 1.8427e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1292/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3814e-05 - rmse: 0.0049\n",
      "Epoch 1292: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9218e-05 - rmse: 0.0054 - val_loss: 2.1500e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1293/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3319e-05 - rmse: 0.0058\n",
      "Epoch 1293: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7538e-05 - rmse: 0.0061 - val_loss: 2.1720e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1294/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5927e-05 - rmse: 0.0051\n",
      "Epoch 1294: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.3844e-05 - rmse: 0.0092 - val_loss: 1.6275e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1295/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2120e-05 - rmse: 0.0065\n",
      "Epoch 1295: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6465e-05 - rmse: 0.0060 - val_loss: 9.7777e-05 - val_rmse: 0.0099 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1296/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7097e-04 - rmse: 0.0131\n",
      "Epoch 1296: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.2447e-05 - rmse: 0.0085 - val_loss: 1.7221e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1297/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2079e-05 - rmse: 0.0057\n",
      "Epoch 1297: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4677e-05 - rmse: 0.0050 - val_loss: 1.5089e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1298/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2293e-05 - rmse: 0.0035\n",
      "Epoch 1298: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3065e-05 - rmse: 0.0036 - val_loss: 1.6097e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1299/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1056e-05 - rmse: 0.0033\n",
      "Epoch 1299: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0933e-05 - rmse: 0.0033 - val_loss: 1.4606e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1300/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5608e-06 - rmse: 0.0019\n",
      "Epoch 1300: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.7533e-06 - rmse: 0.0028 - val_loss: 1.5835e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1301/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0495e-05 - rmse: 0.0032\n",
      "Epoch 1301: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0932e-05 - rmse: 0.0033 - val_loss: 1.4059e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1302/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4970e-05 - rmse: 0.0059\n",
      "Epoch 1302: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9811e-05 - rmse: 0.0045 - val_loss: 1.8490e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1303/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2614e-06 - rmse: 0.0023\n",
      "Epoch 1303: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6337e-05 - rmse: 0.0040 - val_loss: 1.5104e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1304/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1675e-05 - rmse: 0.0056\n",
      "Epoch 1304: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0223e-05 - rmse: 0.0045 - val_loss: 2.4760e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1305/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0096e-05 - rmse: 0.0055\n",
      "Epoch 1305: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4278e-05 - rmse: 0.0049 - val_loss: 1.7471e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1306/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0540e-05 - rmse: 0.0045\n",
      "Epoch 1306: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0092e-05 - rmse: 0.0045 - val_loss: 1.9588e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1307/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9492e-05 - rmse: 0.0070\n",
      "Epoch 1307: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.8208e-05 - rmse: 0.0062 - val_loss: 1.1066e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1308/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7437e-05 - rmse: 0.0042\n",
      "Epoch 1308: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3720e-06 - rmse: 0.0031 - val_loss: 1.4778e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1309/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9104e-06 - rmse: 0.0030\n",
      "Epoch 1309: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3265e-05 - rmse: 0.0048 - val_loss: 1.5366e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1310/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4075e-05 - rmse: 0.0038\n",
      "Epoch 1310: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2515e-05 - rmse: 0.0047 - val_loss: 2.1663e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1311/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6872e-05 - rmse: 0.0061\n",
      "Epoch 1311: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.7469e-05 - rmse: 0.0061 - val_loss: 1.8471e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1312/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6568e-05 - rmse: 0.0060\n",
      "Epoch 1312: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5484e-05 - rmse: 0.0039 - val_loss: 1.5954e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1313/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9324e-06 - rmse: 0.0028\n",
      "Epoch 1313: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6046e-06 - rmse: 0.0029 - val_loss: 1.0121e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1314/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6794e-05 - rmse: 0.0061\n",
      "Epoch 1314: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8295e-05 - rmse: 0.0043 - val_loss: 1.5440e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1315/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4461e-05 - rmse: 0.0038\n",
      "Epoch 1315: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0276e-05 - rmse: 0.0032 - val_loss: 1.6326e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1316/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2306e-05 - rmse: 0.0035\n",
      "Epoch 1316: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.9370e-06 - rmse: 0.0028 - val_loss: 1.5318e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1317/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1260e-06 - rmse: 0.0025\n",
      "Epoch 1317: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.4708e-06 - rmse: 0.0027 - val_loss: 1.9501e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1318/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1310e-05 - rmse: 0.0034\n",
      "Epoch 1318: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3106e-05 - rmse: 0.0036 - val_loss: 1.8021e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1319/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5605e-06 - rmse: 0.0031\n",
      "Epoch 1319: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5182e-05 - rmse: 0.0039 - val_loss: 1.4802e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1320/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2734e-06 - rmse: 0.0025\n",
      "Epoch 1320: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3576e-05 - rmse: 0.0037 - val_loss: 1.6477e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1321/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4009e-06 - rmse: 0.0027\n",
      "Epoch 1321: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.0848e-06 - rmse: 0.0028 - val_loss: 1.5424e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1322/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9568e-06 - rmse: 0.0022\n",
      "Epoch 1322: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4955e-05 - rmse: 0.0039 - val_loss: 1.7031e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1323/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8605e-06 - rmse: 0.0028\n",
      "Epoch 1323: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3605e-05 - rmse: 0.0037 - val_loss: 1.8066e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1324/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1186e-06 - rmse: 0.0027\n",
      "Epoch 1324: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.6086e-06 - rmse: 0.0026 - val_loss: 1.6925e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1325/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9208e-06 - rmse: 0.0022\n",
      "Epoch 1325: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.5968e-06 - rmse: 0.0028 - val_loss: 1.7446e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1326/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7371e-06 - rmse: 0.0024\n",
      "Epoch 1326: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6963e-06 - rmse: 0.0029 - val_loss: 1.2922e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1327/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8784e-06 - rmse: 0.0028\n",
      "Epoch 1327: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0338e-05 - rmse: 0.0032 - val_loss: 1.5150e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1328/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0166e-05 - rmse: 0.0045\n",
      "Epoch 1328: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5979e-05 - rmse: 0.0051 - val_loss: 1.8002e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1329/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2576e-06 - rmse: 0.0023\n",
      "Epoch 1329: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7175e-05 - rmse: 0.0052 - val_loss: 2.5187e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1330/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1371e-05 - rmse: 0.0064\n",
      "Epoch 1330: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.8193e-05 - rmse: 0.0062 - val_loss: 2.5144e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1331/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6551e-05 - rmse: 0.0098\n",
      "Epoch 1331: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.1092e-05 - rmse: 0.0064 - val_loss: 1.1641e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1332/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6685e-05 - rmse: 0.0041\n",
      "Epoch 1332: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0500e-05 - rmse: 0.0045 - val_loss: 1.0926e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1333/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5833e-05 - rmse: 0.0040\n",
      "Epoch 1333: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5454e-05 - rmse: 0.0039 - val_loss: 1.2042e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1334/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8556e-05 - rmse: 0.0043\n",
      "Epoch 1334: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6436e-05 - rmse: 0.0041 - val_loss: 1.1978e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1335/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3604e-05 - rmse: 0.0037\n",
      "Epoch 1335: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3290e-05 - rmse: 0.0036 - val_loss: 1.5179e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1336/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3679e-05 - rmse: 0.0037\n",
      "Epoch 1336: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1689e-05 - rmse: 0.0034 - val_loss: 1.4498e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1337/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6510e-06 - rmse: 0.0028\n",
      "Epoch 1337: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2128e-05 - rmse: 0.0035 - val_loss: 1.5575e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1338/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6183e-06 - rmse: 0.0021\n",
      "Epoch 1338: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8147e-06 - rmse: 0.0031 - val_loss: 1.7702e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1339/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7624e-06 - rmse: 0.0028\n",
      "Epoch 1339: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4693e-06 - rmse: 0.0029 - val_loss: 1.4805e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1340/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0576e-06 - rmse: 0.0028\n",
      "Epoch 1340: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.8926e-06 - rmse: 0.0024 - val_loss: 1.5474e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1341/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5234e-06 - rmse: 0.0019\n",
      "Epoch 1341: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.0880e-06 - rmse: 0.0023 - val_loss: 1.4400e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1342/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6410e-06 - rmse: 0.0026\n",
      "Epoch 1342: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6825e-06 - rmse: 0.0024 - val_loss: 1.4058e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1343/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0034e-06 - rmse: 0.0017\n",
      "Epoch 1343: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.8274e-06 - rmse: 0.0020 - val_loss: 1.5026e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1344/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8597e-06 - rmse: 0.0017\n",
      "Epoch 1344: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.0013e-06 - rmse: 0.0020 - val_loss: 1.5497e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1345/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8518e-06 - rmse: 0.0017\n",
      "Epoch 1345: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2316e-06 - rmse: 0.0021 - val_loss: 1.4046e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1346/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6483e-06 - rmse: 0.0019\n",
      "Epoch 1346: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.1816e-06 - rmse: 0.0025 - val_loss: 1.4755e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1347/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9047e-06 - rmse: 0.0017\n",
      "Epoch 1347: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9604e-06 - rmse: 0.0022 - val_loss: 1.6683e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1348/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9641e-06 - rmse: 0.0022\n",
      "Epoch 1348: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.1928e-06 - rmse: 0.0025 - val_loss: 1.4398e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1349/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9211e-06 - rmse: 0.0022\n",
      "Epoch 1349: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.8543e-06 - rmse: 0.0028 - val_loss: 1.6218e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1350/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7072e-06 - rmse: 0.0022\n",
      "Epoch 1350: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.5082e-06 - rmse: 0.0023 - val_loss: 1.4617e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1351/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8614e-06 - rmse: 0.0020\n",
      "Epoch 1351: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.4687e-06 - rmse: 0.0023 - val_loss: 1.5536e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1352/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1224e-06 - rmse: 0.0020\n",
      "Epoch 1352: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.0545e-06 - rmse: 0.0027 - val_loss: 1.6002e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1353/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6121e-06 - rmse: 0.0024\n",
      "Epoch 1353: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.1656e-06 - rmse: 0.0025 - val_loss: 1.6473e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1354/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0160e-06 - rmse: 0.0022\n",
      "Epoch 1354: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3124e-06 - rmse: 0.0021 - val_loss: 1.5686e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1355/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7184e-06 - rmse: 0.0013\n",
      "Epoch 1355: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9129e-06 - rmse: 0.0022 - val_loss: 1.3289e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1356/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7914e-06 - rmse: 0.0022\n",
      "Epoch 1356: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.3042e-06 - rmse: 0.0027 - val_loss: 1.3373e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1357/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5919e-05 - rmse: 0.0040\n",
      "Epoch 1357: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5233e-05 - rmse: 0.0039 - val_loss: 1.4106e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1358/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9753e-06 - rmse: 0.0022\n",
      "Epoch 1358: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2966e-05 - rmse: 0.0036 - val_loss: 1.2911e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1359/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0697e-06 - rmse: 0.0028\n",
      "Epoch 1359: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3137e-05 - rmse: 0.0048 - val_loss: 1.4345e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1360/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3088e-05 - rmse: 0.0036\n",
      "Epoch 1360: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2989e-05 - rmse: 0.0036 - val_loss: 1.2016e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1361/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5997e-06 - rmse: 0.0024\n",
      "Epoch 1361: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0518e-05 - rmse: 0.0032 - val_loss: 1.6482e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1362/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2892e-05 - rmse: 0.0048\n",
      "Epoch 1362: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2688e-05 - rmse: 0.0036 - val_loss: 1.3656e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1363/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0267e-05 - rmse: 0.0032\n",
      "Epoch 1363: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6533e-06 - rmse: 0.0031 - val_loss: 1.9063e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1364/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2548e-06 - rmse: 0.0029\n",
      "Epoch 1364: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1793e-05 - rmse: 0.0034 - val_loss: 1.7588e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1365/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9901e-06 - rmse: 0.0026\n",
      "Epoch 1365: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3877e-05 - rmse: 0.0037 - val_loss: 2.0213e-04 - val_rmse: 0.0142 - lr: 1.0000e-04\n",
      "Epoch 1366/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2672e-05 - rmse: 0.0036\n",
      "Epoch 1366: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4692e-05 - rmse: 0.0050 - val_loss: 1.1880e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1367/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1135e-05 - rmse: 0.0046\n",
      "Epoch 1367: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.8736e-05 - rmse: 0.0062 - val_loss: 1.7750e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1368/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5910e-05 - rmse: 0.0081\n",
      "Epoch 1368: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3126e-05 - rmse: 0.0086 - val_loss: 1.2930e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1369/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1369e-04 - rmse: 0.0107\n",
      "Epoch 1369: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0920e-04 - rmse: 0.0105 - val_loss: 3.0709e-04 - val_rmse: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 1370/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8756e-04 - rmse: 0.0137\n",
      "Epoch 1370: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5726e-04 - rmse: 0.0125 - val_loss: 1.3795e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1371/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8408e-05 - rmse: 0.0094\n",
      "Epoch 1371: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.5356e-05 - rmse: 0.0081 - val_loss: 2.8839e-04 - val_rmse: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 1372/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3007e-04 - rmse: 0.0114\n",
      "Epoch 1372: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.8830e-05 - rmse: 0.0083 - val_loss: 1.2900e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1373/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4511e-05 - rmse: 0.0074\n",
      "Epoch 1373: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9519e-05 - rmse: 0.0070 - val_loss: 1.3431e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1374/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5952e-05 - rmse: 0.0068\n",
      "Epoch 1374: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2606e-05 - rmse: 0.0057 - val_loss: 1.2891e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1375/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3220e-05 - rmse: 0.0058\n",
      "Epoch 1375: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9799e-05 - rmse: 0.0071 - val_loss: 1.1336e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1376/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0205e-05 - rmse: 0.0055\n",
      "Epoch 1376: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3039e-05 - rmse: 0.0057 - val_loss: 2.2949e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1377/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4798e-05 - rmse: 0.0059\n",
      "Epoch 1377: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4396e-05 - rmse: 0.0049 - val_loss: 2.9610e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 1378/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0889e-05 - rmse: 0.0095\n",
      "Epoch 1378: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6081e-05 - rmse: 0.0060 - val_loss: 2.6368e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1379/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0398e-05 - rmse: 0.0071\n",
      "Epoch 1379: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.8559e-05 - rmse: 0.0053 - val_loss: 1.8463e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1380/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1625e-06 - rmse: 0.0027\n",
      "Epoch 1380: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7911e-05 - rmse: 0.0053 - val_loss: 1.8061e-04 - val_rmse: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 1381/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4721e-06 - rmse: 0.0029\n",
      "Epoch 1381: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7604e-05 - rmse: 0.0042 - val_loss: 1.5147e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1382/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4485e-06 - rmse: 0.0025\n",
      "Epoch 1382: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3144e-05 - rmse: 0.0036 - val_loss: 1.6475e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1383/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2675e-06 - rmse: 0.0027\n",
      "Epoch 1383: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.4148e-06 - rmse: 0.0027 - val_loss: 1.4251e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1384/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3873e-06 - rmse: 0.0021\n",
      "Epoch 1384: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8034e-06 - rmse: 0.0030 - val_loss: 1.8138e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1385/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8311e-06 - rmse: 0.0030\n",
      "Epoch 1385: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1429e-05 - rmse: 0.0034 - val_loss: 1.6614e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1386/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3579e-06 - rmse: 0.0025\n",
      "Epoch 1386: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2918e-05 - rmse: 0.0048 - val_loss: 2.0973e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1387/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1173e-05 - rmse: 0.0064\n",
      "Epoch 1387: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5289e-05 - rmse: 0.0067 - val_loss: 2.0697e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1388/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9809e-05 - rmse: 0.0045\n",
      "Epoch 1388: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.3237e-05 - rmse: 0.0091 - val_loss: 2.5024e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1389/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9915e-05 - rmse: 0.0084\n",
      "Epoch 1389: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7470e-05 - rmse: 0.0094 - val_loss: 2.2566e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1390/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1831e-05 - rmse: 0.0056\n",
      "Epoch 1390: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6096e-04 - rmse: 0.0162 - val_loss: 4.6881e-04 - val_rmse: 0.0217 - lr: 1.0000e-04\n",
      "Epoch 1391/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4187e-04 - rmse: 0.0156\n",
      "Epoch 1391: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5228e-04 - rmse: 0.0188 - val_loss: 3.7318e-04 - val_rmse: 0.0193 - lr: 1.0000e-04\n",
      "Epoch 1392/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3749e-04 - rmse: 0.0154\n",
      "Epoch 1392: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4810e-04 - rmse: 0.0187 - val_loss: 2.7848e-04 - val_rmse: 0.0167 - lr: 1.0000e-04\n",
      "Epoch 1393/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4962e-04 - rmse: 0.0122\n",
      "Epoch 1393: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4046e-04 - rmse: 0.0185 - val_loss: 3.8078e-04 - val_rmse: 0.0195 - lr: 1.0000e-04\n",
      "Epoch 1394/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4412e-04 - rmse: 0.0156\n",
      "Epoch 1394: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1977e-04 - rmse: 0.0205 - val_loss: 1.9426e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1395/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0363e-04 - rmse: 0.0143\n",
      "Epoch 1395: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1368e-04 - rmse: 0.0177 - val_loss: 5.4661e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 1396/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2873e-04 - rmse: 0.0305\n",
      "Epoch 1396: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3894e-04 - rmse: 0.0210 - val_loss: 5.1851e-04 - val_rmse: 0.0228 - lr: 1.0000e-04\n",
      "Epoch 1397/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7775e-04 - rmse: 0.0194\n",
      "Epoch 1397: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2118e-04 - rmse: 0.0228 - val_loss: 2.7582e-04 - val_rmse: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 1398/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0328e-04 - rmse: 0.0102\n",
      "Epoch 1398: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9383e-04 - rmse: 0.0198 - val_loss: 7.1903e-04 - val_rmse: 0.0268 - lr: 1.0000e-04\n",
      "Epoch 1399/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3282e-04 - rmse: 0.0182\n",
      "Epoch 1399: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5812e-04 - rmse: 0.0161 - val_loss: 5.4945e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 1400/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9700e-04 - rmse: 0.0140\n",
      "Epoch 1400: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5717e-04 - rmse: 0.0160 - val_loss: 1.7578e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1401/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1672e-04 - rmse: 0.0178\n",
      "Epoch 1401: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3839e-04 - rmse: 0.0118 - val_loss: 3.3409e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 1402/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2045e-04 - rmse: 0.0110\n",
      "Epoch 1402: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1077e-04 - rmse: 0.0105 - val_loss: 1.3252e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1403/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4792e-05 - rmse: 0.0074\n",
      "Epoch 1403: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3942e-04 - rmse: 0.0118 - val_loss: 2.2408e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1404/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2163e-04 - rmse: 0.0110\n",
      "Epoch 1404: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2505e-04 - rmse: 0.0112 - val_loss: 4.6479e-04 - val_rmse: 0.0216 - lr: 1.0000e-04\n",
      "Epoch 1405/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9477e-04 - rmse: 0.0172\n",
      "Epoch 1405: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0552e-04 - rmse: 0.0143 - val_loss: 1.5009e-04 - val_rmse: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 1406/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0980e-04 - rmse: 0.0105\n",
      "Epoch 1406: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7311e-04 - rmse: 0.0132 - val_loss: 1.9282e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1407/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4174e-04 - rmse: 0.0119\n",
      "Epoch 1407: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5573e-04 - rmse: 0.0125 - val_loss: 1.5715e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1408/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7373e-05 - rmse: 0.0052\n",
      "Epoch 1408: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9784e-04 - rmse: 0.0141 - val_loss: 6.8800e-04 - val_rmse: 0.0262 - lr: 1.0000e-04\n",
      "Epoch 1409/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3177e-04 - rmse: 0.0208\n",
      "Epoch 1409: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3231e-04 - rmse: 0.0182 - val_loss: 3.2651e-04 - val_rmse: 0.0181 - lr: 1.0000e-04\n",
      "Epoch 1410/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0135e-04 - rmse: 0.0101\n",
      "Epoch 1410: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9280e-04 - rmse: 0.0222 - val_loss: 2.2361e-04 - val_rmse: 0.0150 - lr: 1.0000e-04\n",
      "Epoch 1411/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1819e-05 - rmse: 0.0072\n",
      "Epoch 1411: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9797e-04 - rmse: 0.0141 - val_loss: 9.9240e-05 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1412/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5372e-04 - rmse: 0.0159\n",
      "Epoch 1412: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6835e-04 - rmse: 0.0130 - val_loss: 2.6202e-04 - val_rmse: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 1413/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3316e-04 - rmse: 0.0115\n",
      "Epoch 1413: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2714e-05 - rmse: 0.0096 - val_loss: 8.9871e-05 - val_rmse: 0.0095 - lr: 1.0000e-04\n",
      "Epoch 1414/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8255e-05 - rmse: 0.0053\n",
      "Epoch 1414: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0358e-04 - rmse: 0.0102 - val_loss: 2.3323e-04 - val_rmse: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 1415/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5440e-05 - rmse: 0.0092\n",
      "Epoch 1415: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.3477e-05 - rmse: 0.0091 - val_loss: 1.8540e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 1416/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4744e-05 - rmse: 0.0080\n",
      "Epoch 1416: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.4069e-05 - rmse: 0.0080 - val_loss: 1.5552e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1417/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7318e-05 - rmse: 0.0052\n",
      "Epoch 1417: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7309e-05 - rmse: 0.0061 - val_loss: 1.6461e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1418/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3256e-05 - rmse: 0.0036\n",
      "Epoch 1418: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9757e-05 - rmse: 0.0063 - val_loss: 1.4746e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1419/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7765e-06 - rmse: 0.0026\n",
      "Epoch 1419: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9125e-05 - rmse: 0.0054 - val_loss: 8.4943e-05 - val_rmse: 0.0092 - lr: 1.0000e-04\n",
      "Epoch 1420/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0112e-05 - rmse: 0.0063\n",
      "Epoch 1420: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8698e-05 - rmse: 0.0054 - val_loss: 1.3401e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1421/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1604e-05 - rmse: 0.0056\n",
      "Epoch 1421: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8219e-05 - rmse: 0.0043 - val_loss: 1.0852e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1422/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1919e-05 - rmse: 0.0035\n",
      "Epoch 1422: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8945e-05 - rmse: 0.0044 - val_loss: 1.6643e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1423/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2802e-05 - rmse: 0.0048\n",
      "Epoch 1423: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2550e-05 - rmse: 0.0057 - val_loss: 1.2509e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1424/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4480e-06 - rmse: 0.0031\n",
      "Epoch 1424: val_loss did not improve from 0.00008\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.0024e-05 - rmse: 0.0071 - val_loss: 1.9234e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1425/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3930e-05 - rmse: 0.0073\n",
      "Epoch 1425: val_loss improved from 0.00008 to 0.00007, saving model to D:\\TrainedModels\\2022113020221130steadyValidation_MLP_val_0.2_test0.1_128units_OptimalSettings_checkpoint.h5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 8.1073e-05 - rmse: 0.0090 - val_loss: 6.9785e-05 - val_rmse: 0.0084 - lr: 1.0000e-04\n",
      "Epoch 1426/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2782e-05 - rmse: 0.0048\n",
      "Epoch 1426: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7718e-04 - rmse: 0.0133 - val_loss: 2.9622e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 1427/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9792e-05 - rmse: 0.0100\n",
      "Epoch 1427: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5227e-04 - rmse: 0.0159 - val_loss: 9.0782e-04 - val_rmse: 0.0301 - lr: 1.0000e-04\n",
      "Epoch 1428/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2736e-04 - rmse: 0.0288\n",
      "Epoch 1428: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9239e-04 - rmse: 0.0198 - val_loss: 3.2179e-04 - val_rmse: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 1429/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8928e-04 - rmse: 0.0138\n",
      "Epoch 1429: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6003e-04 - rmse: 0.0161 - val_loss: 1.0118e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1430/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1300e-05 - rmse: 0.0096\n",
      "Epoch 1430: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.8766e-05 - rmse: 0.0089 - val_loss: 1.3531e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1431/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5948e-05 - rmse: 0.0093\n",
      "Epoch 1431: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9133e-05 - rmse: 0.0070 - val_loss: 1.5795e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1432/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3734e-05 - rmse: 0.0049\n",
      "Epoch 1432: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8224e-05 - rmse: 0.0062 - val_loss: 1.8694e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1433/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0806e-05 - rmse: 0.0046\n",
      "Epoch 1433: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9849e-05 - rmse: 0.0063 - val_loss: 1.0006e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1434/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3373e-05 - rmse: 0.0048\n",
      "Epoch 1434: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8495e-05 - rmse: 0.0053 - val_loss: 1.9525e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1435/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6208e-05 - rmse: 0.0040\n",
      "Epoch 1435: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8998e-05 - rmse: 0.0062 - val_loss: 1.0708e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1436/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5004e-05 - rmse: 0.0081\n",
      "Epoch 1436: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.5479e-05 - rmse: 0.0067 - val_loss: 2.2171e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1437/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7437e-05 - rmse: 0.0076\n",
      "Epoch 1437: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.2422e-05 - rmse: 0.0079 - val_loss: 1.4010e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1438/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6589e-05 - rmse: 0.0041\n",
      "Epoch 1438: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9205e-05 - rmse: 0.0070 - val_loss: 1.7586e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1439/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2543e-05 - rmse: 0.0057\n",
      "Epoch 1439: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.6697e-05 - rmse: 0.0088 - val_loss: 1.4818e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1440/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1329e-05 - rmse: 0.0046\n",
      "Epoch 1440: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3626e-04 - rmse: 0.0117 - val_loss: 3.0911e-04 - val_rmse: 0.0176 - lr: 1.0000e-04\n",
      "Epoch 1441/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3333e-04 - rmse: 0.0115\n",
      "Epoch 1441: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2773e-04 - rmse: 0.0151 - val_loss: 2.0728e-04 - val_rmse: 0.0144 - lr: 1.0000e-04\n",
      "Epoch 1442/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4695e-05 - rmse: 0.0050\n",
      "Epoch 1442: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3352e-04 - rmse: 0.0116 - val_loss: 2.2287e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1443/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7220e-05 - rmse: 0.0069\n",
      "Epoch 1443: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1884e-04 - rmse: 0.0109 - val_loss: 2.7044e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1444/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6253e-05 - rmse: 0.0087\n",
      "Epoch 1444: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1452e-05 - rmse: 0.0090 - val_loss: 1.5306e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1445/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4105e-05 - rmse: 0.0092\n",
      "Epoch 1445: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0739e-05 - rmse: 0.0055 - val_loss: 2.6641e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1446/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2599e-05 - rmse: 0.0065\n",
      "Epoch 1446: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.7415e-05 - rmse: 0.0099 - val_loss: 1.2922e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1447/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2337e-05 - rmse: 0.0096\n",
      "Epoch 1447: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1924e-04 - rmse: 0.0109 - val_loss: 3.1747e-04 - val_rmse: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 1448/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6162e-05 - rmse: 0.0098\n",
      "Epoch 1448: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2883e-04 - rmse: 0.0114 - val_loss: 1.8203e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1449/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2437e-05 - rmse: 0.0072\n",
      "Epoch 1449: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5676e-04 - rmse: 0.0125 - val_loss: 2.4683e-04 - val_rmse: 0.0157 - lr: 1.0000e-04\n",
      "Epoch 1450/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6635e-04 - rmse: 0.0216\n",
      "Epoch 1450: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0689e-04 - rmse: 0.0202 - val_loss: 2.9378e-04 - val_rmse: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 1451/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6065e-04 - rmse: 0.0127\n",
      "Epoch 1451: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9830e-04 - rmse: 0.0173 - val_loss: 7.4426e-04 - val_rmse: 0.0273 - lr: 1.0000e-04\n",
      "Epoch 1452/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3433e-04 - rmse: 0.0208\n",
      "Epoch 1452: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7140e-04 - rmse: 0.0165 - val_loss: 1.8932e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1453/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1869e-05 - rmse: 0.0056\n",
      "Epoch 1453: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8395e-04 - rmse: 0.0136 - val_loss: 2.4115e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1454/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9063e-04 - rmse: 0.0198\n",
      "Epoch 1454: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1005e-04 - rmse: 0.0145 - val_loss: 2.8194e-04 - val_rmse: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 1455/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8866e-05 - rmse: 0.0043\n",
      "Epoch 1455: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5175e-04 - rmse: 0.0123 - val_loss: 8.2879e-04 - val_rmse: 0.0288 - lr: 1.0000e-04\n",
      "Epoch 1456/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1670e-04 - rmse: 0.0204\n",
      "Epoch 1456: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.1459e-04 - rmse: 0.0204 - val_loss: 4.8294e-04 - val_rmse: 0.0220 - lr: 1.0000e-04\n",
      "Epoch 1457/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1508e-04 - rmse: 0.0147\n",
      "Epoch 1457: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.2295e-04 - rmse: 0.0229 - val_loss: 1.9556e-04 - val_rmse: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 1458/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8968e-04 - rmse: 0.0138\n",
      "Epoch 1458: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6948e-04 - rmse: 0.0164 - val_loss: 2.6464e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1459/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8799e-05 - rmse: 0.0062\n",
      "Epoch 1459: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8145e-05 - rmse: 0.0099 - val_loss: 3.3692e-04 - val_rmse: 0.0184 - lr: 1.0000e-04\n",
      "Epoch 1460/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0841e-04 - rmse: 0.0144\n",
      "Epoch 1460: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.5651e-05 - rmse: 0.0098 - val_loss: 3.3633e-04 - val_rmse: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 1461/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1543e-04 - rmse: 0.0178\n",
      "Epoch 1461: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3621e-04 - rmse: 0.0117 - val_loss: 5.4758e-04 - val_rmse: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 1462/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7915e-04 - rmse: 0.0195\n",
      "Epoch 1462: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2202e-04 - rmse: 0.0149 - val_loss: 1.7724e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1463/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8863e-04 - rmse: 0.0170\n",
      "Epoch 1463: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1764e-04 - rmse: 0.0108 - val_loss: 3.1461e-04 - val_rmse: 0.0177 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1464/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4059e-04 - rmse: 0.0119\n",
      "Epoch 1464: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1309e-04 - rmse: 0.0106 - val_loss: 1.4989e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1465/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1780e-05 - rmse: 0.0065\n",
      "Epoch 1465: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.4783e-05 - rmse: 0.0086 - val_loss: 2.0936e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1466/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2480e-05 - rmse: 0.0035\n",
      "Epoch 1466: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5925e-05 - rmse: 0.0093 - val_loss: 2.3745e-04 - val_rmse: 0.0154 - lr: 1.0000e-04\n",
      "Epoch 1467/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4869e-05 - rmse: 0.0059\n",
      "Epoch 1467: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.7898e-05 - rmse: 0.0076 - val_loss: 1.6774e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1468/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7497e-05 - rmse: 0.0042\n",
      "Epoch 1468: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9033e-05 - rmse: 0.0054 - val_loss: 1.8735e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1469/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9646e-05 - rmse: 0.0089\n",
      "Epoch 1469: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6349e-05 - rmse: 0.0060 - val_loss: 1.4080e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1470/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3344e-05 - rmse: 0.0058\n",
      "Epoch 1470: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0238e-05 - rmse: 0.0055 - val_loss: 2.4409e-04 - val_rmse: 0.0156 - lr: 1.0000e-04\n",
      "Epoch 1471/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0370e-05 - rmse: 0.0045\n",
      "Epoch 1471: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.6562e-05 - rmse: 0.0068 - val_loss: 2.2763e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1472/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2199e-05 - rmse: 0.0072\n",
      "Epoch 1472: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7784e-05 - rmse: 0.0076 - val_loss: 1.2408e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1473/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3371e-05 - rmse: 0.0048\n",
      "Epoch 1473: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6839e-05 - rmse: 0.0061 - val_loss: 1.9035e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1474/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0329e-05 - rmse: 0.0084\n",
      "Epoch 1474: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.1192e-05 - rmse: 0.0078 - val_loss: 2.5091e-04 - val_rmse: 0.0158 - lr: 1.0000e-04\n",
      "Epoch 1475/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3080e-05 - rmse: 0.0079\n",
      "Epoch 1475: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6504e-05 - rmse: 0.0060 - val_loss: 1.8884e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1476/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4431e-05 - rmse: 0.0049\n",
      "Epoch 1476: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6612e-05 - rmse: 0.0061 - val_loss: 1.1386e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1477/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5488e-05 - rmse: 0.0039\n",
      "Epoch 1477: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8887e-05 - rmse: 0.0054 - val_loss: 1.0030e-04 - val_rmse: 0.0100 - lr: 1.0000e-04\n",
      "Epoch 1478/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8351e-05 - rmse: 0.0043\n",
      "Epoch 1478: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0099e-05 - rmse: 0.0045 - val_loss: 1.9991e-04 - val_rmse: 0.0141 - lr: 1.0000e-04\n",
      "Epoch 1479/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6934e-05 - rmse: 0.0041\n",
      "Epoch 1479: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5625e-05 - rmse: 0.0040 - val_loss: 1.3936e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1480/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7177e-05 - rmse: 0.0041\n",
      "Epoch 1480: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0599e-05 - rmse: 0.0045 - val_loss: 1.4342e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1481/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3960e-05 - rmse: 0.0037\n",
      "Epoch 1481: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3045e-05 - rmse: 0.0036 - val_loss: 1.2287e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1482/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3175e-05 - rmse: 0.0036\n",
      "Epoch 1482: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2319e-05 - rmse: 0.0035 - val_loss: 1.2343e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1483/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8041e-06 - rmse: 0.0022\n",
      "Epoch 1483: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4460e-05 - rmse: 0.0038 - val_loss: 1.8154e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1484/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9992e-05 - rmse: 0.0055\n",
      "Epoch 1484: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6305e-05 - rmse: 0.0051 - val_loss: 1.5558e-04 - val_rmse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 1485/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0513e-05 - rmse: 0.0064\n",
      "Epoch 1485: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5929e-05 - rmse: 0.0040 - val_loss: 1.9369e-04 - val_rmse: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 1486/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9152e-06 - rmse: 0.0030\n",
      "Epoch 1486: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9255e-05 - rmse: 0.0063 - val_loss: 1.4135e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1487/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3699e-05 - rmse: 0.0058\n",
      "Epoch 1487: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.7623e-05 - rmse: 0.0076 - val_loss: 1.9173e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1488/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0349e-05 - rmse: 0.0045\n",
      "Epoch 1488: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3218e-05 - rmse: 0.0086 - val_loss: 1.6101e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1489/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7729e-06 - rmse: 0.0031\n",
      "Epoch 1489: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2566e-05 - rmse: 0.0048 - val_loss: 1.7612e-04 - val_rmse: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 1490/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0469e-06 - rmse: 0.0030\n",
      "Epoch 1490: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7226e-05 - rmse: 0.0052 - val_loss: 2.1134e-04 - val_rmse: 0.0145 - lr: 1.0000e-04\n",
      "Epoch 1491/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2066e-05 - rmse: 0.0035\n",
      "Epoch 1491: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.6720e-05 - rmse: 0.0068 - val_loss: 1.2877e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1492/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7501e-05 - rmse: 0.0069\n",
      "Epoch 1492: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.8824e-05 - rmse: 0.0083 - val_loss: 2.5185e-04 - val_rmse: 0.0159 - lr: 1.0000e-04\n",
      "Epoch 1493/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8169e-05 - rmse: 0.0043\n",
      "Epoch 1493: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1603e-05 - rmse: 0.0090 - val_loss: 2.6436e-04 - val_rmse: 0.0163 - lr: 1.0000e-04\n",
      "Epoch 1494/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2644e-05 - rmse: 0.0073\n",
      "Epoch 1494: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.3129e-05 - rmse: 0.0086 - val_loss: 1.8747e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1495/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5257e-05 - rmse: 0.0039\n",
      "Epoch 1495: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.0474e-05 - rmse: 0.0071 - val_loss: 2.2057e-04 - val_rmse: 0.0149 - lr: 1.0000e-04\n",
      "Epoch 1496/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6733e-05 - rmse: 0.0098\n",
      "Epoch 1496: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0892e-04 - rmse: 0.0104 - val_loss: 8.7728e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1497/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1210e-05 - rmse: 0.0046\n",
      "Epoch 1497: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0353e-04 - rmse: 0.0102 - val_loss: 3.5980e-04 - val_rmse: 0.0190 - lr: 1.0000e-04\n",
      "Epoch 1498/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0968e-04 - rmse: 0.0105\n",
      "Epoch 1498: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1948e-05 - rmse: 0.0091 - val_loss: 1.9172e-04 - val_rmse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 1499/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7656e-05 - rmse: 0.0042\n",
      "Epoch 1499: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.2461e-05 - rmse: 0.0091 - val_loss: 1.5935e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1500/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8784e-05 - rmse: 0.0094\n",
      "Epoch 1500: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.9640e-05 - rmse: 0.0070 - val_loss: 1.3174e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1501/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5783e-05 - rmse: 0.0060\n",
      "Epoch 1501: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1172e-05 - rmse: 0.0056 - val_loss: 2.6967e-04 - val_rmse: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 1502/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0481e-05 - rmse: 0.0084\n",
      "Epoch 1502: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1727e-05 - rmse: 0.0065 - val_loss: 1.0848e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1503/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2551e-05 - rmse: 0.0035\n",
      "Epoch 1503: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.8934e-05 - rmse: 0.0054 - val_loss: 1.2662e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1504/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2150e-05 - rmse: 0.0079\n",
      "Epoch 1504: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3683e-05 - rmse: 0.0066 - val_loss: 1.1602e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1505/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7830e-05 - rmse: 0.0042\n",
      "Epoch 1505: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3165e-05 - rmse: 0.0058 - val_loss: 1.6499e-04 - val_rmse: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 1506/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4150e-05 - rmse: 0.0038\n",
      "Epoch 1506: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.0807e-05 - rmse: 0.0071 - val_loss: 2.1743e-04 - val_rmse: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 1507/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9289e-05 - rmse: 0.0063\n",
      "Epoch 1507: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8871e-05 - rmse: 0.0099 - val_loss: 1.4166e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1508/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3026e-05 - rmse: 0.0085\n",
      "Epoch 1508: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.0210e-05 - rmse: 0.0071 - val_loss: 1.7236e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1509/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8031e-05 - rmse: 0.0062\n",
      "Epoch 1509: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2937e-05 - rmse: 0.0073 - val_loss: 1.5763e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1510/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4002e-05 - rmse: 0.0037\n",
      "Epoch 1510: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1990e-05 - rmse: 0.0047 - val_loss: 1.8672e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1511/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8785e-05 - rmse: 0.0062\n",
      "Epoch 1511: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7427e-05 - rmse: 0.0052 - val_loss: 1.2414e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1512/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4097e-05 - rmse: 0.0038\n",
      "Epoch 1512: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4783e-05 - rmse: 0.0038 - val_loss: 1.1942e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1513/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3725e-05 - rmse: 0.0049\n",
      "Epoch 1513: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7043e-05 - rmse: 0.0041 - val_loss: 1.3305e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1514/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8180e-06 - rmse: 0.0022\n",
      "Epoch 1514: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.6333e-06 - rmse: 0.0028 - val_loss: 1.3201e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1515/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5206e-06 - rmse: 0.0021\n",
      "Epoch 1515: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2389e-05 - rmse: 0.0035 - val_loss: 1.0971e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1516/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7292e-05 - rmse: 0.0042\n",
      "Epoch 1516: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2949e-05 - rmse: 0.0057 - val_loss: 1.3004e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1517/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1338e-06 - rmse: 0.0023\n",
      "Epoch 1517: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3658e-05 - rmse: 0.0066 - val_loss: 1.7535e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1518/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8431e-05 - rmse: 0.0062\n",
      "Epoch 1518: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9692e-05 - rmse: 0.0070 - val_loss: 2.5633e-04 - val_rmse: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 1519/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3583e-05 - rmse: 0.0091\n",
      "Epoch 1519: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.5093e-05 - rmse: 0.0074 - val_loss: 1.8458e-04 - val_rmse: 0.0136 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1520/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2909e-05 - rmse: 0.0079\n",
      "Epoch 1520: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.9571e-05 - rmse: 0.0077 - val_loss: 1.5331e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1521/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5245e-05 - rmse: 0.0087\n",
      "Epoch 1521: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3599e-05 - rmse: 0.0097 - val_loss: 1.3940e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1522/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4265e-05 - rmse: 0.0038\n",
      "Epoch 1522: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1505e-05 - rmse: 0.0056 - val_loss: 2.1308e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1523/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6457e-05 - rmse: 0.0041\n",
      "Epoch 1523: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1150e-05 - rmse: 0.0046 - val_loss: 1.1116e-04 - val_rmse: 0.0105 - lr: 1.0000e-04\n",
      "Epoch 1524/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2935e-05 - rmse: 0.0073\n",
      "Epoch 1524: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0796e-05 - rmse: 0.0055 - val_loss: 1.2784e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1525/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7839e-05 - rmse: 0.0042\n",
      "Epoch 1525: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5889e-05 - rmse: 0.0060 - val_loss: 1.8338e-04 - val_rmse: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 1526/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7270e-05 - rmse: 0.0042\n",
      "Epoch 1526: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5276e-05 - rmse: 0.0050 - val_loss: 2.1459e-04 - val_rmse: 0.0146 - lr: 1.0000e-04\n",
      "Epoch 1527/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4128e-05 - rmse: 0.0080\n",
      "Epoch 1527: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3840e-05 - rmse: 0.0058 - val_loss: 1.7521e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1528/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2942e-05 - rmse: 0.0048\n",
      "Epoch 1528: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0201e-05 - rmse: 0.0055 - val_loss: 1.6557e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1529/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7334e-05 - rmse: 0.0052\n",
      "Epoch 1529: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7034e-05 - rmse: 0.0061 - val_loss: 1.0256e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1530/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1006e-06 - rmse: 0.0030\n",
      "Epoch 1530: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8928e-05 - rmse: 0.0062 - val_loss: 1.8785e-04 - val_rmse: 0.0137 - lr: 1.0000e-04\n",
      "Epoch 1531/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2542e-05 - rmse: 0.0057\n",
      "Epoch 1531: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5417e-05 - rmse: 0.0050 - val_loss: 1.2898e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1532/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4895e-06 - rmse: 0.0029\n",
      "Epoch 1532: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0135e-05 - rmse: 0.0045 - val_loss: 1.6750e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1533/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9770e-05 - rmse: 0.0044\n",
      "Epoch 1533: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8322e-05 - rmse: 0.0043 - val_loss: 1.6547e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1534/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9935e-06 - rmse: 0.0032\n",
      "Epoch 1534: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1345e-05 - rmse: 0.0046 - val_loss: 1.1330e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1535/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5506e-05 - rmse: 0.0039\n",
      "Epoch 1535: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2354e-05 - rmse: 0.0057 - val_loss: 1.6068e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1536/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3148e-05 - rmse: 0.0036\n",
      "Epoch 1536: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1069e-05 - rmse: 0.0046 - val_loss: 1.4095e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1537/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6011e-05 - rmse: 0.0040\n",
      "Epoch 1537: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8606e-05 - rmse: 0.0043 - val_loss: 1.3014e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1538/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5618e-05 - rmse: 0.0040\n",
      "Epoch 1538: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2116e-05 - rmse: 0.0035 - val_loss: 8.9049e-05 - val_rmse: 0.0094 - lr: 1.0000e-04\n",
      "Epoch 1539/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9262e-05 - rmse: 0.0044\n",
      "Epoch 1539: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2161e-05 - rmse: 0.0035 - val_loss: 1.5319e-04 - val_rmse: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 1540/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0708e-05 - rmse: 0.0033\n",
      "Epoch 1540: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4600e-05 - rmse: 0.0038 - val_loss: 9.4235e-05 - val_rmse: 0.0097 - lr: 1.0000e-04\n",
      "Epoch 1541/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6014e-05 - rmse: 0.0051\n",
      "Epoch 1541: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5481e-05 - rmse: 0.0039 - val_loss: 1.4080e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1542/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1008e-05 - rmse: 0.0033\n",
      "Epoch 1542: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1329e-05 - rmse: 0.0034 - val_loss: 1.3695e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1543/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6951e-06 - rmse: 0.0029\n",
      "Epoch 1543: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3835e-06 - rmse: 0.0029 - val_loss: 1.4800e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1544/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7208e-06 - rmse: 0.0031\n",
      "Epoch 1544: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1179e-05 - rmse: 0.0033 - val_loss: 1.2639e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1545/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9924e-06 - rmse: 0.0022\n",
      "Epoch 1545: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.1857e-06 - rmse: 0.0025 - val_loss: 1.0554e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1546/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8413e-06 - rmse: 0.0030\n",
      "Epoch 1546: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.6055e-06 - rmse: 0.0024 - val_loss: 1.3743e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1547/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4263e-06 - rmse: 0.0016\n",
      "Epoch 1547: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.9403e-06 - rmse: 0.0020 - val_loss: 1.2716e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1548/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6135e-06 - rmse: 0.0019\n",
      "Epoch 1548: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6608e-06 - rmse: 0.0019 - val_loss: 1.1609e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1549/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4353e-06 - rmse: 0.0019\n",
      "Epoch 1549: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3813e-06 - rmse: 0.0018 - val_loss: 1.4614e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1550/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6207e-06 - rmse: 0.0026\n",
      "Epoch 1550: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9932e-06 - rmse: 0.0022 - val_loss: 1.3039e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1551/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4524e-06 - rmse: 0.0021\n",
      "Epoch 1551: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.5346e-06 - rmse: 0.0021 - val_loss: 1.2449e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1552/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6840e-06 - rmse: 0.0016\n",
      "Epoch 1552: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4487e-06 - rmse: 0.0019 - val_loss: 1.3554e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1553/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1225e-06 - rmse: 0.0018\n",
      "Epoch 1553: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.1234e-06 - rmse: 0.0018 - val_loss: 1.2172e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1554/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5381e-06 - rmse: 0.0016\n",
      "Epoch 1554: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6119e-06 - rmse: 0.0019 - val_loss: 1.3402e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1555/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4343e-06 - rmse: 0.0019\n",
      "Epoch 1555: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.4541e-06 - rmse: 0.0021 - val_loss: 1.6165e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1556/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4445e-05 - rmse: 0.0038\n",
      "Epoch 1556: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.2151e-06 - rmse: 0.0030 - val_loss: 1.3838e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1557/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9459e-06 - rmse: 0.0020\n",
      "Epoch 1557: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.6444e-06 - rmse: 0.0028 - val_loss: 1.0859e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1558/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1529e-06 - rmse: 0.0029\n",
      "Epoch 1558: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.2956e-06 - rmse: 0.0025 - val_loss: 1.2336e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1559/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0624e-06 - rmse: 0.0020\n",
      "Epoch 1559: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.9636e-06 - rmse: 0.0026 - val_loss: 1.2907e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1560/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0367e-06 - rmse: 0.0020\n",
      "Epoch 1560: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8901e-06 - rmse: 0.0022 - val_loss: 1.4077e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1561/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2260e-06 - rmse: 0.0021\n",
      "Epoch 1561: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.1680e-06 - rmse: 0.0020 - val_loss: 1.4174e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1562/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7018e-06 - rmse: 0.0024\n",
      "Epoch 1562: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1208e-06 - rmse: 0.0020 - val_loss: 1.2020e-04 - val_rmse: 0.0110 - lr: 1.0000e-04\n",
      "Epoch 1563/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2639e-06 - rmse: 0.0023\n",
      "Epoch 1563: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.8405e-06 - rmse: 0.0024 - val_loss: 1.3466e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1564/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2422e-06 - rmse: 0.0023\n",
      "Epoch 1564: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.7906e-06 - rmse: 0.0022 - val_loss: 1.6013e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1565/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6410e-06 - rmse: 0.0029\n",
      "Epoch 1565: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.4525e-06 - rmse: 0.0025 - val_loss: 1.4570e-04 - val_rmse: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 1566/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7573e-06 - rmse: 0.0024\n",
      "Epoch 1566: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0740e-06 - rmse: 0.0020 - val_loss: 1.1846e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1567/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4859e-06 - rmse: 0.0023\n",
      "Epoch 1567: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8357e-06 - rmse: 0.0020 - val_loss: 1.2466e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1568/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8996e-06 - rmse: 0.0022\n",
      "Epoch 1568: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7640e-06 - rmse: 0.0024 - val_loss: 1.3353e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1569/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1737e-06 - rmse: 0.0020\n",
      "Epoch 1569: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.4716e-06 - rmse: 0.0023 - val_loss: 1.6721e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1570/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8433e-06 - rmse: 0.0030\n",
      "Epoch 1570: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6968e-05 - rmse: 0.0041 - val_loss: 1.3375e-04 - val_rmse: 0.0116 - lr: 1.0000e-04\n",
      "Epoch 1571/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2981e-06 - rmse: 0.0015\n",
      "Epoch 1571: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2206e-05 - rmse: 0.0047 - val_loss: 1.1304e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1572/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3682e-05 - rmse: 0.0037\n",
      "Epoch 1572: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1132e-05 - rmse: 0.0033 - val_loss: 1.0575e-04 - val_rmse: 0.0103 - lr: 1.0000e-04\n",
      "Epoch 1573/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1841e-05 - rmse: 0.0034\n",
      "Epoch 1573: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3382e-05 - rmse: 0.0037 - val_loss: 1.2929e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1574/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6445e-06 - rmse: 0.0024\n",
      "Epoch 1574: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3226e-06 - rmse: 0.0029 - val_loss: 1.2492e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1575/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3158e-06 - rmse: 0.0018\n",
      "Epoch 1575: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2171e-06 - rmse: 0.0023 - val_loss: 1.1712e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1576/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0308e-06 - rmse: 0.0017\n",
      "Epoch 1576: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0127e-06 - rmse: 0.0020 - val_loss: 1.1557e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1577/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3451e-06 - rmse: 0.0021\n",
      "Epoch 1577: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.5495e-06 - rmse: 0.0021 - val_loss: 1.2648e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1578/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8820e-06 - rmse: 0.0017\n",
      "Epoch 1578: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.5318e-06 - rmse: 0.0021 - val_loss: 1.3077e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1579/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2802e-06 - rmse: 0.0011\n",
      "Epoch 1579: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3324e-06 - rmse: 0.0018 - val_loss: 1.3032e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1580/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2112e-06 - rmse: 0.0021\n",
      "Epoch 1580: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2711e-06 - rmse: 0.0018 - val_loss: 1.3175e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1581/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2614e-06 - rmse: 0.0018\n",
      "Epoch 1581: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7271e-06 - rmse: 0.0019 - val_loss: 1.4234e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1582/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6824e-06 - rmse: 0.0022\n",
      "Epoch 1582: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.0902e-06 - rmse: 0.0020 - val_loss: 1.2658e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1583/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2143e-06 - rmse: 0.0023\n",
      "Epoch 1583: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5028e-06 - rmse: 0.0019 - val_loss: 1.3008e-04 - val_rmse: 0.0114 - lr: 1.0000e-04\n",
      "Epoch 1584/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2195e-06 - rmse: 0.0030\n",
      "Epoch 1584: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.7928e-06 - rmse: 0.0022 - val_loss: 1.1294e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1585/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4665e-06 - rmse: 0.0025\n",
      "Epoch 1585: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.8356e-06 - rmse: 0.0020 - val_loss: 1.3313e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1586/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6178e-06 - rmse: 0.0019\n",
      "Epoch 1586: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6391e-06 - rmse: 0.0019 - val_loss: 1.2355e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1587/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2787e-06 - rmse: 0.0015\n",
      "Epoch 1587: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9096e-06 - rmse: 0.0020 - val_loss: 1.2417e-04 - val_rmse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 1588/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2188e-06 - rmse: 0.0023\n",
      "Epoch 1588: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.0081e-06 - rmse: 0.0025 - val_loss: 1.2661e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1589/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9958e-06 - rmse: 0.0017\n",
      "Epoch 1589: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7641e-06 - rmse: 0.0024 - val_loss: 1.1303e-04 - val_rmse: 0.0106 - lr: 1.0000e-04\n",
      "Epoch 1590/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4277e-06 - rmse: 0.0021\n",
      "Epoch 1590: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.9384e-06 - rmse: 0.0022 - val_loss: 1.3835e-04 - val_rmse: 0.0118 - lr: 1.0000e-04\n",
      "Epoch 1591/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8810e-06 - rmse: 0.0017\n",
      "Epoch 1591: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6652e-06 - rmse: 0.0019 - val_loss: 1.2827e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1592/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7651e-06 - rmse: 0.0019\n",
      "Epoch 1592: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6772e-06 - rmse: 0.0019 - val_loss: 1.2490e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1593/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3292e-06 - rmse: 0.0012\n",
      "Epoch 1593: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7545e-06 - rmse: 0.0017 - val_loss: 1.2692e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1594/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9565e-06 - rmse: 0.0017\n",
      "Epoch 1594: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9434e-06 - rmse: 0.0017 - val_loss: 1.1508e-04 - val_rmse: 0.0107 - lr: 1.0000e-04\n",
      "Epoch 1595/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5406e-06 - rmse: 0.0019\n",
      "Epoch 1595: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0420e-06 - rmse: 0.0017 - val_loss: 1.3688e-04 - val_rmse: 0.0117 - lr: 1.0000e-04\n",
      "Epoch 1596/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5994e-06 - rmse: 0.0016\n",
      "Epoch 1596: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6179e-06 - rmse: 0.0019 - val_loss: 1.1841e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1597/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2699e-06 - rmse: 0.0018\n",
      "Epoch 1597: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.1272e-06 - rmse: 0.0025 - val_loss: 1.4434e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1598/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1570e-06 - rmse: 0.0025\n",
      "Epoch 1598: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.9602e-06 - rmse: 0.0024 - val_loss: 1.7006e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1599/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2036e-05 - rmse: 0.0035\n",
      "Epoch 1599: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4742e-05 - rmse: 0.0038 - val_loss: 1.1557e-04 - val_rmse: 0.0108 - lr: 1.0000e-04\n",
      "Epoch 1600/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7265e-06 - rmse: 0.0026\n",
      "Epoch 1600: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.0680e-05 - rmse: 0.0078 - val_loss: 1.7082e-04 - val_rmse: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 1601/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9742e-05 - rmse: 0.0055\n",
      "Epoch 1601: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.4949e-05 - rmse: 0.0081 - val_loss: 1.2799e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1602/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4669e-04 - rmse: 0.0121\n",
      "Epoch 1602: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1224e-04 - rmse: 0.0106 - val_loss: 2.8616e-04 - val_rmse: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 1603/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2990e-05 - rmse: 0.0079\n",
      "Epoch 1603: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1712e-04 - rmse: 0.0108 - val_loss: 1.4934e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1604/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0993e-04 - rmse: 0.0105\n",
      "Epoch 1604: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4576e-05 - rmse: 0.0092 - val_loss: 2.9462e-04 - val_rmse: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 1605/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1872e-04 - rmse: 0.0109\n",
      "Epoch 1605: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.1623e-05 - rmse: 0.0085 - val_loss: 1.6762e-04 - val_rmse: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 1606/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9078e-05 - rmse: 0.0063\n",
      "Epoch 1606: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8411e-05 - rmse: 0.0070 - val_loss: 1.6081e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1607/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4913e-05 - rmse: 0.0050\n",
      "Epoch 1607: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1539e-05 - rmse: 0.0064 - val_loss: 1.4383e-04 - val_rmse: 0.0120 - lr: 1.0000e-04\n",
      "Epoch 1608/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8697e-06 - rmse: 0.0030\n",
      "Epoch 1608: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6580e-05 - rmse: 0.0052 - val_loss: 1.1944e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1609/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0153e-05 - rmse: 0.0045\n",
      "Epoch 1609: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5750e-05 - rmse: 0.0060 - val_loss: 1.2858e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1610/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6284e-06 - rmse: 0.0022\n",
      "Epoch 1610: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.8725e-05 - rmse: 0.0070 - val_loss: 1.5966e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1611/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2614e-05 - rmse: 0.0036\n",
      "Epoch 1611: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.4126e-05 - rmse: 0.0074 - val_loss: 1.7484e-04 - val_rmse: 0.0132 - lr: 1.0000e-04\n",
      "Epoch 1612/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8101e-05 - rmse: 0.0062\n",
      "Epoch 1612: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.4658e-05 - rmse: 0.0080 - val_loss: 1.6867e-04 - val_rmse: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 1613/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9872e-06 - rmse: 0.0032\n",
      "Epoch 1613: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.7353e-05 - rmse: 0.0076 - val_loss: 1.0155e-04 - val_rmse: 0.0101 - lr: 1.0000e-04\n",
      "Epoch 1614/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3139e-05 - rmse: 0.0048\n",
      "Epoch 1614: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1552e-05 - rmse: 0.0072 - val_loss: 2.2717e-04 - val_rmse: 0.0151 - lr: 1.0000e-04\n",
      "Epoch 1615/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7370e-05 - rmse: 0.0069\n",
      "Epoch 1615: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8307e-05 - rmse: 0.0062 - val_loss: 1.2600e-04 - val_rmse: 0.0112 - lr: 1.0000e-04\n",
      "Epoch 1616/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6522e-05 - rmse: 0.0051\n",
      "Epoch 1616: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4856e-05 - rmse: 0.0039 - val_loss: 1.6117e-04 - val_rmse: 0.0127 - lr: 1.0000e-04\n",
      "Epoch 1617/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1915e-06 - rmse: 0.0027\n",
      "Epoch 1617: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1981e-05 - rmse: 0.0035 - val_loss: 1.4271e-04 - val_rmse: 0.0119 - lr: 1.0000e-04\n",
      "Epoch 1618/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5491e-06 - rmse: 0.0024\n",
      "Epoch 1618: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.1022e-06 - rmse: 0.0025 - val_loss: 1.1987e-04 - val_rmse: 0.0109 - lr: 1.0000e-04\n",
      "Epoch 1619/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5502e-06 - rmse: 0.0021\n",
      "Epoch 1619: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7645e-06 - rmse: 0.0024 - val_loss: 1.5785e-04 - val_rmse: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 1620/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7598e-06 - rmse: 0.0030\n",
      "Epoch 1620: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0282e-05 - rmse: 0.0032 - val_loss: 1.4824e-04 - val_rmse: 0.0122 - lr: 1.0000e-04\n",
      "Epoch 1621/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9652e-06 - rmse: 0.0030\n",
      "Epoch 1621: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1862e-06 - rmse: 0.0029 - val_loss: 1.2802e-04 - val_rmse: 0.0113 - lr: 1.0000e-04\n",
      "Epoch 1622/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5665e-05 - rmse: 0.0040\n",
      "Epoch 1622: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3474e-05 - rmse: 0.0037 - val_loss: 1.0800e-04 - val_rmse: 0.0104 - lr: 1.0000e-04\n",
      "Epoch 1623/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1007e-05 - rmse: 0.0033\n",
      "Epoch 1623: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8952e-05 - rmse: 0.0044 - val_loss: 1.3300e-04 - val_rmse: 0.0115 - lr: 1.0000e-04\n",
      "Epoch 1624/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0146e-05 - rmse: 0.0045\n",
      "Epoch 1624: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3939e-05 - rmse: 0.0058 - val_loss: 9.5760e-05 - val_rmse: 0.0098 - lr: 1.0000e-04\n",
      "Epoch 1625/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0503e-05 - rmse: 0.0045\n",
      "Epoch 1625: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 1625: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.9879e-05 - rmse: 0.0084 - val_loss: 2.3966e-04 - val_rmse: 0.0155 - lr: 1.0000e-04\n",
      "Epoch 1626/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8870e-05 - rmse: 0.0099\n",
      "Epoch 1626: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1381e-05 - rmse: 0.0096 - val_loss: 2.7324e-04 - val_rmse: 0.0165 - lr: 5.0000e-05\n",
      "Epoch 1627/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2388e-04 - rmse: 0.0111\n",
      "Epoch 1627: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3804e-05 - rmse: 0.0086 - val_loss: 1.0572e-04 - val_rmse: 0.0103 - lr: 5.0000e-05\n",
      "Epoch 1628/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0508e-05 - rmse: 0.0071\n",
      "Epoch 1628: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9528e-05 - rmse: 0.0054 - val_loss: 1.8679e-04 - val_rmse: 0.0137 - lr: 5.0000e-05\n",
      "Epoch 1629/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2317e-05 - rmse: 0.0035\n",
      "Epoch 1629: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1737e-05 - rmse: 0.0034 - val_loss: 1.2727e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1630/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6763e-06 - rmse: 0.0019\n",
      "Epoch 1630: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.5729e-06 - rmse: 0.0031 - val_loss: 1.0815e-04 - val_rmse: 0.0104 - lr: 5.0000e-05\n",
      "Epoch 1631/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6621e-06 - rmse: 0.0028\n",
      "Epoch 1631: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.0752e-06 - rmse: 0.0028 - val_loss: 1.5777e-04 - val_rmse: 0.0126 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1632/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8968e-06 - rmse: 0.0030\n",
      "Epoch 1632: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.1529e-06 - rmse: 0.0025 - val_loss: 1.0926e-04 - val_rmse: 0.0105 - lr: 5.0000e-05\n",
      "Epoch 1633/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4245e-05 - rmse: 0.0038\n",
      "Epoch 1633: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1192e-05 - rmse: 0.0033 - val_loss: 1.4105e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1634/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0775e-06 - rmse: 0.0014\n",
      "Epoch 1634: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1889e-05 - rmse: 0.0034 - val_loss: 1.4268e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1635/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1928e-06 - rmse: 0.0025\n",
      "Epoch 1635: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.6333e-06 - rmse: 0.0026 - val_loss: 9.9931e-05 - val_rmse: 0.0100 - lr: 5.0000e-05\n",
      "Epoch 1636/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1126e-05 - rmse: 0.0033\n",
      "Epoch 1636: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.3631e-06 - rmse: 0.0027 - val_loss: 1.4790e-04 - val_rmse: 0.0122 - lr: 5.0000e-05\n",
      "Epoch 1637/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4628e-06 - rmse: 0.0031\n",
      "Epoch 1637: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.0377e-06 - rmse: 0.0025 - val_loss: 1.1917e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1638/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3637e-06 - rmse: 0.0029\n",
      "Epoch 1638: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.9648e-06 - rmse: 0.0022 - val_loss: 1.3772e-04 - val_rmse: 0.0117 - lr: 5.0000e-05\n",
      "Epoch 1639/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2372e-06 - rmse: 0.0021\n",
      "Epoch 1639: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.9484e-06 - rmse: 0.0020 - val_loss: 1.2539e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1640/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7810e-06 - rmse: 0.0017\n",
      "Epoch 1640: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1531e-06 - rmse: 0.0020 - val_loss: 1.2155e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1641/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9730e-06 - rmse: 0.0017\n",
      "Epoch 1641: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7577e-06 - rmse: 0.0017 - val_loss: 1.2113e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1642/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0069e-06 - rmse: 0.0017\n",
      "Epoch 1642: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7774e-06 - rmse: 0.0017 - val_loss: 1.2042e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1643/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7622e-06 - rmse: 0.0017\n",
      "Epoch 1643: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4289e-06 - rmse: 0.0016 - val_loss: 1.1649e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1644/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8493e-06 - rmse: 0.0014\n",
      "Epoch 1644: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4348e-06 - rmse: 0.0016 - val_loss: 1.1570e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1645/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0784e-06 - rmse: 0.0010\n",
      "Epoch 1645: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3896e-06 - rmse: 0.0015 - val_loss: 1.2448e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1646/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4929e-06 - rmse: 0.0016\n",
      "Epoch 1646: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3019e-06 - rmse: 0.0015 - val_loss: 1.1771e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1647/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7799e-06 - rmse: 0.0013\n",
      "Epoch 1647: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7553e-06 - rmse: 0.0017 - val_loss: 1.3100e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1648/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2218e-06 - rmse: 0.0015\n",
      "Epoch 1648: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7860e-06 - rmse: 0.0017 - val_loss: 1.1157e-04 - val_rmse: 0.0106 - lr: 5.0000e-05\n",
      "Epoch 1649/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8928e-06 - rmse: 0.0022\n",
      "Epoch 1649: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.1932e-06 - rmse: 0.0018 - val_loss: 1.2792e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1650/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0386e-06 - rmse: 0.0017\n",
      "Epoch 1650: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4154e-06 - rmse: 0.0016 - val_loss: 1.0815e-04 - val_rmse: 0.0104 - lr: 5.0000e-05\n",
      "Epoch 1651/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8621e-06 - rmse: 0.0020\n",
      "Epoch 1651: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9957e-06 - rmse: 0.0017 - val_loss: 1.2625e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1652/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8524e-06 - rmse: 0.0014\n",
      "Epoch 1652: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5122e-06 - rmse: 0.0016 - val_loss: 1.1948e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1653/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0906e-06 - rmse: 0.0014\n",
      "Epoch 1653: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2322e-06 - rmse: 0.0015 - val_loss: 1.1893e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1654/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0192e-06 - rmse: 0.0014\n",
      "Epoch 1654: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2297e-06 - rmse: 0.0015 - val_loss: 1.2715e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1655/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6787e-06 - rmse: 0.0016\n",
      "Epoch 1655: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4117e-06 - rmse: 0.0016 - val_loss: 1.2540e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1656/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0063e-06 - rmse: 0.0014\n",
      "Epoch 1656: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4312e-06 - rmse: 0.0016 - val_loss: 1.1693e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1657/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8133e-06 - rmse: 0.0017\n",
      "Epoch 1657: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5108e-06 - rmse: 0.0016 - val_loss: 1.3008e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1658/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2970e-06 - rmse: 0.0015\n",
      "Epoch 1658: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.5550e-06 - rmse: 0.0016 - val_loss: 1.2106e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1659/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9889e-06 - rmse: 0.0014\n",
      "Epoch 1659: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0494e-06 - rmse: 0.0014 - val_loss: 1.2764e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1660/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6043e-06 - rmse: 0.0013\n",
      "Epoch 1660: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0918e-06 - rmse: 0.0014 - val_loss: 1.2152e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1661/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8303e-06 - rmse: 0.0020\n",
      "Epoch 1661: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.0134e-06 - rmse: 0.0014 - val_loss: 1.2213e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1662/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0069e-06 - rmse: 0.0014\n",
      "Epoch 1662: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1251e-06 - rmse: 0.0015 - val_loss: 1.3154e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1663/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0124e-06 - rmse: 0.0014\n",
      "Epoch 1663: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0401e-06 - rmse: 0.0014 - val_loss: 1.1656e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1664/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0056e-06 - rmse: 0.0014\n",
      "Epoch 1664: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9459e-06 - rmse: 0.0017 - val_loss: 1.3572e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1665/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2190e-06 - rmse: 0.0023\n",
      "Epoch 1665: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2187e-06 - rmse: 0.0018 - val_loss: 1.2201e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1666/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4795e-06 - rmse: 0.0012\n",
      "Epoch 1666: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4220e-06 - rmse: 0.0016 - val_loss: 1.1670e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1667/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4012e-06 - rmse: 0.0015\n",
      "Epoch 1667: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.1016e-06 - rmse: 0.0018 - val_loss: 1.4038e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1668/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0797e-06 - rmse: 0.0023\n",
      "Epoch 1668: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.3659e-06 - rmse: 0.0021 - val_loss: 1.1845e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1669/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9082e-06 - rmse: 0.0020\n",
      "Epoch 1669: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7404e-06 - rmse: 0.0017 - val_loss: 1.2555e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1670/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0419e-06 - rmse: 0.0014\n",
      "Epoch 1670: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4925e-06 - rmse: 0.0016 - val_loss: 1.1982e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1671/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5102e-06 - rmse: 0.0016\n",
      "Epoch 1671: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7265e-06 - rmse: 0.0017 - val_loss: 1.1989e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1672/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0151e-06 - rmse: 0.0014\n",
      "Epoch 1672: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6348e-06 - rmse: 0.0016 - val_loss: 1.3096e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1673/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3639e-06 - rmse: 0.0018\n",
      "Epoch 1673: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9195e-06 - rmse: 0.0017 - val_loss: 1.2524e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1674/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5095e-06 - rmse: 0.0019\n",
      "Epoch 1674: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7919e-06 - rmse: 0.0017 - val_loss: 1.2111e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1675/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3101e-07 - rmse: 9.6489e-04\n",
      "Epoch 1675: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6712e-06 - rmse: 0.0016 - val_loss: 1.2736e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1676/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4250e-06 - rmse: 0.0016\n",
      "Epoch 1676: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2402e-06 - rmse: 0.0015 - val_loss: 1.2544e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1677/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6134e-06 - rmse: 0.0013\n",
      "Epoch 1677: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2589e-06 - rmse: 0.0015 - val_loss: 1.3150e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1678/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9027e-06 - rmse: 0.0014\n",
      "Epoch 1678: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1892e-06 - rmse: 0.0015 - val_loss: 1.1929e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1679/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7580e-06 - rmse: 0.0013\n",
      "Epoch 1679: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2908e-06 - rmse: 0.0015 - val_loss: 1.3083e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1680/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3446e-06 - rmse: 0.0015\n",
      "Epoch 1680: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1666e-06 - rmse: 0.0015 - val_loss: 1.2927e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1681/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7138e-06 - rmse: 0.0013\n",
      "Epoch 1681: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0850e-06 - rmse: 0.0014 - val_loss: 1.2557e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1682/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0626e-06 - rmse: 0.0014\n",
      "Epoch 1682: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9999e-06 - rmse: 0.0014 - val_loss: 1.2823e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1683/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4741e-06 - rmse: 0.0016\n",
      "Epoch 1683: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9663e-06 - rmse: 0.0014 - val_loss: 1.2353e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1684/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8256e-06 - rmse: 0.0014\n",
      "Epoch 1684: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9544e-06 - rmse: 0.0014 - val_loss: 1.2219e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1685/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2214e-06 - rmse: 0.0015\n",
      "Epoch 1685: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9526e-06 - rmse: 0.0014 - val_loss: 1.2792e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1686/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5285e-06 - rmse: 0.0016\n",
      "Epoch 1686: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9818e-06 - rmse: 0.0014 - val_loss: 1.2813e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1687/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3336e-06 - rmse: 0.0012\n",
      "Epoch 1687: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9839e-06 - rmse: 0.0014 - val_loss: 1.2524e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1688/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1754e-06 - rmse: 0.0015\n",
      "Epoch 1688: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9566e-06 - rmse: 0.0014 - val_loss: 1.2438e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1689/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6714e-06 - rmse: 0.0013\n",
      "Epoch 1689: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8837e-06 - rmse: 0.0014 - val_loss: 1.2997e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1690/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3217e-06 - rmse: 0.0018\n",
      "Epoch 1690: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9158e-06 - rmse: 0.0014 - val_loss: 1.2289e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1691/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0614e-06 - rmse: 0.0014\n",
      "Epoch 1691: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9433e-06 - rmse: 0.0014 - val_loss: 1.2803e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1692/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2506e-06 - rmse: 0.0011\n",
      "Epoch 1692: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8741e-06 - rmse: 0.0014 - val_loss: 1.2369e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1693/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1533e-06 - rmse: 0.0011\n",
      "Epoch 1693: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8683e-06 - rmse: 0.0014 - val_loss: 1.2422e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1694/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9773e-06 - rmse: 0.0017\n",
      "Epoch 1694: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9791e-06 - rmse: 0.0014 - val_loss: 1.2789e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1695/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6992e-07 - rmse: 8.1849e-04\n",
      "Epoch 1695: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8336e-06 - rmse: 0.0014 - val_loss: 1.2427e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1696/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2236e-07 - rmse: 7.8890e-04\n",
      "Epoch 1696: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9492e-06 - rmse: 0.0014 - val_loss: 1.2674e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1697/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1500e-06 - rmse: 0.0011\n",
      "Epoch 1697: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0389e-06 - rmse: 0.0014 - val_loss: 1.3116e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1698/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3084e-07 - rmse: 8.5489e-04\n",
      "Epoch 1698: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9317e-06 - rmse: 0.0014 - val_loss: 1.1998e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1699/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9869e-06 - rmse: 0.0017\n",
      "Epoch 1699: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9156e-06 - rmse: 0.0014 - val_loss: 1.3124e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1700/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7640e-06 - rmse: 0.0013\n",
      "Epoch 1700: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9425e-06 - rmse: 0.0014 - val_loss: 1.2175e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1701/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8826e-06 - rmse: 0.0020\n",
      "Epoch 1701: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6961e-06 - rmse: 0.0016 - val_loss: 1.3134e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1702/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2619e-06 - rmse: 0.0018\n",
      "Epoch 1702: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7533e-06 - rmse: 0.0017 - val_loss: 1.3298e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1703/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7251e-06 - rmse: 0.0013\n",
      "Epoch 1703: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9236e-06 - rmse: 0.0017 - val_loss: 1.1530e-04 - val_rmse: 0.0107 - lr: 5.0000e-05\n",
      "Epoch 1704/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1166e-06 - rmse: 0.0023\n",
      "Epoch 1704: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5950e-06 - rmse: 0.0021 - val_loss: 1.3538e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1705/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8335e-06 - rmse: 0.0022\n",
      "Epoch 1705: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.8396e-06 - rmse: 0.0020 - val_loss: 1.3607e-04 - val_rmse: 0.0117 - lr: 5.0000e-05\n",
      "Epoch 1706/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5306e-06 - rmse: 0.0016\n",
      "Epoch 1706: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6560e-06 - rmse: 0.0016 - val_loss: 1.1277e-04 - val_rmse: 0.0106 - lr: 5.0000e-05\n",
      "Epoch 1707/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3350e-06 - rmse: 0.0018\n",
      "Epoch 1707: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7939e-06 - rmse: 0.0017 - val_loss: 1.3952e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1708/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4432e-06 - rmse: 0.0019\n",
      "Epoch 1708: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5682e-06 - rmse: 0.0016 - val_loss: 1.2542e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1709/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2287e-06 - rmse: 0.0011\n",
      "Epoch 1709: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8191e-06 - rmse: 0.0013 - val_loss: 1.2586e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1710/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3178e-06 - rmse: 0.0018\n",
      "Epoch 1710: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2271e-06 - rmse: 0.0015 - val_loss: 1.1696e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1711/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8747e-06 - rmse: 0.0017\n",
      "Epoch 1711: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3262e-06 - rmse: 0.0015 - val_loss: 1.3316e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1712/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7677e-06 - rmse: 0.0013\n",
      "Epoch 1712: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3628e-06 - rmse: 0.0015 - val_loss: 1.3017e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1713/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2221e-06 - rmse: 0.0015\n",
      "Epoch 1713: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1173e-06 - rmse: 0.0015 - val_loss: 1.1156e-04 - val_rmse: 0.0106 - lr: 5.0000e-05\n",
      "Epoch 1714/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3408e-06 - rmse: 0.0021\n",
      "Epoch 1714: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.1628e-06 - rmse: 0.0018 - val_loss: 1.3826e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1715/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6735e-06 - rmse: 0.0016\n",
      "Epoch 1715: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2449e-06 - rmse: 0.0018 - val_loss: 1.2907e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1716/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9636e-06 - rmse: 0.0017\n",
      "Epoch 1716: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4112e-06 - rmse: 0.0016 - val_loss: 1.1607e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1717/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5534e-06 - rmse: 0.0016\n",
      "Epoch 1717: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.2069e-06 - rmse: 0.0023 - val_loss: 1.3974e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1718/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3230e-06 - rmse: 0.0021\n",
      "Epoch 1718: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4377e-06 - rmse: 0.0019 - val_loss: 1.2995e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1719/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5887e-06 - rmse: 0.0013\n",
      "Epoch 1719: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5223e-06 - rmse: 0.0016 - val_loss: 1.1935e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1720/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8844e-06 - rmse: 0.0017\n",
      "Epoch 1720: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4950e-06 - rmse: 0.0019 - val_loss: 1.2806e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1721/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9632e-06 - rmse: 0.0017\n",
      "Epoch 1721: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5287e-06 - rmse: 0.0016 - val_loss: 1.3982e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1722/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2678e-06 - rmse: 0.0015\n",
      "Epoch 1722: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1660e-06 - rmse: 0.0015 - val_loss: 1.1762e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1723/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7333e-06 - rmse: 0.0013\n",
      "Epoch 1723: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1461e-06 - rmse: 0.0015 - val_loss: 1.3383e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1724/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3538e-06 - rmse: 0.0021\n",
      "Epoch 1724: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5558e-06 - rmse: 0.0016 - val_loss: 1.2484e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1725/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0795e-06 - rmse: 0.0010\n",
      "Epoch 1725: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9267e-06 - rmse: 0.0014 - val_loss: 1.2581e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1726/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3765e-06 - rmse: 0.0015\n",
      "Epoch 1726: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8153e-06 - rmse: 0.0013 - val_loss: 1.2431e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1727/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4130e-06 - rmse: 0.0016\n",
      "Epoch 1727: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7382e-06 - rmse: 0.0013 - val_loss: 1.2906e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1728/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6019e-06 - rmse: 0.0013\n",
      "Epoch 1728: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8230e-06 - rmse: 0.0014 - val_loss: 1.2938e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1729/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7733e-06 - rmse: 0.0017\n",
      "Epoch 1729: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7162e-06 - rmse: 0.0013 - val_loss: 1.2455e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1730/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4162e-06 - rmse: 0.0012\n",
      "Epoch 1730: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0613e-06 - rmse: 0.0014 - val_loss: 1.2708e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1731/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3978e-06 - rmse: 0.0015\n",
      "Epoch 1731: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2952e-06 - rmse: 0.0015 - val_loss: 1.3520e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1732/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3897e-06 - rmse: 0.0015\n",
      "Epoch 1732: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9109e-06 - rmse: 0.0014 - val_loss: 1.2115e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1733/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8110e-06 - rmse: 0.0013\n",
      "Epoch 1733: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0718e-06 - rmse: 0.0014 - val_loss: 1.2766e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1734/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5625e-06 - rmse: 0.0016\n",
      "Epoch 1734: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8066e-06 - rmse: 0.0013 - val_loss: 1.3196e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1735/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9411e-06 - rmse: 0.0014\n",
      "Epoch 1735: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9652e-06 - rmse: 0.0014 - val_loss: 1.1992e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1736/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5120e-06 - rmse: 0.0019\n",
      "Epoch 1736: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9096e-06 - rmse: 0.0017 - val_loss: 1.3870e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1737/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8292e-06 - rmse: 0.0017\n",
      "Epoch 1737: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2276e-06 - rmse: 0.0018 - val_loss: 1.2568e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1738/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3132e-06 - rmse: 0.0011\n",
      "Epoch 1738: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5692e-06 - rmse: 0.0016 - val_loss: 1.1906e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1739/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3307e-06 - rmse: 0.0015\n",
      "Epoch 1739: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5658e-06 - rmse: 0.0016 - val_loss: 1.3382e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1740/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6536e-06 - rmse: 0.0016\n",
      "Epoch 1740: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5026e-06 - rmse: 0.0016 - val_loss: 1.1633e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1741/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3608e-06 - rmse: 0.0012\n",
      "Epoch 1741: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2676e-06 - rmse: 0.0015 - val_loss: 1.2677e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1742/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0664e-06 - rmse: 0.0010\n",
      "Epoch 1742: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4615e-06 - rmse: 0.0016 - val_loss: 1.2855e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1743/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5910e-06 - rmse: 0.0019\n",
      "Epoch 1743: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0488e-06 - rmse: 0.0014 - val_loss: 1.3352e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1744/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0077e-06 - rmse: 0.0014\n",
      "Epoch 1744: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2778e-06 - rmse: 0.0018 - val_loss: 1.3534e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1745/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2555e-06 - rmse: 0.0015\n",
      "Epoch 1745: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.3875e-06 - rmse: 0.0018 - val_loss: 1.1376e-04 - val_rmse: 0.0107 - lr: 5.0000e-05\n",
      "Epoch 1746/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8367e-06 - rmse: 0.0017\n",
      "Epoch 1746: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7456e-06 - rmse: 0.0017 - val_loss: 1.4477e-04 - val_rmse: 0.0120 - lr: 5.0000e-05\n",
      "Epoch 1747/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2561e-06 - rmse: 0.0021\n",
      "Epoch 1747: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9293e-06 - rmse: 0.0017 - val_loss: 1.2995e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1748/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0256e-06 - rmse: 0.0014\n",
      "Epoch 1748: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4918e-06 - rmse: 0.0016 - val_loss: 1.2023e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1749/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6806e-06 - rmse: 0.0019\n",
      "Epoch 1749: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.0329e-06 - rmse: 0.0017 - val_loss: 1.2660e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1750/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9854e-06 - rmse: 0.0014\n",
      "Epoch 1750: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2307e-06 - rmse: 0.0015 - val_loss: 1.2490e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1751/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5314e-06 - rmse: 0.0012\n",
      "Epoch 1751: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.0668e-06 - rmse: 0.0014 - val_loss: 1.1322e-04 - val_rmse: 0.0106 - lr: 5.0000e-05\n",
      "Epoch 1752/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7891e-06 - rmse: 0.0017\n",
      "Epoch 1752: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8431e-06 - rmse: 0.0017 - val_loss: 1.2380e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1753/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2060e-06 - rmse: 0.0011\n",
      "Epoch 1753: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4322e-06 - rmse: 0.0016 - val_loss: 1.3585e-04 - val_rmse: 0.0117 - lr: 5.0000e-05\n",
      "Epoch 1754/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3546e-06 - rmse: 0.0018\n",
      "Epoch 1754: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8373e-06 - rmse: 0.0017 - val_loss: 1.1931e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1755/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1627e-06 - rmse: 0.0020\n",
      "Epoch 1755: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1445e-06 - rmse: 0.0015 - val_loss: 1.3174e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1756/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6346e-06 - rmse: 0.0016\n",
      "Epoch 1756: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5244e-06 - rmse: 0.0016 - val_loss: 1.3291e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1757/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2425e-06 - rmse: 0.0015\n",
      "Epoch 1757: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.7660e-06 - rmse: 0.0017 - val_loss: 1.1685e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1758/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8626e-06 - rmse: 0.0017\n",
      "Epoch 1758: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.7908e-06 - rmse: 0.0019 - val_loss: 1.2650e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1759/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3414e-06 - rmse: 0.0012\n",
      "Epoch 1759: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2375e-06 - rmse: 0.0015 - val_loss: 1.4097e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1760/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6726e-06 - rmse: 0.0016\n",
      "Epoch 1760: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.8166e-06 - rmse: 0.0017 - val_loss: 1.1518e-04 - val_rmse: 0.0107 - lr: 5.0000e-05\n",
      "Epoch 1761/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5154e-06 - rmse: 0.0023\n",
      "Epoch 1761: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.0734e-06 - rmse: 0.0020 - val_loss: 1.2557e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1762/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7944e-06 - rmse: 0.0013\n",
      "Epoch 1762: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.9640e-06 - rmse: 0.0017 - val_loss: 1.4248e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1763/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.6937e-06 - rmse: 0.0019\n",
      "Epoch 1763: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6543e-06 - rmse: 0.0019 - val_loss: 1.2407e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1764/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8932e-07 - rmse: 9.4304e-04\n",
      "Epoch 1764: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6846e-06 - rmse: 0.0016 - val_loss: 1.1606e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1765/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3254e-06 - rmse: 0.0021\n",
      "Epoch 1765: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.4632e-06 - rmse: 0.0019 - val_loss: 1.3079e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1766/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4000e-06 - rmse: 0.0012\n",
      "Epoch 1766: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4787e-06 - rmse: 0.0019 - val_loss: 1.4735e-04 - val_rmse: 0.0121 - lr: 5.0000e-05\n",
      "Epoch 1767/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7984e-06 - rmse: 0.0017\n",
      "Epoch 1767: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4792e-06 - rmse: 0.0019 - val_loss: 1.2152e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1768/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9790e-06 - rmse: 0.0020\n",
      "Epoch 1768: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5316e-06 - rmse: 0.0019 - val_loss: 1.2403e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1769/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2781e-06 - rmse: 0.0011\n",
      "Epoch 1769: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.9207e-06 - rmse: 0.0017 - val_loss: 1.3168e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1770/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8630e-06 - rmse: 0.0014\n",
      "Epoch 1770: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.0131e-06 - rmse: 0.0017 - val_loss: 1.4178e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1771/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7229e-06 - rmse: 0.0017\n",
      "Epoch 1771: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2688e-06 - rmse: 0.0018 - val_loss: 1.2755e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1772/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0208e-06 - rmse: 0.0017\n",
      "Epoch 1772: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6640e-06 - rmse: 0.0016 - val_loss: 1.2246e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1773/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6338e-06 - rmse: 0.0016\n",
      "Epoch 1773: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.1224e-06 - rmse: 0.0018 - val_loss: 1.3862e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1774/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8271e-06 - rmse: 0.0022\n",
      "Epoch 1774: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.7093e-06 - rmse: 0.0019 - val_loss: 1.4266e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1775/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2322e-06 - rmse: 0.0015\n",
      "Epoch 1775: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6919e-06 - rmse: 0.0016 - val_loss: 1.1239e-04 - val_rmse: 0.0106 - lr: 5.0000e-05\n",
      "Epoch 1776/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2520e-06 - rmse: 0.0023\n",
      "Epoch 1776: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.8828e-06 - rmse: 0.0022 - val_loss: 1.3080e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1777/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0970e-06 - rmse: 0.0014\n",
      "Epoch 1777: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2501e-06 - rmse: 0.0018 - val_loss: 1.4313e-04 - val_rmse: 0.0120 - lr: 5.0000e-05\n",
      "Epoch 1778/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3713e-06 - rmse: 0.0021\n",
      "Epoch 1778: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.5446e-06 - rmse: 0.0021 - val_loss: 1.1645e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1779/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5187e-06 - rmse: 0.0016\n",
      "Epoch 1779: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.2508e-06 - rmse: 0.0021 - val_loss: 1.0752e-04 - val_rmse: 0.0104 - lr: 5.0000e-05\n",
      "Epoch 1780/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8097e-06 - rmse: 0.0020\n",
      "Epoch 1780: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.6986e-06 - rmse: 0.0031 - val_loss: 1.4510e-04 - val_rmse: 0.0120 - lr: 5.0000e-05\n",
      "Epoch 1781/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1185e-06 - rmse: 0.0020\n",
      "Epoch 1781: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 6.0108e-06 - rmse: 0.0025 - val_loss: 1.4235e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1782/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8884e-06 - rmse: 0.0024\n",
      "Epoch 1782: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.7669e-06 - rmse: 0.0024 - val_loss: 1.0466e-04 - val_rmse: 0.0102 - lr: 5.0000e-05\n",
      "Epoch 1783/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6444e-06 - rmse: 0.0013\n",
      "Epoch 1783: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.4666e-06 - rmse: 0.0019 - val_loss: 1.2732e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1784/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2289e-06 - rmse: 0.0011\n",
      "Epoch 1784: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5678e-06 - rmse: 0.0019 - val_loss: 1.3182e-04 - val_rmse: 0.0115 - lr: 5.0000e-05\n",
      "Epoch 1785/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7294e-06 - rmse: 0.0013\n",
      "Epoch 1785: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4966e-06 - rmse: 0.0016 - val_loss: 1.3733e-04 - val_rmse: 0.0117 - lr: 5.0000e-05\n",
      "Epoch 1786/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.2127e-06 - rmse: 0.0018\n",
      "Epoch 1786: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.4872e-06 - rmse: 0.0016 - val_loss: 1.3397e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1787/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5324e-06 - rmse: 0.0012\n",
      "Epoch 1787: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.6967e-06 - rmse: 0.0016 - val_loss: 1.1968e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1788/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1279e-06 - rmse: 0.0025\n",
      "Epoch 1788: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.4594e-06 - rmse: 0.0019 - val_loss: 1.2184e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1789/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9735e-06 - rmse: 0.0014\n",
      "Epoch 1789: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9782e-06 - rmse: 0.0014 - val_loss: 1.3876e-04 - val_rmse: 0.0118 - lr: 5.0000e-05\n",
      "Epoch 1790/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1002e-06 - rmse: 0.0018\n",
      "Epoch 1790: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6155e-06 - rmse: 0.0016 - val_loss: 1.2685e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1791/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6742e-06 - rmse: 0.0013\n",
      "Epoch 1791: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5307e-06 - rmse: 0.0012 - val_loss: 1.2732e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1792/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1915e-07 - rmse: 8.4803e-04\n",
      "Epoch 1792: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7362e-06 - rmse: 0.0013 - val_loss: 1.3668e-04 - val_rmse: 0.0117 - lr: 5.0000e-05\n",
      "Epoch 1793/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9125e-06 - rmse: 0.0014\n",
      "Epoch 1793: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.2203e-06 - rmse: 0.0015 - val_loss: 1.2665e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1794/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6085e-06 - rmse: 0.0013\n",
      "Epoch 1794: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.5788e-06 - rmse: 0.0016 - val_loss: 1.1686e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1795/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9270e-06 - rmse: 0.0024\n",
      "Epoch 1795: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2698e-06 - rmse: 0.0018 - val_loss: 1.3627e-04 - val_rmse: 0.0117 - lr: 5.0000e-05\n",
      "Epoch 1796/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6391e-06 - rmse: 0.0013\n",
      "Epoch 1796: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.4523e-06 - rmse: 0.0016 - val_loss: 1.3432e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1797/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3301e-06 - rmse: 0.0012\n",
      "Epoch 1797: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0959e-06 - rmse: 0.0014 - val_loss: 1.2309e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1798/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0240e-06 - rmse: 0.0010\n",
      "Epoch 1798: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8687e-06 - rmse: 0.0014 - val_loss: 1.2258e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1799/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8873e-06 - rmse: 0.0014\n",
      "Epoch 1799: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0565e-06 - rmse: 0.0014 - val_loss: 1.2465e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1800/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6465e-06 - rmse: 0.0013\n",
      "Epoch 1800: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7489e-06 - rmse: 0.0013 - val_loss: 1.2737e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1801/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6966e-06 - rmse: 0.0013\n",
      "Epoch 1801: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6584e-06 - rmse: 0.0013 - val_loss: 1.2428e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1802/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0803e-06 - rmse: 0.0010\n",
      "Epoch 1802: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8524e-06 - rmse: 0.0014 - val_loss: 1.2330e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1803/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0074e-06 - rmse: 0.0010\n",
      "Epoch 1803: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5339e-06 - rmse: 0.0012 - val_loss: 1.2535e-04 - val_rmse: 0.0112 - lr: 5.0000e-05\n",
      "Epoch 1804/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6306e-06 - rmse: 0.0013\n",
      "Epoch 1804: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6322e-06 - rmse: 0.0013 - val_loss: 1.1909e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1805/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0688e-06 - rmse: 0.0014\n",
      "Epoch 1805: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8377e-06 - rmse: 0.0014 - val_loss: 1.3582e-04 - val_rmse: 0.0117 - lr: 5.0000e-05\n",
      "Epoch 1806/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5070e-06 - rmse: 0.0012\n",
      "Epoch 1806: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9581e-06 - rmse: 0.0014 - val_loss: 1.2879e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1807/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0619e-06 - rmse: 0.0010\n",
      "Epoch 1807: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9177e-06 - rmse: 0.0014 - val_loss: 1.2269e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1808/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7718e-06 - rmse: 0.0013\n",
      "Epoch 1808: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8338e-06 - rmse: 0.0014 - val_loss: 1.2810e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1809/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5689e-07 - rmse: 7.4625e-04\n",
      "Epoch 1809: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5482e-06 - rmse: 0.0012 - val_loss: 1.2764e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1810/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3286e-06 - rmse: 0.0015\n",
      "Epoch 1810: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6948e-06 - rmse: 0.0013 - val_loss: 1.2160e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1811/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5193e-06 - rmse: 0.0012\n",
      "Epoch 1811: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6847e-06 - rmse: 0.0013 - val_loss: 1.2375e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1812/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2085e-06 - rmse: 0.0011\n",
      "Epoch 1812: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5976e-06 - rmse: 0.0013 - val_loss: 1.3105e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1813/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8567e-06 - rmse: 0.0014\n",
      "Epoch 1813: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7790e-06 - rmse: 0.0013 - val_loss: 1.2390e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1814/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7712e-06 - rmse: 0.0013\n",
      "Epoch 1814: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6334e-06 - rmse: 0.0013 - val_loss: 1.3561e-04 - val_rmse: 0.0116 - lr: 5.0000e-05\n",
      "Epoch 1815/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1593e-06 - rmse: 0.0011\n",
      "Epoch 1815: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7882e-06 - rmse: 0.0013 - val_loss: 1.2378e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1816/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6221e-06 - rmse: 0.0013\n",
      "Epoch 1816: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1341e-06 - rmse: 0.0015 - val_loss: 1.1728e-04 - val_rmse: 0.0108 - lr: 5.0000e-05\n",
      "Epoch 1817/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1244e-06 - rmse: 0.0015\n",
      "Epoch 1817: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9205e-06 - rmse: 0.0014 - val_loss: 1.4057e-04 - val_rmse: 0.0119 - lr: 5.0000e-05\n",
      "Epoch 1818/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8836e-06 - rmse: 0.0014\n",
      "Epoch 1818: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.0286e-06 - rmse: 0.0014 - val_loss: 1.2819e-04 - val_rmse: 0.0113 - lr: 5.0000e-05\n",
      "Epoch 1819/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3032e-06 - rmse: 0.0011\n",
      "Epoch 1819: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.3080e-06 - rmse: 0.0015 - val_loss: 1.2344e-04 - val_rmse: 0.0111 - lr: 5.0000e-05\n",
      "Epoch 1820/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7158e-06 - rmse: 0.0013\n",
      "Epoch 1820: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.2719e-06 - rmse: 0.0023 - val_loss: 1.2036e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1821/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3440e-06 - rmse: 0.0015\n",
      "Epoch 1821: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.6001e-06 - rmse: 0.0028 - val_loss: 1.5513e-04 - val_rmse: 0.0125 - lr: 5.0000e-05\n",
      "Epoch 1822/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.9694e-06 - rmse: 0.0032\n",
      "Epoch 1822: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 7.3359e-06 - rmse: 0.0027 - val_loss: 1.3059e-04 - val_rmse: 0.0114 - lr: 5.0000e-05\n",
      "Epoch 1823/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4746e-06 - rmse: 0.0012\n",
      "Epoch 1823: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0889e-05 - rmse: 0.0033 - val_loss: 1.2199e-04 - val_rmse: 0.0110 - lr: 5.0000e-05\n",
      "Epoch 1824/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7293e-05 - rmse: 0.0042\n",
      "Epoch 1824: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2648e-05 - rmse: 0.0036 - val_loss: 1.1881e-04 - val_rmse: 0.0109 - lr: 5.0000e-05\n",
      "Epoch 1825/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6001e-06 - rmse: 0.0016\n",
      "Epoch 1825: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 1825: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.0569e-06 - rmse: 0.0027 - val_loss: 1.5640e-04 - val_rmse: 0.0125 - lr: 5.0000e-05\n",
      "Epoch 1826/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2320e-06 - rmse: 0.0027\n",
      "Epoch 1826: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5.1199e-06 - rmse: 0.0023 - val_loss: 1.2050e-04 - val_rmse: 0.0110 - lr: 2.5000e-05\n",
      "Epoch 1827/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2890e-05 - rmse: 0.0036\n",
      "Epoch 1827: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.1673e-06 - rmse: 0.0025 - val_loss: 1.3668e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1828/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7845e-06 - rmse: 0.0026\n",
      "Epoch 1828: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 4.5206e-06 - rmse: 0.0021 - val_loss: 1.0880e-04 - val_rmse: 0.0104 - lr: 2.5000e-05\n",
      "Epoch 1829/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0840e-06 - rmse: 0.0025\n",
      "Epoch 1829: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4.1326e-06 - rmse: 0.0020 - val_loss: 1.4230e-04 - val_rmse: 0.0119 - lr: 2.5000e-05\n",
      "Epoch 1830/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0257e-06 - rmse: 0.0022\n",
      "Epoch 1830: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.2496e-06 - rmse: 0.0018 - val_loss: 1.2092e-04 - val_rmse: 0.0110 - lr: 2.5000e-05\n",
      "Epoch 1831/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8415e-06 - rmse: 0.0017\n",
      "Epoch 1831: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.6812e-06 - rmse: 0.0016 - val_loss: 1.3102e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1832/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8445e-06 - rmse: 0.0017\n",
      "Epoch 1832: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1802e-06 - rmse: 0.0015 - val_loss: 1.2596e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1833/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8336e-06 - rmse: 0.0017\n",
      "Epoch 1833: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1644e-06 - rmse: 0.0015 - val_loss: 1.2475e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1834/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3734e-06 - rmse: 0.0012\n",
      "Epoch 1834: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7391e-06 - rmse: 0.0013 - val_loss: 1.3077e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1835/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8781e-07 - rmse: 9.9389e-04\n",
      "Epoch 1835: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6913e-06 - rmse: 0.0013 - val_loss: 1.2708e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1836/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9404e-06 - rmse: 0.0014\n",
      "Epoch 1836: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6874e-06 - rmse: 0.0013 - val_loss: 1.2921e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1837/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0151e-06 - rmse: 0.0014\n",
      "Epoch 1837: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6274e-06 - rmse: 0.0013 - val_loss: 1.2367e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1838/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8750e-06 - rmse: 0.0017\n",
      "Epoch 1838: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5126e-06 - rmse: 0.0012 - val_loss: 1.2836e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1839/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0725e-06 - rmse: 0.0014\n",
      "Epoch 1839: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3432e-06 - rmse: 0.0012 - val_loss: 1.2857e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1840/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2757e-06 - rmse: 0.0011\n",
      "Epoch 1840: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3558e-06 - rmse: 0.0012 - val_loss: 1.3012e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1841/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7033e-06 - rmse: 0.0013\n",
      "Epoch 1841: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3494e-06 - rmse: 0.0012 - val_loss: 1.2667e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1842/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4805e-07 - rmse: 6.6936e-04\n",
      "Epoch 1842: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3458e-06 - rmse: 0.0012 - val_loss: 1.3070e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1843/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9739e-07 - rmse: 8.9297e-04\n",
      "Epoch 1843: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3596e-06 - rmse: 0.0012 - val_loss: 1.2488e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1844/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6580e-07 - rmse: 8.1597e-04\n",
      "Epoch 1844: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3593e-06 - rmse: 0.0012 - val_loss: 1.3154e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1845/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6662e-07 - rmse: 9.8317e-04\n",
      "Epoch 1845: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4903e-06 - rmse: 0.0012 - val_loss: 1.2248e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1846/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3708e-07 - rmse: 8.5853e-04\n",
      "Epoch 1846: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6008e-06 - rmse: 0.0013 - val_loss: 1.3291e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1847/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1187e-06 - rmse: 0.0011\n",
      "Epoch 1847: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5107e-06 - rmse: 0.0012 - val_loss: 1.2794e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1848/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8216e-07 - rmse: 9.9104e-04\n",
      "Epoch 1848: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5558e-06 - rmse: 0.0012 - val_loss: 1.3086e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1849/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0873e-06 - rmse: 0.0010\n",
      "Epoch 1849: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4846e-06 - rmse: 0.0012 - val_loss: 1.2704e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1850/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9915e-07 - rmse: 7.7405e-04\n",
      "Epoch 1850: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3426e-06 - rmse: 0.0012 - val_loss: 1.2824e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1851/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6578e-06 - rmse: 0.0013\n",
      "Epoch 1851: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3629e-06 - rmse: 0.0012 - val_loss: 1.2750e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1852/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9212e-06 - rmse: 0.0014\n",
      "Epoch 1852: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3742e-06 - rmse: 0.0012 - val_loss: 1.3008e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1853/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5918e-06 - rmse: 0.0013\n",
      "Epoch 1853: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3841e-06 - rmse: 0.0012 - val_loss: 1.2542e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1854/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0609e-07 - rmse: 9.5188e-04\n",
      "Epoch 1854: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3168e-06 - rmse: 0.0011 - val_loss: 1.2904e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1855/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3894e-07 - rmse: 7.9934e-04\n",
      "Epoch 1855: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3209e-06 - rmse: 0.0011 - val_loss: 1.2535e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1856/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0058e-06 - rmse: 0.0010\n",
      "Epoch 1856: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3946e-06 - rmse: 0.0012 - val_loss: 1.3117e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1857/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6005e-07 - rmse: 9.2739e-04\n",
      "Epoch 1857: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6091e-06 - rmse: 0.0013 - val_loss: 1.2371e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1858/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4732e-07 - rmse: 9.7330e-04\n",
      "Epoch 1858: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7001e-06 - rmse: 0.0013 - val_loss: 1.3157e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1859/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0314e-06 - rmse: 0.0010\n",
      "Epoch 1859: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4218e-06 - rmse: 0.0012 - val_loss: 1.2548e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1860/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1622e-06 - rmse: 0.0011\n",
      "Epoch 1860: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3988e-06 - rmse: 0.0012 - val_loss: 1.3058e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1861/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6011e-06 - rmse: 0.0013\n",
      "Epoch 1861: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3529e-06 - rmse: 0.0012 - val_loss: 1.2496e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1862/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2570e-06 - rmse: 0.0011\n",
      "Epoch 1862: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4174e-06 - rmse: 0.0012 - val_loss: 1.2982e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1863/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1695e-06 - rmse: 0.0015\n",
      "Epoch 1863: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2618e-06 - rmse: 0.0011 - val_loss: 1.2840e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1864/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5959e-06 - rmse: 0.0013\n",
      "Epoch 1864: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3257e-06 - rmse: 0.0012 - val_loss: 1.3320e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1865/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0316e-06 - rmse: 0.0014\n",
      "Epoch 1865: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4805e-06 - rmse: 0.0012 - val_loss: 1.2515e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1866/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5995e-06 - rmse: 0.0016\n",
      "Epoch 1866: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4070e-06 - rmse: 0.0012 - val_loss: 1.3060e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1867/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5941e-06 - rmse: 0.0016\n",
      "Epoch 1867: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3069e-06 - rmse: 0.0011 - val_loss: 1.2452e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1868/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3691e-07 - rmse: 9.1483e-04\n",
      "Epoch 1868: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3166e-06 - rmse: 0.0011 - val_loss: 1.3189e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1869/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0831e-06 - rmse: 0.0010\n",
      "Epoch 1869: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3454e-06 - rmse: 0.0012 - val_loss: 1.2570e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1870/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1243e-06 - rmse: 0.0011\n",
      "Epoch 1870: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3258e-06 - rmse: 0.0012 - val_loss: 1.2973e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1871/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4676e-06 - rmse: 0.0012\n",
      "Epoch 1871: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2864e-06 - rmse: 0.0011 - val_loss: 1.2713e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1872/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5657e-06 - rmse: 0.0013\n",
      "Epoch 1872: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3914e-06 - rmse: 0.0012 - val_loss: 1.3175e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1873/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0017e-06 - rmse: 0.0010\n",
      "Epoch 1873: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4064e-06 - rmse: 0.0012 - val_loss: 1.2948e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1874/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8813e-06 - rmse: 0.0014\n",
      "Epoch 1874: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4011e-06 - rmse: 0.0012 - val_loss: 1.2651e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1875/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0261e-06 - rmse: 0.0010\n",
      "Epoch 1875: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6200e-06 - rmse: 0.0013 - val_loss: 1.3365e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1876/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7741e-06 - rmse: 0.0013\n",
      "Epoch 1876: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4974e-06 - rmse: 0.0012 - val_loss: 1.2436e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1877/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1335e-06 - rmse: 0.0011\n",
      "Epoch 1877: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6364e-06 - rmse: 0.0013 - val_loss: 1.3590e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 1878/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0734e-06 - rmse: 0.0014\n",
      "Epoch 1878: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8242e-06 - rmse: 0.0014 - val_loss: 1.1879e-04 - val_rmse: 0.0109 - lr: 2.5000e-05\n",
      "Epoch 1879/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3094e-06 - rmse: 0.0023\n",
      "Epoch 1879: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.5384e-06 - rmse: 0.0016 - val_loss: 1.3862e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 1880/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0834e-06 - rmse: 0.0018\n",
      "Epoch 1880: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8315e-06 - rmse: 0.0014 - val_loss: 1.2271e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1881/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8362e-06 - rmse: 0.0014\n",
      "Epoch 1881: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.9861e-06 - rmse: 0.0014 - val_loss: 1.2822e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1882/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0593e-06 - rmse: 0.0010\n",
      "Epoch 1882: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6529e-06 - rmse: 0.0013 - val_loss: 1.2991e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1883/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6374e-07 - rmse: 9.2937e-04\n",
      "Epoch 1883: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5274e-06 - rmse: 0.0012 - val_loss: 1.2762e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1884/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9247e-06 - rmse: 0.0014\n",
      "Epoch 1884: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5116e-06 - rmse: 0.0012 - val_loss: 1.3360e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1885/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6592e-06 - rmse: 0.0013\n",
      "Epoch 1885: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4833e-06 - rmse: 0.0012 - val_loss: 1.2614e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1886/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7000e-06 - rmse: 0.0013\n",
      "Epoch 1886: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4535e-06 - rmse: 0.0012 - val_loss: 1.2943e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1887/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1359e-07 - rmse: 8.4474e-04\n",
      "Epoch 1887: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2621e-06 - rmse: 0.0011 - val_loss: 1.2803e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1888/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4779e-06 - rmse: 0.0012\n",
      "Epoch 1888: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2656e-06 - rmse: 0.0011 - val_loss: 1.3065e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1889/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7022e-07 - rmse: 6.0846e-04\n",
      "Epoch 1889: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2550e-06 - rmse: 0.0011 - val_loss: 1.2563e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1890/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1430e-07 - rmse: 9.0238e-04\n",
      "Epoch 1890: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3665e-06 - rmse: 0.0012 - val_loss: 1.3097e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1891/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7948e-06 - rmse: 0.0013\n",
      "Epoch 1891: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4661e-06 - rmse: 0.0012 - val_loss: 1.3051e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1892/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3913e-06 - rmse: 0.0012\n",
      "Epoch 1892: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4128e-06 - rmse: 0.0012 - val_loss: 1.2422e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1893/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4147e-07 - rmse: 8.0092e-04\n",
      "Epoch 1893: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3889e-06 - rmse: 0.0012 - val_loss: 1.3592e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 1894/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5170e-06 - rmse: 0.0012\n",
      "Epoch 1894: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6789e-06 - rmse: 0.0013 - val_loss: 1.2367e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1895/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3728e-06 - rmse: 0.0012\n",
      "Epoch 1895: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3352e-06 - rmse: 0.0012 - val_loss: 1.3177e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1896/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2101e-06 - rmse: 0.0011\n",
      "Epoch 1896: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3316e-06 - rmse: 0.0012 - val_loss: 1.2718e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1897/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6424e-06 - rmse: 0.0013\n",
      "Epoch 1897: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2348e-06 - rmse: 0.0011 - val_loss: 1.3149e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1898/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3242e-06 - rmse: 0.0012\n",
      "Epoch 1898: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3034e-06 - rmse: 0.0011 - val_loss: 1.2534e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1899/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3939e-06 - rmse: 0.0012\n",
      "Epoch 1899: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3057e-06 - rmse: 0.0011 - val_loss: 1.2897e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1900/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5924e-06 - rmse: 0.0013\n",
      "Epoch 1900: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3691e-06 - rmse: 0.0012 - val_loss: 1.3011e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1901/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1817e-07 - rmse: 7.1984e-04\n",
      "Epoch 1901: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2500e-06 - rmse: 0.0011 - val_loss: 1.2499e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1902/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1065e-06 - rmse: 0.0015\n",
      "Epoch 1902: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3776e-06 - rmse: 0.0012 - val_loss: 1.3138e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1903/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1078e-06 - rmse: 0.0011\n",
      "Epoch 1903: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4671e-06 - rmse: 0.0012 - val_loss: 1.2769e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1904/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1916e-06 - rmse: 0.0015\n",
      "Epoch 1904: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3670e-06 - rmse: 0.0012 - val_loss: 1.2695e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1905/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4457e-06 - rmse: 0.0012\n",
      "Epoch 1905: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4244e-06 - rmse: 0.0012 - val_loss: 1.3044e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1906/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0393e-06 - rmse: 0.0010\n",
      "Epoch 1906: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3095e-06 - rmse: 0.0011 - val_loss: 1.2791e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1907/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0876e-06 - rmse: 0.0010\n",
      "Epoch 1907: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3639e-06 - rmse: 0.0012 - val_loss: 1.3116e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1908/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7555e-06 - rmse: 0.0013\n",
      "Epoch 1908: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3352e-06 - rmse: 0.0012 - val_loss: 1.2946e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1909/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8438e-07 - rmse: 6.9598e-04\n",
      "Epoch 1909: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3619e-06 - rmse: 0.0012 - val_loss: 1.2987e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1910/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9323e-07 - rmse: 9.4511e-04\n",
      "Epoch 1910: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2318e-06 - rmse: 0.0011 - val_loss: 1.2553e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1911/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5784e-07 - rmse: 8.7054e-04\n",
      "Epoch 1911: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2594e-06 - rmse: 0.0011 - val_loss: 1.3213e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1912/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8498e-06 - rmse: 0.0014\n",
      "Epoch 1912: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2388e-06 - rmse: 0.0011 - val_loss: 1.2717e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1913/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5486e-06 - rmse: 0.0016\n",
      "Epoch 1913: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2826e-06 - rmse: 0.0011 - val_loss: 1.3039e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1914/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1099e-06 - rmse: 0.0015\n",
      "Epoch 1914: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2479e-06 - rmse: 0.0011 - val_loss: 1.2898e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1915/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1449e-06 - rmse: 0.0011\n",
      "Epoch 1915: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1883e-06 - rmse: 0.0011 - val_loss: 1.3003e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1916/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0959e-06 - rmse: 0.0010\n",
      "Epoch 1916: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2188e-06 - rmse: 0.0011 - val_loss: 1.2516e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1917/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1285e-06 - rmse: 0.0011\n",
      "Epoch 1917: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2179e-06 - rmse: 0.0011 - val_loss: 1.3286e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1918/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2668e-06 - rmse: 0.0011\n",
      "Epoch 1918: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2817e-06 - rmse: 0.0011 - val_loss: 1.2362e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1919/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9599e-06 - rmse: 0.0014\n",
      "Epoch 1919: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5144e-06 - rmse: 0.0012 - val_loss: 1.3147e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1920/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6931e-06 - rmse: 0.0013\n",
      "Epoch 1920: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4622e-06 - rmse: 0.0012 - val_loss: 1.3313e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1921/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1736e-06 - rmse: 0.0015\n",
      "Epoch 1921: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6541e-06 - rmse: 0.0013 - val_loss: 1.2068e-04 - val_rmse: 0.0110 - lr: 2.5000e-05\n",
      "Epoch 1922/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8214e-06 - rmse: 0.0020\n",
      "Epoch 1922: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6223e-06 - rmse: 0.0013 - val_loss: 1.3463e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1923/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1978e-06 - rmse: 0.0015\n",
      "Epoch 1923: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8303e-06 - rmse: 0.0014 - val_loss: 1.2729e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1924/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0599e-06 - rmse: 0.0010\n",
      "Epoch 1924: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4895e-06 - rmse: 0.0012 - val_loss: 1.2699e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1925/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6462e-07 - rmse: 8.1525e-04\n",
      "Epoch 1925: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3691e-06 - rmse: 0.0012 - val_loss: 1.3410e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1926/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1644e-06 - rmse: 0.0011\n",
      "Epoch 1926: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3051e-06 - rmse: 0.0011 - val_loss: 1.2876e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1927/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3934e-06 - rmse: 0.0012\n",
      "Epoch 1927: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3185e-06 - rmse: 0.0011 - val_loss: 1.3089e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1928/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8382e-06 - rmse: 0.0014\n",
      "Epoch 1928: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2775e-06 - rmse: 0.0011 - val_loss: 1.3340e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1929/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1549e-06 - rmse: 0.0011\n",
      "Epoch 1929: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2954e-06 - rmse: 0.0011 - val_loss: 1.2568e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1930/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8306e-06 - rmse: 0.0014\n",
      "Epoch 1930: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4290e-06 - rmse: 0.0012 - val_loss: 1.3712e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 1931/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4011e-06 - rmse: 0.0012\n",
      "Epoch 1931: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5770e-06 - rmse: 0.0013 - val_loss: 1.2701e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1932/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5749e-07 - rmse: 9.2601e-04\n",
      "Epoch 1932: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4685e-06 - rmse: 0.0012 - val_loss: 1.2775e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1933/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8094e-06 - rmse: 0.0013\n",
      "Epoch 1933: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2256e-06 - rmse: 0.0011 - val_loss: 1.3250e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1934/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6044e-06 - rmse: 0.0013\n",
      "Epoch 1934: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3573e-06 - rmse: 0.0012 - val_loss: 1.2644e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1935/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1417e-07 - rmse: 9.5612e-04\n",
      "Epoch 1935: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2895e-06 - rmse: 0.0011 - val_loss: 1.3072e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1936/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1900e-06 - rmse: 0.0011\n",
      "Epoch 1936: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2616e-06 - rmse: 0.0011 - val_loss: 1.3280e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1937/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0367e-06 - rmse: 0.0014\n",
      "Epoch 1937: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2494e-06 - rmse: 0.0011 - val_loss: 1.2740e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1938/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6098e-06 - rmse: 0.0013\n",
      "Epoch 1938: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1893e-06 - rmse: 0.0011 - val_loss: 1.3322e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1939/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5528e-06 - rmse: 0.0012\n",
      "Epoch 1939: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2387e-06 - rmse: 0.0011 - val_loss: 1.2818e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1940/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5509e-06 - rmse: 0.0012\n",
      "Epoch 1940: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2991e-06 - rmse: 0.0011 - val_loss: 1.3013e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1941/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0992e-06 - rmse: 0.0014\n",
      "Epoch 1941: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2511e-06 - rmse: 0.0011 - val_loss: 1.3310e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1942/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4399e-06 - rmse: 0.0016\n",
      "Epoch 1942: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2713e-06 - rmse: 0.0011 - val_loss: 1.3113e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1943/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0159e-06 - rmse: 0.0010\n",
      "Epoch 1943: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2132e-06 - rmse: 0.0011 - val_loss: 1.2993e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1944/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6188e-06 - rmse: 0.0013\n",
      "Epoch 1944: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1757e-06 - rmse: 0.0011 - val_loss: 1.3419e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1945/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4677e-06 - rmse: 0.0012\n",
      "Epoch 1945: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3112e-06 - rmse: 0.0011 - val_loss: 1.2729e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1946/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0515e-06 - rmse: 0.0010\n",
      "Epoch 1946: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3721e-06 - rmse: 0.0012 - val_loss: 1.2788e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1947/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7631e-06 - rmse: 0.0013\n",
      "Epoch 1947: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6055e-06 - rmse: 0.0013 - val_loss: 1.3780e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 1948/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1967e-07 - rmse: 9.5900e-04\n",
      "Epoch 1948: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7046e-06 - rmse: 0.0013 - val_loss: 1.2419e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1949/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3829e-06 - rmse: 0.0015\n",
      "Epoch 1949: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5891e-06 - rmse: 0.0013 - val_loss: 1.3588e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 1950/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1930e-07 - rmse: 8.4812e-04\n",
      "Epoch 1950: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4031e-06 - rmse: 0.0012 - val_loss: 1.3336e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1951/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0717e-07 - rmse: 9.5246e-04\n",
      "Epoch 1951: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5744e-06 - rmse: 0.0013 - val_loss: 1.2719e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1952/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2928e-06 - rmse: 0.0011\n",
      "Epoch 1952: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8661e-06 - rmse: 0.0014 - val_loss: 1.4080e-04 - val_rmse: 0.0119 - lr: 2.5000e-05\n",
      "Epoch 1953/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0840e-06 - rmse: 0.0014\n",
      "Epoch 1953: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1197e-06 - rmse: 0.0015 - val_loss: 1.2873e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1954/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3624e-07 - rmse: 7.3228e-04\n",
      "Epoch 1954: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4209e-06 - rmse: 0.0012 - val_loss: 1.2672e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1955/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4632e-06 - rmse: 0.0016\n",
      "Epoch 1955: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5300e-06 - rmse: 0.0012 - val_loss: 1.3448e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1956/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2681e-07 - rmse: 9.0929e-04\n",
      "Epoch 1956: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3855e-06 - rmse: 0.0012 - val_loss: 1.3037e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1957/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0863e-06 - rmse: 0.0010\n",
      "Epoch 1957: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2903e-06 - rmse: 0.0011 - val_loss: 1.2823e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1958/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0981e-07 - rmse: 9.5384e-04\n",
      "Epoch 1958: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2808e-06 - rmse: 0.0011 - val_loss: 1.3421e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1959/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0355e-06 - rmse: 0.0010\n",
      "Epoch 1959: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3112e-06 - rmse: 0.0011 - val_loss: 1.2464e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1960/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3425e-06 - rmse: 0.0015\n",
      "Epoch 1960: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7837e-06 - rmse: 0.0013 - val_loss: 1.2897e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1961/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7694e-06 - rmse: 0.0013\n",
      "Epoch 1961: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.6537e-06 - rmse: 0.0013 - val_loss: 1.3925e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 1962/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0403e-06 - rmse: 0.0017\n",
      "Epoch 1962: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1461e-06 - rmse: 0.0015 - val_loss: 1.2630e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1963/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3686e-06 - rmse: 0.0012\n",
      "Epoch 1963: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4434e-06 - rmse: 0.0012 - val_loss: 1.2699e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1964/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9381e-07 - rmse: 8.9096e-04\n",
      "Epoch 1964: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3693e-06 - rmse: 0.0012 - val_loss: 1.3370e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1965/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.6184e-06 - rmse: 0.0016\n",
      "Epoch 1965: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4015e-06 - rmse: 0.0012 - val_loss: 1.3695e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 1966/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1645e-06 - rmse: 0.0011\n",
      "Epoch 1966: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2765e-06 - rmse: 0.0011 - val_loss: 1.2928e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1967/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9325e-07 - rmse: 8.3261e-04\n",
      "Epoch 1967: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2230e-06 - rmse: 0.0011 - val_loss: 1.3303e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1968/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3962e-07 - rmse: 9.1631e-04\n",
      "Epoch 1968: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2329e-06 - rmse: 0.0011 - val_loss: 1.2927e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1969/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0191e-06 - rmse: 0.0014\n",
      "Epoch 1969: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2681e-06 - rmse: 0.0011 - val_loss: 1.3207e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1970/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0810e-06 - rmse: 0.0010\n",
      "Epoch 1970: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3760e-06 - rmse: 0.0012 - val_loss: 1.3241e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1971/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2277e-07 - rmse: 9.0706e-04\n",
      "Epoch 1971: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3460e-06 - rmse: 0.0012 - val_loss: 1.2320e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1972/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4617e-06 - rmse: 0.0012\n",
      "Epoch 1972: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4705e-06 - rmse: 0.0012 - val_loss: 1.3846e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 1973/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1309e-06 - rmse: 0.0011\n",
      "Epoch 1973: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8410e-06 - rmse: 0.0014 - val_loss: 1.3217e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1974/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4533e-06 - rmse: 0.0012\n",
      "Epoch 1974: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3614e-06 - rmse: 0.0012 - val_loss: 1.2544e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1975/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7119e-07 - rmse: 9.3337e-04\n",
      "Epoch 1975: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3380e-06 - rmse: 0.0012 - val_loss: 1.3232e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1976/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4034e-06 - rmse: 0.0012\n",
      "Epoch 1976: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2135e-06 - rmse: 0.0011 - val_loss: 1.2595e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1977/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6171e-06 - rmse: 0.0013\n",
      "Epoch 1977: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4528e-06 - rmse: 0.0012 - val_loss: 1.3475e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1978/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0610e-06 - rmse: 0.0014\n",
      "Epoch 1978: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7932e-06 - rmse: 0.0013 - val_loss: 1.3314e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1979/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4348e-06 - rmse: 0.0012\n",
      "Epoch 1979: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4979e-06 - rmse: 0.0012 - val_loss: 1.2675e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1980/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5989e-07 - rmse: 7.4826e-04\n",
      "Epoch 1980: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3853e-06 - rmse: 0.0012 - val_loss: 1.4382e-04 - val_rmse: 0.0120 - lr: 2.5000e-05\n",
      "Epoch 1981/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5823e-06 - rmse: 0.0013\n",
      "Epoch 1981: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4423e-06 - rmse: 0.0012 - val_loss: 1.2411e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 1982/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8948e-06 - rmse: 0.0014\n",
      "Epoch 1982: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9286e-06 - rmse: 0.0014 - val_loss: 1.3276e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1983/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3537e-07 - rmse: 8.5753e-04\n",
      "Epoch 1983: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5125e-06 - rmse: 0.0012 - val_loss: 1.3758e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 1984/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7331e-06 - rmse: 0.0013\n",
      "Epoch 1984: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2991e-06 - rmse: 0.0011 - val_loss: 1.2526e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1985/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1656e-07 - rmse: 9.0364e-04\n",
      "Epoch 1985: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4239e-06 - rmse: 0.0012 - val_loss: 1.4086e-04 - val_rmse: 0.0119 - lr: 2.5000e-05\n",
      "Epoch 1986/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2313e-06 - rmse: 0.0015\n",
      "Epoch 1986: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8126e-06 - rmse: 0.0013 - val_loss: 1.2632e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1987/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1043e-06 - rmse: 0.0015\n",
      "Epoch 1987: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9180e-06 - rmse: 0.0014 - val_loss: 1.2824e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1988/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4654e-06 - rmse: 0.0012\n",
      "Epoch 1988: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3516e-06 - rmse: 0.0012 - val_loss: 1.3365e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1989/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5038e-06 - rmse: 0.0012\n",
      "Epoch 1989: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3020e-06 - rmse: 0.0011 - val_loss: 1.2592e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1990/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3281e-07 - rmse: 9.6582e-04\n",
      "Epoch 1990: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2943e-06 - rmse: 0.0011 - val_loss: 1.3224e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1991/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8643e-06 - rmse: 0.0014\n",
      "Epoch 1991: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1983e-06 - rmse: 0.0011 - val_loss: 1.2993e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 1992/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8809e-06 - rmse: 0.0017\n",
      "Epoch 1992: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3631e-06 - rmse: 0.0012 - val_loss: 1.2628e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1993/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0432e-06 - rmse: 0.0010\n",
      "Epoch 1993: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3260e-06 - rmse: 0.0012 - val_loss: 1.3289e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 1994/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0388e-06 - rmse: 0.0010\n",
      "Epoch 1994: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2205e-06 - rmse: 0.0011 - val_loss: 1.2810e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 1995/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4143e-06 - rmse: 0.0012\n",
      "Epoch 1995: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3316e-06 - rmse: 0.0012 - val_loss: 1.2951e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1996/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0071e-06 - rmse: 0.0010\n",
      "Epoch 1996: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3132e-06 - rmse: 0.0011 - val_loss: 1.3875e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 1997/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4048e-06 - rmse: 0.0012\n",
      "Epoch 1997: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2850e-06 - rmse: 0.0011 - val_loss: 1.2529e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 1998/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6291e-06 - rmse: 0.0013\n",
      "Epoch 1998: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3304e-06 - rmse: 0.0012 - val_loss: 1.3514e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 1999/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6791e-06 - rmse: 0.0013\n",
      "Epoch 1999: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3164e-06 - rmse: 0.0011 - val_loss: 1.3256e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 2000/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8727e-06 - rmse: 0.0014\n",
      "Epoch 2000: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2950e-06 - rmse: 0.0011 - val_loss: 1.2643e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 2001/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2218e-07 - rmse: 8.4981e-04\n",
      "Epoch 2001: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5508e-06 - rmse: 0.0012 - val_loss: 1.4014e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 2002/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8051e-06 - rmse: 0.0013\n",
      "Epoch 2002: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3721e-06 - rmse: 0.0012 - val_loss: 1.2674e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 2003/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3452e-06 - rmse: 0.0012\n",
      "Epoch 2003: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5003e-06 - rmse: 0.0012 - val_loss: 1.2750e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 2004/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2578e-06 - rmse: 0.0011\n",
      "Epoch 2004: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3794e-06 - rmse: 0.0012 - val_loss: 1.3632e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 2005/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1063e-06 - rmse: 0.0015\n",
      "Epoch 2005: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4585e-06 - rmse: 0.0012 - val_loss: 1.2787e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 2006/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5995e-06 - rmse: 0.0013\n",
      "Epoch 2006: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4174e-06 - rmse: 0.0012 - val_loss: 1.2229e-04 - val_rmse: 0.0111 - lr: 2.5000e-05\n",
      "Epoch 2007/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4100e-06 - rmse: 0.0012\n",
      "Epoch 2007: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4319e-06 - rmse: 0.0012 - val_loss: 1.3203e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 2008/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2756e-07 - rmse: 9.6310e-04\n",
      "Epoch 2008: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1597e-06 - rmse: 0.0011 - val_loss: 1.3192e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 2009/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6203e-07 - rmse: 9.8083e-04\n",
      "Epoch 2009: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1659e-06 - rmse: 0.0011 - val_loss: 1.2823e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 2010/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2403e-06 - rmse: 0.0011\n",
      "Epoch 2010: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1224e-06 - rmse: 0.0011 - val_loss: 1.3013e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 2011/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5248e-07 - rmse: 8.0776e-04\n",
      "Epoch 2011: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2089e-06 - rmse: 0.0011 - val_loss: 1.3874e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 2012/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3673e-06 - rmse: 0.0015\n",
      "Epoch 2012: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5016e-06 - rmse: 0.0012 - val_loss: 1.3000e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 2013/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3618e-07 - rmse: 9.1443e-04\n",
      "Epoch 2013: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3682e-06 - rmse: 0.0012 - val_loss: 1.2755e-04 - val_rmse: 0.0113 - lr: 2.5000e-05\n",
      "Epoch 2014/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1201e-06 - rmse: 0.0015\n",
      "Epoch 2014: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5889e-06 - rmse: 0.0013 - val_loss: 1.4515e-04 - val_rmse: 0.0120 - lr: 2.5000e-05\n",
      "Epoch 2015/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.7179e-06 - rmse: 0.0019\n",
      "Epoch 2015: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1594e-06 - rmse: 0.0015 - val_loss: 1.3606e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n",
      "Epoch 2016/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9117e-06 - rmse: 0.0014\n",
      "Epoch 2016: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2309e-06 - rmse: 0.0011 - val_loss: 1.2654e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 2017/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5238e-06 - rmse: 0.0012\n",
      "Epoch 2017: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.7235e-06 - rmse: 0.0013 - val_loss: 1.3284e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 2018/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2265e-06 - rmse: 0.0011\n",
      "Epoch 2018: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3772e-06 - rmse: 0.0012 - val_loss: 1.3898e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 2019/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1446e-07 - rmse: 9.0247e-04\n",
      "Epoch 2019: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2708e-06 - rmse: 0.0011 - val_loss: 1.2498e-04 - val_rmse: 0.0112 - lr: 2.5000e-05\n",
      "Epoch 2020/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0936e-06 - rmse: 0.0010\n",
      "Epoch 2020: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5816e-06 - rmse: 0.0013 - val_loss: 1.3924e-04 - val_rmse: 0.0118 - lr: 2.5000e-05\n",
      "Epoch 2021/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1806e-06 - rmse: 0.0011\n",
      "Epoch 2021: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3345e-06 - rmse: 0.0012 - val_loss: 1.3278e-04 - val_rmse: 0.0115 - lr: 2.5000e-05\n",
      "Epoch 2022/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6392e-07 - rmse: 8.7403e-04\n",
      "Epoch 2022: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1896e-06 - rmse: 0.0011 - val_loss: 1.3074e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 2023/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0071e-06 - rmse: 0.0010\n",
      "Epoch 2023: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1538e-06 - rmse: 0.0011 - val_loss: 1.3755e-04 - val_rmse: 0.0117 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2024/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8834e-07 - rmse: 6.9881e-04\n",
      "Epoch 2024: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1232e-06 - rmse: 0.0011 - val_loss: 1.3042e-04 - val_rmse: 0.0114 - lr: 2.5000e-05\n",
      "Epoch 2025/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5607e-07 - rmse: 6.7533e-04\n",
      "Epoch 2025: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 2025: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1101e-06 - rmse: 0.0011 - val_loss: 1.3441e-04 - val_rmse: 0.0116 - lr: 2.5000e-05\n",
      "Epoch 2026/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2926e-06 - rmse: 0.0015\n",
      "Epoch 2026: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1356e-06 - rmse: 0.0011 - val_loss: 1.3239e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2027/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9788e-07 - rmse: 9.4756e-04\n",
      "Epoch 2027: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0900e-06 - rmse: 0.0010 - val_loss: 1.3343e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2028/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0307e-06 - rmse: 0.0010\n",
      "Epoch 2028: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0679e-06 - rmse: 0.0010 - val_loss: 1.3285e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2029/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0571e-06 - rmse: 0.0010\n",
      "Epoch 2029: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0711e-06 - rmse: 0.0010 - val_loss: 1.3043e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2030/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4311e-07 - rmse: 9.1821e-04\n",
      "Epoch 2030: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1014e-06 - rmse: 0.0010 - val_loss: 1.3224e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2031/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1415e-06 - rmse: 0.0011\n",
      "Epoch 2031: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0722e-06 - rmse: 0.0010 - val_loss: 1.3313e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2032/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1990e-07 - rmse: 9.5912e-04\n",
      "Epoch 2032: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0659e-06 - rmse: 0.0010 - val_loss: 1.3138e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2033/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2190e-06 - rmse: 0.0011\n",
      "Epoch 2033: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0115e-06 - rmse: 0.0010 - val_loss: 1.3407e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2034/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4659e-07 - rmse: 9.2010e-04\n",
      "Epoch 2034: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1269e-06 - rmse: 0.0011 - val_loss: 1.2921e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2035/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1107e-06 - rmse: 0.0011\n",
      "Epoch 2035: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2453e-06 - rmse: 0.0011 - val_loss: 1.3449e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2036/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5825e-07 - rmse: 7.4716e-04\n",
      "Epoch 2036: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1258e-06 - rmse: 0.0011 - val_loss: 1.3135e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2037/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2932e-06 - rmse: 0.0011\n",
      "Epoch 2037: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0524e-06 - rmse: 0.0010 - val_loss: 1.3223e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2038/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0992e-06 - rmse: 0.0010\n",
      "Epoch 2038: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0743e-06 - rmse: 0.0010 - val_loss: 1.3120e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2039/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8126e-07 - rmse: 9.9058e-04\n",
      "Epoch 2039: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0587e-06 - rmse: 0.0010 - val_loss: 1.3348e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2040/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3835e-07 - rmse: 9.6869e-04\n",
      "Epoch 2040: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0600e-06 - rmse: 0.0010 - val_loss: 1.3070e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2041/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1179e-06 - rmse: 0.0011\n",
      "Epoch 2041: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1287e-06 - rmse: 0.0011 - val_loss: 1.3422e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2042/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0764e-07 - rmse: 7.1249e-04\n",
      "Epoch 2042: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0888e-06 - rmse: 0.0010 - val_loss: 1.3116e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2043/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5876e-06 - rmse: 0.0013\n",
      "Epoch 2043: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0898e-06 - rmse: 0.0010 - val_loss: 1.3266e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2044/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7491e-06 - rmse: 0.0013\n",
      "Epoch 2044: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0528e-06 - rmse: 0.0010 - val_loss: 1.3244e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2045/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1851e-07 - rmse: 7.8646e-04\n",
      "Epoch 2045: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0581e-06 - rmse: 0.0010 - val_loss: 1.3553e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2046/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1621e-07 - rmse: 9.5719e-04\n",
      "Epoch 2046: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0787e-06 - rmse: 0.0010 - val_loss: 1.3033e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2047/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7342e-06 - rmse: 0.0013\n",
      "Epoch 2047: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1148e-06 - rmse: 0.0011 - val_loss: 1.3333e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2048/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7361e-07 - rmse: 8.2074e-04\n",
      "Epoch 2048: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0695e-06 - rmse: 0.0010 - val_loss: 1.3146e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2049/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6165e-07 - rmse: 9.2825e-04\n",
      "Epoch 2049: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0423e-06 - rmse: 0.0010 - val_loss: 1.3329e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2050/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6361e-07 - rmse: 9.8163e-04\n",
      "Epoch 2050: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0474e-06 - rmse: 0.0010 - val_loss: 1.3320e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2051/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6667e-06 - rmse: 0.0013\n",
      "Epoch 2051: val_loss did not improve from 0.00007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0936e-06 - rmse: 0.0010 - val_loss: 1.3143e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2052/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1022e-07 - rmse: 7.8117e-04\n",
      "Epoch 2052: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1073e-06 - rmse: 0.0011 - val_loss: 1.3371e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2053/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1190e-06 - rmse: 0.0011\n",
      "Epoch 2053: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1366e-06 - rmse: 0.0011 - val_loss: 1.3169e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2054/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5047e-06 - rmse: 0.0012\n",
      "Epoch 2054: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0401e-06 - rmse: 0.0010 - val_loss: 1.3479e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2055/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9940e-07 - rmse: 8.9409e-04\n",
      "Epoch 2055: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0643e-06 - rmse: 0.0010 - val_loss: 1.3107e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2056/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4288e-07 - rmse: 4.9283e-04\n",
      "Epoch 2056: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0376e-06 - rmse: 0.0010 - val_loss: 1.3272e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2057/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0770e-07 - rmse: 8.9872e-04\n",
      "Epoch 2057: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0358e-06 - rmse: 0.0010 - val_loss: 1.3294e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2058/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2044e-06 - rmse: 0.0011\n",
      "Epoch 2058: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0221e-06 - rmse: 0.0010 - val_loss: 1.3233e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2059/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7735e-06 - rmse: 0.0013\n",
      "Epoch 2059: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0679e-06 - rmse: 0.0010 - val_loss: 1.3100e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2060/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3266e-07 - rmse: 7.9540e-04\n",
      "Epoch 2060: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0410e-06 - rmse: 0.0010 - val_loss: 1.3414e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2061/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7356e-06 - rmse: 0.0013\n",
      "Epoch 2061: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0568e-06 - rmse: 0.0010 - val_loss: 1.3216e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2062/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5252e-07 - rmse: 7.4332e-04\n",
      "Epoch 2062: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1231e-06 - rmse: 0.0011 - val_loss: 1.3103e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2063/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.2178e-07 - rmse: 8.4958e-04\n",
      "Epoch 2063: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0834e-06 - rmse: 0.0010 - val_loss: 1.3410e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2064/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3463e-06 - rmse: 0.0012\n",
      "Epoch 2064: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0916e-06 - rmse: 0.0010 - val_loss: 1.3187e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2065/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.8232e-07 - rmse: 6.9450e-04\n",
      "Epoch 2065: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0552e-06 - rmse: 0.0010 - val_loss: 1.3276e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2066/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8452e-07 - rmse: 8.8573e-04\n",
      "Epoch 2066: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0828e-06 - rmse: 0.0010 - val_loss: 1.3252e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2067/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4103e-06 - rmse: 0.0012\n",
      "Epoch 2067: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0082e-06 - rmse: 0.0010 - val_loss: 1.3523e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2068/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2733e-06 - rmse: 0.0011\n",
      "Epoch 2068: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0508e-06 - rmse: 0.0010 - val_loss: 1.3138e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2069/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2581e-06 - rmse: 0.0011\n",
      "Epoch 2069: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0823e-06 - rmse: 0.0010 - val_loss: 1.3523e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2070/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8132e-07 - rmse: 8.2542e-04\n",
      "Epoch 2070: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0629e-06 - rmse: 0.0010 - val_loss: 1.3148e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2071/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7815e-07 - rmse: 5.2740e-04\n",
      "Epoch 2071: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0679e-06 - rmse: 0.0010 - val_loss: 1.3334e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2072/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8924e-07 - rmse: 8.8839e-04\n",
      "Epoch 2072: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0238e-06 - rmse: 0.0010 - val_loss: 1.3192e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2073/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8976e-07 - rmse: 9.9487e-04\n",
      "Epoch 2073: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0208e-06 - rmse: 0.0010 - val_loss: 1.3244e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2074/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1171e-07 - rmse: 7.1534e-04\n",
      "Epoch 2074: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0093e-06 - rmse: 0.0010 - val_loss: 1.3327e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2075/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2386e-06 - rmse: 0.0015\n",
      "Epoch 2075: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0231e-06 - rmse: 0.0010 - val_loss: 1.3273e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2076/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7958e-07 - rmse: 9.3786e-04\n",
      "Epoch 2076: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0383e-06 - rmse: 0.0010 - val_loss: 1.3207e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2077/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4629e-07 - rmse: 8.0392e-04\n",
      "Epoch 2077: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0510e-06 - rmse: 0.0010 - val_loss: 1.3531e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2078/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4685e-07 - rmse: 6.6847e-04\n",
      "Epoch 2078: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0528e-06 - rmse: 0.0010 - val_loss: 1.3168e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2079/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9268e-07 - rmse: 8.9033e-04\n",
      "Epoch 2079: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1967e-06 - rmse: 0.0011 - val_loss: 1.3679e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2080/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7627e-06 - rmse: 0.0013\n",
      "Epoch 2080: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1939e-06 - rmse: 0.0011 - val_loss: 1.3135e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2081/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0448e-06 - rmse: 0.0010\n",
      "Epoch 2081: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0704e-06 - rmse: 0.0010 - val_loss: 1.3388e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2082/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3161e-07 - rmse: 7.2912e-04\n",
      "Epoch 2082: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1491e-06 - rmse: 0.0011 - val_loss: 1.3201e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2083/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0999e-06 - rmse: 0.0010\n",
      "Epoch 2083: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1262e-06 - rmse: 0.0011 - val_loss: 1.3494e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2084/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1698e-06 - rmse: 0.0015\n",
      "Epoch 2084: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0948e-06 - rmse: 0.0010 - val_loss: 1.3063e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2085/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9651e-07 - rmse: 7.7234e-04\n",
      "Epoch 2085: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0995e-06 - rmse: 0.0010 - val_loss: 1.3425e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2086/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5848e-07 - rmse: 8.1147e-04\n",
      "Epoch 2086: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0549e-06 - rmse: 0.0010 - val_loss: 1.3212e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2087/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2858e-06 - rmse: 0.0011\n",
      "Epoch 2087: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0173e-06 - rmse: 0.0010 - val_loss: 1.3470e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2088/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2103e-06 - rmse: 0.0011\n",
      "Epoch 2088: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0567e-06 - rmse: 0.0010 - val_loss: 1.3151e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2089/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2264e-06 - rmse: 0.0011\n",
      "Epoch 2089: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0578e-06 - rmse: 0.0010 - val_loss: 1.3399e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2090/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0970e-06 - rmse: 0.0010\n",
      "Epoch 2090: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0521e-06 - rmse: 0.0010 - val_loss: 1.3341e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2091/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2281e-06 - rmse: 0.0011\n",
      "Epoch 2091: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0252e-06 - rmse: 0.0010 - val_loss: 1.3276e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2092/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8347e-06 - rmse: 0.0014\n",
      "Epoch 2092: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0170e-06 - rmse: 0.0010 - val_loss: 1.3324e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2093/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5734e-07 - rmse: 9.7844e-04\n",
      "Epoch 2093: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0143e-06 - rmse: 0.0010 - val_loss: 1.3360e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2094/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6496e-07 - rmse: 9.3003e-04\n",
      "Epoch 2094: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0274e-06 - rmse: 0.0010 - val_loss: 1.3176e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2095/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.8734e-07 - rmse: 5.3604e-04\n",
      "Epoch 2095: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0303e-06 - rmse: 0.0010 - val_loss: 1.3339e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2096/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5973e-07 - rmse: 8.7162e-04\n",
      "Epoch 2096: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0415e-06 - rmse: 0.0010 - val_loss: 1.3319e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2097/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2776e-07 - rmse: 6.5404e-04\n",
      "Epoch 2097: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1035e-06 - rmse: 0.0011 - val_loss: 1.3201e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2098/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4774e-07 - rmse: 9.2073e-04\n",
      "Epoch 2098: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0328e-06 - rmse: 0.0010 - val_loss: 1.3232e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2099/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6016e-07 - rmse: 9.2745e-04\n",
      "Epoch 2099: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0143e-06 - rmse: 0.0010 - val_loss: 1.3375e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2100/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3323e-06 - rmse: 0.0012\n",
      "Epoch 2100: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0709e-06 - rmse: 0.0010 - val_loss: 1.3161e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2101/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0342e-07 - rmse: 8.9634e-04\n",
      "Epoch 2101: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1255e-06 - rmse: 0.0011 - val_loss: 1.3431e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2102/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3131e-07 - rmse: 7.9455e-04\n",
      "Epoch 2102: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3081e-06 - rmse: 0.0011 - val_loss: 1.3228e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2103/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3028e-07 - rmse: 7.2820e-04\n",
      "Epoch 2103: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3608e-06 - rmse: 0.0012 - val_loss: 1.3783e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2104/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.5139e-07 - rmse: 5.9278e-04\n",
      "Epoch 2104: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3948e-06 - rmse: 0.0012 - val_loss: 1.2732e-04 - val_rmse: 0.0113 - lr: 1.2500e-05\n",
      "Epoch 2105/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4642e-06 - rmse: 0.0016\n",
      "Epoch 2105: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2400e-06 - rmse: 0.0011 - val_loss: 1.3555e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2106/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2486e-06 - rmse: 0.0011\n",
      "Epoch 2106: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2618e-06 - rmse: 0.0011 - val_loss: 1.3314e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2107/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9853e-06 - rmse: 0.0014\n",
      "Epoch 2107: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.7475e-07 - rmse: 9.8729e-04 - val_loss: 1.3467e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2108/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5781e-07 - rmse: 8.7052e-04\n",
      "Epoch 2108: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0304e-06 - rmse: 0.0010 - val_loss: 1.3179e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2109/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2184e-06 - rmse: 0.0011\n",
      "Epoch 2109: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1452e-06 - rmse: 0.0011 - val_loss: 1.3345e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2110/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7236e-06 - rmse: 0.0013\n",
      "Epoch 2110: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3861e-06 - rmse: 0.0012 - val_loss: 1.3683e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2111/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9873e-07 - rmse: 9.4802e-04\n",
      "Epoch 2111: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3932e-06 - rmse: 0.0012 - val_loss: 1.3114e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2112/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4704e-06 - rmse: 0.0012\n",
      "Epoch 2112: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3083e-06 - rmse: 0.0011 - val_loss: 1.3369e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2113/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1623e-07 - rmse: 7.1849e-04\n",
      "Epoch 2113: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1789e-06 - rmse: 0.0011 - val_loss: 1.3066e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2114/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1041e-07 - rmse: 7.8128e-04\n",
      "Epoch 2114: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1084e-06 - rmse: 0.0011 - val_loss: 1.3611e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2115/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2032e-06 - rmse: 0.0011\n",
      "Epoch 2115: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1398e-06 - rmse: 0.0011 - val_loss: 1.3186e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2116/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3809e-06 - rmse: 0.0012\n",
      "Epoch 2116: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1401e-06 - rmse: 0.0011 - val_loss: 1.3841e-04 - val_rmse: 0.0118 - lr: 1.2500e-05\n",
      "Epoch 2117/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2355e-06 - rmse: 0.0011\n",
      "Epoch 2117: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3158e-06 - rmse: 0.0011 - val_loss: 1.3250e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2118/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2791e-07 - rmse: 9.6328e-04\n",
      "Epoch 2118: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0503e-06 - rmse: 0.0010 - val_loss: 1.3691e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2119/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2543e-07 - rmse: 9.0853e-04\n",
      "Epoch 2119: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0188e-06 - rmse: 0.0010 - val_loss: 1.3231e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2120/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5665e-07 - rmse: 6.7576e-04\n",
      "Epoch 2120: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0137e-06 - rmse: 0.0010 - val_loss: 1.3456e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2121/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0072e-06 - rmse: 0.0010\n",
      "Epoch 2121: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8272e-07 - rmse: 9.9132e-04 - val_loss: 1.3228e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2122/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9667e-07 - rmse: 8.9257e-04\n",
      "Epoch 2122: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0763e-06 - rmse: 0.0010 - val_loss: 1.3616e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2123/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0352e-06 - rmse: 0.0010\n",
      "Epoch 2123: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0191e-06 - rmse: 0.0010 - val_loss: 1.3188e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2124/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2323e-07 - rmse: 7.2335e-04\n",
      "Epoch 2124: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0341e-06 - rmse: 0.0010 - val_loss: 1.3398e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2125/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5210e-07 - rmse: 6.7239e-04\n",
      "Epoch 2125: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.9532e-07 - rmse: 9.9765e-04 - val_loss: 1.3427e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2126/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4822e-06 - rmse: 0.0012\n",
      "Epoch 2126: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8021e-07 - rmse: 9.9005e-04 - val_loss: 1.3331e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2127/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4825e-06 - rmse: 0.0012\n",
      "Epoch 2127: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9733e-07 - rmse: 9.9867e-04 - val_loss: 1.3681e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2128/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2218e-06 - rmse: 0.0011\n",
      "Epoch 2128: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0488e-06 - rmse: 0.0010 - val_loss: 1.3244e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2129/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6572e-07 - rmse: 6.8243e-04\n",
      "Epoch 2129: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.7339e-07 - rmse: 9.8661e-04 - val_loss: 1.3599e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2130/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4713e-06 - rmse: 0.0012\n",
      "Epoch 2130: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.9661e-07 - rmse: 9.9830e-04 - val_loss: 1.3176e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2131/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3216e-06 - rmse: 0.0011\n",
      "Epoch 2131: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0004e-06 - rmse: 0.0010 - val_loss: 1.3511e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2132/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0658e-07 - rmse: 7.7883e-04\n",
      "Epoch 2132: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0207e-06 - rmse: 0.0010 - val_loss: 1.3177e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2133/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5047e-06 - rmse: 0.0012\n",
      "Epoch 2133: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0823e-06 - rmse: 0.0010 - val_loss: 1.3583e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2134/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4579e-06 - rmse: 0.0012\n",
      "Epoch 2134: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0539e-06 - rmse: 0.0010 - val_loss: 1.3445e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2135/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9944e-07 - rmse: 9.4839e-04\n",
      "Epoch 2135: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8096e-07 - rmse: 9.9043e-04 - val_loss: 1.3439e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2136/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2322e-07 - rmse: 6.5055e-04\n",
      "Epoch 2136: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0073e-06 - rmse: 0.0010 - val_loss: 1.3536e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2137/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3401e-06 - rmse: 0.0012\n",
      "Epoch 2137: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0474e-06 - rmse: 0.0010 - val_loss: 1.3227e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2138/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3051e-06 - rmse: 0.0011\n",
      "Epoch 2138: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9588e-07 - rmse: 9.9794e-04 - val_loss: 1.3510e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2139/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5072e-07 - rmse: 8.6644e-04\n",
      "Epoch 2139: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7316e-07 - rmse: 9.8649e-04 - val_loss: 1.3490e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2140/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.3547e-07 - rmse: 5.7920e-04\n",
      "Epoch 2140: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6765e-07 - rmse: 9.8369e-04 - val_loss: 1.3535e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2141/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7454e-07 - rmse: 9.8719e-04\n",
      "Epoch 2141: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0378e-06 - rmse: 0.0010 - val_loss: 1.3432e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2142/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5595e-07 - rmse: 7.4562e-04\n",
      "Epoch 2142: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0138e-06 - rmse: 0.0010 - val_loss: 1.3295e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2143/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7798e-07 - rmse: 6.9136e-04\n",
      "Epoch 2143: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0181e-06 - rmse: 0.0010 - val_loss: 1.3530e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2144/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.3414e-07 - rmse: 7.3085e-04\n",
      "Epoch 2144: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8945e-07 - rmse: 9.9471e-04 - val_loss: 1.3242e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2145/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4779e-06 - rmse: 0.0012\n",
      "Epoch 2145: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0119e-06 - rmse: 0.0010 - val_loss: 1.3785e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2146/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7451e-07 - rmse: 8.8006e-04\n",
      "Epoch 2146: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0799e-06 - rmse: 0.0010 - val_loss: 1.3160e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2147/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0361e-07 - rmse: 6.3531e-04\n",
      "Epoch 2147: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0376e-06 - rmse: 0.0010 - val_loss: 1.3468e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2148/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2004e-06 - rmse: 0.0011\n",
      "Epoch 2148: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8978e-07 - rmse: 9.9488e-04 - val_loss: 1.3185e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2149/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7445e-06 - rmse: 0.0013\n",
      "Epoch 2149: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8040e-07 - rmse: 9.9015e-04 - val_loss: 1.3508e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2150/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8391e-07 - rmse: 8.2699e-04\n",
      "Epoch 2150: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5968e-07 - rmse: 9.7963e-04 - val_loss: 1.3196e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2151/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5735e-06 - rmse: 0.0013\n",
      "Epoch 2151: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0128e-06 - rmse: 0.0010 - val_loss: 1.3684e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2152/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1775e-07 - rmse: 9.0430e-04\n",
      "Epoch 2152: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.6893e-07 - rmse: 9.8434e-04 - val_loss: 1.3337e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2153/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2899e-07 - rmse: 9.1049e-04\n",
      "Epoch 2153: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9860e-07 - rmse: 9.9930e-04 - val_loss: 1.3411e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2154/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5678e-07 - rmse: 8.6993e-04\n",
      "Epoch 2154: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6202e-07 - rmse: 9.8082e-04 - val_loss: 1.3554e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2155/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0105e-07 - rmse: 9.4923e-04\n",
      "Epoch 2155: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.6016e-07 - rmse: 9.7988e-04 - val_loss: 1.3420e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2156/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1831e-06 - rmse: 0.0011\n",
      "Epoch 2156: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4889e-07 - rmse: 9.7411e-04 - val_loss: 1.3485e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2157/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0563e-06 - rmse: 0.0010\n",
      "Epoch 2157: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9097e-07 - rmse: 9.9547e-04 - val_loss: 1.3377e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2158/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9220e-06 - rmse: 0.0014\n",
      "Epoch 2158: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0359e-06 - rmse: 0.0010 - val_loss: 1.3653e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2159/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9449e-07 - rmse: 7.0320e-04\n",
      "Epoch 2159: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9634e-07 - rmse: 9.9817e-04 - val_loss: 1.3245e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2160/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4860e-06 - rmse: 0.0012\n",
      "Epoch 2160: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0231e-06 - rmse: 0.0010 - val_loss: 1.3697e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2161/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6281e-07 - rmse: 9.2887e-04\n",
      "Epoch 2161: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0051e-06 - rmse: 0.0010 - val_loss: 1.3277e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2162/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5973e-06 - rmse: 0.0013\n",
      "Epoch 2162: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0898e-06 - rmse: 0.0010 - val_loss: 1.3642e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2163/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6852e-07 - rmse: 7.5400e-04\n",
      "Epoch 2163: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8779e-07 - rmse: 9.9388e-04 - val_loss: 1.3424e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2164/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3360e-06 - rmse: 0.0012\n",
      "Epoch 2164: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6871e-07 - rmse: 9.8423e-04 - val_loss: 1.3597e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2165/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3033e-07 - rmse: 8.5459e-04\n",
      "Epoch 2165: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0212e-06 - rmse: 0.0010 - val_loss: 1.3477e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2166/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0879e-07 - rmse: 7.1330e-04\n",
      "Epoch 2166: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6835e-07 - rmse: 9.8405e-04 - val_loss: 1.3421e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2167/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1126e-07 - rmse: 9.5460e-04\n",
      "Epoch 2167: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9758e-07 - rmse: 9.9879e-04 - val_loss: 1.3400e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2168/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2114e-07 - rmse: 6.4895e-04\n",
      "Epoch 2168: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0422e-06 - rmse: 0.0010 - val_loss: 1.4104e-04 - val_rmse: 0.0119 - lr: 1.2500e-05\n",
      "Epoch 2169/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0565e-06 - rmse: 0.0010\n",
      "Epoch 2169: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3572e-06 - rmse: 0.0012 - val_loss: 1.2984e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2170/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0221e-06 - rmse: 0.0014\n",
      "Epoch 2170: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4246e-06 - rmse: 0.0012 - val_loss: 1.3509e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2171/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8339e-07 - rmse: 7.6380e-04\n",
      "Epoch 2171: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0575e-06 - rmse: 0.0010 - val_loss: 1.3283e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2172/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0891e-07 - rmse: 9.5337e-04\n",
      "Epoch 2172: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7974e-07 - rmse: 9.8982e-04 - val_loss: 1.3813e-04 - val_rmse: 0.0118 - lr: 1.2500e-05\n",
      "Epoch 2173/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2286e-06 - rmse: 0.0011\n",
      "Epoch 2173: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0619e-06 - rmse: 0.0010 - val_loss: 1.3410e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2174/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2710e-06 - rmse: 0.0011\n",
      "Epoch 2174: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8193e-07 - rmse: 9.9092e-04 - val_loss: 1.3545e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2175/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5438e-06 - rmse: 0.0012\n",
      "Epoch 2175: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1715e-06 - rmse: 0.0011 - val_loss: 1.3875e-04 - val_rmse: 0.0118 - lr: 1.2500e-05\n",
      "Epoch 2176/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6189e-07 - rmse: 7.4959e-04\n",
      "Epoch 2176: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2059e-06 - rmse: 0.0011 - val_loss: 1.3083e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2177/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3323e-06 - rmse: 0.0012\n",
      "Epoch 2177: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1570e-06 - rmse: 0.0011 - val_loss: 1.3422e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2178/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4827e-07 - rmse: 8.6502e-04\n",
      "Epoch 2178: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1209e-06 - rmse: 0.0011 - val_loss: 1.3639e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2179/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5893e-07 - rmse: 8.7116e-04\n",
      "Epoch 2179: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1481e-06 - rmse: 0.0011 - val_loss: 1.2933e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2180/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1226e-06 - rmse: 0.0011\n",
      "Epoch 2180: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3173e-06 - rmse: 0.0011 - val_loss: 1.4158e-04 - val_rmse: 0.0119 - lr: 1.2500e-05\n",
      "Epoch 2181/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9203e-06 - rmse: 0.0017\n",
      "Epoch 2181: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4453e-06 - rmse: 0.0012 - val_loss: 1.3178e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2182/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0283e-07 - rmse: 7.0911e-04\n",
      "Epoch 2182: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5790e-07 - rmse: 9.7872e-04 - val_loss: 1.3815e-04 - val_rmse: 0.0118 - lr: 1.2500e-05\n",
      "Epoch 2183/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.2670e-06 - rmse: 0.0015\n",
      "Epoch 2183: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2898e-06 - rmse: 0.0011 - val_loss: 1.3062e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2184/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7576e-07 - rmse: 9.3582e-04\n",
      "Epoch 2184: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4110e-06 - rmse: 0.0012 - val_loss: 1.4011e-04 - val_rmse: 0.0118 - lr: 1.2500e-05\n",
      "Epoch 2185/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4071e-06 - rmse: 0.0016\n",
      "Epoch 2185: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5420e-06 - rmse: 0.0012 - val_loss: 1.3567e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2186/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5653e-07 - rmse: 8.1026e-04\n",
      "Epoch 2186: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.2282e-06 - rmse: 0.0011 - val_loss: 1.3275e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2187/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2622e-06 - rmse: 0.0011\n",
      "Epoch 2187: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1010e-06 - rmse: 0.0010 - val_loss: 1.3479e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2188/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0158e-07 - rmse: 9.4951e-04\n",
      "Epoch 2188: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0712e-06 - rmse: 0.0010 - val_loss: 1.3543e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2189/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3080e-06 - rmse: 0.0011\n",
      "Epoch 2189: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7984e-07 - rmse: 9.8987e-04 - val_loss: 1.3397e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2190/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2319e-06 - rmse: 0.0011\n",
      "Epoch 2190: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0368e-06 - rmse: 0.0010 - val_loss: 1.3789e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2191/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1778e-06 - rmse: 0.0011\n",
      "Epoch 2191: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9247e-07 - rmse: 9.9623e-04 - val_loss: 1.3247e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2192/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0762e-06 - rmse: 0.0010\n",
      "Epoch 2192: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7775e-07 - rmse: 9.8881e-04 - val_loss: 1.3734e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2193/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7772e-07 - rmse: 9.3687e-04\n",
      "Epoch 2193: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5746e-07 - rmse: 9.7850e-04 - val_loss: 1.3611e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2194/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4758e-06 - rmse: 0.0012\n",
      "Epoch 2194: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.2014e-07 - rmse: 9.5924e-04 - val_loss: 1.3716e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2195/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4963e-07 - rmse: 8.0600e-04\n",
      "Epoch 2195: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0592e-06 - rmse: 0.0010 - val_loss: 1.3258e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2196/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1496e-06 - rmse: 0.0011\n",
      "Epoch 2196: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.7294e-07 - rmse: 9.8638e-04 - val_loss: 1.3840e-04 - val_rmse: 0.0118 - lr: 1.2500e-05\n",
      "Epoch 2197/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0544e-06 - rmse: 0.0010\n",
      "Epoch 2197: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0992e-06 - rmse: 0.0010 - val_loss: 1.3463e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2198/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2031e-06 - rmse: 0.0011\n",
      "Epoch 2198: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8155e-07 - rmse: 9.9073e-04 - val_loss: 1.3442e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2199/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0373e-06 - rmse: 0.0010\n",
      "Epoch 2199: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0121e-06 - rmse: 0.0010 - val_loss: 1.3732e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2200/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1351e-07 - rmse: 7.1659e-04\n",
      "Epoch 2200: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0089e-06 - rmse: 0.0010 - val_loss: 1.3380e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2201/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6647e-06 - rmse: 0.0013\n",
      "Epoch 2201: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0971e-06 - rmse: 0.0010 - val_loss: 1.3943e-04 - val_rmse: 0.0118 - lr: 1.2500e-05\n",
      "Epoch 2202/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4164e-07 - rmse: 8.6119e-04\n",
      "Epoch 2202: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1772e-06 - rmse: 0.0011 - val_loss: 1.3437e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2203/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7922e-07 - rmse: 8.8273e-04\n",
      "Epoch 2203: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.3869e-06 - rmse: 0.0012 - val_loss: 1.2942e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2204/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7484e-07 - rmse: 9.8734e-04\n",
      "Epoch 2204: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3284e-06 - rmse: 0.0012 - val_loss: 1.4591e-04 - val_rmse: 0.0121 - lr: 1.2500e-05\n",
      "Epoch 2205/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7466e-06 - rmse: 0.0017\n",
      "Epoch 2205: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.8204e-06 - rmse: 0.0013 - val_loss: 1.2904e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2206/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4909e-06 - rmse: 0.0012\n",
      "Epoch 2206: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.5882e-06 - rmse: 0.0013 - val_loss: 1.3411e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2207/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.5557e-07 - rmse: 8.0967e-04\n",
      "Epoch 2207: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3270e-06 - rmse: 0.0012 - val_loss: 1.3519e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2208/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9888e-07 - rmse: 8.3599e-04\n",
      "Epoch 2208: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3328e-06 - rmse: 0.0012 - val_loss: 1.2827e-04 - val_rmse: 0.0113 - lr: 1.2500e-05\n",
      "Epoch 2209/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7657e-06 - rmse: 0.0013\n",
      "Epoch 2209: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3555e-06 - rmse: 0.0012 - val_loss: 1.4363e-04 - val_rmse: 0.0120 - lr: 1.2500e-05\n",
      "Epoch 2210/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5803e-06 - rmse: 0.0013\n",
      "Epoch 2210: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.4315e-06 - rmse: 0.0012 - val_loss: 1.2741e-04 - val_rmse: 0.0113 - lr: 1.2500e-05\n",
      "Epoch 2211/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7805e-06 - rmse: 0.0013\n",
      "Epoch 2211: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4223e-06 - rmse: 0.0012 - val_loss: 1.3763e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2212/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7636e-07 - rmse: 8.2241e-04\n",
      "Epoch 2212: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2127e-06 - rmse: 0.0011 - val_loss: 1.3431e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2213/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0936e-06 - rmse: 0.0010\n",
      "Epoch 2213: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0720e-06 - rmse: 0.0010 - val_loss: 1.3418e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2214/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2851e-07 - rmse: 7.9278e-04\n",
      "Epoch 2214: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0463e-06 - rmse: 0.0010 - val_loss: 1.3605e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2215/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2010e-06 - rmse: 0.0011\n",
      "Epoch 2215: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0047e-06 - rmse: 0.0010 - val_loss: 1.3548e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2216/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.9162e-07 - rmse: 8.3164e-04\n",
      "Epoch 2216: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4000e-07 - rmse: 9.6954e-04 - val_loss: 1.3470e-04 - val_rmse: 0.0116 - lr: 1.2500e-05\n",
      "Epoch 2217/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3178e-07 - rmse: 9.6529e-04\n",
      "Epoch 2217: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3158e-07 - rmse: 9.6519e-04 - val_loss: 1.3759e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2218/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.1234e-07 - rmse: 9.5516e-04\n",
      "Epoch 2218: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1660e-07 - rmse: 9.5739e-04 - val_loss: 1.3252e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2219/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.8913e-07 - rmse: 7.6755e-04\n",
      "Epoch 2219: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0198e-06 - rmse: 0.0010 - val_loss: 1.3746e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2220/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7027e-06 - rmse: 0.0013\n",
      "Epoch 2220: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9445e-07 - rmse: 9.9722e-04 - val_loss: 1.3632e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2221/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6880e-07 - rmse: 6.8469e-04\n",
      "Epoch 2221: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0916e-06 - rmse: 0.0010 - val_loss: 1.3195e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2222/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4842e-07 - rmse: 8.0524e-04\n",
      "Epoch 2222: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9594e-07 - rmse: 9.9797e-04 - val_loss: 1.4398e-04 - val_rmse: 0.0120 - lr: 1.2500e-05\n",
      "Epoch 2223/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4127e-06 - rmse: 0.0012\n",
      "Epoch 2223: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7221e-06 - rmse: 0.0013 - val_loss: 1.3303e-04 - val_rmse: 0.0115 - lr: 1.2500e-05\n",
      "Epoch 2224/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.7066e-07 - rmse: 7.5542e-04\n",
      "Epoch 2224: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0619e-06 - rmse: 0.0010 - val_loss: 1.3707e-04 - val_rmse: 0.0117 - lr: 1.2500e-05\n",
      "Epoch 2225/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3500e-07 - rmse: 9.6695e-04\n",
      "Epoch 2225: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 2225: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1456e-06 - rmse: 0.0011 - val_loss: 1.3047e-04 - val_rmse: 0.0114 - lr: 1.2500e-05\n",
      "Epoch 2226/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1003e-06 - rmse: 0.0010\n",
      "Epoch 2226: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1627e-06 - rmse: 0.0011 - val_loss: 1.3577e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2227/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3660e-07 - rmse: 9.6778e-04\n",
      "Epoch 2227: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4882e-07 - rmse: 9.7407e-04 - val_loss: 1.3447e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2228/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9570e-07 - rmse: 6.2905e-04\n",
      "Epoch 2228: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.3667e-07 - rmse: 9.6782e-04 - val_loss: 1.3816e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2229/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5833e-07 - rmse: 8.7082e-04\n",
      "Epoch 2229: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.3600e-07 - rmse: 9.6747e-04 - val_loss: 1.3537e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2230/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4645e-07 - rmse: 7.3922e-04\n",
      "Epoch 2230: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.3709e-07 - rmse: 9.6803e-04 - val_loss: 1.3493e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2231/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5719e-06 - rmse: 0.0013\n",
      "Epoch 2231: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0357e-06 - rmse: 0.0010 - val_loss: 1.3693e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2232/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.3204e-07 - rmse: 9.1216e-04\n",
      "Epoch 2232: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7332e-07 - rmse: 9.8657e-04 - val_loss: 1.3646e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2233/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2464e-06 - rmse: 0.0011\n",
      "Epoch 2233: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.9210e-07 - rmse: 9.4451e-04 - val_loss: 1.3807e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2234/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2267e-06 - rmse: 0.0011\n",
      "Epoch 2234: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1568e-07 - rmse: 9.5691e-04 - val_loss: 1.3350e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2235/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5826e-07 - rmse: 9.2642e-04\n",
      "Epoch 2235: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.6857e-07 - rmse: 9.8416e-04 - val_loss: 1.3628e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2236/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5917e-07 - rmse: 8.7131e-04\n",
      "Epoch 2236: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.9571e-07 - rmse: 9.4642e-04 - val_loss: 1.3488e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2237/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4539e-07 - rmse: 8.0336e-04\n",
      "Epoch 2237: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0403e-07 - rmse: 9.5081e-04 - val_loss: 1.3709e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2238/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1895e-07 - rmse: 8.4791e-04\n",
      "Epoch 2238: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9576e-07 - rmse: 9.4644e-04 - val_loss: 1.3667e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2239/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2812e-06 - rmse: 0.0011\n",
      "Epoch 2239: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4895e-07 - rmse: 9.7414e-04 - val_loss: 1.3530e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2240/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5296e-07 - rmse: 9.2356e-04\n",
      "Epoch 2240: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1553e-07 - rmse: 9.5683e-04 - val_loss: 1.3594e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2241/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6618e-07 - rmse: 8.7532e-04\n",
      "Epoch 2241: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9276e-07 - rmse: 9.9637e-04 - val_loss: 1.3513e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2242/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3010e-07 - rmse: 7.9379e-04\n",
      "Epoch 2242: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8307e-07 - rmse: 9.9150e-04 - val_loss: 1.3701e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2243/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3305e-06 - rmse: 0.0012\n",
      "Epoch 2243: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.2106e-07 - rmse: 9.5972e-04 - val_loss: 1.3426e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2244/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2467e-07 - rmse: 7.2434e-04\n",
      "Epoch 2244: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7083e-07 - rmse: 9.3318e-04 - val_loss: 1.3640e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2245/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3843e-07 - rmse: 6.6214e-04\n",
      "Epoch 2245: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9645e-07 - rmse: 9.4681e-04 - val_loss: 1.3544e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2246/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9357e-07 - rmse: 8.9083e-04\n",
      "Epoch 2246: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.9203e-07 - rmse: 9.4447e-04 - val_loss: 1.3555e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2247/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5619e-06 - rmse: 0.0012\n",
      "Epoch 2247: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6858e-07 - rmse: 9.3197e-04 - val_loss: 1.3669e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2248/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7094e-06 - rmse: 0.0013\n",
      "Epoch 2248: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8485e-07 - rmse: 9.4066e-04 - val_loss: 1.3499e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2249/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6537e-07 - rmse: 9.8253e-04\n",
      "Epoch 2249: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1307e-07 - rmse: 9.5555e-04 - val_loss: 1.3676e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2250/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7508e-07 - rmse: 8.2163e-04\n",
      "Epoch 2250: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9407e-07 - rmse: 9.4555e-04 - val_loss: 1.3527e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2251/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6894e-06 - rmse: 0.0013\n",
      "Epoch 2251: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9863e-07 - rmse: 9.4796e-04 - val_loss: 1.3583e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2252/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2045e-07 - rmse: 9.5940e-04\n",
      "Epoch 2252: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0421e-07 - rmse: 9.5090e-04 - val_loss: 1.3548e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2253/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2325e-07 - rmse: 7.2336e-04\n",
      "Epoch 2253: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9673e-07 - rmse: 9.4696e-04 - val_loss: 1.3577e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2254/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.2035e-07 - rmse: 9.0573e-04\n",
      "Epoch 2254: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9554e-07 - rmse: 9.4633e-04 - val_loss: 1.3672e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2255/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3608e-06 - rmse: 0.0012\n",
      "Epoch 2255: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5348e-07 - rmse: 9.7646e-04 - val_loss: 1.3454e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2256/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0621e-07 - rmse: 9.5195e-04\n",
      "Epoch 2256: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7730e-07 - rmse: 9.3665e-04 - val_loss: 1.3844e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2257/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0576e-06 - rmse: 0.0010\n",
      "Epoch 2257: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.9874e-07 - rmse: 9.4802e-04 - val_loss: 1.3422e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2258/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.4897e-07 - rmse: 8.0559e-04\n",
      "Epoch 2258: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.5885e-07 - rmse: 9.7921e-04 - val_loss: 1.3668e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2259/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.9480e-07 - rmse: 5.4296e-04\n",
      "Epoch 2259: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.0829e-07 - rmse: 9.5304e-04 - val_loss: 1.3624e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2260/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7333e-07 - rmse: 8.2056e-04\n",
      "Epoch 2260: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.9615e-07 - rmse: 9.4665e-04 - val_loss: 1.3468e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2261/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3036e-06 - rmse: 0.0011\n",
      "Epoch 2261: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1562e-07 - rmse: 9.5688e-04 - val_loss: 1.3748e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2262/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5782e-06 - rmse: 0.0013\n",
      "Epoch 2262: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0624e-07 - rmse: 9.5196e-04 - val_loss: 1.3529e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2263/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5088e-07 - rmse: 7.4221e-04\n",
      "Epoch 2263: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6776e-07 - rmse: 9.8375e-04 - val_loss: 1.3774e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2264/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.6630e-07 - rmse: 8.1627e-04\n",
      "Epoch 2264: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9348e-07 - rmse: 9.4524e-04 - val_loss: 1.3554e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2265/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.5010e-07 - rmse: 9.2201e-04\n",
      "Epoch 2265: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6313e-07 - rmse: 9.2905e-04 - val_loss: 1.3839e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2266/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1239e-06 - rmse: 0.0011\n",
      "Epoch 2266: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1457e-07 - rmse: 9.5633e-04 - val_loss: 1.3548e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2267/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2737e-07 - rmse: 6.5373e-04\n",
      "Epoch 2267: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1021e-07 - rmse: 9.5405e-04 - val_loss: 1.3590e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2268/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.0093e-07 - rmse: 7.0776e-04\n",
      "Epoch 2268: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7790e-07 - rmse: 9.3697e-04 - val_loss: 1.3553e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2269/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8409e-07 - rmse: 8.8549e-04\n",
      "Epoch 2269: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0021e-06 - rmse: 0.0010 - val_loss: 1.3788e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2270/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.9205e-07 - rmse: 9.4448e-04\n",
      "Epoch 2270: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7479e-07 - rmse: 9.8732e-04 - val_loss: 1.3293e-04 - val_rmse: 0.0115 - lr: 6.2500e-06\n",
      "Epoch 2271/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8199e-06 - rmse: 0.0013\n",
      "Epoch 2271: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0628e-06 - rmse: 0.0010 - val_loss: 1.3600e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2272/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.8888e-07 - rmse: 8.8819e-04\n",
      "Epoch 2272: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0159e-06 - rmse: 0.0010 - val_loss: 1.3615e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2273/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0804e-06 - rmse: 0.0010\n",
      "Epoch 2273: val_loss did not improve from 0.00007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0035e-06 - rmse: 0.0010 - val_loss: 1.3708e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2274/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0203e-06 - rmse: 0.0010\n",
      "Epoch 2274: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3347e-07 - rmse: 9.6616e-04 - val_loss: 1.3630e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2275/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4111e-07 - rmse: 7.3560e-04\n",
      "Epoch 2275: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6022e-07 - rmse: 9.2748e-04 - val_loss: 1.3718e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2276/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8712e-07 - rmse: 6.2219e-04\n",
      "Epoch 2276: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6696e-07 - rmse: 9.3111e-04 - val_loss: 1.3548e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2277/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4594e-06 - rmse: 0.0012\n",
      "Epoch 2277: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9523e-07 - rmse: 9.4617e-04 - val_loss: 1.3577e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2278/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6989e-07 - rmse: 9.8483e-04\n",
      "Epoch 2278: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6411e-07 - rmse: 9.2958e-04 - val_loss: 1.3647e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2279/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.5501e-07 - rmse: 5.0499e-04\n",
      "Epoch 2279: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8157e-07 - rmse: 9.3892e-04 - val_loss: 1.3697e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2280/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6306e-07 - rmse: 6.8048e-04\n",
      "Epoch 2280: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.8246e-07 - rmse: 9.3939e-04 - val_loss: 1.3594e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2281/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9118e-07 - rmse: 7.6888e-04\n",
      "Epoch 2281: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7211e-07 - rmse: 9.3387e-04 - val_loss: 1.3708e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2282/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2680e-06 - rmse: 0.0011\n",
      "Epoch 2282: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6960e-07 - rmse: 9.3252e-04 - val_loss: 1.3646e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2283/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2840e-07 - rmse: 7.2691e-04\n",
      "Epoch 2283: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5401e-07 - rmse: 9.2412e-04 - val_loss: 1.3633e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2284/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2914e-06 - rmse: 0.0011\n",
      "Epoch 2284: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5606e-07 - rmse: 9.2524e-04 - val_loss: 1.3630e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2285/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.7570e-07 - rmse: 9.8778e-04\n",
      "Epoch 2285: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7765e-07 - rmse: 9.3683e-04 - val_loss: 1.3566e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2286/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9044e-07 - rmse: 7.0032e-04\n",
      "Epoch 2286: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6901e-07 - rmse: 9.3221e-04 - val_loss: 1.3646e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2287/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7971e-07 - rmse: 6.9261e-04\n",
      "Epoch 2287: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7653e-07 - rmse: 9.3623e-04 - val_loss: 1.3709e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2288/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0696e-06 - rmse: 0.0010\n",
      "Epoch 2288: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5059e-07 - rmse: 9.2228e-04 - val_loss: 1.3477e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2289/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.5682e-07 - rmse: 7.4620e-04\n",
      "Epoch 2289: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7581e-07 - rmse: 9.3585e-04 - val_loss: 1.3750e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2290/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1544e-06 - rmse: 0.0011\n",
      "Epoch 2290: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8217e-07 - rmse: 9.9105e-04 - val_loss: 1.3566e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2291/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4865e-07 - rmse: 6.6981e-04\n",
      "Epoch 2291: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.5309e-07 - rmse: 9.7626e-04 - val_loss: 1.3769e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2292/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0910e-07 - rmse: 8.9950e-04\n",
      "Epoch 2292: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.5621e-07 - rmse: 9.7786e-04 - val_loss: 1.3592e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2293/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4286e-06 - rmse: 0.0012\n",
      "Epoch 2293: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1586e-07 - rmse: 9.5701e-04 - val_loss: 1.3650e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2294/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0741e-07 - rmse: 7.7937e-04\n",
      "Epoch 2294: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3368e-07 - rmse: 9.6627e-04 - val_loss: 1.3708e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2295/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.9505e-07 - rmse: 7.7140e-04\n",
      "Epoch 2295: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8236e-07 - rmse: 9.3934e-04 - val_loss: 1.3506e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2296/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0646e-06 - rmse: 0.0010\n",
      "Epoch 2296: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6017e-07 - rmse: 9.2745e-04 - val_loss: 1.3804e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2297/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4730e-07 - rmse: 9.2049e-04\n",
      "Epoch 2297: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.0430e-07 - rmse: 9.5095e-04 - val_loss: 1.3528e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2298/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1926e-06 - rmse: 0.0011\n",
      "Epoch 2298: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.0958e-07 - rmse: 9.5372e-04 - val_loss: 1.3736e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2299/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4825e-06 - rmse: 0.0012\n",
      "Epoch 2299: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.8020e-07 - rmse: 9.3819e-04 - val_loss: 1.3566e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2300/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4741e-06 - rmse: 0.0012\n",
      "Epoch 2300: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6151e-07 - rmse: 9.2818e-04 - val_loss: 1.3740e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2301/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.4847e-07 - rmse: 5.9031e-04\n",
      "Epoch 2301: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5825e-07 - rmse: 9.2642e-04 - val_loss: 1.3620e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2302/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.8503e-07 - rmse: 4.3015e-04\n",
      "Epoch 2302: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6194e-07 - rmse: 9.2841e-04 - val_loss: 1.3717e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2303/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6132e-07 - rmse: 8.7254e-04\n",
      "Epoch 2303: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6079e-07 - rmse: 9.2779e-04 - val_loss: 1.3641e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2304/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3044e-06 - rmse: 0.0011\n",
      "Epoch 2304: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5174e-07 - rmse: 9.2290e-04 - val_loss: 1.3617e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2305/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.0994e-07 - rmse: 5.5672e-04\n",
      "Epoch 2305: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5610e-07 - rmse: 9.2526e-04 - val_loss: 1.3810e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2306/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1193e-07 - rmse: 7.1549e-04\n",
      "Epoch 2306: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6112e-07 - rmse: 9.2797e-04 - val_loss: 1.3639e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2307/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3745e-06 - rmse: 0.0012\n",
      "Epoch 2307: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4361e-07 - rmse: 9.1848e-04 - val_loss: 1.3686e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2308/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3935e-07 - rmse: 9.6920e-04\n",
      "Epoch 2308: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1011e-07 - rmse: 9.5400e-04 - val_loss: 1.3535e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2309/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2318e-06 - rmse: 0.0011\n",
      "Epoch 2309: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8173e-07 - rmse: 9.3901e-04 - val_loss: 1.3689e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2310/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5565e-07 - rmse: 9.7758e-04\n",
      "Epoch 2310: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1603e-07 - rmse: 9.5709e-04 - val_loss: 1.3737e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2311/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2374e-06 - rmse: 0.0011\n",
      "Epoch 2311: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0546e-06 - rmse: 0.0010 - val_loss: 1.3522e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2312/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2789e-06 - rmse: 0.0011\n",
      "Epoch 2312: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0676e-06 - rmse: 0.0010 - val_loss: 1.3730e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2313/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1983e-06 - rmse: 0.0011\n",
      "Epoch 2313: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7113e-07 - rmse: 9.8546e-04 - val_loss: 1.3578e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2314/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7646e-07 - rmse: 9.3620e-04\n",
      "Epoch 2314: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.0767e-07 - rmse: 9.5272e-04 - val_loss: 1.3714e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2315/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6214e-07 - rmse: 9.8089e-04\n",
      "Epoch 2315: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6114e-07 - rmse: 9.2798e-04 - val_loss: 1.3760e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2316/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4582e-07 - rmse: 8.6361e-04\n",
      "Epoch 2316: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4447e-07 - rmse: 9.1895e-04 - val_loss: 1.3455e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2317/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.4541e-07 - rmse: 7.3852e-04\n",
      "Epoch 2317: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9197e-07 - rmse: 9.4444e-04 - val_loss: 1.3907e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2318/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0548e-07 - rmse: 9.5157e-04\n",
      "Epoch 2318: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4206e-07 - rmse: 9.7060e-04 - val_loss: 1.3603e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2319/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0265e-07 - rmse: 8.9591e-04\n",
      "Epoch 2319: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.0036e-07 - rmse: 9.4887e-04 - val_loss: 1.3687e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2320/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6277e-06 - rmse: 0.0013\n",
      "Epoch 2320: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9023e-07 - rmse: 9.4352e-04 - val_loss: 1.3680e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2321/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8737e-07 - rmse: 9.9367e-04\n",
      "Epoch 2321: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.8134e-07 - rmse: 9.3879e-04 - val_loss: 1.3692e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2322/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3168e-06 - rmse: 0.0011\n",
      "Epoch 2322: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5944e-07 - rmse: 9.2706e-04 - val_loss: 1.3691e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2323/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7718e-07 - rmse: 9.3658e-04\n",
      "Epoch 2323: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.0696e-07 - rmse: 9.5234e-04 - val_loss: 1.3719e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2324/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3789e-07 - rmse: 8.5901e-04\n",
      "Epoch 2324: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8392e-07 - rmse: 9.4017e-04 - val_loss: 1.3788e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2325/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7138e-07 - rmse: 8.7828e-04\n",
      "Epoch 2325: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3956e-07 - rmse: 9.1628e-04 - val_loss: 1.3543e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2326/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4631e-07 - rmse: 8.6389e-04\n",
      "Epoch 2326: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0203e-07 - rmse: 9.4975e-04 - val_loss: 1.3854e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2327/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.9395e-07 - rmse: 7.0281e-04\n",
      "Epoch 2327: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9280e-07 - rmse: 9.4488e-04 - val_loss: 1.3679e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2328/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3672e-07 - rmse: 7.9795e-04\n",
      "Epoch 2328: val_loss did not improve from 0.00007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0448e-07 - rmse: 9.5104e-04 - val_loss: 1.3756e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2329/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0600e-06 - rmse: 0.0010\n",
      "Epoch 2329: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7629e-07 - rmse: 9.3611e-04 - val_loss: 1.3766e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2330/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2569e-06 - rmse: 0.0011\n",
      "Epoch 2330: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6752e-07 - rmse: 9.3141e-04 - val_loss: 1.3732e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2331/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5031e-07 - rmse: 9.7484e-04\n",
      "Epoch 2331: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4626e-07 - rmse: 9.1992e-04 - val_loss: 1.3718e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2332/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0021e-07 - rmse: 6.3262e-04\n",
      "Epoch 2332: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8836e-07 - rmse: 9.4253e-04 - val_loss: 1.3509e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2333/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8283e-07 - rmse: 9.3959e-04\n",
      "Epoch 2333: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4835e-07 - rmse: 9.2106e-04 - val_loss: 1.3846e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2334/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4152e-06 - rmse: 0.0012\n",
      "Epoch 2334: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5935e-07 - rmse: 9.2701e-04 - val_loss: 1.3795e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2335/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6643e-07 - rmse: 8.7546e-04\n",
      "Epoch 2335: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5559e-07 - rmse: 9.2498e-04 - val_loss: 1.3683e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2336/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1055e-07 - rmse: 4.5885e-04\n",
      "Epoch 2336: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5218e-07 - rmse: 9.2313e-04 - val_loss: 1.3629e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2337/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.5520e-07 - rmse: 8.6902e-04\n",
      "Epoch 2337: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6178e-07 - rmse: 9.2832e-04 - val_loss: 1.3800e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2338/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0243e-06 - rmse: 0.0010\n",
      "Epoch 2338: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8110e-07 - rmse: 9.3867e-04 - val_loss: 1.3782e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2339/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2653e-07 - rmse: 9.6256e-04\n",
      "Epoch 2339: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3755e-07 - rmse: 9.1518e-04 - val_loss: 1.3825e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2340/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.8140e-07 - rmse: 8.2547e-04\n",
      "Epoch 2340: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4422e-07 - rmse: 9.1881e-04 - val_loss: 1.3539e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2341/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3910e-07 - rmse: 9.6907e-04\n",
      "Epoch 2341: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5489e-07 - rmse: 9.2460e-04 - val_loss: 1.3724e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2342/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1057e-07 - rmse: 7.8139e-04\n",
      "Epoch 2342: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3682e-07 - rmse: 9.1478e-04 - val_loss: 1.3641e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2343/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5509e-07 - rmse: 6.7460e-04\n",
      "Epoch 2343: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7579e-07 - rmse: 9.3584e-04 - val_loss: 1.3856e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2344/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2019e-06 - rmse: 0.0011\n",
      "Epoch 2344: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8490e-07 - rmse: 9.4069e-04 - val_loss: 1.3756e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2345/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8114e-07 - rmse: 9.3869e-04\n",
      "Epoch 2345: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4552e-07 - rmse: 9.1952e-04 - val_loss: 1.3706e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2346/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.0984e-07 - rmse: 7.8092e-04\n",
      "Epoch 2346: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5953e-07 - rmse: 9.2711e-04 - val_loss: 1.3593e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2347/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3121e-06 - rmse: 0.0011\n",
      "Epoch 2347: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.2842e-07 - rmse: 9.1018e-04 - val_loss: 1.3844e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2348/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1539e-06 - rmse: 0.0011\n",
      "Epoch 2348: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3778e-07 - rmse: 9.1530e-04 - val_loss: 1.3607e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2349/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0777e-07 - rmse: 6.3857e-04\n",
      "Epoch 2349: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6018e-07 - rmse: 9.2746e-04 - val_loss: 1.3815e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2350/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1444e-06 - rmse: 0.0011\n",
      "Epoch 2350: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7571e-07 - rmse: 9.8778e-04 - val_loss: 1.3330e-04 - val_rmse: 0.0115 - lr: 6.2500e-06\n",
      "Epoch 2351/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.3854e-07 - rmse: 8.5939e-04\n",
      "Epoch 2351: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.7374e-07 - rmse: 9.8678e-04 - val_loss: 1.4092e-04 - val_rmse: 0.0119 - lr: 6.2500e-06\n",
      "Epoch 2352/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6764e-07 - rmse: 9.3147e-04\n",
      "Epoch 2352: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.6727e-07 - rmse: 9.8350e-04 - val_loss: 1.3519e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2353/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.2339e-07 - rmse: 7.2345e-04\n",
      "Epoch 2353: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0123e-06 - rmse: 0.0010 - val_loss: 1.4106e-04 - val_rmse: 0.0119 - lr: 6.2500e-06\n",
      "Epoch 2354/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.0130e-07 - rmse: 6.3348e-04\n",
      "Epoch 2354: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.1261e-06 - rmse: 0.0011 - val_loss: 1.3499e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2355/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.6842e-07 - rmse: 6.8441e-04\n",
      "Epoch 2355: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0461e-06 - rmse: 0.0010 - val_loss: 1.4022e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2356/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1009e-06 - rmse: 0.0010\n",
      "Epoch 2356: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0107e-06 - rmse: 0.0010 - val_loss: 1.3698e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2357/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2229e-06 - rmse: 0.0011\n",
      "Epoch 2357: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1697e-07 - rmse: 9.5759e-04 - val_loss: 1.3827e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2358/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1495e-07 - rmse: 9.0275e-04\n",
      "Epoch 2358: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7192e-07 - rmse: 9.3376e-04 - val_loss: 1.3766e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2359/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.7823e-07 - rmse: 9.3714e-04\n",
      "Epoch 2359: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5803e-07 - rmse: 9.2630e-04 - val_loss: 1.3680e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2360/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.8600e-07 - rmse: 9.4128e-04\n",
      "Epoch 2360: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.2300e-07 - rmse: 9.0720e-04 - val_loss: 1.3885e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2361/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1762e-06 - rmse: 0.0011\n",
      "Epoch 2361: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5964e-07 - rmse: 9.2717e-04 - val_loss: 1.3804e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2362/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.6539e-07 - rmse: 8.7487e-04\n",
      "Epoch 2362: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6390e-07 - rmse: 9.2946e-04 - val_loss: 1.3803e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2363/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1878e-06 - rmse: 0.0011\n",
      "Epoch 2363: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.5313e-07 - rmse: 9.2365e-04 - val_loss: 1.3526e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2364/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.0497e-06 - rmse: 0.0014\n",
      "Epoch 2364: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1125e-07 - rmse: 9.5459e-04 - val_loss: 1.3702e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2365/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2562e-07 - rmse: 6.5239e-04\n",
      "Epoch 2365: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8293e-07 - rmse: 9.3964e-04 - val_loss: 1.3768e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2366/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.0121e-07 - rmse: 8.3738e-04\n",
      "Epoch 2366: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6884e-07 - rmse: 9.3212e-04 - val_loss: 1.3593e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2367/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.1410e-07 - rmse: 9.0228e-04\n",
      "Epoch 2367: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6487e-07 - rmse: 9.2998e-04 - val_loss: 1.3617e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2368/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.4624e-07 - rmse: 8.6385e-04\n",
      "Epoch 2368: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1702e-07 - rmse: 9.0389e-04 - val_loss: 1.3675e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2369/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4197e-07 - rmse: 6.6481e-04\n",
      "Epoch 2369: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6277e-07 - rmse: 9.2886e-04 - val_loss: 1.3931e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2370/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.1225e-07 - rmse: 7.1572e-04\n",
      "Epoch 2370: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7620e-07 - rmse: 9.3606e-04 - val_loss: 1.3737e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2371/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5300e-06 - rmse: 0.0012\n",
      "Epoch 2371: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3500e-07 - rmse: 9.1378e-04 - val_loss: 1.3708e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2372/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2048e-06 - rmse: 0.0011\n",
      "Epoch 2372: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7547e-07 - rmse: 9.3567e-04 - val_loss: 1.3587e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2373/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8344e-07 - rmse: 6.1922e-04\n",
      "Epoch 2373: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.3756e-07 - rmse: 9.1518e-04 - val_loss: 1.3824e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2374/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6818e-07 - rmse: 9.3176e-04\n",
      "Epoch 2374: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.8578e-07 - rmse: 9.4116e-04 - val_loss: 1.3602e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2375/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.3241e-07 - rmse: 7.9524e-04\n",
      "Epoch 2375: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4367e-07 - rmse: 9.1852e-04 - val_loss: 1.3809e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2376/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3977e-07 - rmse: 9.6942e-04\n",
      "Epoch 2376: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.8252e-07 - rmse: 9.3942e-04 - val_loss: 1.3827e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2377/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4590e-07 - rmse: 6.6775e-04\n",
      "Epoch 2377: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4135e-07 - rmse: 9.1725e-04 - val_loss: 1.3688e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2378/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2047e-07 - rmse: 7.8770e-04\n",
      "Epoch 2378: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7435e-07 - rmse: 9.3507e-04 - val_loss: 1.3848e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2379/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2277e-06 - rmse: 0.0011\n",
      "Epoch 2379: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0654e-07 - rmse: 9.5212e-04 - val_loss: 1.3706e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2380/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.4243e-07 - rmse: 9.7079e-04\n",
      "Epoch 2380: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0169e-06 - rmse: 0.0010 - val_loss: 1.4079e-04 - val_rmse: 0.0119 - lr: 6.2500e-06\n",
      "Epoch 2381/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.6194e-07 - rmse: 9.2841e-04\n",
      "Epoch 2381: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8002e-07 - rmse: 9.8996e-04 - val_loss: 1.3429e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2382/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.1141e-07 - rmse: 8.4345e-04\n",
      "Epoch 2382: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8969e-07 - rmse: 9.9483e-04 - val_loss: 1.4291e-04 - val_rmse: 0.0120 - lr: 6.2500e-06\n",
      "Epoch 2383/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3394e-06 - rmse: 0.0012\n",
      "Epoch 2383: val_loss did not improve from 0.00007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0808e-06 - rmse: 0.0010 - val_loss: 1.3450e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2384/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2854e-06 - rmse: 0.0011\n",
      "Epoch 2384: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0057e-06 - rmse: 0.0010 - val_loss: 1.4228e-04 - val_rmse: 0.0119 - lr: 6.2500e-06\n",
      "Epoch 2385/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3518e-06 - rmse: 0.0012\n",
      "Epoch 2385: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0250e-06 - rmse: 0.0010 - val_loss: 1.3303e-04 - val_rmse: 0.0115 - lr: 6.2500e-06\n",
      "Epoch 2386/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3902e-07 - rmse: 6.6259e-04\n",
      "Epoch 2386: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8308e-07 - rmse: 9.9150e-04 - val_loss: 1.3972e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2387/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.3165e-07 - rmse: 9.6522e-04\n",
      "Epoch 2387: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.4543e-07 - rmse: 9.7233e-04 - val_loss: 1.3597e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2388/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5157e-06 - rmse: 0.0012\n",
      "Epoch 2388: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2498e-07 - rmse: 9.6176e-04 - val_loss: 1.3741e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2389/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0628e-06 - rmse: 0.0010\n",
      "Epoch 2389: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3718e-07 - rmse: 9.1498e-04 - val_loss: 1.3727e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2390/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.7281e-07 - rmse: 8.2025e-04\n",
      "Epoch 2390: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.7294e-07 - rmse: 9.3431e-04 - val_loss: 1.3810e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2391/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.1919e-07 - rmse: 7.8688e-04\n",
      "Epoch 2391: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.1431e-07 - rmse: 9.0239e-04 - val_loss: 1.3542e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2392/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2717e-06 - rmse: 0.0011\n",
      "Epoch 2392: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3841e-07 - rmse: 9.6872e-04 - val_loss: 1.3627e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2393/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.8314e-07 - rmse: 9.9153e-04\n",
      "Epoch 2393: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.8811e-07 - rmse: 9.4240e-04 - val_loss: 1.3843e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2394/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.8914e-07 - rmse: 6.2381e-04\n",
      "Epoch 2394: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.3552e-07 - rmse: 9.6722e-04 - val_loss: 1.3685e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2395/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1792e-06 - rmse: 0.0011\n",
      "Epoch 2395: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.8011e-07 - rmse: 9.9001e-04 - val_loss: 1.3884e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2396/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6097e-06 - rmse: 0.0013\n",
      "Epoch 2396: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.3580e-07 - rmse: 9.6737e-04 - val_loss: 1.3659e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2397/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4796e-07 - rmse: 6.6930e-04\n",
      "Epoch 2397: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0172e-06 - rmse: 0.0010 - val_loss: 1.3993e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2398/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.7292e-07 - rmse: 8.7916e-04\n",
      "Epoch 2398: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.5181e-07 - rmse: 9.7561e-04 - val_loss: 1.3318e-04 - val_rmse: 0.0115 - lr: 6.2500e-06\n",
      "Epoch 2399/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0969e-06 - rmse: 0.0010\n",
      "Epoch 2399: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.8631e-07 - rmse: 9.9313e-04 - val_loss: 1.3848e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2400/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0656e-06 - rmse: 0.0010\n",
      "Epoch 2400: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.5158e-07 - rmse: 9.2281e-04 - val_loss: 1.3695e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2401/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7.9246e-07 - rmse: 8.9020e-04\n",
      "Epoch 2401: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3552e-07 - rmse: 9.1407e-04 - val_loss: 1.3861e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2402/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.6761e-07 - rmse: 9.8367e-04\n",
      "Epoch 2402: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3837e-07 - rmse: 9.1562e-04 - val_loss: 1.3583e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2403/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.1292e-07 - rmse: 6.4259e-04\n",
      "Epoch 2403: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4503e-07 - rmse: 9.1926e-04 - val_loss: 1.3784e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2404/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2275e-06 - rmse: 0.0011\n",
      "Epoch 2404: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.3389e-07 - rmse: 9.1318e-04 - val_loss: 1.3717e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2405/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.0184e-07 - rmse: 8.9546e-04\n",
      "Epoch 2405: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6493e-07 - rmse: 9.3002e-04 - val_loss: 1.3663e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2406/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.5678e-07 - rmse: 6.7586e-04\n",
      "Epoch 2406: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.0815e-07 - rmse: 8.9897e-04 - val_loss: 1.3798e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2407/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.1833e-07 - rmse: 5.6420e-04\n",
      "Epoch 2407: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.3795e-07 - rmse: 9.1539e-04 - val_loss: 1.3583e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2408/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.0612e-07 - rmse: 9.5190e-04\n",
      "Epoch 2408: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.2717e-07 - rmse: 9.0949e-04 - val_loss: 1.3978e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2409/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4718e-06 - rmse: 0.0012\n",
      "Epoch 2409: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.1303e-07 - rmse: 9.5553e-04 - val_loss: 1.3702e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2410/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3.9413e-07 - rmse: 6.2780e-04\n",
      "Epoch 2410: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.8091e-07 - rmse: 9.3857e-04 - val_loss: 1.3791e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2411/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.4412e-07 - rmse: 6.6642e-04\n",
      "Epoch 2411: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.4755e-07 - rmse: 9.2063e-04 - val_loss: 1.3816e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2412/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.5916e-07 - rmse: 9.7937e-04\n",
      "Epoch 2412: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.4854e-07 - rmse: 9.2116e-04 - val_loss: 1.3563e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2413/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.2259e-07 - rmse: 6.5007e-04\n",
      "Epoch 2413: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.0734e-07 - rmse: 9.5254e-04 - val_loss: 1.4006e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2414/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4123e-06 - rmse: 0.0012\n",
      "Epoch 2414: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1.0304e-06 - rmse: 0.0010 - val_loss: 1.3315e-04 - val_rmse: 0.0115 - lr: 6.2500e-06\n",
      "Epoch 2415/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6.2589e-07 - rmse: 7.9113e-04\n",
      "Epoch 2415: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0423e-06 - rmse: 0.0010 - val_loss: 1.4094e-04 - val_rmse: 0.0119 - lr: 6.2500e-06\n",
      "Epoch 2416/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9.2236e-07 - rmse: 9.6040e-04\n",
      "Epoch 2416: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.4596e-07 - rmse: 9.7261e-04 - val_loss: 1.3438e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2417/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0520e-06 - rmse: 0.0010\n",
      "Epoch 2417: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6351e-07 - rmse: 9.2925e-04 - val_loss: 1.3893e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2418/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5443e-06 - rmse: 0.0012\n",
      "Epoch 2418: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1292e-07 - rmse: 9.5547e-04 - val_loss: 1.3473e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2419/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8.4891e-07 - rmse: 9.2136e-04\n",
      "Epoch 2419: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.9123e-07 - rmse: 9.4405e-04 - val_loss: 1.3588e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2420/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7072e-07 - rmse: 6.8609e-04\n",
      "Epoch 2420: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.9580e-07 - rmse: 9.9790e-04 - val_loss: 1.4071e-04 - val_rmse: 0.0119 - lr: 6.2500e-06\n",
      "Epoch 2421/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5549e-06 - rmse: 0.0012\n",
      "Epoch 2421: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 9.2290e-07 - rmse: 9.6067e-04 - val_loss: 1.3401e-04 - val_rmse: 0.0116 - lr: 6.2500e-06\n",
      "Epoch 2422/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1955e-06 - rmse: 0.0011\n",
      "Epoch 2422: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 8.6239e-07 - rmse: 9.2865e-04 - val_loss: 1.3802e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2423/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.3358e-07 - rmse: 6.5846e-04\n",
      "Epoch 2423: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.2291e-07 - rmse: 9.0714e-04 - val_loss: 1.3740e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2424/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4.7233e-07 - rmse: 6.8727e-04\n",
      "Epoch 2424: val_loss did not improve from 0.00007\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1881e-07 - rmse: 9.0488e-04 - val_loss: 1.3953e-04 - val_rmse: 0.0118 - lr: 6.2500e-06\n",
      "Epoch 2425/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0370e-06 - rmse: 0.0010Restoring model weights from the end of the best epoch: 1425.\n",
      "\n",
      "Epoch 2425: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 2425: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9494e-07 - rmse: 9.4601e-04 - val_loss: 1.3605e-04 - val_rmse: 0.0117 - lr: 6.2500e-06\n",
      "Epoch 2425: early stopping\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "history = model.fit([x_train, x_para_train], y_train, batch_size=batch_size,\n",
    "                    validation_data=([x_val, x_para_val], y_val),\n",
    "                    steps_per_epoch = STEP_SIZE_TRAIN, validation_steps=VALIDATION_STEPS,\n",
    "                    epochs=10000, shuffle=True, callbacks=[es, ckpt, rp])\n",
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79204ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:02:58.338960\n"
     ]
    }
   ],
   "source": [
    "time = end - start\n",
    "print(\"Training time:\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fbce220",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = \"D:\\\\VAWT_data\\\\flap_steady\\\\flap_steady\\\\result\\\\\"+\"20221130AeroCNN1_optimalSettings\\\\test\"+str(test_rate)+\"Cdonly\"\n",
    "if not os.path.exists(storage_dir):\n",
    "    os.makedirs(storage_dir)\n",
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77cc60d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = history.history\n",
    "plt.plot(hist['loss'], lw=2, label='Training loss')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation loss')\n",
    "plt.title('Training loss (mean squared error)\\nAeroCNN-I, optimal settings, $C_d$ only', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "#plt.ylim([5e-5, 1e-1])\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"TrainingLoss_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e42b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2)\n",
    "plt.plot(hist['val_rmse'], lw=2)\n",
    "plt.title('Root Mean Squared Error, optimal settings, $C_d$ only', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "983b76cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist['rmse'], lw=2, label='Training RMSE')\n",
    "plt.plot(hist['val_loss'], lw=2, label='Validation RMSE')\n",
    "plt.title('Root Mean Squared Error\\nAeroCNN-I, optimal settings, $C_d$ only', size=15)\n",
    "plt.xlabel('Epoch', size=15)\n",
    "plt.yscale('log')\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "saveName = \"RMSE_test\"+str(test_rate) + \".jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c76ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 16ms/step - loss: 4.8823e-05 - rmse: 0.0070\n"
     ]
    }
   ],
   "source": [
    "train_results = model.evaluate([x_train, x_para_train], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abc70d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step - loss: 3.1827e-04 - rmse: 0.0178\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate([x_test, x_para_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "745feda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "decoded_train_ = model.predict([x_train, x_para_train])\n",
    "decoded_val_ = model.predict([x_val, x_para_val])\n",
    "decoded_test_ = model.predict([x_test, x_para_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a50468a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = np.unique(np.where(np.isin(cd, y_train)))\n",
    "index_val = np.unique(np.where(np.isin(cd, y_val)))\n",
    "index_test = np.unique(np.where(np.isin(cd, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5d3a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train*(np.max(cd_orig)-np.min(cd_orig))+np.min(cd_orig)\n",
    "y_val = y_val*(np.max(cd_orig)-np.min(cd_orig))+np.min(cd_orig)\n",
    "y_test = y_test*(np.max(cd_orig)-np.min(cd_orig))+np.min(cd_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "494df8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_train = decoded_train_*(np.max(cd_orig)-np.min(cd_orig))+np.min(cd_orig)\n",
    "decoded_val = decoded_val_*(np.max(cd_orig)-np.min(cd_orig))+np.min(cd_orig)\n",
    "decoded_test = decoded_test_*(np.max(cd_orig)-np.min(cd_orig))+np.min(cd_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55d01da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\\\TrainedModels\\\\20221130\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "os.chdir(model_directory)\n",
    "model_name = \"20221130steadyValidation_AeroCNN1_val_\"+str(val_rate)+\"_test\"+str(test_rate)+ \"_\" + str(n_units) +\"units_optimalSettings_Cdonly.h5\"\n",
    "model.save(model_name, overwrite=True, include_optimizer=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c38a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train_abs = np.abs(decoded_train - y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3ff0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_val_abs = np.abs(decoded_val - y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c0dfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test_abs = np.abs(decoded_test - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e21d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31fb219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014035146276532389\n"
     ]
    }
   ],
   "source": [
    "l2_error_train = np.sqrt(np.sum((decoded_train - y_train)**2) / np.sum(y_train**2))\n",
    "print(l2_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69d47034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01926383310178078\n"
     ]
    }
   ],
   "source": [
    "l2_error_val = np.sqrt(np.sum((decoded_val - y_val)**2) / np.sum(y_val**2))\n",
    "print(l2_error_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3770434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04155218468132563\n"
     ]
    }
   ],
   "source": [
    "l2_error_test = np.sqrt(np.sum((decoded_test - y_test)**2) / np.sum(y_test**2))\n",
    "print(l2_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9785cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_train_list = []\n",
    "for i in range(0, len(x_train)):\n",
    "    l2_error_train_data = np.sqrt(np.sum((decoded_train[i] - y_train[i])**2) / np.sum(y_train[i]**2))\n",
    "    l2_error_train_list.append(l2_error_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab588246",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_error_val_list = []\n",
    "for i in range(0, len(x_val)):\n",
    "    l2_error_val_data = np.sqrt(np.sum((decoded_val[i] - y_val[i])**2) / np.sum((y_val[i]+1e-07)**2))\n",
    "    l2_error_val_list.append(l2_error_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72b6a696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_error_test_list = []\n",
    "for i in range(0, len(x_test)):\n",
    "    l2_error_test_data = np.sqrt(np.sum((decoded_test[i] - y_test[i])**2) / np.sum(y_test[i]**2))\n",
    "    l2_error_test_list.append(l2_error_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd795141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1,x_train.shape[0],x_train.shape[0]),\n",
    "         l2_error_train*np.ones(x_train.shape[0],), 'k', lw=2.5)\n",
    "plt.scatter(np.linspace(1, x_train.shape[0], x_train.shape[0]), l2_error_train_list, c='b')\n",
    "plt.xlabel('Index', fontsize=15)\n",
    "plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('$L_2$ error norm distribution - AeroCNN-I, training\\nValidation rate {0}, test rate {1}, optimal settings ($C_d$)'.format(\n",
    "    val_rate, test_rate), fontsize=15\n",
    "         )\n",
    "plt.grid()\n",
    "saveName = \"trainingErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8f39cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1,x_val.shape[0],x_val.shape[0]),\n",
    "         l2_error_val*np.ones(x_val.shape[0],), 'k', lw=2.5)\n",
    "plt.scatter(np.linspace(1, x_val.shape[0], x_val.shape[0]), l2_error_val_list, c='b')\n",
    "plt.xlabel('Index', fontsize=15)\n",
    "plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('$L_2$ error norm distribution - AeroCNN-I, validation\\nValidation rate {0}, test rate {1}, optimal settings ($C_d$)'.format(\n",
    "    val_rate, test_rate), fontsize=15\n",
    "         )\n",
    "plt.grid()\n",
    "saveName = \"validationErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "342bbd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1,x_test.shape[0],x_test.shape[0]),\n",
    "         l2_error_test*np.ones(x_test.shape[0],), 'k', lw=2.5)\n",
    "plt.scatter(np.linspace(1, x_test.shape[0], x_test.shape[0]), l2_error_test_list, c='b')\n",
    "plt.xlabel('Index', fontsize=15)\n",
    "plt.ylabel('$L_2$ error norm', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.title('$L_2$ error norm distribution - AeroCNN-I, test\\nValidation rate {0}, test rate {1}, optimal settings ($C_d$)'.format(\n",
    "    val_rate, test_rate), fontsize=15\n",
    "         )\n",
    "plt.grid()\n",
    "saveName = \"testErrorDistribution.jpg\"\n",
    "plt.savefig(saveName, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8978795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict([x, x_para])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9badb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CD_trainTestSplit_Plot(i, CD, cd, aTrain, aTest, iTrain, iTest):\n",
    "    \n",
    "    title_0_Cd = 'Gurney flap not attached (NACA0018)\\n$C_D$ prediction, L2 error=%.4f' % l2_error_Cd\n",
    "    #title_0_Cl = 'Gurney flap not attached (NACA0018)\\n$C_L$ prediction, L2 error=%.4f' % l2_error_Cl\n",
    "    \n",
    "    title_n_Cd = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_D$ prediction, L2 error=%.4f'%(l2_error_Cd)\n",
    "    #title_n_Cl = 'Gurney flap attached h=%.2f, '%(h[i]) + r'$\\beta$=%d'%(beta[i])+'\\n$C_L$ prediction, L2 error=%.4f'%(l2_error_Cl)\n",
    "    \n",
    "    if i==0:\n",
    "#         title_Cd, title_Cl = title_0_Cd, title_0_Cl\n",
    "#         savename1,savename2 = \"CdComparison_NACA0018.jpg\", \"ClComparison_NACA0018.jpg\"\n",
    "        title_Cd = title_0_Cd\n",
    "        savename1 = \"CdComparison_NACA0018.jpg\"\n",
    "    else:\n",
    "#         title_Cd, title_Cl = title_n_Cd, title_n_Cl\n",
    "        title_Cd = title_n_Cd\n",
    "        savename1 = \"CdComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "#         savename2 = \"ClComparison_h\"+str(h[i])+\"_beta\"+str(beta[i])+\".jpg\"\n",
    "    \n",
    "    # CD graph plot\n",
    "    plt.plot(alpha, CD, 'k-', label='Ground truth')\n",
    "    plt.plot(alpha, cd, 'k--', label='Predicted value')\n",
    "    plt.scatter(aTrain, iTrain, color='b', label='Training set')\n",
    "    plt.scatter(aVal, iVal, color='g', label='Validation set')\n",
    "    plt.scatter(aTest, iTest, color='r', label='Test set')\n",
    "    plt.xlabel('AoA (degree)')\n",
    "    plt.ylabel('$C_D$')\n",
    "    plt.title(title_Cd, fontsize=15)        \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylim([0, 0.12])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename1, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    '''# CL graph plot\n",
    "    plt.plot(alpha, CL, 'k-', label='Ground truth')\n",
    "    plt.plot(alpha, cl, 'k--', label='Predicted value')\n",
    "    plt.scatter(aTrain, iTrain[:,1]*(np.max(cl_orig)-np.min(cl_orig)) + np.min(cl_orig), color='b', label='Training set')\n",
    "    plt.scatter(aTest, iTest[:,1]*(np.max(cl_orig)-np.min(cl_orig)) + np.min(cl_orig), color='r', label='Test set')\n",
    "    plt.xlabel('AoA (degree)')\n",
    "    plt.ylabel('$C_L$')\n",
    "    plt.title(title_Cl, fontsize=15)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylim([0, 2])\n",
    "    plt.grid()\n",
    "    plt.savefig(savename2, dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36887acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "NACA0018 airfoil without Gurney flap\n",
      "L2 error of Cd: 0.0131\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0237\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0217\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "L2 error of Cd: 0.0146\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0673\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0215\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0184\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0095\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0270\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cd: 0.0330\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0198\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cd: 0.0162\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cd: 0.0105\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "L2 error of Cd: 0.0177\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "L2 error of Cd: 0.0486\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = [0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03]\n",
    "beta = [0, 30, 45, 60, 75, 90, 30, 45, 60, 75, 90, 30, 45, 60, 75, 90]\n",
    "for i in range(0, 16):\n",
    "    # Index from each dataset\n",
    "    iTrain = []\n",
    "    iVal = []\n",
    "    iTest = []\n",
    "    \n",
    "    # Index from input data (alpha, in this case)\n",
    "    alpha_train = []\n",
    "    alpha_val = []\n",
    "    alpha_test = []\n",
    "    \n",
    "    predictedValue = model.predict([x[n_alpha*i:n_alpha*(i+1),:],x_para[n_alpha*i:n_alpha*(i+1),:]])\n",
    "    y_corres = y[n_alpha*i:n_alpha*(i+1),:]\n",
    "    \n",
    "    l2_error_Cd = np.sqrt(np.sum((predictedValue - y_corres)**2) / np.sum(y_corres**2))\n",
    "    \n",
    "    if i==0:\n",
    "        print('NACA0018 airfoil without Gurney flap\\nL2 error of Cd: {0:0.4f}'.format(l2_error_Cd))\n",
    "    else:\n",
    "        print('L2 error of Cd: {0:0.4f}'.format(l2_error_Cd))\n",
    "    \n",
    "    cd = predicted[n_alpha*i:n_alpha*(i+1)]*(np.max(cd_orig)-np.min(cd_orig)) + np.min(cd_orig)\n",
    "    CD = y[n_alpha*i:n_alpha*(i+1)]*(np.max(cd_orig)-np.min(cd_orig)) + np.min(cd_orig)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*n_alpha) & ((index_train>=i*n_alpha)))]):\n",
    "        iTrain.append(predicted[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*n_alpha) & ((index_val>=i*n_alpha)))]):\n",
    "        iVal.append(predicted[index])    \n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*n_alpha) & (index_test>=i*n_alpha))]):\n",
    "        iTest.append(predicted[index])\n",
    "        \n",
    "    iTrain = np.array(iTrain)*(np.max(cd_orig)-np.min(cd_orig)) + np.min(cd_orig)\n",
    "    iTest = np.array(iTest)*(np.max(cd_orig)-np.min(cd_orig)) + np.min(cd_orig)\n",
    "    iVal = np.array(iVal)*(np.max(cd_orig)-np.min(cd_orig)) + np.min(cd_orig)\n",
    "    \n",
    "    for ii, index in enumerate(index_train[np.where((index_train<(i+1)*n_alpha) & ((index_train>=i*n_alpha)))]):\n",
    "        alpha_train.append(aa[index])\n",
    "    for kk, index in enumerate(index_val[np.where((index_val<(i+1)*n_alpha) & ((index_val>=i*n_alpha)))]):\n",
    "        alpha_val.append(aa[index])\n",
    "    for jj, index in enumerate(index_test[np.where((index_test<(i+1)*n_alpha) & ((index_test>=i*n_alpha)))]):\n",
    "        alpha_test.append(aa[index])\n",
    "        \n",
    "    aTrain = np.array(alpha_train)*np.max(alpha)\n",
    "    aVal = np.array(alpha_val)*np.max(alpha)\n",
    "    aTest = np.array(alpha_test)*np.max(alpha)\n",
    "    \n",
    "    CD_trainTestSplit_Plot(i, CD, cd, aTrain, aTest, iTrain, iTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805a612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ac6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c3f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad9cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cb680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
